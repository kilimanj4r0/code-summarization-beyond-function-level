{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import logging\n",
    "\n",
    "# logging.set_verbosity_error()\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../../data\")\n",
    "VECTOR_STORES_DIR = DATA_DIR / \"vector-stores\"\n",
    "PREPROCESSED_DATA_DIR = DATA_DIR / \"preprocessed\"\n",
    "PREDICTED_DATA_DIR = DATA_DIR / \"predicted\"\n",
    "\n",
    "levels = [\"method\", \"class\", \"repo\"]\n",
    "datasets = [\n",
    "    \"mce\",  # method (+few shot), class\n",
    "    \"mcsn\",  # method (+few shot), repo (+few shot)\n",
    "]\n",
    "\n",
    "LEVEL = levels[2]\n",
    "DATASET = datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>method_name</th>\n",
       "      <th>method_code</th>\n",
       "      <th>method_summary</th>\n",
       "      <th>original_method_code</th>\n",
       "      <th>method_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>HttpHook.run</td>\n",
       "      <td>def run(self, endpoint, data=None, headers=Non...</td>\n",
       "      <td>Performs the request</td>\n",
       "      <td>def run(self, endpoint, data=None, headers=Non...</td>\n",
       "      <td>airflow/hooks/http_hook.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        repo_name   method_name  \\\n",
       "0  apache/airflow  HttpHook.run   \n",
       "\n",
       "                                         method_code        method_summary  \\\n",
       "0  def run(self, endpoint, data=None, headers=Non...  Performs the request   \n",
       "\n",
       "                                original_method_code  \\\n",
       "0  def run(self, endpoint, data=None, headers=Non...   \n",
       "\n",
       "                  method_path  \n",
       "0  airflow/hooks/http_hook.py  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = PREPROCESSED_DATA_DIR / f\"method-level-{DATASET}.jsonl\"\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LEVEL != 'class':\n",
    "    fs_file_path = PREPROCESSED_DATA_DIR / f\"method-level-{DATASET}-few-shot.jsonl\"\n",
    "    fs_df = pd.read_json(fs_file_path, lines=True)\n",
    "    fs_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "    \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
    "    \"deepseek-ai/deepseek-coder-33b-instruct\",\n",
    "    \"bigcode/starcoder2-15b-instruct-v0.1\",\n",
    "    \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\"\n",
    "]\n",
    "\n",
    "idx = 3\n",
    "MODEL_NAME = model_names[idx]\n",
    "MODEL_KEY = \"dsc\" if idx in [0, 1, 2] else \"sc\" if idx == 3 else \"ll\"\n",
    "\n",
    "MODEL_MAX_LENGTH = 16384  # From model config\n",
    "PROMPT_MAX_LENGTH = 3000  # From experiments with maximum possible prompt\n",
    "\n",
    "MODEL_DIR = PREDICTED_DATA_DIR / MODEL_NAME.split(\"/\")[-1]\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(repo_name):\n",
    "    return FAISS.load_local(\n",
    "        VECTOR_STORES_DIR / repo_name.replace(\"/\", \"_\"),\n",
    "        EMBEDDINGS,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "\n",
    "if LEVEL == 'repo':\n",
    "    REPOS = df[\"repo_name\"].unique().tolist()\n",
    "    EMBEDDINGS = HuggingFaceBgeEmbeddings(\n",
    "        model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "        model_kwargs={'device': 'cuda:2'},\n",
    "        encode_kwargs={'normalize_embeddings': True},\n",
    "    )\n",
    "    VECTOR_STORES = {repo_name: load_vector_store(repo_name) for repo_name in REPOS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"You're a specialized AI assisting with Python code summaries, deeply knowledgeable in computer science.\\n\"\n",
    "\n",
    "METHOD_INSTRUCTION = \"Concisely summarize the Python code provided in 1-3 sentences.\"\n",
    "\n",
    "CLASS_CONTEXT_INSTRUCTION = (\n",
    "    \"Consider the following class code as additional context for your response:\\n\"\n",
    ")\n",
    "CLASS_INSTRUCTION = (\n",
    "    \"Concisely summarize the following Python function in 1-3 sentences:\\n\"\n",
    ")\n",
    "\n",
    "REPO_CONTEXT_INSTRUCTION = \"You have the following repository context, which includes fragments of code with their corresponding paths and lines from the repository:\\n\\n\"\n",
    "REPO_INSTRUCTION_PREFIX = \"Your task is to summarize the Python function located at \"\n",
    "REPO_INSTRUCTION_SUFFIX = (\n",
    "    \" concisely in 1-3 sentences, based on the provided context:\\n\"\n",
    ")\n",
    "\n",
    "K = 50\n",
    "\n",
    "\n",
    "def retrieve_repo_context(method_code, repo_name, original_method_code, k=K):\n",
    "    context = VECTOR_STORES[repo_name].similarity_search(method_code, k=k)\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"File path: {d.metadata['file_path']}\\nFile content:\\n```\\n{d.page_content}\\n```\"\n",
    "            for d in context\n",
    "            if d.page_content not in original_method_code\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def construct_few_shot_list(df, num=0, repo_level=False):\n",
    "    df = df.sample(num, random_state=42)\n",
    "    few_shot = []\n",
    "    for code, summary in zip(df[\"method_code\"], df[\"method_summary\"]):\n",
    "        if repo_level:\n",
    "            few_shot.append({\"role\": \"user\", \"content\": f\"{CLASS_INSTRUCTION}\\n{code}\"})\n",
    "        else:\n",
    "            few_shot.append(\n",
    "                {\"role\": \"user\", \"content\": f\"{code}\\n{METHOD_INSTRUCTION}\"}\n",
    "            )\n",
    "        few_shot.append({\"role\": \"assistant\", \"content\": f\"{summary}\"})\n",
    "    return few_shot\n",
    "\n",
    "\n",
    "# def get_repo_context(repo_context, tokenizer, percent=1.0):\n",
    "#     # Calcuate max_index according to model params and percent\n",
    "#     max_length = MODEL_MAX_LENGTH - PROMPT_MAX_LENGTH\n",
    "#     max_index = int(max_length * percent)\n",
    "#     join_str = '\"File path: '\n",
    "#     # Tokenize the repo_context\n",
    "#     ids = tokenizer.encode(repo_context)\n",
    "#     # Truncate the tokens array according to percent of repo_context\n",
    "#     ids = ids[:max_index]\n",
    "#     # Split by join_str and trim last element because it can be truncated\n",
    "#     new_repo_context = tokenizer.decode(ids).split(join_str)[:-1]\n",
    "#     # Join back\n",
    "#     new_repo_context = join_str.join(new_repo_context)\n",
    "#     return new_repo_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488152cb4a824f349a34f15072dc17ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"auto\"  # 1\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for documents length in tokens\n",
    "# import numpy as np\n",
    "# k = 50\n",
    "# for idx in range(10):\n",
    "#     lengths = []\n",
    "#     tokens = []\n",
    "#     docs = VECTOR_STORES[df['repo_name'].iloc[idx]].similarity_search(df['method_code'].iloc[idx], k=k)\n",
    "#     for doc in docs:\n",
    "#         tok_doc = tokenizer.encode(doc.page_content)\n",
    "#         lengths.append(len(doc.page_content))\n",
    "#         tokens.append(len(tok_doc))\n",
    "#     print(np.mean(lengths), np.mean(tokens), np.mean(tokens) * k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory footprint: 32.25 GB\n",
      "Memory allocated: 7.59 GB\n",
      "Number of parameters: 15.96 B\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "print(f\"Memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Number of parameters: {model.num_parameters() / 1e9:.2f} B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_CONFIGS = {\n",
    "    \"dsc\" : {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    },\n",
    "    \"sc\" : {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"eos_token_id\": [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"###\")\n",
    "        ],\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    },\n",
    "    \"ll\" : {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"eos_token_id\": [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_level_pipeline(method_code, model, tokenizer, model_key, few_shots=[]):\n",
    "    system_message = {\n",
    "        \"role\": \"system\" if model_key != \"sc\" else \"user\",\n",
    "        \"content\": SYSTEM,\n",
    "    }\n",
    "    main_message = {\"role\": \"user\", \"content\": f\"{method_code}\\n{METHOD_INSTRUCTION}\"}\n",
    "    messages = [system_message] + few_shots + [main_message]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        **GENERATION_CONFIGS[model_key],\n",
    "    )\n",
    "    return (\n",
    "        tokenizer.decode(outputs[0][len(inputs[0]) :], skip_special_tokens=True)\n",
    "        .split(\"###\")[0]\n",
    "        .split(\"</s>\")[0]\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "\n",
    "def class_level_pipeline(method_code, class_context, model, tokenizer, model_key):\n",
    "    system_message = {\n",
    "        \"role\": \"system\" if model_key != \"sc\" else \"user\",\n",
    "        \"content\": SYSTEM,\n",
    "    }\n",
    "    class_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{CLASS_CONTEXT_INSTRUCTION}{class_context}\",\n",
    "    }\n",
    "    main_message = {\"role\": \"user\", \"content\": f\"{CLASS_INSTRUCTION}{method_code}\"}\n",
    "    messages = [system_message, class_message, main_message]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        **GENERATION_CONFIGS[model_key],\n",
    "    )\n",
    "    return (\n",
    "        tokenizer.decode(outputs[0][len(inputs[0]) :], skip_special_tokens=True)\n",
    "        .split(\"###\")[0]\n",
    "        .split(\"</s>\")[0]\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "\n",
    "def repo_level_pipeline(row, model, tokenizer, model_key, few_shots=[], k=K):\n",
    "    method_code, method_path, repo_name, original_method_code = row\n",
    "    system_message = {\n",
    "        \"role\": \"system\" if model_key != \"sc\" else \"user\",\n",
    "        \"content\": SYSTEM,\n",
    "    }\n",
    "    if k > 0:\n",
    "        repo_context = retrieve_repo_context(\n",
    "            method_code, repo_name, original_method_code, k=k\n",
    "        )\n",
    "        repo_message = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{REPO_CONTEXT_INSTRUCTION}{repo_context}\",\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        repo_message = []\n",
    "    main_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{REPO_INSTRUCTION_PREFIX}{method_path}{REPO_INSTRUCTION_SUFFIX}{method_code}\",\n",
    "    }\n",
    "    messages = [system_message] + repo_message + few_shots + [main_message]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            **GENERATION_CONFIGS[model_key],\n",
    "        )\n",
    "    return (\n",
    "        tokenizer.decode(outputs[0][len(inputs[0]) :], skip_special_tokens=True)\n",
    "        .split(\"###\")[0]\n",
    "        .split(\"</s>\")[0]\n",
    "        .strip()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEW_SHOTS = 0 if LEVEL == \"class\" else 0\n",
    "if LEVEL == \"repo\":\n",
    "    NUM_FEW_SHOTS = 10\n",
    "\n",
    "few_shots = (\n",
    "    []\n",
    "    if LEVEL == \"class\"\n",
    "    else (\n",
    "        construct_few_shot_list(fs_df, num=NUM_FEW_SHOTS)\n",
    "        if LEVEL == \"method\"\n",
    "        else construct_few_shot_list(fs_df, num=NUM_FEW_SHOTS, repo_level=True)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tqdm.pandas()\n",
    "if LEVEL == 'method':\n",
    "    df[\"pred_summary\"] = df[\"method_code\"].progress_apply(\n",
    "        lambda x: method_level_pipeline(x, model, tokenizer, MODEL_KEY, few_shots)\n",
    "    )\n",
    "elif LEVEL == 'class':\n",
    "    df[\"class_pred_summary\"] = df.progress_apply(\n",
    "        lambda x: class_level_pipeline(x.get(\"method_code\"), x.get(\"class_code\"), model, tokenizer, MODEL_KEY), axis=1\n",
    "    )\n",
    "    df[\"skeleton_pred_summary\"] = df.progress_apply(\n",
    "        lambda x: class_level_pipeline(x.get(\"method_code\"), x.get(\"skeleton\"), model, tokenizer, MODEL_KEY), axis=1\n",
    "    )\n",
    "elif LEVEL == 'repo':\n",
    "    df[\"pred_summary\"] = df[[\"method_code\", \"method_path\", \"repo_name\", \"original_method_code\"]].progress_apply(\n",
    "        lambda x: repo_level_pipeline(x, model, tokenizer, MODEL_KEY, few_shots=few_shots, k=50), axis=1\n",
    "    )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "if LEVEL != 'class':\n",
    "    df.to_json(\n",
    "        MODEL_DIR / f\"{LEVEL}-level-{DATASET}-few-shot-{NUM_FEW_SHOTS}-pred.jsonl\",\n",
    "        orient=\"records\",\n",
    "        lines=True,\n",
    "    )\n",
    "else:\n",
    "    df.to_json(\n",
    "        MODEL_DIR / f\"{LEVEL}-level-{DATASET}-pred.jsonl\",\n",
    "        orient=\"records\",\n",
    "        lines=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>method_name</th>\n",
       "      <th>method_code</th>\n",
       "      <th>method_summary</th>\n",
       "      <th>original_method_code</th>\n",
       "      <th>method_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>Azure/azure-sdk-for-python</td>\n",
       "      <td>ServiceManagementService.add_disk</td>\n",
       "      <td>def add_disk(self, has_operating_system, label...</td>\n",
       "      <td>Adds a disk to the user image repository. The ...</td>\n",
       "      <td>def add_disk(self, has_operating_system, label...</td>\n",
       "      <td>azure-servicemanagement-legacy/azure/servicema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>HdfsSensorRegex.poke</td>\n",
       "      <td>def poke(self, context):\\n        sb = self.ho...</td>\n",
       "      <td>poke matching files in a directory with self.r...</td>\n",
       "      <td>def poke(self, context):\\n        \"\"\"\\n       ...</td>\n",
       "      <td>airflow/contrib/sensors/hdfs_sensor.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>Azure/azure-sdk-for-python</td>\n",
       "      <td>ServiceManagementService.delete_dns_server</td>\n",
       "      <td>def delete_dns_server(self, service_name, depl...</td>\n",
       "      <td>Deletes a DNS server from a deployment.</td>\n",
       "      <td>def delete_dns_server(self, service_name, depl...</td>\n",
       "      <td>azure-servicemanagement-legacy/azure/servicema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>Azure/azure-sdk-for-python</td>\n",
       "      <td>_convert_etree_element_to_queue</td>\n",
       "      <td>def _convert_etree_element_to_queue(entry_elem...</td>\n",
       "      <td>Converts entry element to queue object. The fo...</td>\n",
       "      <td>def _convert_etree_element_to_queue(entry_elem...</td>\n",
       "      <td>azure-servicebus/azure/servicebus/control_clie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>AWSAthenaHook.get_conn</td>\n",
       "      <td>def get_conn(self):\\n        if not self.conn:...</td>\n",
       "      <td>check if aws conn exists already or create one...</td>\n",
       "      <td>def get_conn(self):\\n        \"\"\"\\n        chec...</td>\n",
       "      <td>airflow/contrib/hooks/aws_athena_hook.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      repo_name                                 method_name  \\\n",
       "529  Azure/azure-sdk-for-python           ServiceManagementService.add_disk   \n",
       "132              apache/airflow                        HdfsSensorRegex.poke   \n",
       "512  Azure/azure-sdk-for-python  ServiceManagementService.delete_dns_server   \n",
       "641  Azure/azure-sdk-for-python             _convert_etree_element_to_queue   \n",
       "303              apache/airflow                      AWSAthenaHook.get_conn   \n",
       "\n",
       "                                           method_code  \\\n",
       "529  def add_disk(self, has_operating_system, label...   \n",
       "132  def poke(self, context):\\n        sb = self.ho...   \n",
       "512  def delete_dns_server(self, service_name, depl...   \n",
       "641  def _convert_etree_element_to_queue(entry_elem...   \n",
       "303  def get_conn(self):\\n        if not self.conn:...   \n",
       "\n",
       "                                        method_summary  \\\n",
       "529  Adds a disk to the user image repository. The ...   \n",
       "132  poke matching files in a directory with self.r...   \n",
       "512            Deletes a DNS server from a deployment.   \n",
       "641  Converts entry element to queue object. The fo...   \n",
       "303  check if aws conn exists already or create one...   \n",
       "\n",
       "                                  original_method_code  \\\n",
       "529  def add_disk(self, has_operating_system, label...   \n",
       "132  def poke(self, context):\\n        \"\"\"\\n       ...   \n",
       "512  def delete_dns_server(self, service_name, depl...   \n",
       "641  def _convert_etree_element_to_queue(entry_elem...   \n",
       "303  def get_conn(self):\\n        \"\"\"\\n        chec...   \n",
       "\n",
       "                                           method_path  \n",
       "529  azure-servicemanagement-legacy/azure/servicema...  \n",
       "132             airflow/contrib/sensors/hdfs_sensor.py  \n",
       "512  azure-servicemanagement-legacy/azure/servicema...  \n",
       "641  azure-servicebus/azure/servicebus/control_clie...  \n",
       "303           airflow/contrib/hooks/aws_athena_hook.py  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "# tdf = df.sample(1)\n",
    "tdf = df.iloc[341:342,]\n",
    "display(tdf)\n",
    "tdf[\"pred_summary\"] = tdf[[\"method_code\", \"method_path\", \"repo_name\", \"original_method_code\"]].progress_apply(\n",
    "    lambda x: repo_level_pipeline(\n",
    "        x,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        MODEL_KEY,\n",
    "        few_shots,\n",
    "        k=50,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "for row in tdf.itertuples():\n",
    "    print(\"=\" * 20)\n",
    "    print(row.method_summary)\n",
    "    print(\"=\" * 20)\n",
    "    print(row.pred_summary)\n",
    "    print(\"=\" * 20)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717\n"
     ]
    }
   ],
   "source": [
    "idx = 7\n",
    "method_code = df[\"method_code\"].iloc[idx]\n",
    "method_summary = df[\"method_summary\"].iloc[idx]\n",
    "# class_context = df[\"class_code\"].iloc[idx]\n",
    "method_path = df[\"method_path\"].iloc[idx]\n",
    "repo_context = ''\n",
    "\n",
    "system_message = {\"role\": \"system\" if MODEL_KEY != \"sc\" else \"user\", \"content\": SYSTEM}\n",
    "# main_message = {\"role\": \"user\", \"content\": f\"{method_code}\\n{METHOD_INSTRUCTION}\"}\n",
    "\n",
    "# class_message = {\"role\": \"user\", \"content\": f\"{CLASS_CONTEXT_INSTRUCTION}{class_context}\"}\n",
    "# main_message = {\"role\": \"user\", \"content\": f\"{CLASS_INSTRUCTION}{method_code}\"}\n",
    "repo_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"{REPO_CONTEXT_INSTRUCTION}{repo_context}\",\n",
    "}\n",
    "main_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"{REPO_INSTRUCTION_PREFIX}{method_path}{REPO_INSTRUCTION_SUFFIX}{method_code}\",\n",
    "}\n",
    "messages = [system_message, repo_message] + few_shots + [main_message]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, return_tensors=\"pt\"#, tokenize=False\n",
    ").to(model.device)\n",
    "print(len(inputs[0]))\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "#     outputs = model.generate(\n",
    "#         inputs,\n",
    "#         **GENERATION_CONFIGS[MODEL_KEY],\n",
    "#     )\n",
    "# print(f\"Generated in {time.time() - start_time} seconds.\")\n",
    "# print(tokenizer.decode(inputs[0]))\n",
    "# print(tokenizer.decode(outputs[0][len(inputs[0]) :], skip_special_tokens=True).split(\"###\")[0].split(\"</s>\")[0].strip())\n",
    "# print(\"GOLD Response:\")\n",
    "# print(method_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vladimir-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
