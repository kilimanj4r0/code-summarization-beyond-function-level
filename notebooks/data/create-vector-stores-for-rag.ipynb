{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vector Stores for each repository from Modified CodeSearchNet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import logging\n",
    "import warnings\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>method_name</th>\n",
       "      <th>method_code</th>\n",
       "      <th>method_summary</th>\n",
       "      <th>original_method_code</th>\n",
       "      <th>method_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>HttpHook.run</td>\n",
       "      <td>def run(self, endpoint, data=None, headers=Non...</td>\n",
       "      <td>Performs the request</td>\n",
       "      <td>def run(self, endpoint, data=None, headers=Non...</td>\n",
       "      <td>airflow/hooks/http_hook.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        repo_name   method_name  \\\n",
       "0  apache/airflow  HttpHook.run   \n",
       "\n",
       "                                         method_code        method_summary  \\\n",
       "0  def run(self, endpoint, data=None, headers=Non...  Performs the request   \n",
       "\n",
       "                                original_method_code  \\\n",
       "0  def run(self, endpoint, data=None, headers=Non...   \n",
       "\n",
       "                  method_path  \n",
       "0  airflow/hooks/http_hook.py  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = Path(\"../../data\")\n",
    "\n",
    "REPOS_DIR = DATA_DIR / \"repos\"\n",
    "VECTOR_STORES_DIR = DATA_DIR / \"vector-stores\"\n",
    "PREPROCESSED_DATA_DIR = DATA_DIR / \"preprocessed\"\n",
    "\n",
    "DATASET = \"mcsn\"\n",
    "file_path = PREPROCESSED_DATA_DIR / f\"method-level-{DATASET}.jsonl\"\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={'device': 'cuda:2'},\n",
    "    encode_kwargs={'normalize_embeddings': True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apache/airflow',\n",
       " 'Azure/azure-sdk-for-python',\n",
       " 'streamlink/streamlink',\n",
       " 'open-mmlab/mmcv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_list = df[\"repo_name\"].unique().tolist()\n",
    "repo_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is done only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and index repositories\n",
    "# for repo_name in tqdm(repo_list[1:]):\n",
    "#     clone_url = f\"https://github.com/{repo_name}.git\"\n",
    "#     repo_path = REPOS_DIR / repo_name.replace(\"/\", \"_\")\n",
    "#     persist_path = VECTOR_STORES_DIR / repo_name.replace(\"/\", \"_\")\n",
    "#     shutil.rmtree(repo_path, ignore_errors=True)\n",
    "#     shutil.rmtree(persist_path, ignore_errors=True)\n",
    "#     loader = GitLoader(\n",
    "#         clone_url=None if repo_path.exists() else clone_url,\n",
    "#         repo_path=repo_path.absolute(),\n",
    "#         branch=\"master\" if repo_name == \"streamlink/streamlink\" else \"main\",\n",
    "#         file_filter=lambda file_path: file_path.endswith(\".py\"),\n",
    "#     )\n",
    "#     print(f\"Loading {repo_name}...\")\n",
    "#     documents = loader.load()\n",
    "#     print(f\"Loaded {len(documents)} documents from {repo_name} into {repo_path}.\")\n",
    "#     python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "#         language=Language.PYTHON,\n",
    "#         chunk_size=1000,\n",
    "#         chunk_overlap=100,\n",
    "#     )  # Splits code by definitions of classes and functions, then by lines\n",
    "#     print(\"Splitting documents...\")\n",
    "#     chunks = python_splitter.split_documents(documents)\n",
    "#     print(f\"Splitted documents into {len(chunks)} chunks from {repo_name}.\")\n",
    "#     print(f\"Persisting {repo_name}...\")\n",
    "#     vector_store = await FAISS.afrom_documents(\n",
    "#         chunks, EMBEDDINGS\n",
    "#     )\n",
    "#     vector_store.save_local(persist_path)\n",
    "#     print(f\"Persisted {repo_name} into {persist_path}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See repo statistics\n",
    "# for repo_name in tqdm(repo_list):\n",
    "#     repo_path = REPOS_DIR / repo_name.replace(\"/\", \"_\")\n",
    "#     loader = GitLoader(\n",
    "#         clone_url=None,\n",
    "#         repo_path=repo_path.absolute(),\n",
    "#         branch=\"master\" if repo_name == \"streamlink/streamlink\" else \"main\",\n",
    "#         file_filter=lambda file_path: file_path.endswith(\".py\"),\n",
    "#     )\n",
    "#     documents = loader.load()\n",
    "#     python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "#         language=Language.PYTHON,\n",
    "#         chunk_size=1000,\n",
    "#         chunk_overlap=100,\n",
    "#     )\n",
    "#     chunks = python_splitter.split_documents(documents)\n",
    "#     print(repo_name, len(documents), len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load indexed repos and test retrive repo context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(repo_name):\n",
    "    return FAISS.load_local(\n",
    "        VECTOR_STORES_DIR / repo_name.replace(\"/\", \"_\"),\n",
    "        EMBEDDINGS,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "\n",
    "\n",
    "VECTOR_STORES = {repo_name: load_vector_store(repo_name) for repo_name in repo_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>method_name</th>\n",
       "      <th>method_code</th>\n",
       "      <th>method_summary</th>\n",
       "      <th>original_method_code</th>\n",
       "      <th>method_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>DbApiHook.insert_rows</td>\n",
       "      <td>def insert_rows(self, table, rows, target_fiel...</td>\n",
       "      <td>A generic way to insert a set of tuples into a...</td>\n",
       "      <td>def insert_rows(self, table, rows, target_fiel...</td>\n",
       "      <td>airflow/hooks/dbapi_hook.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure/azure-sdk-for-python</td>\n",
       "      <td>_MinidomXmlToObject.get_entry_properties_from_...</td>\n",
       "      <td>def get_entry_properties_from_node(entry, incl...</td>\n",
       "      <td>get properties from entry xml</td>\n",
       "      <td>def get_entry_properties_from_node(entry, incl...</td>\n",
       "      <td>azure-servicemanagement-legacy/azure/servicema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>streamlink/streamlink</td>\n",
       "      <td>format_time</td>\n",
       "      <td>def format_time(elapsed):\\n    hours = int(ela...</td>\n",
       "      <td>Formats elapsed seconds into a human readable ...</td>\n",
       "      <td>def format_time(elapsed):\\n    \"\"\"Formats elap...</td>\n",
       "      <td>src/streamlink_cli/utils/progress.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>frames2video</td>\n",
       "      <td>def frames2video(frame_dir,\\n                 ...</td>\n",
       "      <td>Read the frame images from a directory and joi...</td>\n",
       "      <td>def frames2video(frame_dir,\\n                 ...</td>\n",
       "      <td>mmcv/video/io.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    repo_name  \\\n",
       "0              apache/airflow   \n",
       "1  Azure/azure-sdk-for-python   \n",
       "2       streamlink/streamlink   \n",
       "3             open-mmlab/mmcv   \n",
       "\n",
       "                                         method_name  \\\n",
       "0                              DbApiHook.insert_rows   \n",
       "1  _MinidomXmlToObject.get_entry_properties_from_...   \n",
       "2                                        format_time   \n",
       "3                                       frames2video   \n",
       "\n",
       "                                         method_code  \\\n",
       "0  def insert_rows(self, table, rows, target_fiel...   \n",
       "1  def get_entry_properties_from_node(entry, incl...   \n",
       "2  def format_time(elapsed):\\n    hours = int(ela...   \n",
       "3  def frames2video(frame_dir,\\n                 ...   \n",
       "\n",
       "                                      method_summary  \\\n",
       "0  A generic way to insert a set of tuples into a...   \n",
       "1                      get properties from entry xml   \n",
       "2  Formats elapsed seconds into a human readable ...   \n",
       "3  Read the frame images from a directory and joi...   \n",
       "\n",
       "                                original_method_code  \\\n",
       "0  def insert_rows(self, table, rows, target_fiel...   \n",
       "1  def get_entry_properties_from_node(entry, incl...   \n",
       "2  def format_time(elapsed):\\n    \"\"\"Formats elap...   \n",
       "3  def frames2video(frame_dir,\\n                 ...   \n",
       "\n",
       "                                         method_path  \n",
       "0                        airflow/hooks/dbapi_hook.py  \n",
       "1  azure-servicemanagement-legacy/azure/servicema...  \n",
       "2               src/streamlink_cli/utils/progress.py  \n",
       "3                                   mmcv/video/io.py  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = [df[df[\"repo_name\"] == repo_name].sample(1, random_state=42) for repo_name in repo_list]\n",
    "tdf = pd.concat(samples, ignore_index=True)\n",
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 70\n",
    "\n",
    "\n",
    "def retrieve_repo_context(method_code, repo_name):\n",
    "    context = VECTOR_STORES[repo_name].similarity_search(method_code, k=K)\n",
    "    return \"\\n\\n\".join(\n",
    "        [f\"File path: {d.metadata['file_path']}\\nFile content:\\n```{d.page_content}```\" for d in context]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "tdf[\"repo_context\"] = tdf.progress_apply(\n",
    "    lambda x: retrieve_repo_context(x.get(\"method_code\"), x.get(\"repo_name\")), axis=1\n",
    ")\n",
    "for c in tdf.repo_context:\n",
    "    print(len(c))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "    \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
    "    \"deepseek-ai/deepseek-coder-33b-instruct\",\n",
    "    \"bigcode/starcoder2-15b-instruct-v0.1\",\n",
    "    \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\"\n",
    "]\n",
    "\n",
    "idx = 1\n",
    "MODEL_NAME = model_names[idx]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache/airflow\n",
      "13779\n",
      "Azure/azure-sdk-for-python\n",
      "12557\n",
      "streamlink/streamlink\n",
      "14144\n",
      "open-mmlab/mmcv\n",
      "14821\n",
      "CPU times: user 1min 21s, sys: 9.33 s, total: 1min 30s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for row in tdf.itertuples():\n",
    "    print(row.repo_name)\n",
    "    overall_context_length = 0\n",
    "    relevance_scores = []\n",
    "    context = VECTOR_STORES[row.repo_name].similarity_search_with_relevance_scores(row.method_code, k=70)\n",
    "    # context = VECTOR_STORES[row.repo_name].max_marginal_relevance_search(\n",
    "    #     row.method_code, k=K, fetch_k=K * 5)\n",
    "    for c in context:\n",
    "        if c[0].page_content not in row.original_method_code:\n",
    "            overall_context_length += len(tokenizer.encode(c[0].page_content))\n",
    "            relevance_scores.append(c[1])\n",
    "    print(overall_context_length)\n",
    "    # print(max(relevance_scores), sum(relevance_scores) / len(relevance_scores), min(relevance_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_content': 'else:\\n                    lst.append(str(cell))\\n            values = tuple(lst)\\n            sql = f\"INSERT /*+ APPEND */ INTO {table} {target_fields} VALUES ({\\',\\'.join(values)})\"\\n            cur.execute(sql)\\n            if i % commit_every == 0:\\n                conn.commit()  # type: ignore[attr-defined]\\n                self.log.info(\"Loaded %s into %s rows so far\", i, table)\\n        conn.commit()  # type: ignore[attr-defined]\\n        cur.close()\\n        conn.close()  # type: ignore[attr-defined]\\n        self.log.info(\"Done loading. Loaded a total of %s rows\", i)',\n",
       " 'metadata': {'source': 'airflow/providers/oracle/hooks/oracle.py',\n",
       "  'file_path': 'airflow/providers/oracle/hooks/oracle.py',\n",
       "  'file_name': 'oracle.py',\n",
       "  'file_type': '.py'},\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vladimir-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
