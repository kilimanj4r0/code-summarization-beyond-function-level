{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CodeSearchNet dataset for RAG\n",
    "\n",
    "To work with the repository context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_dataset(\"code_search_net\", 'python', split='test', trust_remote_code=True)\n",
    "ds = load_dataset(\"code_x_glue_ct_code_to_text\", 'python', split='test', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>repo</th>\n",
       "      <th>path</th>\n",
       "      <th>func_name</th>\n",
       "      <th>original_string</th>\n",
       "      <th>language</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>sha</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>soimort/you-get</td>\n",
       "      <td>src/you_get/extractors/miomio.py</td>\n",
       "      <td>sina_xml_to_url_list</td>\n",
       "      <td>def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...</td>\n",
       "      <td>python</td>\n",
       "      <td>def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...</td>\n",
       "      <td>[def, sina_xml_to_url_list, (, xml_data, ), :,...</td>\n",
       "      <td>str-&gt;list\\n    Convert XML to URL List.\\n    F...</td>\n",
       "      <td>[str, -, &gt;, list, Convert, XML, to, URL, List,...</td>\n",
       "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>soimort/you-get</td>\n",
       "      <td>src/you_get/extractors/dailymotion.py</td>\n",
       "      <td>dailymotion_download</td>\n",
       "      <td>def dailymotion_download(url, output_dir='.', ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def dailymotion_download(url, output_dir='.', ...</td>\n",
       "      <td>[def, dailymotion_download, (, url, ,, output_...</td>\n",
       "      <td>Downloads Dailymotion videos by URL.</td>\n",
       "      <td>[Downloads, Dailymotion, videos, by, URL, .]</td>\n",
       "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>soimort/you-get</td>\n",
       "      <td>src/you_get/extractors/sina.py</td>\n",
       "      <td>sina_download</td>\n",
       "      <td>def sina_download(url, output_dir='.', merge=T...</td>\n",
       "      <td>python</td>\n",
       "      <td>def sina_download(url, output_dir='.', merge=T...</td>\n",
       "      <td>[def, sina_download, (, url, ,, output_dir, =,...</td>\n",
       "      <td>Downloads Sina videos by URL.</td>\n",
       "      <td>[Downloads, Sina, videos, by, URL, .]</td>\n",
       "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>soimort/you-get</td>\n",
       "      <td>src/you_get/util/log.py</td>\n",
       "      <td>sprint</td>\n",
       "      <td>def sprint(text, *colors):\\n    \"\"\"Format text...</td>\n",
       "      <td>python</td>\n",
       "      <td>def sprint(text, *colors):\\n    \"\"\"Format text...</td>\n",
       "      <td>[def, sprint, (, text, ,, *, colors, ), :, ret...</td>\n",
       "      <td>Format text with color or other effects into A...</td>\n",
       "      <td>[Format, text, with, color, or, other, effects...</td>\n",
       "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>soimort/you-get</td>\n",
       "      <td>src/you_get/util/log.py</td>\n",
       "      <td>print_log</td>\n",
       "      <td>def print_log(text, *colors):\\n    \"\"\"Print a ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def print_log(text, *colors):\\n    \"\"\"Print a ...</td>\n",
       "      <td>[def, print_log, (, text, ,, *, colors, ), :, ...</td>\n",
       "      <td>Print a log message to standard error.</td>\n",
       "      <td>[Print, a, log, message, to, standard, error, .]</td>\n",
       "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14913</th>\n",
       "      <td>14913</td>\n",
       "      <td>JIC-CSB/jicbioimage.illustrate</td>\n",
       "      <td>jicbioimage/illustrate/__init__.py</td>\n",
       "      <td>AnnotatedImage.from_grayscale</td>\n",
       "      <td>def from_grayscale(im, channels_on=(True, True...</td>\n",
       "      <td>python</td>\n",
       "      <td>def from_grayscale(im, channels_on=(True, True...</td>\n",
       "      <td>[def, from_grayscale, (, im, ,, channels_on, =...</td>\n",
       "      <td>Return a canvas from a grayscale image.\\n\\n   ...</td>\n",
       "      <td>[Return, a, canvas, from, a, grayscale, image, .]</td>\n",
       "      <td>d88ddf81ee3eb3949677e2ef746af8169ce88092</td>\n",
       "      <td>https://github.com/JIC-CSB/jicbioimage.illustr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14914</th>\n",
       "      <td>14914</td>\n",
       "      <td>un33k/django-toolware</td>\n",
       "      <td>toolware/utils/generic.py</td>\n",
       "      <td>get_uuid</td>\n",
       "      <td>def get_uuid(length=32, version=1):\\n    \"\"\"\\n...</td>\n",
       "      <td>python</td>\n",
       "      <td>def get_uuid(length=32, version=1):\\n    \"\"\"\\n...</td>\n",
       "      <td>[def, get_uuid, (, length, =, 32, ,, version, ...</td>\n",
       "      <td>Returns a unique ID of a given length.\\n    Us...</td>\n",
       "      <td>[Returns, a, unique, ID, of, a, given, length,...</td>\n",
       "      <td>973f3e003dc38b812897dab88455bee37dcaf931</td>\n",
       "      <td>https://github.com/un33k/django-toolware/blob/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14915</th>\n",
       "      <td>14915</td>\n",
       "      <td>un33k/django-toolware</td>\n",
       "      <td>toolware/utils/generic.py</td>\n",
       "      <td>get_unique_key_from_get</td>\n",
       "      <td>def get_unique_key_from_get(get_dict):\\n    \"\"...</td>\n",
       "      <td>python</td>\n",
       "      <td>def get_unique_key_from_get(get_dict):\\n    \"\"...</td>\n",
       "      <td>[def, get_unique_key_from_get, (, get_dict, ),...</td>\n",
       "      <td>Build a unique key from get data</td>\n",
       "      <td>[Build, a, unique, key, from, get, data]</td>\n",
       "      <td>973f3e003dc38b812897dab88455bee37dcaf931</td>\n",
       "      <td>https://github.com/un33k/django-toolware/blob/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14916</th>\n",
       "      <td>14916</td>\n",
       "      <td>un33k/django-toolware</td>\n",
       "      <td>toolware/utils/generic.py</td>\n",
       "      <td>get_domain</td>\n",
       "      <td>def get_domain(url):\\n    \"\"\" Returns domain n...</td>\n",
       "      <td>python</td>\n",
       "      <td>def get_domain(url):\\n    \"\"\" Returns domain n...</td>\n",
       "      <td>[def, get_domain, (, url, ), :, if, 'http', no...</td>\n",
       "      <td>Returns domain name portion of a URL</td>\n",
       "      <td>[Returns, domain, name, portion, of, a, URL]</td>\n",
       "      <td>973f3e003dc38b812897dab88455bee37dcaf931</td>\n",
       "      <td>https://github.com/un33k/django-toolware/blob/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14917</th>\n",
       "      <td>14917</td>\n",
       "      <td>un33k/django-toolware</td>\n",
       "      <td>toolware/utils/generic.py</td>\n",
       "      <td>get_url_args</td>\n",
       "      <td>def get_url_args(url):\\n    \"\"\" Returns a dict...</td>\n",
       "      <td>python</td>\n",
       "      <td>def get_url_args(url):\\n    \"\"\" Returns a dict...</td>\n",
       "      <td>[def, get_url_args, (, url, ), :, url_data, =,...</td>\n",
       "      <td>Returns a dictionary from a URL params</td>\n",
       "      <td>[Returns, a, dictionary, from, a, URL, params]</td>\n",
       "      <td>973f3e003dc38b812897dab88455bee37dcaf931</td>\n",
       "      <td>https://github.com/un33k/django-toolware/blob/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14918 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                            repo  \\\n",
       "0          0                 soimort/you-get   \n",
       "1          1                 soimort/you-get   \n",
       "2          2                 soimort/you-get   \n",
       "3          3                 soimort/you-get   \n",
       "4          4                 soimort/you-get   \n",
       "...      ...                             ...   \n",
       "14913  14913  JIC-CSB/jicbioimage.illustrate   \n",
       "14914  14914           un33k/django-toolware   \n",
       "14915  14915           un33k/django-toolware   \n",
       "14916  14916           un33k/django-toolware   \n",
       "14917  14917           un33k/django-toolware   \n",
       "\n",
       "                                        path                      func_name  \\\n",
       "0           src/you_get/extractors/miomio.py           sina_xml_to_url_list   \n",
       "1      src/you_get/extractors/dailymotion.py           dailymotion_download   \n",
       "2             src/you_get/extractors/sina.py                  sina_download   \n",
       "3                    src/you_get/util/log.py                         sprint   \n",
       "4                    src/you_get/util/log.py                      print_log   \n",
       "...                                      ...                            ...   \n",
       "14913     jicbioimage/illustrate/__init__.py  AnnotatedImage.from_grayscale   \n",
       "14914              toolware/utils/generic.py                       get_uuid   \n",
       "14915              toolware/utils/generic.py        get_unique_key_from_get   \n",
       "14916              toolware/utils/generic.py                     get_domain   \n",
       "14917              toolware/utils/generic.py                   get_url_args   \n",
       "\n",
       "                                         original_string language  \\\n",
       "0      def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...   python   \n",
       "1      def dailymotion_download(url, output_dir='.', ...   python   \n",
       "2      def sina_download(url, output_dir='.', merge=T...   python   \n",
       "3      def sprint(text, *colors):\\n    \"\"\"Format text...   python   \n",
       "4      def print_log(text, *colors):\\n    \"\"\"Print a ...   python   \n",
       "...                                                  ...      ...   \n",
       "14913  def from_grayscale(im, channels_on=(True, True...   python   \n",
       "14914  def get_uuid(length=32, version=1):\\n    \"\"\"\\n...   python   \n",
       "14915  def get_unique_key_from_get(get_dict):\\n    \"\"...   python   \n",
       "14916  def get_domain(url):\\n    \"\"\" Returns domain n...   python   \n",
       "14917  def get_url_args(url):\\n    \"\"\" Returns a dict...   python   \n",
       "\n",
       "                                                    code  \\\n",
       "0      def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...   \n",
       "1      def dailymotion_download(url, output_dir='.', ...   \n",
       "2      def sina_download(url, output_dir='.', merge=T...   \n",
       "3      def sprint(text, *colors):\\n    \"\"\"Format text...   \n",
       "4      def print_log(text, *colors):\\n    \"\"\"Print a ...   \n",
       "...                                                  ...   \n",
       "14913  def from_grayscale(im, channels_on=(True, True...   \n",
       "14914  def get_uuid(length=32, version=1):\\n    \"\"\"\\n...   \n",
       "14915  def get_unique_key_from_get(get_dict):\\n    \"\"...   \n",
       "14916  def get_domain(url):\\n    \"\"\" Returns domain n...   \n",
       "14917  def get_url_args(url):\\n    \"\"\" Returns a dict...   \n",
       "\n",
       "                                             code_tokens  \\\n",
       "0      [def, sina_xml_to_url_list, (, xml_data, ), :,...   \n",
       "1      [def, dailymotion_download, (, url, ,, output_...   \n",
       "2      [def, sina_download, (, url, ,, output_dir, =,...   \n",
       "3      [def, sprint, (, text, ,, *, colors, ), :, ret...   \n",
       "4      [def, print_log, (, text, ,, *, colors, ), :, ...   \n",
       "...                                                  ...   \n",
       "14913  [def, from_grayscale, (, im, ,, channels_on, =...   \n",
       "14914  [def, get_uuid, (, length, =, 32, ,, version, ...   \n",
       "14915  [def, get_unique_key_from_get, (, get_dict, ),...   \n",
       "14916  [def, get_domain, (, url, ), :, if, 'http', no...   \n",
       "14917  [def, get_url_args, (, url, ), :, url_data, =,...   \n",
       "\n",
       "                                               docstring  \\\n",
       "0      str->list\\n    Convert XML to URL List.\\n    F...   \n",
       "1                   Downloads Dailymotion videos by URL.   \n",
       "2                          Downloads Sina videos by URL.   \n",
       "3      Format text with color or other effects into A...   \n",
       "4                 Print a log message to standard error.   \n",
       "...                                                  ...   \n",
       "14913  Return a canvas from a grayscale image.\\n\\n   ...   \n",
       "14914  Returns a unique ID of a given length.\\n    Us...   \n",
       "14915                   Build a unique key from get data   \n",
       "14916               Returns domain name portion of a URL   \n",
       "14917             Returns a dictionary from a URL params   \n",
       "\n",
       "                                        docstring_tokens  \\\n",
       "0      [str, -, >, list, Convert, XML, to, URL, List,...   \n",
       "1           [Downloads, Dailymotion, videos, by, URL, .]   \n",
       "2                  [Downloads, Sina, videos, by, URL, .]   \n",
       "3      [Format, text, with, color, or, other, effects...   \n",
       "4       [Print, a, log, message, to, standard, error, .]   \n",
       "...                                                  ...   \n",
       "14913  [Return, a, canvas, from, a, grayscale, image, .]   \n",
       "14914  [Returns, a, unique, ID, of, a, given, length,...   \n",
       "14915           [Build, a, unique, key, from, get, data]   \n",
       "14916       [Returns, domain, name, portion, of, a, URL]   \n",
       "14917     [Returns, a, dictionary, from, a, URL, params]   \n",
       "\n",
       "                                            sha  \\\n",
       "0      b746ac01c9f39de94cac2d56f665285b0523b974   \n",
       "1      b746ac01c9f39de94cac2d56f665285b0523b974   \n",
       "2      b746ac01c9f39de94cac2d56f665285b0523b974   \n",
       "3      b746ac01c9f39de94cac2d56f665285b0523b974   \n",
       "4      b746ac01c9f39de94cac2d56f665285b0523b974   \n",
       "...                                         ...   \n",
       "14913  d88ddf81ee3eb3949677e2ef746af8169ce88092   \n",
       "14914  973f3e003dc38b812897dab88455bee37dcaf931   \n",
       "14915  973f3e003dc38b812897dab88455bee37dcaf931   \n",
       "14916  973f3e003dc38b812897dab88455bee37dcaf931   \n",
       "14917  973f3e003dc38b812897dab88455bee37dcaf931   \n",
       "\n",
       "                                                     url  \n",
       "0      https://github.com/soimort/you-get/blob/b746ac...  \n",
       "1      https://github.com/soimort/you-get/blob/b746ac...  \n",
       "2      https://github.com/soimort/you-get/blob/b746ac...  \n",
       "3      https://github.com/soimort/you-get/blob/b746ac...  \n",
       "4      https://github.com/soimort/you-get/blob/b746ac...  \n",
       "...                                                  ...  \n",
       "14913  https://github.com/JIC-CSB/jicbioimage.illustr...  \n",
       "14914  https://github.com/un33k/django-toolware/blob/...  \n",
       "14915  https://github.com/un33k/django-toolware/blob/...  \n",
       "14916  https://github.com/un33k/django-toolware/blob/...  \n",
       "14917  https://github.com/un33k/django-toolware/blob/...  \n",
       "\n",
       "[14918 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ds.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all unique repository names\n",
    "repos = set(df['repo'].unique())\n",
    "len(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_repos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 6\n",
      "page 7\n",
      "page 8\n",
      "page 9\n",
      "page 10\n"
     ]
    }
   ],
   "source": [
    "# Get top repositories from github\n",
    "top_repos = list(top_repos)\n",
    "for i in range(1, 11):\n",
    "    try:\n",
    "        repos_stars = requests.get(f'https://api.github.com/search/repositories?q=language:python&sort=stars&order=desc&per_page=100&page={i}')\n",
    "        repos_stars = repos_stars.json()['items']\n",
    "    except:\n",
    "        print(repos_stars.content)\n",
    "        continue\n",
    "    try:\n",
    "        repos_forks = requests.get(f'https://api.github.com/search/repositories?q=language:python&sort=forks&order=desc&per_page=100&page={i}')\n",
    "        repos_forks = repos_forks.json()['items']\n",
    "    except:\n",
    "        print(repos_forks.content)\n",
    "        continue\n",
    "    print(f'page {i}')\n",
    "    top_repos.extend([repo['full_name'] for repo in repos_stars + repos_forks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2185"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_repos = set(top_repos)\n",
    "len(top_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Azure/azure-sdk-for-python',\n",
       " 'apache/airflow',\n",
       " 'dagster-io/dagster',\n",
       " 'soimort/you-get',\n",
       " 'open-mmlab/mmcv',\n",
       " 'pytorch/vision',\n",
       " 'streamlink/streamlink']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_repos = list(top_repos & repos)\n",
    "best_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>repo</th>\n",
       "      <th>path</th>\n",
       "      <th>func_name</th>\n",
       "      <th>original_string</th>\n",
       "      <th>language</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>sha</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/executors/base_executor.py</td>\n",
       "      <td>BaseExecutor.has_task</td>\n",
       "      <td>def has_task(self, task_instance):\\n        \"\"...</td>\n",
       "      <td>python</td>\n",
       "      <td>def has_task(self, task_instance):\\n        \"\"...</td>\n",
       "      <td>[def, has_task, (, self, ,, task_instance, ), ...</td>\n",
       "      <td>Checks if a task is either queued or running i...</td>\n",
       "      <td>[Checks, if, a, task, is, either, queued, or, ...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/executors/base_executor.py</td>\n",
       "      <td>BaseExecutor.get_event_buffer</td>\n",
       "      <td>def get_event_buffer(self, dag_ids=None):\\n   ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def get_event_buffer(self, dag_ids=None):\\n   ...</td>\n",
       "      <td>[def, get_event_buffer, (, self, ,, dag_ids, =...</td>\n",
       "      <td>Returns and flush the event buffer. In case da...</td>\n",
       "      <td>[Returns, and, flush, the, event, buffer, ., I...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/contrib/hooks/snowflake_hook.py</td>\n",
       "      <td>SnowflakeHook.get_conn</td>\n",
       "      <td>def get_conn(self):\\n        \"\"\"\\n        Retu...</td>\n",
       "      <td>python</td>\n",
       "      <td>def get_conn(self):\\n        \"\"\"\\n        Retu...</td>\n",
       "      <td>[def, get_conn, (, self, ), :, conn_config, =,...</td>\n",
       "      <td>Returns a snowflake.connection object</td>\n",
       "      <td>[Returns, a, snowflake, ., connection, object]</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/contrib/hooks/snowflake_hook.py</td>\n",
       "      <td>SnowflakeHook._get_aws_credentials</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        \"\"\"\\n...</td>\n",
       "      <td>python</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        \"\"\"\\n...</td>\n",
       "      <td>[def, _get_aws_credentials, (, self, ), :, if,...</td>\n",
       "      <td>returns aws_access_key_id, aws_secret_access_k...</td>\n",
       "      <td>[returns, aws_access_key_id, aws_secret_access...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/contrib/hooks/grpc_hook.py</td>\n",
       "      <td>GrpcHook._get_field</td>\n",
       "      <td>def _get_field(self, field_name, default=None)...</td>\n",
       "      <td>python</td>\n",
       "      <td>def _get_field(self, field_name, default=None)...</td>\n",
       "      <td>[def, _get_field, (, self, ,, field_name, ,, d...</td>\n",
       "      <td>Fetches a field from extras, and returns it. T...</td>\n",
       "      <td>[Fetches, a, field, from, extras, and, returns...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>2350</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "      <td>flow2rgb</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>python</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>[def, flow2rgb, (, flow, ,, color_wheel, =, No...</td>\n",
       "      <td>Convert flow map to RGB image.\\n\\n    Args:\\n ...</td>\n",
       "      <td>[Convert, flow, map, to, RGB, image, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>2351</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "      <td>make_color_wheel</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    \"\"\"Build...</td>\n",
       "      <td>python</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    \"\"\"Build...</td>\n",
       "      <td>[def, make_color_wheel, (, bins, =, None, ), :...</td>\n",
       "      <td>Build a color wheel.\\n\\n    Args:\\n        bin...</td>\n",
       "      <td>[Build, a, color, wheel, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>2352</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>examples/train_cifar10.py</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>[def, accuracy, (, output, ,, target, ,, topk,...</td>\n",
       "      <td>Computes the precision@k for the specified val...</td>\n",
       "      <td>[Computes, the, precision]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>2353</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "      <td>scatter</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>[def, scatter, (, inputs, ,, target_gpus, ,, d...</td>\n",
       "      <td>Scatter inputs to target gpus.\\n\\n    The only...</td>\n",
       "      <td>[Scatter, inputs, to, target, gpus, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>2354</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "      <td>scatter_kwargs</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>python</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>[def, scatter_kwargs, (, inputs, ,, kwargs, ,,...</td>\n",
       "      <td>Scatter with support for kwargs dictionary</td>\n",
       "      <td>[Scatter, with, support, for, kwargs, dictionary]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>987 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             repo                                     path  \\\n",
       "25      25   apache/airflow       airflow/executors/base_executor.py   \n",
       "26      26   apache/airflow       airflow/executors/base_executor.py   \n",
       "27      27   apache/airflow  airflow/contrib/hooks/snowflake_hook.py   \n",
       "28      28   apache/airflow  airflow/contrib/hooks/snowflake_hook.py   \n",
       "29      29   apache/airflow       airflow/contrib/hooks/grpc_hook.py   \n",
       "...    ...              ...                                      ...   \n",
       "2350  2350  open-mmlab/mmcv            mmcv/visualization/optflow.py   \n",
       "2351  2351  open-mmlab/mmcv            mmcv/visualization/optflow.py   \n",
       "2352  2352  open-mmlab/mmcv                examples/train_cifar10.py   \n",
       "2353  2353  open-mmlab/mmcv          mmcv/parallel/scatter_gather.py   \n",
       "2354  2354  open-mmlab/mmcv          mmcv/parallel/scatter_gather.py   \n",
       "\n",
       "                               func_name  \\\n",
       "25                 BaseExecutor.has_task   \n",
       "26         BaseExecutor.get_event_buffer   \n",
       "27                SnowflakeHook.get_conn   \n",
       "28    SnowflakeHook._get_aws_credentials   \n",
       "29                   GrpcHook._get_field   \n",
       "...                                  ...   \n",
       "2350                            flow2rgb   \n",
       "2351                    make_color_wheel   \n",
       "2352                            accuracy   \n",
       "2353                             scatter   \n",
       "2354                      scatter_kwargs   \n",
       "\n",
       "                                        original_string language  \\\n",
       "25    def has_task(self, task_instance):\\n        \"\"...   python   \n",
       "26    def get_event_buffer(self, dag_ids=None):\\n   ...   python   \n",
       "27    def get_conn(self):\\n        \"\"\"\\n        Retu...   python   \n",
       "28    def _get_aws_credentials(self):\\n        \"\"\"\\n...   python   \n",
       "29    def _get_field(self, field_name, default=None)...   python   \n",
       "...                                                 ...      ...   \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...   python   \n",
       "2351  def make_color_wheel(bins=None):\\n    \"\"\"Build...   python   \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...   python   \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...   python   \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...   python   \n",
       "\n",
       "                                                   code  \\\n",
       "25    def has_task(self, task_instance):\\n        \"\"...   \n",
       "26    def get_event_buffer(self, dag_ids=None):\\n   ...   \n",
       "27    def get_conn(self):\\n        \"\"\"\\n        Retu...   \n",
       "28    def _get_aws_credentials(self):\\n        \"\"\"\\n...   \n",
       "29    def _get_field(self, field_name, default=None)...   \n",
       "...                                                 ...   \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...   \n",
       "2351  def make_color_wheel(bins=None):\\n    \"\"\"Build...   \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...   \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...   \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...   \n",
       "\n",
       "                                            code_tokens  \\\n",
       "25    [def, has_task, (, self, ,, task_instance, ), ...   \n",
       "26    [def, get_event_buffer, (, self, ,, dag_ids, =...   \n",
       "27    [def, get_conn, (, self, ), :, conn_config, =,...   \n",
       "28    [def, _get_aws_credentials, (, self, ), :, if,...   \n",
       "29    [def, _get_field, (, self, ,, field_name, ,, d...   \n",
       "...                                                 ...   \n",
       "2350  [def, flow2rgb, (, flow, ,, color_wheel, =, No...   \n",
       "2351  [def, make_color_wheel, (, bins, =, None, ), :...   \n",
       "2352  [def, accuracy, (, output, ,, target, ,, topk,...   \n",
       "2353  [def, scatter, (, inputs, ,, target_gpus, ,, d...   \n",
       "2354  [def, scatter_kwargs, (, inputs, ,, kwargs, ,,...   \n",
       "\n",
       "                                              docstring  \\\n",
       "25    Checks if a task is either queued or running i...   \n",
       "26    Returns and flush the event buffer. In case da...   \n",
       "27                Returns a snowflake.connection object   \n",
       "28    returns aws_access_key_id, aws_secret_access_k...   \n",
       "29    Fetches a field from extras, and returns it. T...   \n",
       "...                                                 ...   \n",
       "2350  Convert flow map to RGB image.\\n\\n    Args:\\n ...   \n",
       "2351  Build a color wheel.\\n\\n    Args:\\n        bin...   \n",
       "2352  Computes the precision@k for the specified val...   \n",
       "2353  Scatter inputs to target gpus.\\n\\n    The only...   \n",
       "2354         Scatter with support for kwargs dictionary   \n",
       "\n",
       "                                       docstring_tokens  \\\n",
       "25    [Checks, if, a, task, is, either, queued, or, ...   \n",
       "26    [Returns, and, flush, the, event, buffer, ., I...   \n",
       "27       [Returns, a, snowflake, ., connection, object]   \n",
       "28    [returns, aws_access_key_id, aws_secret_access...   \n",
       "29    [Fetches, a, field, from, extras, and, returns...   \n",
       "...                                                 ...   \n",
       "2350            [Convert, flow, map, to, RGB, image, .]   \n",
       "2351                        [Build, a, color, wheel, .]   \n",
       "2352                         [Computes, the, precision]   \n",
       "2353             [Scatter, inputs, to, target, gpus, .]   \n",
       "2354  [Scatter, with, support, for, kwargs, dictionary]   \n",
       "\n",
       "                                           sha  \\\n",
       "25    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "26    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "27    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "28    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "29    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "...                                        ...   \n",
       "2350  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2351  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2352  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2353  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2354  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "\n",
       "                                                    url  \n",
       "25    https://github.com/apache/airflow/blob/b69c686...  \n",
       "26    https://github.com/apache/airflow/blob/b69c686...  \n",
       "27    https://github.com/apache/airflow/blob/b69c686...  \n",
       "28    https://github.com/apache/airflow/blob/b69c686...  \n",
       "29    https://github.com/apache/airflow/blob/b69c686...  \n",
       "...                                                 ...  \n",
       "2350  https://github.com/open-mmlab/mmcv/blob/0d77f6...  \n",
       "2351  https://github.com/open-mmlab/mmcv/blob/0d77f6...  \n",
       "2352  https://github.com/open-mmlab/mmcv/blob/0d77f6...  \n",
       "2353  https://github.com/open-mmlab/mmcv/blob/0d77f6...  \n",
       "2354  https://github.com/open-mmlab/mmcv/blob/0d77f6...  \n",
       "\n",
       "[987 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter df for already chosen repos\n",
    "df = df[\n",
    "    df[\"repo\"].isin(\n",
    "        [\n",
    "            \"Azure/azure-sdk-for-python\",\n",
    "            \"apache/airflow\",\n",
    "            \"open-mmlab/mmcv\",\n",
    "            \"streamlink/streamlink\",\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>repo</th>\n",
       "      <th>path</th>\n",
       "      <th>func_name</th>\n",
       "      <th>original_string</th>\n",
       "      <th>language</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>sha</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>soimort/you-get</td>\n",
       "      <td>src/you_get/extractors/miomio.py</td>\n",
       "      <td>sina_xml_to_url_list</td>\n",
       "      <td>def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...</td>\n",
       "      <td>python</td>\n",
       "      <td>def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...</td>\n",
       "      <td>[def, sina_xml_to_url_list, (, xml_data, ), :,...</td>\n",
       "      <td>str-&gt;list\\n    Convert XML to URL List.\\n    F...</td>\n",
       "      <td>[str, -, &gt;, list, Convert, XML, to, URL, List,...</td>\n",
       "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>soimort/you-get</td>\n",
       "      <td>src/you_get/extractors/dailymotion.py</td>\n",
       "      <td>dailymotion_download</td>\n",
       "      <td>def dailymotion_download(url, output_dir='.', ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def dailymotion_download(url, output_dir='.', ...</td>\n",
       "      <td>[def, dailymotion_download, (, url, ,, output_...</td>\n",
       "      <td>Downloads Dailymotion videos by URL.</td>\n",
       "      <td>[Downloads, Dailymotion, videos, by, URL, .]</td>\n",
       "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>soimort/you-get</td>\n",
       "      <td>src/you_get/extractors/sina.py</td>\n",
       "      <td>sina_download</td>\n",
       "      <td>def sina_download(url, output_dir='.', merge=T...</td>\n",
       "      <td>python</td>\n",
       "      <td>def sina_download(url, output_dir='.', merge=T...</td>\n",
       "      <td>[def, sina_download, (, url, ,, output_dir, =,...</td>\n",
       "      <td>Downloads Sina videos by URL.</td>\n",
       "      <td>[Downloads, Sina, videos, by, URL, .]</td>\n",
       "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>soimort/you-get</td>\n",
       "      <td>src/you_get/util/log.py</td>\n",
       "      <td>sprint</td>\n",
       "      <td>def sprint(text, *colors):\\n    \"\"\"Format text...</td>\n",
       "      <td>python</td>\n",
       "      <td>def sprint(text, *colors):\\n    \"\"\"Format text...</td>\n",
       "      <td>[def, sprint, (, text, ,, *, colors, ), :, ret...</td>\n",
       "      <td>Format text with color or other effects into A...</td>\n",
       "      <td>[Format, text, with, color, or, other, effects...</td>\n",
       "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>soimort/you-get</td>\n",
       "      <td>src/you_get/util/log.py</td>\n",
       "      <td>print_log</td>\n",
       "      <td>def print_log(text, *colors):\\n    \"\"\"Print a ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def print_log(text, *colors):\\n    \"\"\"Print a ...</td>\n",
       "      <td>[def, print_log, (, text, ,, *, colors, ), :, ...</td>\n",
       "      <td>Print a log message to standard error.</td>\n",
       "      <td>[Print, a, log, message, to, standard, error, .]</td>\n",
       "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985</th>\n",
       "      <td>2985</td>\n",
       "      <td>dagster-io/dagster</td>\n",
       "      <td>python_modules/dagster/dagster/core/execution_...</td>\n",
       "      <td>_execute_core_transform</td>\n",
       "      <td>def _execute_core_transform(transform_context,...</td>\n",
       "      <td>python</td>\n",
       "      <td>def _execute_core_transform(transform_context,...</td>\n",
       "      <td>[def, _execute_core_transform, (, transform_co...</td>\n",
       "      <td>Execute the user-specified transform for the s...</td>\n",
       "      <td>[Execute, the, user, -, specified, transform, ...</td>\n",
       "      <td>4119f8c773089de64831b1dfb9e168e353d401dc</td>\n",
       "      <td>https://github.com/dagster-io/dagster/blob/411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>2986</td>\n",
       "      <td>dagster-io/dagster</td>\n",
       "      <td>python_modules/dagster/dagster/core/types/deco...</td>\n",
       "      <td>as_dagster_type</td>\n",
       "      <td>def as_dagster_type(\\n    existing_type,\\n    ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def as_dagster_type(\\n    existing_type,\\n    ...</td>\n",
       "      <td>[def, as_dagster_type, (, existing_type, ,, na...</td>\n",
       "      <td>Takes a python cls and creates a type for it i...</td>\n",
       "      <td>[Takes, a, python, cls, and, creates, a, type,...</td>\n",
       "      <td>4119f8c773089de64831b1dfb9e168e353d401dc</td>\n",
       "      <td>https://github.com/dagster-io/dagster/blob/411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>2987</td>\n",
       "      <td>dagster-io/dagster</td>\n",
       "      <td>python_modules/dagster/dagster/core/definition...</td>\n",
       "      <td>resource</td>\n",
       "      <td>def resource(config_field=None, description=No...</td>\n",
       "      <td>python</td>\n",
       "      <td>def resource(config_field=None, description=No...</td>\n",
       "      <td>[def, resource, (, config_field, =, None, ,, d...</td>\n",
       "      <td>A decorator for creating a resource. The decor...</td>\n",
       "      <td>[A, decorator, for, creating, a, resource, ., ...</td>\n",
       "      <td>4119f8c773089de64831b1dfb9e168e353d401dc</td>\n",
       "      <td>https://github.com/dagster-io/dagster/blob/411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>2988</td>\n",
       "      <td>dagster-io/dagster</td>\n",
       "      <td>python_modules/libraries/dagster-pagerduty/dag...</td>\n",
       "      <td>PagerDutyService.EventV2_create</td>\n",
       "      <td>def EventV2_create(\\n        self,\\n        su...</td>\n",
       "      <td>python</td>\n",
       "      <td>def EventV2_create(\\n        self,\\n        su...</td>\n",
       "      <td>[def, EventV2_create, (, self, ,, summary, ,, ...</td>\n",
       "      <td>Events API v2 enables you to add PagerDuty's a...</td>\n",
       "      <td>[Events, API, v2, enables, you, to, add, Pager...</td>\n",
       "      <td>4119f8c773089de64831b1dfb9e168e353d401dc</td>\n",
       "      <td>https://github.com/dagster-io/dagster/blob/411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>2989</td>\n",
       "      <td>dagster-io/dagster</td>\n",
       "      <td>python_modules/dagster-airflow/dagster_airflow...</td>\n",
       "      <td>coalesce_execution_steps</td>\n",
       "      <td>def coalesce_execution_steps(execution_plan):\\...</td>\n",
       "      <td>python</td>\n",
       "      <td>def coalesce_execution_steps(execution_plan):\\...</td>\n",
       "      <td>[def, coalesce_execution_steps, (, execution_p...</td>\n",
       "      <td>Groups execution steps by solid, in topologica...</td>\n",
       "      <td>[Groups, execution, steps, by, solid, in, topo...</td>\n",
       "      <td>4119f8c773089de64831b1dfb9e168e353d401dc</td>\n",
       "      <td>https://github.com/dagster-io/dagster/blob/411...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1092 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                repo  \\\n",
       "0        0     soimort/you-get   \n",
       "1        1     soimort/you-get   \n",
       "2        2     soimort/you-get   \n",
       "3        3     soimort/you-get   \n",
       "4        4     soimort/you-get   \n",
       "...    ...                 ...   \n",
       "2985  2985  dagster-io/dagster   \n",
       "2986  2986  dagster-io/dagster   \n",
       "2987  2987  dagster-io/dagster   \n",
       "2988  2988  dagster-io/dagster   \n",
       "2989  2989  dagster-io/dagster   \n",
       "\n",
       "                                                   path  \\\n",
       "0                      src/you_get/extractors/miomio.py   \n",
       "1                 src/you_get/extractors/dailymotion.py   \n",
       "2                        src/you_get/extractors/sina.py   \n",
       "3                               src/you_get/util/log.py   \n",
       "4                               src/you_get/util/log.py   \n",
       "...                                                 ...   \n",
       "2985  python_modules/dagster/dagster/core/execution_...   \n",
       "2986  python_modules/dagster/dagster/core/types/deco...   \n",
       "2987  python_modules/dagster/dagster/core/definition...   \n",
       "2988  python_modules/libraries/dagster-pagerduty/dag...   \n",
       "2989  python_modules/dagster-airflow/dagster_airflow...   \n",
       "\n",
       "                            func_name  \\\n",
       "0                sina_xml_to_url_list   \n",
       "1                dailymotion_download   \n",
       "2                       sina_download   \n",
       "3                              sprint   \n",
       "4                           print_log   \n",
       "...                               ...   \n",
       "2985          _execute_core_transform   \n",
       "2986                  as_dagster_type   \n",
       "2987                         resource   \n",
       "2988  PagerDutyService.EventV2_create   \n",
       "2989         coalesce_execution_steps   \n",
       "\n",
       "                                        original_string language  \\\n",
       "0     def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...   python   \n",
       "1     def dailymotion_download(url, output_dir='.', ...   python   \n",
       "2     def sina_download(url, output_dir='.', merge=T...   python   \n",
       "3     def sprint(text, *colors):\\n    \"\"\"Format text...   python   \n",
       "4     def print_log(text, *colors):\\n    \"\"\"Print a ...   python   \n",
       "...                                                 ...      ...   \n",
       "2985  def _execute_core_transform(transform_context,...   python   \n",
       "2986  def as_dagster_type(\\n    existing_type,\\n    ...   python   \n",
       "2987  def resource(config_field=None, description=No...   python   \n",
       "2988  def EventV2_create(\\n        self,\\n        su...   python   \n",
       "2989  def coalesce_execution_steps(execution_plan):\\...   python   \n",
       "\n",
       "                                                   code  \\\n",
       "0     def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...   \n",
       "1     def dailymotion_download(url, output_dir='.', ...   \n",
       "2     def sina_download(url, output_dir='.', merge=T...   \n",
       "3     def sprint(text, *colors):\\n    \"\"\"Format text...   \n",
       "4     def print_log(text, *colors):\\n    \"\"\"Print a ...   \n",
       "...                                                 ...   \n",
       "2985  def _execute_core_transform(transform_context,...   \n",
       "2986  def as_dagster_type(\\n    existing_type,\\n    ...   \n",
       "2987  def resource(config_field=None, description=No...   \n",
       "2988  def EventV2_create(\\n        self,\\n        su...   \n",
       "2989  def coalesce_execution_steps(execution_plan):\\...   \n",
       "\n",
       "                                            code_tokens  \\\n",
       "0     [def, sina_xml_to_url_list, (, xml_data, ), :,...   \n",
       "1     [def, dailymotion_download, (, url, ,, output_...   \n",
       "2     [def, sina_download, (, url, ,, output_dir, =,...   \n",
       "3     [def, sprint, (, text, ,, *, colors, ), :, ret...   \n",
       "4     [def, print_log, (, text, ,, *, colors, ), :, ...   \n",
       "...                                                 ...   \n",
       "2985  [def, _execute_core_transform, (, transform_co...   \n",
       "2986  [def, as_dagster_type, (, existing_type, ,, na...   \n",
       "2987  [def, resource, (, config_field, =, None, ,, d...   \n",
       "2988  [def, EventV2_create, (, self, ,, summary, ,, ...   \n",
       "2989  [def, coalesce_execution_steps, (, execution_p...   \n",
       "\n",
       "                                              docstring  \\\n",
       "0     str->list\\n    Convert XML to URL List.\\n    F...   \n",
       "1                  Downloads Dailymotion videos by URL.   \n",
       "2                         Downloads Sina videos by URL.   \n",
       "3     Format text with color or other effects into A...   \n",
       "4                Print a log message to standard error.   \n",
       "...                                                 ...   \n",
       "2985  Execute the user-specified transform for the s...   \n",
       "2986  Takes a python cls and creates a type for it i...   \n",
       "2987  A decorator for creating a resource. The decor...   \n",
       "2988  Events API v2 enables you to add PagerDuty's a...   \n",
       "2989  Groups execution steps by solid, in topologica...   \n",
       "\n",
       "                                       docstring_tokens  \\\n",
       "0     [str, -, >, list, Convert, XML, to, URL, List,...   \n",
       "1          [Downloads, Dailymotion, videos, by, URL, .]   \n",
       "2                 [Downloads, Sina, videos, by, URL, .]   \n",
       "3     [Format, text, with, color, or, other, effects...   \n",
       "4      [Print, a, log, message, to, standard, error, .]   \n",
       "...                                                 ...   \n",
       "2985  [Execute, the, user, -, specified, transform, ...   \n",
       "2986  [Takes, a, python, cls, and, creates, a, type,...   \n",
       "2987  [A, decorator, for, creating, a, resource, ., ...   \n",
       "2988  [Events, API, v2, enables, you, to, add, Pager...   \n",
       "2989  [Groups, execution, steps, by, solid, in, topo...   \n",
       "\n",
       "                                           sha  \\\n",
       "0     b746ac01c9f39de94cac2d56f665285b0523b974   \n",
       "1     b746ac01c9f39de94cac2d56f665285b0523b974   \n",
       "2     b746ac01c9f39de94cac2d56f665285b0523b974   \n",
       "3     b746ac01c9f39de94cac2d56f665285b0523b974   \n",
       "4     b746ac01c9f39de94cac2d56f665285b0523b974   \n",
       "...                                        ...   \n",
       "2985  4119f8c773089de64831b1dfb9e168e353d401dc   \n",
       "2986  4119f8c773089de64831b1dfb9e168e353d401dc   \n",
       "2987  4119f8c773089de64831b1dfb9e168e353d401dc   \n",
       "2988  4119f8c773089de64831b1dfb9e168e353d401dc   \n",
       "2989  4119f8c773089de64831b1dfb9e168e353d401dc   \n",
       "\n",
       "                                                    url  \n",
       "0     https://github.com/soimort/you-get/blob/b746ac...  \n",
       "1     https://github.com/soimort/you-get/blob/b746ac...  \n",
       "2     https://github.com/soimort/you-get/blob/b746ac...  \n",
       "3     https://github.com/soimort/you-get/blob/b746ac...  \n",
       "4     https://github.com/soimort/you-get/blob/b746ac...  \n",
       "...                                                 ...  \n",
       "2985  https://github.com/dagster-io/dagster/blob/411...  \n",
       "2986  https://github.com/dagster-io/dagster/blob/411...  \n",
       "2987  https://github.com/dagster-io/dagster/blob/411...  \n",
       "2988  https://github.com/dagster-io/dagster/blob/411...  \n",
       "2989  https://github.com/dagster-io/dagster/blob/411...  \n",
       "\n",
       "[1092 rows x 12 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter df for best_repos\n",
    "df = df[df['repo'].isin(best_repos)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str->list\n",
      "    Convert XML to URL List.\n",
      "    From Biligrab.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Downloads Dailymotion videos by URL.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Downloads Sina videos by URL.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Format text with color or other effects into ANSI escaped string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Print a log message to standard error.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Print an error log message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "What a Terrible Failure!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Detect operating system.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "str->None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "str->dict\n",
      "    Information for CKPlayer API content.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Splicing URLs according to video ID to get video details\n",
      "----------------------------------------------------------------------------------------------------\n",
      "str->list of str\n",
      "        Give you the real URLs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts a string to a valid filename.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Downloads CBS videos by URL.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Override the original one\n",
      "        Ugly ugly dirty hack\n",
      "----------------------------------------------------------------------------------------------------\n",
      "str, str, str, bool, bool ->None\n",
      "\n",
      "    Download Acfun video by vid.\n",
      "\n",
      "    Call Acfun API, decide which site to use, and pass the job to its\n",
      "    extractor.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scans through a string for substrings matched some patterns.\n",
      "\n",
      "    Args:\n",
      "        text: A string to be scanned.\n",
      "        patterns: a list of regex pattern.\n",
      "\n",
      "    Returns:\n",
      "        a list if matched. empty if not.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses the query string of a URL and returns the value of a parameter.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        param: A string representing the name of the parameter.\n",
      "\n",
      "    Returns:\n",
      "        The value of the parameter.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the content of a URL via sending a HTTP GET request.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        headers: Request headers used by the client.\n",
      "        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n",
      "\n",
      "    Returns:\n",
      "        The content as a string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Post the content of a URL via sending a HTTP POST request.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        headers: Request headers used by the client.\n",
      "        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n",
      "\n",
      "    Returns:\n",
      "        The content as a string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses host name and port number from a string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "str->str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "JSON, int, int, int->str\n",
      "    \n",
      "    Get a proper title with courseid+topicID+partID.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "int->None\n",
      "    \n",
      "    Download a WHOLE course.\n",
      "    Reuse the API call to save time.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "int, int, int->None\n",
      "    \n",
      "    Download ONE PART of the course.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a task is either queued or running in this executor\n",
      "\n",
      "        :param task_instance: TaskInstance\n",
      "        :return: True if the task is known to this executor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns and flush the event buffer. In case dag_ids is specified\n",
      "        it will only return and flush events for the given dag_ids. Otherwise\n",
      "        it returns and flushes all\n",
      "\n",
      "        :param dag_ids: to dag_ids to return events for, if None returns all\n",
      "        :return: a dict of events\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a snowflake.connection object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "returns aws_access_key_id, aws_secret_access_key\n",
      "        from extra\n",
      "\n",
      "        intended to be used by external import and export statements\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetches a field from extras, and returns it. This is some Airflow\n",
      "        magic. The grpc hook type adds custom UI elements\n",
      "        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n",
      "        They get formatted as shown below.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executes SQL using psycopg2 copy_expert method.\n",
      "        Necessary to execute COPY command without access to a superuser.\n",
      "\n",
      "        Note: if this method is called with a \"COPY FROM\" statement and\n",
      "        the specified input file does not exist, it creates an empty\n",
      "        file and no data is loaded, but the operation succeeds.\n",
      "        So if users want to be aware when the input file does not exist,\n",
      "        they have to check its existence by themselves.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dumps a database table into a tab-delimited file\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uploads the file to Google cloud storage\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the max partition for a table.\n",
      "\n",
      "    :param schema: The hive schema the table lives in\n",
      "    :type schema: str\n",
      "    :param table: The hive table you are interested in, supports the dot\n",
      "        notation as in \"my_database.my_table\", if a dot is found,\n",
      "        the schema param is disregarded\n",
      "    :type table: str\n",
      "    :param metastore_conn_id: The hive connection you are interested in.\n",
      "        If your default is set you don't need to use this parameter.\n",
      "    :type metastore_conn_id: str\n",
      "    :param filter_map: partition_key:partition_value map used for partition filtering,\n",
      "                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n",
      "                       Only partitions matching all partition_key:partition_value\n",
      "                       pairs will be considered as candidates of max partition.\n",
      "    :type filter_map: map\n",
      "    :param field: the field to get the max value from. If there's only\n",
      "        one partition field, this will be inferred\n",
      "    :type field: str\n",
      "\n",
      "    >>> max_partition('airflow.static_babynames_partitioned')\n",
      "    '2015-01-01'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a mysql connection object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the state of a TaskInstance at the command line.\n",
      "    >>> airflow task_state tutorial sleep 2015-01-01\n",
      "    success\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Runs forever, monitoring the child processes of @gunicorn_master_proc and\n",
      "    restarting workers occasionally.\n",
      "    Each iteration of the loop traverses one edge of this state transition\n",
      "    diagram, where each state (node) represents\n",
      "    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n",
      "    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n",
      "    The horizontal transition at ? happens after the new worker parses all the\n",
      "    dags (so it could take a while!)\n",
      "       V ────────────────────────────────────────────────────────────────────────┐\n",
      "    [n / n] ──TTIN──> [ [n, n+bs) / n + bs ]  ────?───> [n + bs / n + bs] ──TTOU─┘\n",
      "       ^                          ^───────────────┘\n",
      "       │\n",
      "       │      ┌────────────────v\n",
      "       └──────┴────── [ [0, n) / n ] <─── start\n",
      "    We change the number of workers by sending TTIN and TTOU to the gunicorn\n",
      "    master process, which increases and decreases the number of child workers\n",
      "    respectively. Gunicorn guarantees that on TTOU workers are terminated\n",
      "    gracefully and that the oldest worker is terminated.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Cloud Translate\n",
      "\n",
      "        :return: Google Cloud Translate client object.\n",
      "        :rtype: Client\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Translate a string or list of strings.\n",
      "\n",
      "        See https://cloud.google.com/translate/docs/translating-text\n",
      "\n",
      "        :type values: str or list\n",
      "        :param values: String or list of strings to translate.\n",
      "\n",
      "        :type target_language: str\n",
      "        :param target_language: The language to translate results into. This\n",
      "                                is required by the API and defaults to\n",
      "                                the target language of the current instance.\n",
      "\n",
      "        :type format_: str\n",
      "        :param format_: (Optional) One of ``text`` or ``html``, to specify\n",
      "                        if the input text is plain text or HTML.\n",
      "\n",
      "        :type source_language: str or None\n",
      "        :param source_language: (Optional) The language of the text to\n",
      "                                be translated.\n",
      "\n",
      "        :type model: str or None\n",
      "        :param model: (Optional) The model used to translate the text, such\n",
      "                      as ``'base'`` or ``'nmt'``.\n",
      "\n",
      "        :rtype: str or list\n",
      "        :returns: A list of dictionaries for each queried value. Each\n",
      "                  dictionary typically contains three keys (though not\n",
      "                  all will be present in all cases)\n",
      "\n",
      "                  * ``detectedSourceLanguage``: The detected language (as an\n",
      "                    ISO 639-1 language code) of the text.\n",
      "                  * ``translatedText``: The translation of the text into the\n",
      "                    target language.\n",
      "                  * ``input``: The corresponding input value.\n",
      "                  * ``model``: The model used to translate the text.\n",
      "\n",
      "                  If only a single value is passed, then only a single\n",
      "                  dictionary will be returned.\n",
      "        :raises: :class:`~exceptions.ValueError` if the number of\n",
      "                 values and translations differ.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves a resource containing information about a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: A Cloud SQL instance resource.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new Cloud SQL instance.\n",
      "\n",
      "        :param body: Body required by the Cloud SQL insert API, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates settings of a Cloud SQL instance.\n",
      "\n",
      "        Caution: This is not a partial update, so you must include values for\n",
      "        all the settings that you want to retain.\n",
      "\n",
      "        :param body: Body required by the Cloud SQL patch API, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n",
      "        :type body: dict\n",
      "        :param instance: Cloud SQL instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a Cloud SQL instance.\n",
      "\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :param instance: Cloud SQL instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves a database resource from a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database in the instance.\n",
      "        :type database: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: A Cloud SQL database resource, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new database inside a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates a database resource inside a Cloud SQL instance.\n",
      "\n",
      "        This method supports patch semantics.\n",
      "        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database to be updated in the instance.\n",
      "        :type database: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a database from a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database to be deleted in the instance.\n",
      "        :type database: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n",
      "        or CSV file.\n",
      "\n",
      "        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n",
      "            project ID.\n",
      "        :type instance: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts Cloud SQL Proxy.\n",
      "\n",
      "        You have to remember to stop the proxy if you started it!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Stops running proxy.\n",
      "\n",
      "        You should stop the proxy after you stop using it.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns version of the Cloud SQL Proxy.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create connection in the Connection table, according to whether it uses\n",
      "        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the dynamically created connection from the Connection table.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete the dynamically created connection from the Connection table.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n",
      "        lifecycle per task.\n",
      "\n",
      "        :return: The Cloud SQL Proxy runner.\n",
      "        :rtype: CloudSqlProxyRunner\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieve database hook. This is the actual Postgres or MySQL database hook\n",
      "        that uses proxy or connects directly to the Google Cloud SQL database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clean up database hook after it was used.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reserve free TCP port to be used by Cloud SQL Proxy\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Replaces invalid MLEngine job_id characters with '_'.\n",
      "\n",
      "    This also adds a leading 'z' in case job_id starts with an invalid\n",
      "    character.\n",
      "\n",
      "    Args:\n",
      "        job_id: A job_id str that may have invalid characters.\n",
      "\n",
      "    Returns:\n",
      "        A valid job_id representation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Extract error code from ftp exception\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Remove any existing DAG runs for the perf test DAGs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Remove any existing task instances for the perf test DAGs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Toggle the pause state of the DAGs in the test.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Print operational metrics for the scheduler test.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Override the scheduler heartbeat to determine when the test is complete\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Invoke Lambda Function\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates Operators needed for model evaluation and returns.\n",
      "\n",
      "    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n",
      "    calling MLEngineBatchPredictionOperator, then summarize and validate\n",
      "    the result via Cloud Dataflow using DataFlowPythonOperator.\n",
      "\n",
      "    For details and pricing about Batch prediction, please refer to the website\n",
      "    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n",
      "    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n",
      "\n",
      "    It returns three chained operators for prediction, summary, and validation,\n",
      "    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n",
      "    respectively.\n",
      "    (<prefix> should contain only alphanumeric characters or hyphen.)\n",
      "\n",
      "    The upstream and downstream can be set accordingly like:\n",
      "      pred, _, val = create_evaluate_ops(...)\n",
      "      pred.set_upstream(upstream_op)\n",
      "      ...\n",
      "      downstream_op.set_upstream(val)\n",
      "\n",
      "    Callers will provide two python callables, metric_fn and validate_fn, in\n",
      "    order to customize the evaluation behavior as they wish.\n",
      "    - metric_fn receives a dictionary per instance derived from json in the\n",
      "      batch prediction result. The keys might vary depending on the model.\n",
      "      It should return a tuple of metrics.\n",
      "    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n",
      "      generated over all instances.\n",
      "      The key/value of the dictionary matches to what's given by\n",
      "      metric_fn_and_keys arg.\n",
      "      The dictionary contains an additional metric, 'count' to represent the\n",
      "      total number of instances received for evaluation.\n",
      "      The function would raise an exception to mark the task as failed, in a\n",
      "      case the validation result is not okay to proceed (i.e. to set the trained\n",
      "      version as default).\n",
      "\n",
      "    Typical examples are like this:\n",
      "\n",
      "    def get_metric_fn_and_keys():\n",
      "        import math  # imports should be outside of the metric_fn below.\n",
      "        def error_and_squared_error(inst):\n",
      "            label = float(inst['input_label'])\n",
      "            classes = float(inst['classes'])  # 0 or 1\n",
      "            err = abs(classes-label)\n",
      "            squared_err = math.pow(classes-label, 2)\n",
      "            return (err, squared_err)  # returns a tuple.\n",
      "        return error_and_squared_error, ['err', 'mse']  # key order must match.\n",
      "\n",
      "    def validate_err_and_count(summary):\n",
      "        if summary['err'] > 0.2:\n",
      "            raise ValueError('Too high err>0.2; summary=%s' % summary)\n",
      "        if summary['mse'] > 0.05:\n",
      "            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n",
      "        if summary['count'] < 1000:\n",
      "            raise ValueError('Too few instances<1000; summary=%s' % summary)\n",
      "        return summary\n",
      "\n",
      "    For the details on the other BatchPrediction-related arguments (project_id,\n",
      "    job_id, region, data_format, input_paths, prediction_path, model_uri),\n",
      "    please refer to MLEngineBatchPredictionOperator too.\n",
      "\n",
      "    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n",
      "        hyphen are allowed (no underscores), since this will be used as dataflow\n",
      "        job name, which doesn't allow other characters.\n",
      "    :type task_prefix: str\n",
      "\n",
      "    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n",
      "    :type data_format: str\n",
      "\n",
      "    :param input_paths: a list of input paths to be sent to BatchPrediction.\n",
      "    :type input_paths: list[str]\n",
      "\n",
      "    :param prediction_path: GCS path to put the prediction results in.\n",
      "    :type prediction_path: str\n",
      "\n",
      "    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n",
      "        - metric_fn is a function that accepts a dictionary (for an instance),\n",
      "          and returns a tuple of metric(s) that it calculates.\n",
      "        - metric_keys is a list of strings to denote the key of each metric.\n",
      "    :type metric_fn_and_keys: tuple of a function and a list[str]\n",
      "\n",
      "    :param validate_fn: a function to validate whether the averaged metric(s) is\n",
      "        good enough to push the model.\n",
      "    :type validate_fn: function\n",
      "\n",
      "    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n",
      "        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n",
      "        the job_id argument.\n",
      "    :type batch_prediction_job_id: str\n",
      "\n",
      "    :param project_id: the Google Cloud Platform project id in which to execute\n",
      "        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n",
      "        `default_args['project_id']` will be used.\n",
      "    :type project_id: str\n",
      "\n",
      "    :param region: the Google Cloud Platform region in which to execute Cloud ML\n",
      "        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n",
      "        `default_args['region']` will be used.\n",
      "    :type region: str\n",
      "\n",
      "    :param dataflow_options: options to run Dataflow jobs. If None, then the\n",
      "        `dag`'s `default_args['dataflow_default_options']` will be used.\n",
      "    :type dataflow_options: dictionary\n",
      "\n",
      "    :param model_uri: GCS path of the model exported by Tensorflow using\n",
      "        tensorflow.estimator.export_savedmodel(). It cannot be used with\n",
      "        model_name or version_name below. See MLEngineBatchPredictionOperator for\n",
      "        more detail.\n",
      "    :type model_uri: str\n",
      "\n",
      "    :param model_name: Used to indicate a model to use for prediction. Can be\n",
      "        used in combination with version_name, but cannot be used together with\n",
      "        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n",
      "        then the `dag`'s `default_args['model_name']` will be used.\n",
      "    :type model_name: str\n",
      "\n",
      "    :param version_name: Used to indicate a model version to use for prediction,\n",
      "        in combination with model_name. Cannot be used together with model_uri.\n",
      "        See MLEngineBatchPredictionOperator for more detail. If None, then the\n",
      "        `dag`'s `default_args['version_name']` will be used.\n",
      "    :type version_name: str\n",
      "\n",
      "    :param dag: The `DAG` to use for all Operators.\n",
      "    :type dag: airflow.models.DAG\n",
      "\n",
      "    :returns: a tuple of three operators, (prediction, summary, validation)\n",
      "    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n",
      "                  PythonOperator)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates the directory specified by path, creating intermediate directories\n",
      "    as necessary. If directory already exists, this is a no-op.\n",
      "\n",
      "    :param path: The directory to create\n",
      "    :type path: str\n",
      "    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n",
      "    :type mode: int\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A small helper function to convert a string to a numeric value\n",
      "    if appropriate\n",
      "\n",
      "    :param s: the string to be converted\n",
      "    :type s: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Make a naive datetime.datetime in a given time zone aware.\n",
      "\n",
      "    :param value: datetime\n",
      "    :param timezone: timezone\n",
      "    :return: localized datetime in settings.TIMEZONE or timezone\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Make an aware datetime.datetime naive in a given time zone.\n",
      "\n",
      "    :param value: datetime\n",
      "    :param timezone: timezone\n",
      "    :return: naive datetime\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n",
      "\n",
      "    :return: datetime.datetime\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Establish a connection to druid broker.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns http session for use with requests\n",
      "\n",
      "        :param headers: additional headers to be passed through as a dictionary\n",
      "        :type headers: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Performs the request\n",
      "\n",
      "        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n",
      "        :type endpoint: str\n",
      "        :param data: payload to be uploaded or request parameters\n",
      "        :type data: dict\n",
      "        :param headers: additional headers to be passed through as a dictionary\n",
      "        :type headers: dict\n",
      "        :param extra_options: additional options to be used when executing the request\n",
      "            i.e. {'check_response': False} to avoid checking raising exceptions on non\n",
      "            2XX or 3XX status codes\n",
      "        :type extra_options: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n",
      "        status codes\n",
      "\n",
      "        :param response: A requests response object\n",
      "        :type response: requests.response\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Grabs extra options like timeout and actually runs the request,\n",
      "        checking for the result\n",
      "\n",
      "        :param session: the session to be used to execute the request\n",
      "        :type session: requests.Session\n",
      "        :param prepped_request: the prepared request generated in run()\n",
      "        :type prepped_request: session.prepare_request\n",
      "        :param extra_options: additional options to be used when executing the request\n",
      "            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n",
      "            or 3XX status codes\n",
      "        :type extra_options: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Contextmanager that will create and teardown a session.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Function decorator that provides a session if it isn't provided.\n",
      "    If you want to reuse a session or run the function as part of a\n",
      "    database transaction, you pass it to the function, if not this wrapper\n",
      "    will create one and close it for you.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clear out the database\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses some DatabaseError to provide a better error message\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a set of records from Presto\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a pandas dataframe from a sql query.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute the statement against Presto. Can be used to create views.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A generic way to insert a set of tuples into a table.\n",
      "\n",
      "        :param table: Name of the target table\n",
      "        :type table: str\n",
      "        :param rows: The rows to insert into the table\n",
      "        :type rows: iterable of tuples\n",
      "        :param target_fields: The names of the columns to fill in the table\n",
      "        :type target_fields: iterable of strings\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a cosmos db client.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a collection exists in CosmosDB.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new collection in the CosmosDB database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a database exists in CosmosDB.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new database in CosmosDB.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an existing database in CosmosDB.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an existing collection in the CosmosDB database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Insert a list of new documents into an existing collection in the CosmosDB database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete an existing document out of a collection in the CosmosDB database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a document from an existing collection in the CosmosDB database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the Cloud Function with the given name.\n",
      "\n",
      "        :param name: Name of the function.\n",
      "        :type name: str\n",
      "        :return: A Cloud Functions object representing the function.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new function in Cloud Function in the location specified in the body.\n",
      "\n",
      "        :param location: The location of the function.\n",
      "        :type location: str\n",
      "        :param body: The body required by the Cloud Functions insert API.\n",
      "        :type body: dict\n",
      "        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n",
      "            If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates Cloud Functions according to the specified update mask.\n",
      "\n",
      "        :param name: The name of the function.\n",
      "        :type name: str\n",
      "        :param body: The body required by the cloud function patch API.\n",
      "        :type body: dict\n",
      "        :param update_mask: The update mask - array of fields that should be patched.\n",
      "        :type update_mask: [str]\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uploads zip file with sources.\n",
      "\n",
      "        :param location: The location where the function is created.\n",
      "        :type location: str\n",
      "        :param zip_path: The path of the valid .zip file to upload.\n",
      "        :type zip_path: str\n",
      "        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n",
      "            If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: The upload URL that was returned by generateUploadUrl method.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified Cloud Function.\n",
      "\n",
      "        :param name: The name of the function.\n",
      "        :type name: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wrapper around the private _get_dep_statuses method that contains some global\n",
      "        checks for all dependencies.\n",
      "\n",
      "        :param ti: the task instance to get the dependency status for\n",
      "        :type ti: airflow.models.TaskInstance\n",
      "        :param session: database session\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "        :param dep_context: the context for which this dependency should be evaluated for\n",
      "        :type dep_context: DepContext\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns whether or not this dependency is met for a given task instance. A\n",
      "        dependency is considered met if all of the dependency statuses it reports are\n",
      "        passing.\n",
      "\n",
      "        :param ti: the task instance to see if this dependency is met for\n",
      "        :type ti: airflow.models.TaskInstance\n",
      "        :param session: database session\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "        :param dep_context: The context this dependency is being checked under that stores\n",
      "            state that can be used by this dependency.\n",
      "        :type dep_context: BaseDepContext\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns an iterable of strings that explain why this dependency wasn't met.\n",
      "\n",
      "        :param ti: the task instance to see if this dependency is met for\n",
      "        :type ti: airflow.models.TaskInstance\n",
      "        :param session: database session\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "        :param dep_context: The context this dependency is being checked under that stores\n",
      "            state that can be used by this dependency.\n",
      "        :type dep_context: BaseDepContext\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses a config file for s3 credentials. Can currently\n",
      "    parse boto, s3cmd.conf and AWS SDK config formats\n",
      "\n",
      "    :param config_file_name: path to the config file\n",
      "    :type config_file_name: str\n",
      "    :param config_format: config type. One of \"boto\", \"s3cmd\" or \"aws\".\n",
      "        Defaults to \"boto\"\n",
      "    :type config_format: str\n",
      "    :param profile: profile name in AWS type config file\n",
      "    :type profile: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the underlying `botocore.Credentials` object.\n",
      "\n",
      "        This contains the following authentication attributes: access_key, secret_key and token.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns verticaql connection object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ensure all logging output has been flushed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If the path contains a folder with a .zip suffix, then\n",
      "    the folder is treated as a zip archive and path to zip is returned.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Traverse a directory and look for Python files.\n",
      "\n",
      "    :param directory: the directory to traverse\n",
      "    :type directory: unicode\n",
      "    :param safe_mode: whether to use a heuristic to determine whether a file\n",
      "        contains Airflow DAG definitions\n",
      "    :return: a list of paths to Python files in the specified directory\n",
      "    :rtype: list[unicode]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Construct a TaskInstance from the database based on the primary key\n",
      "\n",
      "        :param session: DB session.\n",
      "        :param lock_for_update: if True, indicates that the database should\n",
      "            lock the TaskInstance (issuing a FOR UPDATE clause) until the\n",
      "            session is committed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Launch DagFileProcessorManager processor and start DAG parsing loop in manager.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send termination signal to DAG parsing processor manager\n",
      "        and expect it to terminate all DAG file processors.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method to clean up DAG file processors to avoid leaving orphan processes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Use multiple processes to parse and generate tasks for the\n",
      "        DAGs in parallel. By processing them in separate processes,\n",
      "        we can get parallelism and isolation from potentially harmful\n",
      "        user code.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parse DAG files repeatedly in a standalone loop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parse DAG files in a loop controlled by DagParsingSignal.\n",
      "        Actual DAG parsing loop will run once upon receiving one\n",
      "        agent heartbeat message and will report done when finished the loop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Refresh file paths from dag dir if we haven't done it for too long.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Occasionally print out stats about how fast the files are getting processed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clears import errors for files that no longer exist.\n",
      "\n",
      "        :param session: session for ORM operations\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Print out stats about how files are getting processed.\n",
      "\n",
      "        :param known_file_paths: a list of file paths that may contain Airflow\n",
      "            DAG definitions\n",
      "        :type known_file_paths: list[unicode]\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Update this with a new set of paths to DAG definition files.\n",
      "\n",
      "        :param new_file_paths: list of paths to DAG definition files\n",
      "        :type new_file_paths: list[unicode]\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sleeps until all the processors are done.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This should be periodically called by the manager loop. This method will\n",
      "        kick off new processes to process DAG definition files and read the\n",
      "        results from the finished processors.\n",
      "\n",
      "        :return: a list of SimpleDags that were produced by processors that\n",
      "            have finished since the last time this was called\n",
      "        :rtype: list[airflow.utils.dag_processing.SimpleDag]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Kill all child processes on exit since we don't want to leave\n",
      "        them as orphaned.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens a ssh connection to the remote host.\n",
      "\n",
      "        :rtype: paramiko.client.SSHClient\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a transfer job that runs periodically.\n",
      "\n",
      "        :param body: (Required) A request body, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n",
      "        :type body: dict\n",
      "        :return: transfer job.\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the latest state of a long-running operation in Google Storage\n",
      "        Transfer Service.\n",
      "\n",
      "        :param job_name: (Required) Name of the job to be fetched\n",
      "        :type job_name: str\n",
      "        :param project_id: (Optional) the ID of the project that owns the Transfer\n",
      "            Job. If set to None or missing, the default project_id from the GCP\n",
      "            connection is used.\n",
      "        :type project_id: str\n",
      "        :return: Transfer Job\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists long-running operations in Google Storage Transfer\n",
      "        Service that match the specified filter.\n",
      "\n",
      "        :param filter: (Required) A request filter, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter\n",
      "        :type filter: dict\n",
      "        :return: List of Transfer Jobs\n",
      "        :rtype: list[dict]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates a transfer job that runs periodically.\n",
      "\n",
      "        :param job_name: (Required) Name of the job to be updated\n",
      "        :type job_name: str\n",
      "        :param body: A request body, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n",
      "        :type body: dict\n",
      "        :return: If successful, TransferJob.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a transfer job. This is a soft delete. After a transfer job is\n",
      "        deleted, the job and all the transfer executions are subject to garbage\n",
      "        collection. Transfer jobs become eligible for garbage collection\n",
      "        30 days after soft delete.\n",
      "\n",
      "        :param job_name: (Required) Name of the job to be deleted\n",
      "        :type job_name: str\n",
      "        :param project_id: (Optional) the ID of the project that owns the Transfer\n",
      "            Job. If set to None or missing, the default project_id from the GCP\n",
      "            connection is used.\n",
      "        :type project_id: str\n",
      "        :rtype: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cancels an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param operation_name: Name of the transfer operation.\n",
      "        :type operation_name: str\n",
      "        :rtype: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pauses an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param operation_name: (Required) Name of the transfer operation.\n",
      "        :type operation_name: str\n",
      "        :rtype: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resumes an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param operation_name: (Required) Name of the transfer operation.\n",
      "        :type operation_name: str\n",
      "        :rtype: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Waits until the job reaches the expected state.\n",
      "\n",
      "        :param job: Transfer job\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n",
      "        :type job: dict\n",
      "        :param expected_statuses: State that is expected\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status\n",
      "        :type expected_statuses: set[str]\n",
      "        :param timeout:\n",
      "        :type timeout: time in which the operation must end in seconds\n",
      "        :rtype: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns all task reschedules for the task instance and try number,\n",
      "        in ascending order.\n",
      "\n",
      "        :param task_instance: the task instance to find task reschedules for\n",
      "        :type task_instance: airflow.models.TaskInstance\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the number of slots open at the moment\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Runs command and returns stdout\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Remove an option if it exists in config from a file or\n",
      "        default config. If both of config have the same option, this removes\n",
      "        the option in both configs unless remove_default=False.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the section as a dict. Values are converted to int, float, bool\n",
      "        as required.\n",
      "\n",
      "        :param section: section from the config\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Allocate IDs for incomplete keys.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n",
      "\n",
      "        :param partial_keys: a list of partial keys.\n",
      "        :type partial_keys: list\n",
      "        :return: a list of full keys.\n",
      "        :rtype: list\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Begins a new transaction.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction\n",
      "\n",
      "        :return: a transaction handle.\n",
      "        :rtype: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Commit a transaction, optionally creating, deleting or modifying some entities.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit\n",
      "\n",
      "        :param body: the body of the commit request.\n",
      "        :type body: dict\n",
      "        :return: the response body of the commit request.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lookup some entities by key.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup\n",
      "\n",
      "        :param keys: the keys to lookup.\n",
      "        :type keys: list\n",
      "        :param read_consistency: the read consistency to use. default, strong or eventual.\n",
      "                                 Cannot be used with a transaction.\n",
      "        :type read_consistency: str\n",
      "        :param transaction: the transaction to use, if any.\n",
      "        :type transaction: str\n",
      "        :return: the response body of the lookup request.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Roll back a transaction.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback\n",
      "\n",
      "        :param transaction: the transaction to roll back.\n",
      "        :type transaction: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run a query for entities.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery\n",
      "\n",
      "        :param body: the body of the query request.\n",
      "        :type body: dict\n",
      "        :return: the batch of query results.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the latest state of a long-running operation.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get\n",
      "\n",
      "        :param name: the name of the operation resource.\n",
      "        :type name: str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the long-running operation.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete\n",
      "\n",
      "        :param name: the name of the operation resource.\n",
      "        :type name: str\n",
      "        :return: none if successful.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Poll backup operation state until it's completed.\n",
      "\n",
      "        :param name: the name of the operation resource\n",
      "        :type name: str\n",
      "        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.\n",
      "        :type polling_interval_in_seconds: int\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Export entities from Cloud Datastore to Cloud Storage for backup.\n",
      "\n",
      "        .. note::\n",
      "            Keep in mind that this requests the Admin API not the Data API.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export\n",
      "\n",
      "        :param bucket: The name of the Cloud Storage bucket.\n",
      "        :type bucket: str\n",
      "        :param namespace: The Cloud Storage namespace path.\n",
      "        :type namespace: str\n",
      "        :param entity_filter: Description of what data from the project is included in the export.\n",
      "        :type entity_filter: dict\n",
      "        :param labels: Client-assigned labels.\n",
      "        :type labels: dict of str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Import a backup from Cloud Storage to Cloud Datastore.\n",
      "\n",
      "        .. note::\n",
      "            Keep in mind that this requests the Admin API not the Data API.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import\n",
      "\n",
      "        :param bucket: The name of the Cloud Storage bucket.\n",
      "        :type bucket: str\n",
      "        :param file: the metadata file written by the projects.export operation.\n",
      "        :type file: str\n",
      "        :param namespace: The Cloud Storage namespace path.\n",
      "        :type namespace: str\n",
      "        :param entity_filter: specify which kinds/namespaces are to be imported.\n",
      "        :type entity_filter: dict\n",
      "        :param labels: Client-assigned labels.\n",
      "        :type labels: dict of str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Publish a message to a topic or an endpoint.\n",
      "\n",
      "        :param target_arn: either a TopicArn or an EndpointArn\n",
      "        :type target_arn: str\n",
      "        :param message: the default message you want to send\n",
      "        :param message: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetch the hostname using the callable from the config or using\n",
      "    `socket.getfqdn` as a fallback.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Cloud Natural Language service.\n",
      "\n",
      "        :return: Cloud Natural Language service object\n",
      "        :rtype: google.cloud.language_v1.LanguageServiceClient\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Finds named entities in the text along with entity types,\n",
      "        salience, mentions for each entity, and other properties.\n",
      "\n",
      "        :param document: Input document.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type document: dict or class google.cloud.language_v1.types.Document\n",
      "        :param encoding_type: The encoding type used by the API to calculate offsets.\n",
      "        :type encoding_type: google.cloud.language_v1.types.EncodingType\n",
      "        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "        :param metadata: Additional metadata that is provided to the method.\n",
      "        :type metadata: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A convenience method that provides all the features that analyzeSentiment,\n",
      "        analyzeEntities, and analyzeSyntax provide in one call.\n",
      "\n",
      "        :param document: Input document.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type document: dict or google.cloud.language_v1.types.Document\n",
      "        :param features: The enabled features.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Features\n",
      "        :type features: dict or google.cloud.language_v1.enums.Features\n",
      "        :param encoding_type: The encoding type used by the API to calculate offsets.\n",
      "        :type encoding_type: google.cloud.language_v1.types.EncodingType\n",
      "        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "        :param metadata: Additional metadata that is provided to the method.\n",
      "        :type metadata: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnnotateTextResponse\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Classifies a document into categories.\n",
      "\n",
      "        :param document: Input document.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type document: dict or class google.cloud.language_v1.types.Document\n",
      "        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "        :param metadata: Additional metadata that is provided to the method.\n",
      "        :type metadata: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets template fields for specific operator class.\n",
      "\n",
      "    :param fullname: Full path to operator class.\n",
      "        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``\n",
      "    :return: List of template field\n",
      "    :rtype: list[str]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A role that allows you to include a list of template fields in the middle of the text. This is especially\n",
      "    useful when writing guides describing how to use the operator.\n",
      "    The result is a list of fields where each field is shorted in the literal block.\n",
      "\n",
      "    Sample usage::\n",
      "\n",
      "    :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`\n",
      "\n",
      "    For further information look at:\n",
      "\n",
      "    * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted\n",
      "      Text Roles)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Properly close pooled database connections\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ensures that certain subfolders of AIRFLOW_HOME are on the classpath\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the returned Celery result from the Airflow task\n",
      "        ID provided to the sensor, and returns True if the\n",
      "        celery result has been finished execution.\n",
      "\n",
      "        :param context: Airflow's execution context\n",
      "        :type context: dict\n",
      "        :return: True if task has been executed, otherwise False\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return true if the ticket cache contains \"conf\" information as is found\n",
      "    in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the\n",
      "    Sun Java Krb5LoginModule in Java6, so we need to take an action to work\n",
      "    around it.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Transforms a SQLAlchemy model instance into a dictionary\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Yield successive chunks of a given size from a list of items\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reduce the given list of items by splitting it into chunks\n",
      "    of the given size and passing each chunk through the reducer\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given a number of tasks, builds a dependency chain.\n",
      "\n",
      "    chain(task_1, task_2, task_3, task_4)\n",
      "\n",
      "    is equivalent to\n",
      "\n",
      "    task_1.set_downstream(task_2)\n",
      "    task_2.set_downstream(task_3)\n",
      "    task_3.set_downstream(task_4)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a pretty ascii table from tuples\n",
      "\n",
      "    If namedtuple are used, the table will have headers\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given task instance, try_number, filename_template, return the rendered log\n",
      "    filename\n",
      "\n",
      "    :param ti: task instance\n",
      "    :param try_number: try_number of the task\n",
      "    :param filename_template: filename template, which can be jinja template or\n",
      "        python string template\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a Google Cloud Dataproc service object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Awaits for Google Cloud Dataproc Operation to complete.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Coerces content or all values of content if it is a dict to a string. The\n",
      "    function will throw if content contains non-string or non-numeric types.\n",
      "\n",
      "    The reason why we have this function is because the ``self.json`` field must be a\n",
      "    dict with only string values. This is because ``render_template`` will fail\n",
      "    for numerical values.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Handles the Airflow + Databricks lifecycle logic for a Databricks operator\n",
      "\n",
      "    :param operator: Databricks operator being handled\n",
      "    :param context: Airflow context\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run an pig script using the pig cli\n",
      "\n",
      "        >>> ph = PigCliHook()\n",
      "        >>> result = ph.run_cli(\"ls /;\")\n",
      "        >>> (\"hdfs://\" in result)\n",
      "        True\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetch and return the state of the given celery task. The scope of this function is\n",
      "    global so that it can be called by subprocesses in the pool.\n",
      "\n",
      "    :param celery_task: a tuple of the Celery task key and the async Celery object used\n",
      "        to fetch the task's state\n",
      "    :type celery_task: tuple(str, celery.result.AsyncResult)\n",
      "    :return: a tuple of the Celery task key and the Celery state of the task\n",
      "    :rtype: tuple[str, str]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "How many Celery tasks should each worker process send.\n",
      "\n",
      "        :return: Number of tasks that should be sent per process\n",
      "        :rtype: int\n",
      "----------------------------------------------------------------------------------------------------\n",
      "How many Celery tasks should be sent to each worker process.\n",
      "\n",
      "        :return: Number of tasks that should be used per process\n",
      "        :rtype: int\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Like a Python builtin dict object, setdefault returns the current value\n",
      "        for a key, and if it isn't there, stores the default value and returns it.\n",
      "\n",
      "        :param key: Dict key for this Variable\n",
      "        :type key: str\n",
      "        :param default: Default value to set and return if the variable\n",
      "            isn't already in the DB\n",
      "        :type default: Mixed\n",
      "        :param deserialize_json: Store this as a JSON encoded value in the DB\n",
      "            and un-encode it when retrieving a value\n",
      "        :return: Mixed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a Google MLEngine service object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Launches a MLEngine job and wait for it to reach a terminal state.\n",
      "\n",
      "        :param project_id: The Google Cloud project id within which MLEngine\n",
      "            job will be launched.\n",
      "        :type project_id: str\n",
      "\n",
      "        :param job: MLEngine Job object that should be provided to the MLEngine\n",
      "            API, such as: ::\n",
      "\n",
      "                {\n",
      "                  'jobId': 'my_job_id',\n",
      "                  'trainingInput': {\n",
      "                    'scaleTier': 'STANDARD_1',\n",
      "                    ...\n",
      "                  }\n",
      "                }\n",
      "\n",
      "        :type job: dict\n",
      "\n",
      "        :param use_existing_job_fn: In case that a MLEngine job with the same\n",
      "            job_id already exist, this method (if provided) will decide whether\n",
      "            we should use this existing job, continue waiting for it to finish\n",
      "            and returning the job object. It should accepts a MLEngine job\n",
      "            object, and returns a boolean value indicating whether it is OK to\n",
      "            reuse the existing job. If 'use_existing_job_fn' is not provided,\n",
      "            we by default reuse the existing MLEngine job.\n",
      "        :type use_existing_job_fn: function\n",
      "\n",
      "        :return: The MLEngine job object if the job successfully reach a\n",
      "            terminal state (which might be FAILED or CANCELLED state).\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets a MLEngine job based on the job name.\n",
      "\n",
      "        :return: MLEngine job object if succeed.\n",
      "        :rtype: dict\n",
      "\n",
      "        Raises:\n",
      "            googleapiclient.errors.HttpError: if HTTP error is returned from server\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Waits for the Job to reach a terminal state.\n",
      "\n",
      "        This method will periodically check the job state until the job reach\n",
      "        a terminal state.\n",
      "\n",
      "        Raises:\n",
      "            googleapiclient.errors.HttpError: if HTTP error is returned when getting\n",
      "            the job\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates the Version on Google Cloud ML Engine.\n",
      "\n",
      "        Returns the operation if the version was created successfully and\n",
      "        raises an error otherwise.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets a version to be the default. Blocks until finished.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists all available versions of a model. Blocks until finished.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the given version of a model. Blocks until finished.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a Model. Blocks until finished.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets a Model. Blocks until finished.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Write batch items to dynamodb table with provisioned throughout capacity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Integrate plugins to the context.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new instance of the configured executor if none exists and returns it\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new instance of the named executor.\n",
      "    In case the executor name is not know in airflow,\n",
      "    look for it in the plugins\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Handles error callbacks when using Segment with segment_debug_mode set to True\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a mssql connection object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Trigger a new dag run for a Dag with an execution date of now unless\n",
      "    specified in the data.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete all DB records related to the specified Dag.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a JSON with a task's public instance variables.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get all pools.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a pool.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete pool.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a new container group\n",
      "\n",
      "        :param resource_group: the name of the resource group\n",
      "        :type resource_group: str\n",
      "        :param name: the name of the container group\n",
      "        :type name: str\n",
      "        :param container_group: the properties of the container group\n",
      "        :type container_group: azure.mgmt.containerinstance.models.ContainerGroup\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the state and exitcode of a container group\n",
      "\n",
      "        :param resource_group: the name of the resource group\n",
      "        :type resource_group: str\n",
      "        :param name: the name of the container group\n",
      "        :type name: str\n",
      "        :return: A tuple with the state, exitcode, and details.\n",
      "            If the exitcode is unknown 0 is returned.\n",
      "        :rtype: tuple(state,exitcode,details)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the messages of a container group\n",
      "\n",
      "        :param resource_group: the name of the resource group\n",
      "        :type resource_group: str\n",
      "        :param name: the name of the container group\n",
      "        :type name: str\n",
      "        :return: A list of the event messages\n",
      "        :rtype: list[str]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the tail from logs of a container group\n",
      "\n",
      "        :param resource_group: the name of the resource group\n",
      "        :type resource_group: str\n",
      "        :param name: the name of the container group\n",
      "        :type name: str\n",
      "        :param tail: the size of the tail\n",
      "        :type tail: int\n",
      "        :return: A list of log messages\n",
      "        :rtype: list[str]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a container group\n",
      "\n",
      "        :param resource_group: the name of the resource group\n",
      "        :type resource_group: str\n",
      "        :param name: the name of the container group\n",
      "        :type name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Test if a container group exists\n",
      "\n",
      "        :param resource_group: the name of the resource group\n",
      "        :type resource_group: str\n",
      "        :param name: the name of the container group\n",
      "        :type name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Function decorator that Looks for an argument named \"default_args\", and\n",
      "    fills the unspecified arguments from it.\n",
      "\n",
      "    Since python2.* isn't clear about which arguments are missing when\n",
      "    calling a function, and that this can be quite confusing with multi-level\n",
      "    inheritance and argument defaults, this decorator also alerts with\n",
      "    specific information about the missing arguments.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Builds an ingest query for an HDFS TSV load.\n",
      "\n",
      "        :param static_path: The path on hdfs where the data is\n",
      "        :type static_path: str\n",
      "        :param columns: List of all the columns that are available\n",
      "        :type columns: list\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check for message on subscribed channels and write to xcom the message with key ``message``\n",
      "\n",
      "        An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``\n",
      "\n",
      "        :param context: the context object\n",
      "        :type context: dict\n",
      "        :return: ``True`` if message (with type 'message') is available or ``False`` if not\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a set of dag runs for the given search criteria.\n",
      "\n",
      "        :param dag_id: the dag_id to find dag runs for\n",
      "        :type dag_id: int, list\n",
      "        :param run_id: defines the the run id for this dag run\n",
      "        :type run_id: str\n",
      "        :param execution_date: the execution date\n",
      "        :type execution_date: datetime.datetime\n",
      "        :param state: the state of the dag run\n",
      "        :type state: airflow.utils.state.State\n",
      "        :param external_trigger: whether this dag run is externally triggered\n",
      "        :type external_trigger: bool\n",
      "        :param no_backfills: return no backfills (True), return all (False).\n",
      "            Defaults to False\n",
      "        :type no_backfills: bool\n",
      "        :param session: database session\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the task instances for this dag run\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the task instance specified by task_id for this dag run\n",
      "\n",
      "        :param task_id: the task id\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The previous DagRun, if there is one\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The previous, SCHEDULED DagRun, if there is one\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Determines the overall state of the DagRun based on the state\n",
      "        of its TaskInstances.\n",
      "\n",
      "        :return: State\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Verifies the DagRun by checking for removed tasks or tasks that are not in the\n",
      "        database yet. It will set state to removed or add the task if required.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "We need to get the headers in addition to the body answer\n",
      "    to get the location from them\n",
      "    This function uses jenkins_request method from python-jenkins library\n",
      "    with just the return call changed\n",
      "\n",
      "    :param jenkins_server: The server to query\n",
      "    :param req: The request to execute\n",
      "    :return: Dict containing the response body (key body)\n",
      "        and the headers coming along (headers)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given a context, this function provides a dictionary of values that can be used to\n",
      "    externally reconstruct relations between dags, dag_runs, tasks and task_instances.\n",
      "    Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if\n",
      "    in_env_var_format is set to True.\n",
      "\n",
      "    :param context: The context for the task_instance of interest.\n",
      "    :type context: dict\n",
      "    :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.\n",
      "    :type in_env_var_format: bool\n",
      "    :return: task_instance context as dict.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This function decides whether or not to Trigger the remote DAG\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends a single datapoint metric to DataDog\n",
      "\n",
      "        :param metric_name: The name of the metric\n",
      "        :type metric_name: str\n",
      "        :param datapoint: A single integer or float related to the metric\n",
      "        :type datapoint: int or float\n",
      "        :param tags: A list of tags associated with the metric\n",
      "        :type tags: list\n",
      "        :param type_: Type of your metric: gauge, rate, or count\n",
      "        :type type_: str\n",
      "        :param interval: If the type of the metric is rate or count, define the corresponding interval\n",
      "        :type interval: int\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries datadog for a specific metric, potentially with some\n",
      "        function applied to it and returns the results.\n",
      "\n",
      "        :param query: The datadog query to execute (see datadog docs)\n",
      "        :type query: str\n",
      "        :param from_seconds_ago: How many seconds ago to start querying for.\n",
      "        :type from_seconds_ago: int\n",
      "        :param to_seconds_ago: Up to how many seconds ago to query for.\n",
      "        :type to_seconds_ago: int\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the DAG out of the dictionary, and refreshes it if expired\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fail given zombie tasks, which are tasks that haven't\n",
      "        had a heartbeat for too long, in the current DagBag.\n",
      "\n",
      "        :param zombies: zombie task instances to kill.\n",
      "        :type zombies: airflow.utils.dag_processing.SimpleTaskInstance\n",
      "        :param session: DB session.\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds the DAG into the bag, recurses into sub dags.\n",
      "        Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given a file path or a folder, this method looks for python modules,\n",
      "        imports them and adds them to the dagbag collection.\n",
      "\n",
      "        Note that if a ``.airflowignore`` file is found while processing\n",
      "        the directory, it will behave much like a ``.gitignore``,\n",
      "        ignoring files that match any of the regex patterns specified\n",
      "        in the file.\n",
      "\n",
      "        **Note**: The patterns in .airflowignore are treated as\n",
      "        un-anchored regexes, not shell-like glob patterns.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prints a report around DagBag loading stats\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add or subtract days from a YYYY-MM-DD\n",
      "\n",
      "    :param ds: anchor date in ``YYYY-MM-DD`` format to add to\n",
      "    :type ds: str\n",
      "    :param days: number of days to add to the ds, you can use negative values\n",
      "    :type days: int\n",
      "\n",
      "    >>> ds_add('2015-01-01', 5)\n",
      "    '2015-01-06'\n",
      "    >>> ds_add('2015-01-06', -5)\n",
      "    '2015-01-01'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Takes an input string and outputs another string\n",
      "    as specified in the output format\n",
      "\n",
      "    :param ds: input string which contains a date\n",
      "    :type ds: str\n",
      "    :param input_format: input string format. E.g. %Y-%m-%d\n",
      "    :type input_format: str\n",
      "    :param output_format: output string format  E.g. %Y-%m-%d\n",
      "    :type output_format: str\n",
      "\n",
      "    >>> ds_format('2015-01-01', \"%Y-%m-%d\", \"%m-%d-%y\")\n",
      "    '01-01-15'\n",
      "    >>> ds_format('1/5/2015', \"%m/%d/%Y\",  \"%Y-%m-%d\")\n",
      "    '2015-01-05'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "poke matching files in a directory with self.regex\n",
      "\n",
      "        :return: Bool depending on the search criteria\n",
      "----------------------------------------------------------------------------------------------------\n",
      "poke for a non empty directory\n",
      "\n",
      "        :return: Bool depending on the search criteria\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clears a set of task instances, but makes sure the running ones\n",
      "    get killed.\n",
      "\n",
      "    :param tis: a list of task instances\n",
      "    :param session: current session\n",
      "    :param activate_dag_runs: flag to check for active dag run\n",
      "    :param dag: DAG object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the try number that this task number will be when it is actually\n",
      "        run.\n",
      "\n",
      "        If the TI is currently running, this will match the column in the\n",
      "        databse, in all othercases this will be incremenetd\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Generates the shell command required to execute this task instance.\n",
      "\n",
      "        :param dag_id: DAG ID\n",
      "        :type dag_id: unicode\n",
      "        :param task_id: Task ID\n",
      "        :type task_id: unicode\n",
      "        :param execution_date: Execution date for the task\n",
      "        :type execution_date: datetime\n",
      "        :param mark_success: Whether to mark the task as successful\n",
      "        :type mark_success: bool\n",
      "        :param ignore_all_deps: Ignore all ignorable dependencies.\n",
      "            Overrides the other ignore_* parameters.\n",
      "        :type ignore_all_deps: bool\n",
      "        :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs\n",
      "            (e.g. for Backfills)\n",
      "        :type ignore_depends_on_past: bool\n",
      "        :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past\n",
      "            and trigger rule\n",
      "        :type ignore_task_deps: bool\n",
      "        :param ignore_ti_state: Ignore the task instance's previous failure/success\n",
      "        :type ignore_ti_state: bool\n",
      "        :param local: Whether to run the task locally\n",
      "        :type local: bool\n",
      "        :param pickle_id: If the DAG was serialized to the DB, the ID\n",
      "            associated with the pickled DAG\n",
      "        :type pickle_id: unicode\n",
      "        :param file_path: path to the file containing the DAG definition\n",
      "        :param raw: raw mode (needs more details)\n",
      "        :param job_id: job ID (needs more details)\n",
      "        :param pool: the Airflow pool that the task should run in\n",
      "        :type pool: unicode\n",
      "        :param cfg_path: the Path to the configuration file\n",
      "        :type cfg_path: basestring\n",
      "        :return: shell command that can be used to run the task instance\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the very latest state from the database, if a session is passed,\n",
      "        we use and looking up the state becomes part of the session, otherwise\n",
      "        a new session is used.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Forces the task instance's state to FAILED in the database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Refreshes the task instance from the database based on the primary key\n",
      "\n",
      "        :param lock_for_update: if True, indicates that the database should\n",
      "            lock the TaskInstance (issuing a FOR UPDATE clause) until the\n",
      "            session is committed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clears all XCom data from the database for the task instance\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a tuple that identifies the task instance uniquely\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks whether the dependents of this task instance have all succeeded.\n",
      "        This is meant to be used by wait_for_downstream.\n",
      "\n",
      "        This is useful when you do not want to start processing the next\n",
      "        schedule of a task until the dependents are done. For instance,\n",
      "        if the task DROPs and recreates a table.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get datetime of the next retry if the task instance fails. For exponential\n",
      "        backoff, retry_delay is used as base and will be converted to seconds.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks on whether the task instance is in the right state and timeframe\n",
      "        to be retried.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a boolean as to whether the slot pool has room for this\n",
      "        task to run\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the DagRun for this TaskInstance\n",
      "\n",
      "        :param session:\n",
      "        :return: DagRun\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Make an XCom available for tasks to pull.\n",
      "\n",
      "        :param key: A key for the XCom\n",
      "        :type key: str\n",
      "        :param value: A value for the XCom. The value is pickled and stored\n",
      "            in the database.\n",
      "        :type value: any pickleable object\n",
      "        :param execution_date: if provided, the XCom will not be visible until\n",
      "            this date. This can be used, for example, to send a message to a\n",
      "            task on a future date without it being immediately visible.\n",
      "        :type execution_date: datetime\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pull XComs that optionally meet certain criteria.\n",
      "\n",
      "        The default value for `key` limits the search to XComs\n",
      "        that were returned by other tasks (as opposed to those that were pushed\n",
      "        manually). To remove this filter, pass key=None (or any desired value).\n",
      "\n",
      "        If a single task_id string is provided, the result is the value of the\n",
      "        most recent matching XCom from that task_id. If multiple task_ids are\n",
      "        provided, a tuple of matching values is returned. None is returned\n",
      "        whenever no matches are found.\n",
      "\n",
      "        :param key: A key for the XCom. If provided, only XComs with matching\n",
      "            keys will be returned. The default key is 'return_value', also\n",
      "            available as a constant XCOM_RETURN_KEY. This key is automatically\n",
      "            given to XComs returned by tasks (as opposed to being pushed\n",
      "            manually). To remove the filter, pass key=None.\n",
      "        :type key: str\n",
      "        :param task_ids: Only XComs from tasks with matching ids will be\n",
      "            pulled. Can pass None to remove the filter.\n",
      "        :type task_ids: str or iterable of strings (representing task_ids)\n",
      "        :param dag_id: If provided, only pulls XComs from this DAG.\n",
      "            If None (default), the DAG of the calling task is used.\n",
      "        :type dag_id: str\n",
      "        :param include_prior_dates: If False, only XComs from the current\n",
      "            execution_date are returned. If True, XComs from previous dates\n",
      "            are returned as well.\n",
      "        :type include_prior_dates: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets the log context.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Close and upload local log file to remote storage Wasb.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Google Compute Engine.\n",
      "\n",
      "        :return: Google Compute Engine services object\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts an existing instance defined by project_id, zone and resource_id.\n",
      "        Must be called with keyword arguments rather than positional.\n",
      "\n",
      "        :param zone: Google Cloud Platform zone where the instance exists\n",
      "        :type zone: str\n",
      "        :param resource_id: Name of the Compute Engine instance resource\n",
      "        :type resource_id: str\n",
      "        :param project_id: Optional, Google Cloud Platform project ID where the\n",
      "            Compute Engine Instance exists. If set to None or missing,\n",
      "            the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets machine type of an instance defined by project_id, zone and resource_id.\n",
      "        Must be called with keyword arguments rather than positional.\n",
      "\n",
      "        :param zone: Google Cloud Platform zone where the instance exists.\n",
      "        :type zone: str\n",
      "        :param resource_id: Name of the Compute Engine instance resource\n",
      "        :type resource_id: str\n",
      "        :param body: Body required by the Compute Engine setMachineType API,\n",
      "            as described in\n",
      "            https://cloud.google.com/compute/docs/reference/rest/v1/instances/setMachineType\n",
      "        :type body: dict\n",
      "        :param project_id: Optional, Google Cloud Platform project ID where the\n",
      "            Compute Engine Instance exists. If set to None or missing,\n",
      "            the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves instance template by project_id and resource_id.\n",
      "        Must be called with keyword arguments rather than positional.\n",
      "\n",
      "        :param resource_id: Name of the instance template\n",
      "        :type resource_id: str\n",
      "        :param project_id: Optional, Google Cloud Platform project ID where the\n",
      "            Compute Engine Instance exists. If set to None or missing,\n",
      "            the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: Instance template representation as object according to\n",
      "            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Inserts instance template using body specified\n",
      "        Must be called with keyword arguments rather than positional.\n",
      "\n",
      "        :param body: Instance template representation as object according to\n",
      "            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n",
      "        :type body: dict\n",
      "        :param request_id: Optional, unique request_id that you might add to achieve\n",
      "            full idempotence (for example when client call times out repeating the request\n",
      "            with the same request id will not create a new instance template again)\n",
      "            It should be in UUID format as defined in RFC 4122\n",
      "        :type request_id: str\n",
      "        :param project_id: Optional, Google Cloud Platform project ID where the\n",
      "            Compute Engine Instance exists. If set to None or missing,\n",
      "            the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves Instance Group Manager by project_id, zone and resource_id.\n",
      "        Must be called with keyword arguments rather than positional.\n",
      "\n",
      "        :param zone: Google Cloud Platform zone where the Instance Group Manager exists\n",
      "        :type zone: str\n",
      "        :param resource_id: Name of the Instance Group Manager\n",
      "        :type resource_id: str\n",
      "        :param project_id: Optional, Google Cloud Platform project ID where the\n",
      "            Compute Engine Instance exists. If set to None or missing,\n",
      "            the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: Instance group manager representation as object according to\n",
      "            https://cloud.google.com/compute/docs/reference/rest/beta/instanceGroupManagers\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Patches Instance Group Manager with the specified body.\n",
      "        Must be called with keyword arguments rather than positional.\n",
      "\n",
      "        :param zone: Google Cloud Platform zone where the Instance Group Manager exists\n",
      "        :type zone: str\n",
      "        :param resource_id: Name of the Instance Group Manager\n",
      "        :type resource_id: str\n",
      "        :param body: Instance Group Manager representation as json-merge-patch object\n",
      "            according to\n",
      "            https://cloud.google.com/compute/docs/reference/rest/beta/instanceTemplates/patch\n",
      "        :type body: dict\n",
      "        :param request_id: Optional, unique request_id that you might add to achieve\n",
      "            full idempotence (for example when client call times out repeating the request\n",
      "            with the same request id will not create a new instance template again).\n",
      "            It should be in UUID format as defined in RFC 4122\n",
      "        :type request_id: str\n",
      "        :param project_id: Optional, Google Cloud Platform project ID where the\n",
      "            Compute Engine Instance exists. If set to None or missing,\n",
      "            the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Waits for the named operation to complete - checks status of the async call.\n",
      "\n",
      "        :param operation_name: name of the operation\n",
      "        :type operation_name: str\n",
      "        :param zone: optional region of the request (might be None for global operations)\n",
      "        :type zone: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if bucket_name exists.\n",
      "\n",
      "        :param bucket_name: the name of the bucket\n",
      "        :type bucket_name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates an Amazon S3 bucket.\n",
      "\n",
      "        :param bucket_name: The name of the bucket\n",
      "        :type bucket_name: str\n",
      "        :param region_name: The name of the aws region in which to create the bucket.\n",
      "        :type region_name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks that a prefix exists in a bucket\n",
      "\n",
      "        :param bucket_name: the name of the bucket\n",
      "        :type bucket_name: str\n",
      "        :param prefix: a key prefix\n",
      "        :type prefix: str\n",
      "        :param delimiter: the delimiter marks key hierarchy.\n",
      "        :type delimiter: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists prefixes in a bucket under prefix\n",
      "\n",
      "        :param bucket_name: the name of the bucket\n",
      "        :type bucket_name: str\n",
      "        :param prefix: a key prefix\n",
      "        :type prefix: str\n",
      "        :param delimiter: the delimiter marks key hierarchy.\n",
      "        :type delimiter: str\n",
      "        :param page_size: pagination size\n",
      "        :type page_size: int\n",
      "        :param max_items: maximum items to return\n",
      "        :type max_items: int\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists keys in a bucket under prefix and not containing delimiter\n",
      "\n",
      "        :param bucket_name: the name of the bucket\n",
      "        :type bucket_name: str\n",
      "        :param prefix: a key prefix\n",
      "        :type prefix: str\n",
      "        :param delimiter: the delimiter marks key hierarchy.\n",
      "        :type delimiter: str\n",
      "        :param page_size: pagination size\n",
      "        :type page_size: int\n",
      "        :param max_items: maximum items to return\n",
      "        :type max_items: int\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a key exists in a bucket\n",
      "\n",
      "        :param key: S3 key that will point to the file\n",
      "        :type key: str\n",
      "        :param bucket_name: Name of the bucket in which the file is stored\n",
      "        :type bucket_name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a boto3.s3.Object\n",
      "\n",
      "        :param key: the path to the key\n",
      "        :type key: str\n",
      "        :param bucket_name: the name of the bucket\n",
      "        :type bucket_name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reads a key from S3\n",
      "\n",
      "        :param key: S3 key that will point to the file\n",
      "        :type key: str\n",
      "        :param bucket_name: Name of the bucket in which the file is stored\n",
      "        :type bucket_name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reads a key with S3 Select.\n",
      "\n",
      "        :param key: S3 key that will point to the file\n",
      "        :type key: str\n",
      "        :param bucket_name: Name of the bucket in which the file is stored\n",
      "        :type bucket_name: str\n",
      "        :param expression: S3 Select expression\n",
      "        :type expression: str\n",
      "        :param expression_type: S3 Select expression type\n",
      "        :type expression_type: str\n",
      "        :param input_serialization: S3 Select input data serialization format\n",
      "        :type input_serialization: dict\n",
      "        :param output_serialization: S3 Select output data serialization format\n",
      "        :type output_serialization: dict\n",
      "        :return: retrieved subset of original data by S3 Select\n",
      "        :rtype: str\n",
      "\n",
      "        .. seealso::\n",
      "            For more details about S3 Select parameters:\n",
      "            http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.select_object_content\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks that a key matching a wildcard expression exists in a bucket\n",
      "\n",
      "        :param wildcard_key: the path to the key\n",
      "        :type wildcard_key: str\n",
      "        :param bucket_name: the name of the bucket\n",
      "        :type bucket_name: str\n",
      "        :param delimiter: the delimiter marks key hierarchy\n",
      "        :type delimiter: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a boto3.s3.Object object matching the wildcard expression\n",
      "\n",
      "        :param wildcard_key: the path to the key\n",
      "        :type wildcard_key: str\n",
      "        :param bucket_name: the name of the bucket\n",
      "        :type bucket_name: str\n",
      "        :param delimiter: the delimiter marks key hierarchy\n",
      "        :type delimiter: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a local file to S3\n",
      "\n",
      "        :param filename: name of the file to load.\n",
      "        :type filename: str\n",
      "        :param key: S3 key that will point to the file\n",
      "        :type key: str\n",
      "        :param bucket_name: Name of the bucket in which to store the file\n",
      "        :type bucket_name: str\n",
      "        :param replace: A flag to decide whether or not to overwrite the key\n",
      "            if it already exists. If replace is False and the key exists, an\n",
      "            error will be raised.\n",
      "        :type replace: bool\n",
      "        :param encrypt: If True, the file will be encrypted on the server-side\n",
      "            by S3 and will be stored in an encrypted form while at rest in S3.\n",
      "        :type encrypt: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a string to S3\n",
      "\n",
      "        This is provided as a convenience to drop a string in S3. It uses the\n",
      "        boto infrastructure to ship a file to s3.\n",
      "\n",
      "        :param string_data: str to set as content for the key.\n",
      "        :type string_data: str\n",
      "        :param key: S3 key that will point to the file\n",
      "        :type key: str\n",
      "        :param bucket_name: Name of the bucket in which to store the file\n",
      "        :type bucket_name: str\n",
      "        :param replace: A flag to decide whether or not to overwrite the key\n",
      "            if it already exists\n",
      "        :type replace: bool\n",
      "        :param encrypt: If True, the file will be encrypted on the server-side\n",
      "            by S3 and will be stored in an encrypted form while at rest in S3.\n",
      "        :type encrypt: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads bytes to S3\n",
      "\n",
      "        This is provided as a convenience to drop a string in S3. It uses the\n",
      "        boto infrastructure to ship a file to s3.\n",
      "\n",
      "        :param bytes_data: bytes to set as content for the key.\n",
      "        :type bytes_data: bytes\n",
      "        :param key: S3 key that will point to the file\n",
      "        :type key: str\n",
      "        :param bucket_name: Name of the bucket in which to store the file\n",
      "        :type bucket_name: str\n",
      "        :param replace: A flag to decide whether or not to overwrite the key\n",
      "            if it already exists\n",
      "        :type replace: bool\n",
      "        :param encrypt: If True, the file will be encrypted on the server-side\n",
      "            by S3 and will be stored in an encrypted form while at rest in S3.\n",
      "        :type encrypt: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a file object to S3\n",
      "\n",
      "        :param file_obj: The file-like object to set as the content for the S3 key.\n",
      "        :type file_obj: file-like object\n",
      "        :param key: S3 key that will point to the file\n",
      "        :type key: str\n",
      "        :param bucket_name: Name of the bucket in which to store the file\n",
      "        :type bucket_name: str\n",
      "        :param replace: A flag that indicates whether to overwrite the key\n",
      "            if it already exists.\n",
      "        :type replace: bool\n",
      "        :param encrypt: If True, S3 encrypts the file on the server,\n",
      "            and the file is stored in encrypted form at rest in S3.\n",
      "        :type encrypt: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a copy of an object that is already stored in S3.\n",
      "\n",
      "        Note: the S3 connection used here needs to have access to both\n",
      "        source and destination bucket/key.\n",
      "\n",
      "        :param source_bucket_key: The key of the source object.\n",
      "\n",
      "            It can be either full s3:// style url or relative path from root level.\n",
      "\n",
      "            When it's specified as a full s3:// url, please omit source_bucket_name.\n",
      "        :type source_bucket_key: str\n",
      "        :param dest_bucket_key: The key of the object to copy to.\n",
      "\n",
      "            The convention to specify `dest_bucket_key` is the same\n",
      "            as `source_bucket_key`.\n",
      "        :type dest_bucket_key: str\n",
      "        :param source_bucket_name: Name of the S3 bucket where the source object is in.\n",
      "\n",
      "            It should be omitted when `source_bucket_key` is provided as a full s3:// url.\n",
      "        :type source_bucket_name: str\n",
      "        :param dest_bucket_name: Name of the S3 bucket to where the object is copied.\n",
      "\n",
      "            It should be omitted when `dest_bucket_key` is provided as a full s3:// url.\n",
      "        :type dest_bucket_name: str\n",
      "        :param source_version_id: Version ID of the source object (OPTIONAL)\n",
      "        :type source_version_id: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries cassandra and returns a cursor to the results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts a user type to RECORD that contains n fields, where n is the\n",
      "        number of attributes. Each element in the user type class will be converted to its\n",
      "        corresponding data type in BQ.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send an email with html content using sendgrid.\n",
      "\n",
      "    To use this plugin:\n",
      "    0. include sendgrid subpackage as part of your Airflow installation, e.g.,\n",
      "    pip install 'apache-airflow[sendgrid]'\n",
      "    1. update [email] backend in airflow.cfg, i.e.,\n",
      "    [email]\n",
      "    email_backend = airflow.contrib.utils.sendgrid.send_email\n",
      "    2. configure Sendgrid specific environment variables at all Airflow instances:\n",
      "    SENDGRID_MAIL_FROM={your-mail-from}\n",
      "    SENDGRID_API_KEY={your-sendgrid-api-key}.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Cloud Speech.\n",
      "\n",
      "        :return: Google Cloud Speech client object.\n",
      "        :rtype: google.cloud.speech_v1.SpeechClient\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Recognizes audio input\n",
      "\n",
      "        :param config: information to the recognizer that specifies how to process the request.\n",
      "            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig\n",
      "        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig\n",
      "        :param audio: audio data to be recognized\n",
      "            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio\n",
      "        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio\n",
      "        :param retry: (Optional) A retry object used to retry requests. If None is specified,\n",
      "            requests will not be retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.\n",
      "            Note that if retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Call the SparkSqlHook to run the provided sql query\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load AirflowPlugin subclasses from the entrypoints\n",
      "    provided. The entry_point group should be 'airflow.plugins'.\n",
      "\n",
      "    :param entry_points: A collection of entrypoints to search for plugins\n",
      "    :type entry_points: Generator[setuptools.EntryPoint, None, None]\n",
      "    :param airflow_plugins: A collection of existing airflow plugins to\n",
      "        ensure we don't load duplicates\n",
      "    :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]\n",
      "    :rtype: list[airflow.plugins_manager.AirflowPlugin]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check whether a potential object is a subclass of\n",
      "    the AirflowPlugin class.\n",
      "\n",
      "    :param plugin_obj: potential subclass of AirflowPlugin\n",
      "    :param existing_plugins: Existing list of AirflowPlugin subclasses\n",
      "    :return: Whether or not the obj is a valid subclass of\n",
      "        AirflowPlugin\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets tasks instances to skipped from the same dag run.\n",
      "\n",
      "        :param dag_run: the DagRun for which to set the tasks to skipped\n",
      "        :param execution_date: execution_date\n",
      "        :param tasks: tasks to skip (not task_ids)\n",
      "        :param session: db session to use\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a AzureDLFileSystem object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if a file exists on Azure Data Lake.\n",
      "\n",
      "        :param file_path: Path and name of the file.\n",
      "        :type file_path: str\n",
      "        :return: True if the file exists, False otherwise.\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a file to Azure Data Lake.\n",
      "\n",
      "        :param local_path: local path. Can be single file, directory (in which case,\n",
      "            upload recursively) or glob pattern. Recursive glob patterns using `**`\n",
      "            are not supported.\n",
      "        :type local_path: str\n",
      "        :param remote_path: Remote path to upload to; if multiple files, this is the\n",
      "            directory root to write within.\n",
      "        :type remote_path: str\n",
      "        :param nthreads: Number of threads to use. If None, uses the number of cores.\n",
      "        :type nthreads: int\n",
      "        :param overwrite: Whether to forcibly overwrite existing files/directories.\n",
      "            If False and remote path is a directory, will quit regardless if any files\n",
      "            would be overwritten or not. If True, only matching filenames are actually\n",
      "            overwritten.\n",
      "        :type overwrite: bool\n",
      "        :param buffersize: int [2**22]\n",
      "            Number of bytes for internal buffer. This block cannot be bigger than\n",
      "            a chunk and cannot be smaller than a block.\n",
      "        :type buffersize: int\n",
      "        :param blocksize: int [2**22]\n",
      "            Number of bytes for a block. Within each chunk, we write a smaller\n",
      "            block for each API call. This block cannot be bigger than a chunk.\n",
      "        :type blocksize: int\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List files in Azure Data Lake Storage\n",
      "\n",
      "        :param path: full path/globstring to use to list files in ADLS\n",
      "        :type path: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run Presto Query on Athena\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncompress gz and bz2 files\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries MSSQL and returns a cursor of results.\n",
      "\n",
      "        :return: mssql cursor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decorates function to execute function at the same time submitting action_logging\n",
      "    but in CLI context. It will call action logger callbacks twice,\n",
      "    one for pre-execution and the other one for post-execution.\n",
      "\n",
      "    Action logger will be called with below keyword parameters:\n",
      "        sub_command : name of sub-command\n",
      "        start_datetime : start datetime instance by utc\n",
      "        end_datetime : end datetime instance by utc\n",
      "        full_command : full command line arguments\n",
      "        user : current user\n",
      "        log : airflow.models.log.Log ORM instance\n",
      "        dag_id : dag id (optional)\n",
      "        task_id : task_id (optional)\n",
      "        execution_date : execution date (optional)\n",
      "        error : exception instance if there's an exception\n",
      "\n",
      "    :param f: function instance\n",
      "    :return: wrapped function\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Builds metrics dict from function args\n",
      "    It assumes that function arguments is from airflow.bin.cli module's function\n",
      "    and has Namespace instance where it optionally contains \"dag_id\", \"task_id\",\n",
      "    and \"execution_date\".\n",
      "\n",
      "    :param func_name: name of function\n",
      "    :param namespace: Namespace instance from argparse\n",
      "    :return: dict with metrics\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create the specified cgroup.\n",
      "\n",
      "        :param path: The path of the cgroup to create.\n",
      "        E.g. cpu/mygroup/mysubgroup\n",
      "        :return: the Node associated with the created cgroup.\n",
      "        :rtype: cgroupspy.nodes.Node\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete the specified cgroup.\n",
      "\n",
      "        :param path: The path of the cgroup to delete.\n",
      "        E.g. cpu/mygroup/mysubgroup\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The purpose of this function is to be robust to improper connections\n",
      "        settings provided by users, specifically in the host field.\n",
      "\n",
      "        For example -- when users supply ``https://xx.cloud.databricks.com`` as the\n",
      "        host, we must strip out the protocol to get the host.::\n",
      "\n",
      "            h = DatabricksHook()\n",
      "            assert h._parse_host('https://xx.cloud.databricks.com') == \\\n",
      "                'xx.cloud.databricks.com'\n",
      "\n",
      "        In the case where users supply the correct ``xx.cloud.databricks.com`` as the\n",
      "        host, this function is a no-op.::\n",
      "\n",
      "            assert h._parse_host('xx.cloud.databricks.com') == 'xx.cloud.databricks.com'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Utility function to perform an API call with retries\n",
      "\n",
      "        :param endpoint_info: Tuple of method and endpoint\n",
      "        :type endpoint_info: tuple[string, string]\n",
      "        :param json: Parameters for this API call.\n",
      "        :type json: dict\n",
      "        :return: If the api call returns a OK status code,\n",
      "            this function returns the response in JSON. Otherwise,\n",
      "            we throw an AirflowException.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sign into Salesforce, only if we are not already signed in.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Make a query to Salesforce.\n",
      "\n",
      "        :param query: The query to make to Salesforce.\n",
      "        :type query: str\n",
      "        :return: The query result.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the description of an object from Salesforce.\n",
      "        This description is the object's schema and\n",
      "        some extra metadata that Salesforce stores for each object.\n",
      "\n",
      "        :param obj: The name of the Salesforce object that we are getting a description of.\n",
      "        :type obj: str\n",
      "        :return: the description of the Salesforce object.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a list of all available fields for an object.\n",
      "\n",
      "        :param obj: The name of the Salesforce object that we are getting a description of.\n",
      "        :type obj: str\n",
      "        :return: the names of the fields.\n",
      "        :rtype: list of str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get all instances of the `object` from Salesforce.\n",
      "        For each model, only get the fields specified in fields.\n",
      "\n",
      "        All we really do underneath the hood is run:\n",
      "            SELECT <fields> FROM <obj>;\n",
      "\n",
      "        :param obj: The object name to get from Salesforce.\n",
      "        :type obj: str\n",
      "        :param fields: The fields to get from the object.\n",
      "        :type fields: iterable\n",
      "        :return: all instances of the object from Salesforce.\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a column of a dataframe to UNIX timestamps if applicable\n",
      "\n",
      "        :param column: A Series object representing a column of a dataframe.\n",
      "        :type column: pd.Series\n",
      "        :return: a new series that maintains the same index as the original\n",
      "        :rtype: pd.Series\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Write query results to file.\n",
      "\n",
      "        Acceptable formats are:\n",
      "            - csv:\n",
      "                comma-separated-values file. This is the default format.\n",
      "            - json:\n",
      "                JSON array. Each element in the array is a different row.\n",
      "            - ndjson:\n",
      "                JSON array but each element is new-line delimited instead of comma delimited like in `json`\n",
      "\n",
      "        This requires a significant amount of cleanup.\n",
      "        Pandas doesn't handle output to CSV and json in a uniform way.\n",
      "        This is especially painful for datetime types.\n",
      "        Pandas wants to write them as strings in CSV, but as millisecond Unix timestamps.\n",
      "\n",
      "        By default, this function will try and leave all values as they are represented in Salesforce.\n",
      "        You use the `coerce_to_timestamp` flag to force all datetimes to become Unix timestamps (UTC).\n",
      "        This is can be greatly beneficial as it will make all of your datetime fields look the same,\n",
      "        and makes it easier to work with in other database environments\n",
      "\n",
      "        :param query_results: the results from a SQL query\n",
      "        :type query_results: list of dict\n",
      "        :param filename: the name of the file where the data should be dumped to\n",
      "        :type filename: str\n",
      "        :param fmt: the format you want the output in. Default:  'csv'\n",
      "        :type fmt: str\n",
      "        :param coerce_to_timestamp: True if you want all datetime fields to be converted into Unix timestamps.\n",
      "            False if you want them to be left in the same format as they were in Salesforce.\n",
      "            Leaving the value as False will result in datetimes being strings. Default: False\n",
      "        :type coerce_to_timestamp: bool\n",
      "        :param record_time_added: True if you want to add a Unix timestamp field\n",
      "            to the resulting data that marks when the data was fetched from Salesforce. Default: False\n",
      "        :type record_time_added: bool\n",
      "        :return: the dataframe that gets written to the file.\n",
      "        :rtype: pd.Dataframe\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetches PyMongo Client\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetches a mongo collection object for querying.\n",
      "\n",
      "        Uses connection schema as DB unless specified.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Replaces many documents in a mongo collection.\n",
      "\n",
      "        Uses bulk_write with multiple ReplaceOne operations\n",
      "        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.bulk_write\n",
      "\n",
      "        .. note::\n",
      "            If no ``filter_docs``are given, it is assumed that all\n",
      "            replacement documents contain the ``_id`` field which are then\n",
      "            used as filters.\n",
      "\n",
      "        :param mongo_collection: The name of the collection to update.\n",
      "        :type mongo_collection: str\n",
      "        :param docs: The new documents.\n",
      "        :type docs: list[dict]\n",
      "        :param filter_docs: A list of queries that match the documents to replace.\n",
      "            Can be omitted; then the _id fields from docs will be used.\n",
      "        :type filter_docs: list[dict]\n",
      "        :param mongo_db: The name of the database to use.\n",
      "            Can be omitted; then the database from the connection string is used.\n",
      "        :type mongo_db: str\n",
      "        :param upsert: If ``True``, perform an insert if no documents\n",
      "            match the filters for the replace operation.\n",
      "        :type upsert: bool\n",
      "        :param collation: An instance of\n",
      "            :class:`~pymongo.collation.Collation`. This option is only\n",
      "            supported on MongoDB 3.4 and above.\n",
      "        :type collation: pymongo.collation.Collation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks the mail folder for mails containing attachments with the given name.\n",
      "\n",
      "        :param name: The name of the attachment that will be searched for.\n",
      "        :type name: str\n",
      "        :param mail_folder: The mail folder where to look at.\n",
      "        :type mail_folder: str\n",
      "        :param check_regex: Checks the name for a regular expression.\n",
      "        :type check_regex: bool\n",
      "        :returns: True if there is an attachment with the given name and False if not.\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves mail's attachments in the mail folder by its name.\n",
      "\n",
      "        :param name: The name of the attachment that will be downloaded.\n",
      "        :type name: str\n",
      "        :param mail_folder: The mail folder where to look at.\n",
      "        :type mail_folder: str\n",
      "        :param check_regex: Checks the name for a regular expression.\n",
      "        :type check_regex: bool\n",
      "        :param latest_only: If set to True it will only retrieve\n",
      "                            the first matched attachment.\n",
      "        :type latest_only: bool\n",
      "        :param not_found_mode: Specify what should happen if no attachment has been found.\n",
      "                               Supported values are 'raise', 'warn' and 'ignore'.\n",
      "                               If it is set to 'raise' it will raise an exception,\n",
      "                               if set to 'warn' it will only print a warning and\n",
      "                               if set to 'ignore' it won't notify you at all.\n",
      "        :type not_found_mode: str\n",
      "        :returns: a list of tuple each containing the attachment filename and its payload.\n",
      "        :rtype: a list of tuple\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Downloads mail's attachments in the mail folder by its name to the local directory.\n",
      "\n",
      "        :param name: The name of the attachment that will be downloaded.\n",
      "        :type name: str\n",
      "        :param local_output_directory: The output directory on the local machine\n",
      "                                       where the files will be downloaded to.\n",
      "        :type local_output_directory: str\n",
      "        :param mail_folder: The mail folder where to look at.\n",
      "        :type mail_folder: str\n",
      "        :param check_regex: Checks the name for a regular expression.\n",
      "        :type check_regex: bool\n",
      "        :param latest_only: If set to True it will only download\n",
      "                            the first matched attachment.\n",
      "        :type latest_only: bool\n",
      "        :param not_found_mode: Specify what should happen if no attachment has been found.\n",
      "                               Supported values are 'raise', 'warn' and 'ignore'.\n",
      "                               If it is set to 'raise' it will raise an exception,\n",
      "                               if set to 'warn' it will only print a warning and\n",
      "                               if set to 'ignore' it won't notify you at all.\n",
      "        :type not_found_mode: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets all attachments by name for the mail.\n",
      "\n",
      "        :param name: The name of the attachment to look for.\n",
      "        :type name: str\n",
      "        :param check_regex: Checks the name for a regular expression.\n",
      "        :type check_regex: bool\n",
      "        :param find_first: If set to True it will only find the first match and then quit.\n",
      "        :type find_first: bool\n",
      "        :returns: a list of tuples each containing name and payload\n",
      "                  where the attachments name matches the given name.\n",
      "        :rtype: list of tuple\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the file including name and payload.\n",
      "\n",
      "        :returns: the part's name and payload.\n",
      "        :rtype: tuple\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Write batch records to Kinesis Firehose\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Determines whether a task is ready to be rescheduled. Only tasks in\n",
      "        NONE state with at least one row in task_reschedule table are\n",
      "        handled by this dependency class, otherwise this dependency is\n",
      "        considered as passed. This dependency fails if the latest reschedule\n",
      "        request's reschedule date is still in future.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send email using backend specified in EMAIL_BACKEND.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send an email with html content\n",
      "\n",
      "    >>> send_email('test@example.com', 'foo', '<b>Foo</b> bar', ['/dev/null'], dryrun=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processes DateTimes from the DB making sure it is always\n",
      "        returning UTC. Not using timezone.convert_to_utc as that\n",
      "        converts to configured TIMEZONE while the DB might be\n",
      "        running with some other setting. We assume UTC datetimes\n",
      "        in the database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if a blob exists on Azure Blob Storage.\n",
      "\n",
      "        :param container_name: Name of the container.\n",
      "        :type container_name: str\n",
      "        :param blob_name: Name of the blob.\n",
      "        :type blob_name: str\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `BlockBlobService.exists()` takes.\n",
      "        :type kwargs: object\n",
      "        :return: True if the blob exists, False otherwise.\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if a prefix exists on Azure Blob storage.\n",
      "\n",
      "        :param container_name: Name of the container.\n",
      "        :type container_name: str\n",
      "        :param prefix: Prefix of the blob.\n",
      "        :type prefix: str\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `BlockBlobService.list_blobs()` takes.\n",
      "        :type kwargs: object\n",
      "        :return: True if blobs matching the prefix exist, False otherwise.\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a string to Azure Blob Storage.\n",
      "\n",
      "        :param string_data: String to load.\n",
      "        :type string_data: str\n",
      "        :param container_name: Name of the container.\n",
      "        :type container_name: str\n",
      "        :param blob_name: Name of the blob.\n",
      "        :type blob_name: str\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `BlockBlobService.create_blob_from_text()` takes.\n",
      "        :type kwargs: object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read a file from Azure Blob Storage and return as a string.\n",
      "\n",
      "        :param container_name: Name of the container.\n",
      "        :type container_name: str\n",
      "        :param blob_name: Name of the blob.\n",
      "        :type blob_name: str\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `BlockBlobService.create_blob_from_path()` takes.\n",
      "        :type kwargs: object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a file from Azure Blob Storage.\n",
      "\n",
      "        :param container_name: Name of the container.\n",
      "        :type container_name: str\n",
      "        :param blob_name: Name of the blob.\n",
      "        :type blob_name: str\n",
      "        :param is_prefix: If blob_name is a prefix, delete all matching files\n",
      "        :type is_prefix: bool\n",
      "        :param ignore_if_missing: if True, then return success even if the\n",
      "            blob does not exist.\n",
      "        :type ignore_if_missing: bool\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `BlockBlobService.create_blob_from_path()` takes.\n",
      "        :type kwargs: object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BACKPORT FROM PYTHON3 FTPLIB.\n",
      "\n",
      "    List a directory in a standardized format by using MLSD\n",
      "    command (RFC-3659). If path is omitted the current directory\n",
      "    is assumed. \"facts\" is a list of strings representing the type\n",
      "    of information desired (e.g. [\"type\", \"size\", \"perm\"]).\n",
      "\n",
      "    Return a generator object yielding a tuple of two elements\n",
      "    for every file found in path.\n",
      "    First element is the file name, the second one is a dictionary\n",
      "    including a variable number of \"facts\" depending on the server\n",
      "    and whether \"facts\" argument has been provided.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a FTP connection object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a list of files on the remote system.\n",
      "\n",
      "        :param path: full path to the remote directory to list\n",
      "        :type path: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Transfers the remote file to a local location.\n",
      "\n",
      "        If local_full_path_or_buffer is a string path, the file will be put\n",
      "        at that location; if it is a file-like buffer, the file will\n",
      "        be written to the buffer but not closed.\n",
      "\n",
      "        :param remote_full_path: full path to the remote file\n",
      "        :type remote_full_path: str\n",
      "        :param local_full_path_or_buffer: full path to the local file or a\n",
      "            file-like buffer\n",
      "        :type local_full_path_or_buffer: str or file-like buffer\n",
      "        :param callback: callback which is called each time a block of data\n",
      "            is read. if you do not use a callback, these blocks will be written\n",
      "            to the file or buffer passed in. if you do pass in a callback, note\n",
      "            that writing to a file or buffer will need to be handled inside the\n",
      "            callback.\n",
      "            [default: output_handle.write()]\n",
      "        :type callback: callable\n",
      "\n",
      "        :Example::\n",
      "\n",
      "            hook = FTPHook(ftp_conn_id='my_conn')\n",
      "\n",
      "            remote_path = '/path/to/remote/file'\n",
      "            local_path = '/path/to/local/file'\n",
      "\n",
      "            # with a custom callback (in this case displaying progress on each read)\n",
      "            def print_progress(percent_progress):\n",
      "                self.log.info('Percent Downloaded: %s%%' % percent_progress)\n",
      "\n",
      "            total_downloaded = 0\n",
      "            total_file_size = hook.get_size(remote_path)\n",
      "            output_handle = open(local_path, 'wb')\n",
      "            def write_to_file_with_progress(data):\n",
      "                total_downloaded += len(data)\n",
      "                output_handle.write(data)\n",
      "                percent_progress = (total_downloaded / total_file_size) * 100\n",
      "                print_progress(percent_progress)\n",
      "            hook.retrieve_file(remote_path, None, callback=write_to_file_with_progress)\n",
      "\n",
      "            # without a custom callback data is written to the local_path\n",
      "            hook.retrieve_file(remote_path, local_path)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Transfers a local file to the remote location.\n",
      "\n",
      "        If local_full_path_or_buffer is a string path, the file will be read\n",
      "        from that location; if it is a file-like buffer, the file will\n",
      "        be read from the buffer but not closed.\n",
      "\n",
      "        :param remote_full_path: full path to the remote file\n",
      "        :type remote_full_path: str\n",
      "        :param local_full_path_or_buffer: full path to the local file or a\n",
      "            file-like buffer\n",
      "        :type local_full_path_or_buffer: str or file-like buffer\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a datetime object representing the last time the file was modified\n",
      "\n",
      "        :param path: remote file path\n",
      "        :type path: string\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Call the DiscordWebhookHook to post message\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the FileService object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if a directory exists on Azure File Share.\n",
      "\n",
      "        :param share_name: Name of the share.\n",
      "        :type share_name: str\n",
      "        :param directory_name: Name of the directory.\n",
      "        :type directory_name: str\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `FileService.exists()` takes.\n",
      "        :type kwargs: object\n",
      "        :return: True if the file exists, False otherwise.\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if a file exists on Azure File Share.\n",
      "\n",
      "        :param share_name: Name of the share.\n",
      "        :type share_name: str\n",
      "        :param directory_name: Name of the directory.\n",
      "        :type directory_name: str\n",
      "        :param file_name: Name of the file.\n",
      "        :type file_name: str\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `FileService.exists()` takes.\n",
      "        :type kwargs: object\n",
      "        :return: True if the file exists, False otherwise.\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the list of directories and files stored on a Azure File Share.\n",
      "\n",
      "        :param share_name: Name of the share.\n",
      "        :type share_name: str\n",
      "        :param directory_name: Name of the directory.\n",
      "        :type directory_name: str\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `FileService.list_directories_and_files()` takes.\n",
      "        :type kwargs: object\n",
      "        :return: A list of files and directories\n",
      "        :rtype: list\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a new directory on a Azure File Share.\n",
      "\n",
      "        :param share_name: Name of the share.\n",
      "        :type share_name: str\n",
      "        :param directory_name: Name of the directory.\n",
      "        :type directory_name: str\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `FileService.create_directory()` takes.\n",
      "        :type kwargs: object\n",
      "        :return: A list of files and directories\n",
      "        :rtype: list\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a file to Azure File Share.\n",
      "\n",
      "        :param file_path: Path to the file to load.\n",
      "        :type file_path: str\n",
      "        :param share_name: Name of the share.\n",
      "        :type share_name: str\n",
      "        :param directory_name: Name of the directory.\n",
      "        :type directory_name: str\n",
      "        :param file_name: Name of the file.\n",
      "        :type file_name: str\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `FileService.create_file_from_path()` takes.\n",
      "        :type kwargs: object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a string to Azure File Share.\n",
      "\n",
      "        :param string_data: String to load.\n",
      "        :type string_data: str\n",
      "        :param share_name: Name of the share.\n",
      "        :type share_name: str\n",
      "        :param directory_name: Name of the directory.\n",
      "        :type directory_name: str\n",
      "        :param file_name: Name of the file.\n",
      "        :type file_name: str\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `FileService.create_file_from_text()` takes.\n",
      "        :type kwargs: object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a stream to Azure File Share.\n",
      "\n",
      "        :param stream: Opened file/stream to upload as the file content.\n",
      "        :type stream: file-like\n",
      "        :param share_name: Name of the share.\n",
      "        :type share_name: str\n",
      "        :param directory_name: Name of the directory.\n",
      "        :type directory_name: str\n",
      "        :param file_name: Name of the file.\n",
      "        :type file_name: str\n",
      "        :param count: Size of the stream in bytes\n",
      "        :type count: int\n",
      "        :param kwargs: Optional keyword arguments that\n",
      "            `FileService.create_file_from_stream()` takes.\n",
      "        :type kwargs: object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a Google Cloud Storage service object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Copies an object from a bucket to another, with renaming if requested.\n",
      "\n",
      "        destination_bucket or destination_object can be omitted, in which case\n",
      "        source bucket/object is used, but not both.\n",
      "\n",
      "        :param source_bucket: The bucket of the object to copy from.\n",
      "        :type source_bucket: str\n",
      "        :param source_object: The object to copy.\n",
      "        :type source_object: str\n",
      "        :param destination_bucket: The destination of the object to copied to.\n",
      "            Can be omitted; then the same bucket is used.\n",
      "        :type destination_bucket: str\n",
      "        :param destination_object: The (renamed) path of the object if given.\n",
      "            Can be omitted; then the same name is used.\n",
      "        :type destination_object: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a file from Google Cloud Storage.\n",
      "\n",
      "        :param bucket_name: The bucket to fetch from.\n",
      "        :type bucket_name: str\n",
      "        :param object_name: The object to fetch.\n",
      "        :type object_name: str\n",
      "        :param filename: If set, a local file path where the file should be written to.\n",
      "        :type filename: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uploads a local file to Google Cloud Storage.\n",
      "\n",
      "        :param bucket_name: The bucket to upload to.\n",
      "        :type bucket_name: str\n",
      "        :param object_name: The object name to set when uploading the local file.\n",
      "        :type object_name: str\n",
      "        :param filename: The local file path to the file to be uploaded.\n",
      "        :type filename: str\n",
      "        :param mime_type: The MIME type to set when uploading the file.\n",
      "        :type mime_type: str\n",
      "        :param gzip: Option to compress file for upload\n",
      "        :type gzip: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks for the existence of a file in Google Cloud Storage.\n",
      "\n",
      "        :param bucket_name: The Google cloud storage bucket where the object is.\n",
      "        :type bucket_name: str\n",
      "        :param object_name: The name of the blob_name to check in the Google cloud\n",
      "            storage bucket.\n",
      "        :type object_name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if an blob_name is updated in Google Cloud Storage.\n",
      "\n",
      "        :param bucket_name: The Google cloud storage bucket where the object is.\n",
      "        :type bucket_name: str\n",
      "        :param object_name: The name of the object to check in the Google cloud\n",
      "            storage bucket.\n",
      "        :type object_name: str\n",
      "        :param ts: The timestamp to check against.\n",
      "        :type ts: datetime.datetime\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an object from the bucket.\n",
      "\n",
      "        :param bucket_name: name of the bucket, where the object resides\n",
      "        :type bucket_name: str\n",
      "        :param object_name: name of the object to delete\n",
      "        :type object_name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List all objects from the bucket with the give string prefix in name\n",
      "\n",
      "        :param bucket_name: bucket name\n",
      "        :type bucket_name: str\n",
      "        :param versions: if true, list all versions of the objects\n",
      "        :type versions: bool\n",
      "        :param max_results: max count of items to return in a single page of responses\n",
      "        :type max_results: int\n",
      "        :param prefix: prefix string which filters objects whose name begin with\n",
      "            this prefix\n",
      "        :type prefix: str\n",
      "        :param delimiter: filters objects based on the delimiter (for e.g '.csv')\n",
      "        :type delimiter: str\n",
      "        :return: a stream of object names matching the filtering criteria\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the size of a file in Google Cloud Storage.\n",
      "\n",
      "        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n",
      "        :type bucket_name: str\n",
      "        :param object_name: The name of the object to check in the Google\n",
      "            cloud storage bucket_name.\n",
      "        :type object_name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the CRC32c checksum of an object in Google Cloud Storage.\n",
      "\n",
      "        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n",
      "        :type bucket_name: str\n",
      "        :param object_name: The name of the object to check in the Google cloud\n",
      "            storage bucket_name.\n",
      "        :type object_name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the MD5 hash of an object in Google Cloud Storage.\n",
      "\n",
      "        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n",
      "        :type bucket_name: str\n",
      "        :param object_name: The name of the object to check in the Google cloud\n",
      "            storage bucket_name.\n",
      "        :type object_name: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new bucket. Google Cloud Storage uses a flat namespace, so\n",
      "        you can't create a bucket with a name that is already in use.\n",
      "\n",
      "        .. seealso::\n",
      "            For more information, see Bucket Naming Guidelines:\n",
      "            https://cloud.google.com/storage/docs/bucketnaming.html#requirements\n",
      "\n",
      "        :param bucket_name: The name of the bucket.\n",
      "        :type bucket_name: str\n",
      "        :param resource: An optional dict with parameters for creating the bucket.\n",
      "            For information on available parameters, see Cloud Storage API doc:\n",
      "            https://cloud.google.com/storage/docs/json_api/v1/buckets/insert\n",
      "        :type resource: dict\n",
      "        :param storage_class: This defines how objects in the bucket are stored\n",
      "            and determines the SLA and the cost of storage. Values include\n",
      "\n",
      "            - ``MULTI_REGIONAL``\n",
      "            - ``REGIONAL``\n",
      "            - ``STANDARD``\n",
      "            - ``NEARLINE``\n",
      "            - ``COLDLINE``.\n",
      "\n",
      "            If this value is not specified when the bucket is\n",
      "            created, it will default to STANDARD.\n",
      "        :type storage_class: str\n",
      "        :param location: The location of the bucket.\n",
      "            Object data for objects in the bucket resides in physical storage\n",
      "            within this region. Defaults to US.\n",
      "\n",
      "            .. seealso::\n",
      "                https://developers.google.com/storage/docs/bucket-locations\n",
      "\n",
      "        :type location: str\n",
      "        :param project_id: The ID of the GCP Project.\n",
      "        :type project_id: str\n",
      "        :param labels: User-provided labels, in key/value pairs.\n",
      "        :type labels: dict\n",
      "        :return: If successful, it returns the ``id`` of the bucket.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Composes a list of existing object into a new object in the same storage bucket_name\n",
      "\n",
      "        Currently it only supports up to 32 objects that can be concatenated\n",
      "        in a single operation\n",
      "\n",
      "        https://cloud.google.com/storage/docs/json_api/v1/objects/compose\n",
      "\n",
      "        :param bucket_name: The name of the bucket containing the source objects.\n",
      "            This is also the same bucket to store the composed destination object.\n",
      "        :type bucket_name: str\n",
      "        :param source_objects: The list of source objects that will be composed\n",
      "            into a single object.\n",
      "        :type source_objects: list\n",
      "        :param destination_object: The path of the object if given.\n",
      "        :type destination_object: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns true if training job's secondary status message has changed.\n",
      "\n",
      "    :param current_job_description: Current job description, returned from DescribeTrainingJob call.\n",
      "    :type current_job_description: dict\n",
      "    :param prev_job_description: Previous job description, returned from DescribeTrainingJob call.\n",
      "    :type prev_job_description: dict\n",
      "\n",
      "    :return: Whether the secondary status message of a training job changed or not.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a string contains start time and the secondary training job status message.\n",
      "\n",
      "    :param job_description: Returned response from DescribeTrainingJob call\n",
      "    :type job_description: dict\n",
      "    :param prev_description: Previous job description from DescribeTrainingJob call\n",
      "    :type prev_description: dict\n",
      "\n",
      "    :return: Job status string to be printed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tar the local file or directory and upload to s3\n",
      "\n",
      "        :param path: local file or directory\n",
      "        :type path: str\n",
      "        :param key: s3 key\n",
      "        :type key: str\n",
      "        :param bucket: s3 bucket\n",
      "        :type bucket: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Extract the S3 operations from the configuration and execute them.\n",
      "\n",
      "        :param config: config of SageMaker operation\n",
      "        :type config: dict\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if an S3 URL exists\n",
      "\n",
      "        :param s3url: S3 url\n",
      "        :type s3url: str\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Establish an AWS connection for retrieving logs during training\n",
      "\n",
      "        :rtype: CloudWatchLogs.Client\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a training job\n",
      "\n",
      "        :param config: the config for training\n",
      "        :type config: dict\n",
      "        :param wait_for_completion: if the program should keep running until job finishes\n",
      "        :type wait_for_completion: bool\n",
      "        :param check_interval: the time interval in seconds which the operator\n",
      "            will check the status of any SageMaker job\n",
      "        :type check_interval: int\n",
      "        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n",
      "            SageMaker jobs that run longer than this will fail. Setting this to\n",
      "            None implies no timeout for any SageMaker job.\n",
      "        :type max_ingestion_time: int\n",
      "        :return: A response to training job creation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a tuning job\n",
      "\n",
      "        :param config: the config for tuning\n",
      "        :type config: dict\n",
      "        :param wait_for_completion: if the program should keep running until job finishes\n",
      "        :type wait_for_completion: bool\n",
      "        :param check_interval: the time interval in seconds which the operator\n",
      "            will check the status of any SageMaker job\n",
      "        :type check_interval: int\n",
      "        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n",
      "            SageMaker jobs that run longer than this will fail. Setting this to\n",
      "            None implies no timeout for any SageMaker job.\n",
      "        :type max_ingestion_time: int\n",
      "        :return: A response to tuning job creation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a transform job\n",
      "\n",
      "        :param config: the config for transform job\n",
      "        :type config: dict\n",
      "        :param wait_for_completion: if the program should keep running until job finishes\n",
      "        :type wait_for_completion: bool\n",
      "        :param check_interval: the time interval in seconds which the operator\n",
      "            will check the status of any SageMaker job\n",
      "        :type check_interval: int\n",
      "        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n",
      "            SageMaker jobs that run longer than this will fail. Setting this to\n",
      "            None implies no timeout for any SageMaker job.\n",
      "        :type max_ingestion_time: int\n",
      "        :return: A response to transform job creation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create an endpoint\n",
      "\n",
      "        :param config: the config for endpoint\n",
      "        :type config: dict\n",
      "        :param wait_for_completion: if the program should keep running until job finishes\n",
      "        :type wait_for_completion: bool\n",
      "        :param check_interval: the time interval in seconds which the operator\n",
      "            will check the status of any SageMaker job\n",
      "        :type check_interval: int\n",
      "        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n",
      "            SageMaker jobs that run longer than this will fail. Setting this to\n",
      "            None implies no timeout for any SageMaker job.\n",
      "        :type max_ingestion_time: int\n",
      "        :return: A response to endpoint creation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the training job info associated with job_name and print CloudWatch logs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check status of a SageMaker job\n",
      "\n",
      "        :param job_name: name of the job to check status\n",
      "        :type job_name: str\n",
      "        :param key: the key of the response dict\n",
      "            that points to the state\n",
      "        :type key: str\n",
      "        :param describe_function: the function used to retrieve the status\n",
      "        :type describe_function: python callable\n",
      "        :param args: the arguments for the function\n",
      "        :param check_interval: the time interval in seconds which the operator\n",
      "            will check the status of any SageMaker job\n",
      "        :type check_interval: int\n",
      "        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n",
      "            SageMaker jobs that run longer than this will fail. Setting this to\n",
      "            None implies no timeout for any SageMaker job.\n",
      "        :type max_ingestion_time: int\n",
      "        :param non_terminal_states: the set of nonterminal states\n",
      "        :type non_terminal_states: set\n",
      "        :return: response of describe call after job is done\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Display the logs for a given training job, optionally tailing them until the\n",
      "        job is complete.\n",
      "\n",
      "        :param job_name: name of the training job to check status and display logs for\n",
      "        :type job_name: str\n",
      "        :param non_terminal_states: the set of non_terminal states\n",
      "        :type non_terminal_states: set\n",
      "        :param failed_states: the set of failed states\n",
      "        :type failed_states: set\n",
      "        :param wait_for_completion: Whether to keep looking for new log entries\n",
      "            until the job completes\n",
      "        :type wait_for_completion: bool\n",
      "        :param check_interval: The interval in seconds between polling for new log entries and job completion\n",
      "        :type check_interval: int\n",
      "        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n",
      "            SageMaker jobs that run longer than this will fail. Setting this to\n",
      "            None implies no timeout for any SageMaker job.\n",
      "        :type max_ingestion_time: int\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute the python dataflow job.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run migrations in 'offline' mode.\n",
      "\n",
      "    This configures the context with just a URL\n",
      "    and not an Engine, though an Engine is acceptable\n",
      "    here as well.  By skipping the Engine creation\n",
      "    we don't even need a DBAPI to be available.\n",
      "\n",
      "    Calls to context.execute() here emit the given string to the\n",
      "    script output.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run migrations in 'online' mode.\n",
      "\n",
      "    In this scenario we need to create an Engine\n",
      "    and associate a connection with the context.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified Cloud Bigtable instance.\n",
      "        Raises google.api_core.exceptions.NotFound if the Cloud Bigtable instance does\n",
      "        not exist.\n",
      "\n",
      "        :param project_id: Optional, Google Cloud Platform project ID where the\n",
      "            BigTable exists. If set to None or missing,\n",
      "            the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :param instance_id: The ID of the Cloud Bigtable instance.\n",
      "        :type instance_id: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates new instance.\n",
      "\n",
      "        :type instance_id: str\n",
      "        :param instance_id: The ID for the new instance.\n",
      "        :type main_cluster_id: str\n",
      "        :param main_cluster_id: The ID for main cluster for the new instance.\n",
      "        :type main_cluster_zone: str\n",
      "        :param main_cluster_zone: The zone for main cluster.\n",
      "            See https://cloud.google.com/bigtable/docs/locations for more details.\n",
      "        :type project_id: str\n",
      "        :param project_id: Optional, Google Cloud Platform project ID where the\n",
      "            BigTable exists. If set to None or missing,\n",
      "            the default project_id from the GCP connection is used.\n",
      "        :type replica_cluster_id: str\n",
      "        :param replica_cluster_id: (optional) The ID for replica cluster for the new\n",
      "            instance.\n",
      "        :type replica_cluster_zone: str\n",
      "        :param replica_cluster_zone: (optional)  The zone for replica cluster.\n",
      "        :type instance_type: enums.Instance.Type\n",
      "        :param instance_type: (optional) The type of the instance.\n",
      "        :type instance_display_name: str\n",
      "        :param instance_display_name: (optional) Human-readable name of the instance.\n",
      "                Defaults to ``instance_id``.\n",
      "        :type instance_labels: dict\n",
      "        :param instance_labels: (optional) Dictionary of labels to associate with the\n",
      "            instance.\n",
      "        :type cluster_nodes: int\n",
      "        :param cluster_nodes: (optional) Number of nodes for cluster.\n",
      "        :type cluster_storage_type: enums.StorageType\n",
      "        :param cluster_storage_type: (optional) The type of storage.\n",
      "        :type timeout: int\n",
      "        :param timeout: (optional) timeout (in seconds) for instance creation.\n",
      "                        If None is not specified, Operator will wait indefinitely.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates the specified Cloud Bigtable table.\n",
      "        Raises ``google.api_core.exceptions.AlreadyExists`` if the table exists.\n",
      "\n",
      "        :type instance: Instance\n",
      "        :param instance: The Cloud Bigtable instance that owns the table.\n",
      "        :type table_id: str\n",
      "        :param table_id: The ID of the table to create in Cloud Bigtable.\n",
      "        :type initial_split_keys: list\n",
      "        :param initial_split_keys: (Optional) A list of row keys in bytes to use to\n",
      "            initially split the table.\n",
      "        :type column_families: dict\n",
      "        :param column_families: (Optional) A map of columns to create. The key is the\n",
      "            column_id str, and the value is a\n",
      "            :class:`google.cloud.bigtable.column_family.GarbageCollectionRule`.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified table in Cloud Bigtable.\n",
      "        Raises google.api_core.exceptions.NotFound if the table does not exist.\n",
      "\n",
      "        :type instance_id: str\n",
      "        :param instance_id: The ID of the Cloud Bigtable instance.\n",
      "        :type table_id: str\n",
      "        :param table_id: The ID of the table in Cloud Bigtable.\n",
      "        :type project_id: str\n",
      "        :param project_id: Optional, Google Cloud Platform project ID where the\n",
      "            BigTable exists. If set to None or missing,\n",
      "            the default project_id from the GCP connection is used.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates number of nodes in the specified Cloud Bigtable cluster.\n",
      "        Raises google.api_core.exceptions.NotFound if the cluster does not exist.\n",
      "\n",
      "        :type instance: Instance\n",
      "        :param instance: The Cloud Bigtable instance that owns the cluster.\n",
      "        :type cluster_id: str\n",
      "        :param cluster_id: The ID of the cluster.\n",
      "        :type nodes: int\n",
      "        :param nodes: The desired number of nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This function creates the command list from available information\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This function prepares a list of hiveconf params\n",
      "        from a dictionary of key value pairs.\n",
      "\n",
      "        :param d:\n",
      "        :type d: dict\n",
      "\n",
      "        >>> hh = HiveCliHook()\n",
      "        >>> hive_conf = {\"hive.exec.dynamic.partition\": \"true\",\n",
      "        ... \"hive.exec.dynamic.partition.mode\": \"nonstrict\"}\n",
      "        >>> hh._prepare_hiveconf(hive_conf)\n",
      "        [\"-hiveconf\", \"hive.exec.dynamic.partition=true\",\\\n",
      " \"-hiveconf\", \"hive.exec.dynamic.partition.mode=nonstrict\"]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a pandas DataFrame into hive.\n",
      "\n",
      "        Hive data types will be inferred if not passed but column names will\n",
      "        not be sanitized.\n",
      "\n",
      "        :param df: DataFrame to load into a Hive table\n",
      "        :type df: pandas.DataFrame\n",
      "        :param table: target Hive table, use dot notation to target a\n",
      "            specific database\n",
      "        :type table: str\n",
      "        :param field_dict: mapping from column name to hive data type.\n",
      "            Note that it must be OrderedDict so as to keep columns' order.\n",
      "        :type field_dict: collections.OrderedDict\n",
      "        :param delimiter: field delimiter in the file\n",
      "        :type delimiter: str\n",
      "        :param encoding: str encoding to use when writing DataFrame to file\n",
      "        :type encoding: str\n",
      "        :param pandas_kwargs: passed to DataFrame.to_csv\n",
      "        :type pandas_kwargs: dict\n",
      "        :param kwargs: passed to self.load_file\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a local file into Hive\n",
      "\n",
      "        Note that the table generated in Hive uses ``STORED AS textfile``\n",
      "        which isn't the most efficient serialization format. If a\n",
      "        large amount of data is loaded and/or if the tables gets\n",
      "        queried considerably, you may want to use this operator only to\n",
      "        stage the data into a temporary table before loading it into its\n",
      "        final destination using a ``HiveOperator``.\n",
      "\n",
      "        :param filepath: local filepath of the file to load\n",
      "        :type filepath: str\n",
      "        :param table: target Hive table, use dot notation to target a\n",
      "            specific database\n",
      "        :type table: str\n",
      "        :param delimiter: field delimiter in the file\n",
      "        :type delimiter: str\n",
      "        :param field_dict: A dictionary of the fields name in the file\n",
      "            as keys and their Hive types as values.\n",
      "            Note that it must be OrderedDict so as to keep columns' order.\n",
      "        :type field_dict: collections.OrderedDict\n",
      "        :param create: whether to create the table if it doesn't exist\n",
      "        :type create: bool\n",
      "        :param overwrite: whether to overwrite the data in table or partition\n",
      "        :type overwrite: bool\n",
      "        :param partition: target partition as a dict of partition columns\n",
      "            and values\n",
      "        :type partition: dict\n",
      "        :param recreate: whether to drop and recreate the table at every\n",
      "            execution\n",
      "        :type recreate: bool\n",
      "        :param tblproperties: TBLPROPERTIES of the hive table being created\n",
      "        :type tblproperties: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a Hive thrift client.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks whether a partition with a given name exists\n",
      "\n",
      "        :param schema: Name of hive schema (database) @table belongs to\n",
      "        :type schema: str\n",
      "        :param table: Name of hive table @partition belongs to\n",
      "        :type schema: str\n",
      "        :partition: Name of the partitions to check for (eg `a=b/c=d`)\n",
      "        :type schema: str\n",
      "        :rtype: bool\n",
      "\n",
      "        >>> hh = HiveMetastoreHook()\n",
      "        >>> t = 'static_babynames_partitioned'\n",
      "        >>> hh.check_for_named_partition('airflow', t, \"ds=2015-01-01\")\n",
      "        True\n",
      "        >>> hh.check_for_named_partition('airflow', t, \"ds=xxx\")\n",
      "        False\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if table exists\n",
      "\n",
      "        >>> hh = HiveMetastoreHook()\n",
      "        >>> hh.table_exists(db='airflow', table_name='static_babynames')\n",
      "        True\n",
      "        >>> hh.table_exists(db='airflow', table_name='does_not_exist')\n",
      "        False\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a Hive connection object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get results of the provided hql in target schema.\n",
      "\n",
      "        :param hql: hql to be executed.\n",
      "        :type hql: str or list\n",
      "        :param schema: target schema, default to 'default'.\n",
      "        :type schema: str\n",
      "        :param fetch_size: max size of result to fetch.\n",
      "        :type fetch_size: int\n",
      "        :param hive_conf: hive_conf to execute alone with the hql.\n",
      "        :type hive_conf: dict\n",
      "        :return: results of hql execution, dict with data (list of results) and header\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute hql in target schema and write results to a csv file.\n",
      "\n",
      "        :param hql: hql to be executed.\n",
      "        :type hql: str or list\n",
      "        :param csv_filepath: filepath of csv to write results into.\n",
      "        :type csv_filepath: str\n",
      "        :param schema: target schema, default to 'default'.\n",
      "        :type schema: str\n",
      "        :param delimiter: delimiter of the csv file, default to ','.\n",
      "        :type delimiter: str\n",
      "        :param lineterminator: lineterminator of the csv file.\n",
      "        :type lineterminator: str\n",
      "        :param output_header: header of the csv file, default to True.\n",
      "        :type output_header: bool\n",
      "        :param fetch_size: number of result rows to write into the csv file, default to 1000.\n",
      "        :type fetch_size: int\n",
      "        :param hive_conf: hive_conf to execute alone with the hql.\n",
      "        :type hive_conf: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a set of records from a Hive query.\n",
      "\n",
      "        :param hql: hql to be executed.\n",
      "        :type hql: str or list\n",
      "        :param schema: target schema, default to 'default'.\n",
      "        :type schema: str\n",
      "        :param hive_conf: hive_conf to execute alone with the hql.\n",
      "        :type hive_conf: dict\n",
      "        :return: result of hive execution\n",
      "        :rtype: list\n",
      "\n",
      "        >>> hh = HiveServer2Hook()\n",
      "        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\"\n",
      "        >>> len(hh.get_records(sql))\n",
      "        100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a pandas dataframe from a Hive query\n",
      "\n",
      "        :param hql: hql to be executed.\n",
      "        :type hql: str or list\n",
      "        :param schema: target schema, default to 'default'.\n",
      "        :type schema: str\n",
      "        :return: result of hql execution\n",
      "        :rtype: DataFrame\n",
      "\n",
      "        >>> hh = HiveServer2Hook()\n",
      "        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\"\n",
      "        >>> df = hh.get_pandas_df(sql)\n",
      "        >>> len(df.index)\n",
      "        100\n",
      "\n",
      "        :return: pandas.DateFrame\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Cloud Vision.\n",
      "\n",
      "        :return: Google Cloud Vision client object.\n",
      "        :rtype: google.cloud.vision_v1.ProductSearchClient\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get Dingding endpoint for sending message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send Dingding message\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method that binds parameters to a SQL query.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method that escapes parameters to a SQL query.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method that casts a BigQuery row to the appropriate data types.\n",
      "    This is useful because BigQuery returns all fields as strings.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "function to check expected type and raise\n",
      "    error if type is not correct\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a BigQuery PEP 249 connection object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a BigQuery service object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks for the existence of a table in Google BigQuery.\n",
      "\n",
      "        :param project_id: The Google cloud project in which to look for the\n",
      "            table. The connection supplied to the hook must provide access to\n",
      "            the specified project.\n",
      "        :type project_id: str\n",
      "        :param dataset_id: The name of the dataset in which to look for the\n",
      "            table.\n",
      "        :type dataset_id: str\n",
      "        :param table_id: The name of the table to check the existence of.\n",
      "        :type table_id: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new, empty table in the dataset.\n",
      "        To create a view, which is defined by a SQL query, parse a dictionary to 'view' kwarg\n",
      "\n",
      "        :param project_id: The project to create the table into.\n",
      "        :type project_id: str\n",
      "        :param dataset_id: The dataset to create the table into.\n",
      "        :type dataset_id: str\n",
      "        :param table_id: The Name of the table to be created.\n",
      "        :type table_id: str\n",
      "        :param schema_fields: If set, the schema field list as defined here:\n",
      "            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema\n",
      "        :type schema_fields: list\n",
      "        :param labels: a dictionary containing labels for the table, passed to BigQuery\n",
      "        :type labels: dict\n",
      "\n",
      "        **Example**: ::\n",
      "\n",
      "            schema_fields=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n",
      "                           {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}]\n",
      "\n",
      "        :param time_partitioning: configure optional time partitioning fields i.e.\n",
      "            partition by field, type and expiration as per API specifications.\n",
      "\n",
      "            .. seealso::\n",
      "                https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#timePartitioning\n",
      "        :type time_partitioning: dict\n",
      "        :param cluster_fields: [Optional] The fields used for clustering.\n",
      "            Must be specified with time_partitioning, data in the table will be first\n",
      "            partitioned and subsequently clustered.\n",
      "            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#clustering.fields\n",
      "        :type cluster_fields: list\n",
      "        :param view: [Optional] A dictionary containing definition for the view.\n",
      "            If set, it will create a view instead of a table:\n",
      "            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view\n",
      "        :type view: dict\n",
      "\n",
      "        **Example**: ::\n",
      "\n",
      "            view = {\n",
      "                \"query\": \"SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 1000\",\n",
      "                \"useLegacySql\": False\n",
      "            }\n",
      "\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Patch information in an existing table.\n",
      "        It only updates fileds that are provided in the request object.\n",
      "\n",
      "        Reference: https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/patch\n",
      "\n",
      "        :param dataset_id: The dataset containing the table to be patched.\n",
      "        :type dataset_id: str\n",
      "        :param table_id: The Name of the table to be patched.\n",
      "        :type table_id: str\n",
      "        :param project_id: The project containing the table to be patched.\n",
      "        :type project_id: str\n",
      "        :param description: [Optional] A user-friendly description of this table.\n",
      "        :type description: str\n",
      "        :param expiration_time: [Optional] The time when this table expires,\n",
      "            in milliseconds since the epoch.\n",
      "        :type expiration_time: int\n",
      "        :param external_data_configuration: [Optional] A dictionary containing\n",
      "            properties of a table stored outside of BigQuery.\n",
      "        :type external_data_configuration: dict\n",
      "        :param friendly_name: [Optional] A descriptive name for this table.\n",
      "        :type friendly_name: str\n",
      "        :param labels: [Optional] A dictionary containing labels associated with this table.\n",
      "        :type labels: dict\n",
      "        :param schema: [Optional] If set, the schema field list as defined here:\n",
      "            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema\n",
      "            The supported schema modifications and unsupported schema modification are listed here:\n",
      "            https://cloud.google.com/bigquery/docs/managing-table-schemas\n",
      "            **Example**: ::\n",
      "\n",
      "                schema=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n",
      "                               {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}]\n",
      "\n",
      "        :type schema: list\n",
      "        :param time_partitioning: [Optional] A dictionary containing time-based partitioning\n",
      "             definition for the table.\n",
      "        :type time_partitioning: dict\n",
      "        :param view: [Optional] A dictionary containing definition for the view.\n",
      "            If set, it will patch a view instead of a table:\n",
      "            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view\n",
      "            **Example**: ::\n",
      "\n",
      "                view = {\n",
      "                    \"query\": \"SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 500\",\n",
      "                    \"useLegacySql\": False\n",
      "                }\n",
      "\n",
      "        :type view: dict\n",
      "        :param require_partition_filter: [Optional] If true, queries over the this table require a\n",
      "            partition filter. If false, queries over the table\n",
      "        :type require_partition_filter: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cancel all started queries that have not yet completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete an existing table from the dataset;\n",
      "        If the table does not exist, return an error unless ignore_if_missing\n",
      "        is set to True.\n",
      "\n",
      "        :param deletion_dataset_table: A dotted\n",
      "            ``(<project>.|<project>:)<dataset>.<table>`` that indicates which table\n",
      "            will be deleted.\n",
      "        :type deletion_dataset_table: str\n",
      "        :param ignore_if_missing: if True, then return success even if the\n",
      "            requested table does not exist.\n",
      "        :type ignore_if_missing: bool\n",
      "        :return:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "creates a new, empty table in the dataset;\n",
      "        If the table already exists, update the existing table.\n",
      "        Since BigQuery does not natively allow table upserts, this is not an\n",
      "        atomic operation.\n",
      "\n",
      "        :param dataset_id: the dataset to upsert the table into.\n",
      "        :type dataset_id: str\n",
      "        :param table_resource: a table resource. see\n",
      "            https://cloud.google.com/bigquery/docs/reference/v2/tables#resource\n",
      "        :type table_resource: dict\n",
      "        :param project_id: the project to upsert the table into.  If None,\n",
      "            project will be self.project_id.\n",
      "        :return:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Grant authorized view access of a dataset to a view table.\n",
      "        If this view has already been granted access to the dataset, do nothing.\n",
      "        This method is not atomic.  Running it may clobber a simultaneous update.\n",
      "\n",
      "        :param source_dataset: the source dataset\n",
      "        :type source_dataset: str\n",
      "        :param view_dataset: the dataset that the view is in\n",
      "        :type view_dataset: str\n",
      "        :param view_table: the table of the view\n",
      "        :type view_table: str\n",
      "        :param source_project: the project of the source dataset. If None,\n",
      "            self.project_id will be used.\n",
      "        :type source_project: str\n",
      "        :param view_project: the project that the view is in. If None,\n",
      "            self.project_id will be used.\n",
      "        :type view_project: str\n",
      "        :return: the datasets resource of the source dataset.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Method returns dataset_resource if dataset exist\n",
      "        and raised 404 error if dataset does not exist\n",
      "\n",
      "        :param dataset_id: The BigQuery Dataset ID\n",
      "        :type dataset_id: str\n",
      "        :param project_id: The GCP Project ID\n",
      "        :type project_id: str\n",
      "        :return: dataset_resource\n",
      "\n",
      "            .. seealso::\n",
      "                For more information, see Dataset Resource content:\n",
      "                https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets#resource\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Method returns full list of BigQuery datasets in the current project\n",
      "\n",
      "        .. seealso::\n",
      "            For more information, see:\n",
      "            https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list\n",
      "\n",
      "        :param project_id: Google Cloud Project for which you\n",
      "            try to get all datasets\n",
      "        :type project_id: str\n",
      "        :return: datasets_list\n",
      "\n",
      "            Example of returned datasets_list: ::\n",
      "\n",
      "                   {\n",
      "                      \"kind\":\"bigquery#dataset\",\n",
      "                      \"location\":\"US\",\n",
      "                      \"id\":\"your-project:dataset_2_test\",\n",
      "                      \"datasetReference\":{\n",
      "                         \"projectId\":\"your-project\",\n",
      "                         \"datasetId\":\"dataset_2_test\"\n",
      "                      }\n",
      "                   },\n",
      "                   {\n",
      "                      \"kind\":\"bigquery#dataset\",\n",
      "                      \"location\":\"US\",\n",
      "                      \"id\":\"your-project:dataset_1_test\",\n",
      "                      \"datasetReference\":{\n",
      "                         \"projectId\":\"your-project\",\n",
      "                         \"datasetId\":\"dataset_1_test\"\n",
      "                      }\n",
      "                   }\n",
      "                ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Method to stream data into BigQuery one record at a time without needing\n",
      "        to run a load job\n",
      "\n",
      "        .. seealso::\n",
      "            For more information, see:\n",
      "            https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll\n",
      "\n",
      "        :param project_id: The name of the project where we have the table\n",
      "        :type project_id: str\n",
      "        :param dataset_id: The name of the dataset where we have the table\n",
      "        :type dataset_id: str\n",
      "        :param table_id: The name of the table\n",
      "        :type table_id: str\n",
      "        :param rows: the rows to insert\n",
      "        :type rows: list\n",
      "\n",
      "        **Example or rows**:\n",
      "            rows=[{\"json\": {\"a_key\": \"a_value_0\"}}, {\"json\": {\"a_key\": \"a_value_1\"}}]\n",
      "\n",
      "        :param ignore_unknown_values: [Optional] Accept rows that contain values\n",
      "            that do not match the schema. The unknown values are ignored.\n",
      "            The default value  is false, which treats unknown values as errors.\n",
      "        :type ignore_unknown_values: bool\n",
      "        :param skip_invalid_rows: [Optional] Insert all valid rows of a request,\n",
      "            even if invalid rows exist. The default value is false, which causes\n",
      "            the entire request to fail if any invalid rows exist.\n",
      "        :type skip_invalid_rows: bool\n",
      "        :param fail_on_error: [Optional] Force the task to fail if any errors occur.\n",
      "            The default value is false, which indicates the task should not fail\n",
      "            even if any insertion errors occur.\n",
      "        :type fail_on_error: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executes a BigQuery query, and returns the job ID.\n",
      "\n",
      "        :param operation: The query to execute.\n",
      "        :type operation: str\n",
      "        :param parameters: Parameters to substitute into the query.\n",
      "        :type parameters: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute a BigQuery query multiple times with different parameters.\n",
      "\n",
      "        :param operation: The query to execute.\n",
      "        :type operation: str\n",
      "        :param seq_of_parameters: List of dictionary parameters to substitute into the\n",
      "            query.\n",
      "        :type seq_of_parameters: list\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method for fetchone, which returns the next row from a buffer.\n",
      "        If the buffer is empty, attempts to paginate through the result set for\n",
      "        the next page, and load it into the buffer.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries Postgres and returns a cursor to the results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create all the intermediate directories in a remote host\n",
      "\n",
      "    :param sftp_client: A Paramiko SFTP client.\n",
      "    :param remote_directory: Absolute Path of the directory containing the file\n",
      "    :return:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create queue using connection object\n",
      "\n",
      "        :param queue_name: name of the queue.\n",
      "        :type queue_name: str\n",
      "        :param attributes: additional attributes for the queue (default: None)\n",
      "            For details of the attributes parameter see :py:meth:`botocore.client.SQS.create_queue`\n",
      "        :type attributes: dict\n",
      "\n",
      "        :return: dict with the information about the queue\n",
      "            For details of the returned value see :py:meth:`botocore.client.SQS.create_queue`\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send message to the queue\n",
      "\n",
      "        :param queue_url: queue url\n",
      "        :type queue_url: str\n",
      "        :param message_body: the contents of the message\n",
      "        :type message_body: str\n",
      "        :param delay_seconds: seconds to delay the message\n",
      "        :type delay_seconds: int\n",
      "        :param message_attributes: additional attributes for the message (default: None)\n",
      "            For details of the attributes parameter see :py:meth:`botocore.client.SQS.send_message`\n",
      "        :type message_attributes: dict\n",
      "\n",
      "        :return: dict with the information about the message sent\n",
      "            For details of the returned value see :py:meth:`botocore.client.SQS.send_message`\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run the task command.\n",
      "\n",
      "        :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``\n",
      "        :type run_with: list\n",
      "        :param join_args: whether to concatenate the list of command tokens e.g. ``['airflow', 'run']`` vs\n",
      "            ``['airflow run']``\n",
      "        :param join_args: bool\n",
      "        :return: the process that was run\n",
      "        :rtype: subprocess.Popen\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A callback that should be called when this is done running.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parse options and process commands\n",
      "----------------------------------------------------------------------------------------------------\n",
      "generate HTML header content\n",
      "----------------------------------------------------------------------------------------------------\n",
      "generate HTML div\n",
      "----------------------------------------------------------------------------------------------------\n",
      "generate javascript code for the chart\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create X-axis\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create Y-axis\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a sqlite connection object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decorator to log user actions\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decorator to make a view compressed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the last dag run for a dag, None if there was none.\n",
      "    Last dag run can be any type of run eg. scheduled or backfilled.\n",
      "    Overridden DagRuns are ignored.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a dag run from this dag including the tasks associated with this dag.\n",
      "        Returns the dag run.\n",
      "\n",
      "        :param run_id: defines the the run id for this dag run\n",
      "        :type run_id: str\n",
      "        :param execution_date: the execution date of this dag run\n",
      "        :type execution_date: datetime.datetime\n",
      "        :param state: the state of the dag run\n",
      "        :type state: airflow.utils.state.State\n",
      "        :param start_date: the date this dag run should be evaluated\n",
      "        :type start_date: datetime.datetime\n",
      "        :param external_trigger: whether this dag run is externally triggered\n",
      "        :type external_trigger: bool\n",
      "        :param session: database session\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Publish the message to SQS queue\n",
      "\n",
      "        :param context: the context object\n",
      "        :type context: dict\n",
      "        :return: dict with information about the message sent\n",
      "            For details of the returned dict see :py:meth:`botocore.client.SQS.send_message`\n",
      "        :rtype: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "returns a json response from a json serializable python object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens the given file. If the path contains a folder with a .zip suffix, then\n",
      "    the folder is treated as a zip archive, opening the file inside the archive.\n",
      "\n",
      "    :return: a file object, as in `open`, or as in `ZipFile.open`.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Used by cache to get a unique key per URL\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns Gcp Video Intelligence Service client\n",
      "\n",
      "        :rtype: google.cloud.videointelligence_v1.VideoIntelligenceServiceClient\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Performs video annotation.\n",
      "\n",
      "        :param input_uri: Input video location. Currently, only Google Cloud Storage URIs are supported,\n",
      "            which must be specified in the following format: ``gs://bucket-id/object-id``.\n",
      "        :type input_uri: str\n",
      "        :param input_content: The video data bytes.\n",
      "            If unset, the input video(s) should be specified via ``input_uri``.\n",
      "            If set, ``input_uri`` should be unset.\n",
      "        :type input_content: bytes\n",
      "        :param features: Requested video annotation features.\n",
      "        :type features: list[google.cloud.videointelligence_v1.VideoIntelligenceServiceClient.enums.Feature]\n",
      "        :param output_uri: Optional, location where the output (in JSON format) should be stored. Currently,\n",
      "            only Google Cloud Storage URIs are supported, which must be specified in the following format:\n",
      "            ``gs://bucket-id/object-id``.\n",
      "        :type output_uri: str\n",
      "        :param video_context: Optional, Additional video context and/or feature-specific parameters.\n",
      "        :type video_context: dict or google.cloud.videointelligence_v1.types.VideoContext\n",
      "        :param location: Optional, cloud region where annotation should take place. Supported cloud regions:\n",
      "            us-east1, us-west1, europe-west1, asia-east1.\n",
      "            If no region is specified, a region will be determined based on video file location.\n",
      "        :type location: str\n",
      "        :param retry: Retry object used to determine when/if to retry requests.\n",
      "            If None is specified, requests will not be retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: Optional, The amount of time, in seconds, to wait for the request to complete.\n",
      "            Note that if retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "        :param metadata: Optional, Additional metadata that is provided to the method.\n",
      "        :type metadata: seq[tuple[str, str]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get Opsgenie api_key for creating alert\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Overwrite HttpHook get_conn because this hook just needs base_url\n",
      "        and headers, and does not need generic params\n",
      "\n",
      "        :param headers: additional headers to be passed through as a dictionary\n",
      "        :type headers: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute the Opsgenie Alert call\n",
      "\n",
      "        :param payload: Opsgenie API Create Alert payload values\n",
      "            See https://docs.opsgenie.com/docs/alert-api#section-create-alert\n",
      "        :type payload: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Construct the Opsgenie JSON payload. All relevant parameters are combined here\n",
      "        to a valid Opsgenie JSON payload.\n",
      "\n",
      "        :return: Opsgenie payload (dict) to send\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Call the OpsgenieAlertHook to post message\n",
      "----------------------------------------------------------------------------------------------------\n",
      "check if aws conn exists already or create one and return it\n",
      "\n",
      "        :return: boto3 session\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run Presto query on athena with provided config and return submitted query_execution_id\n",
      "\n",
      "        :param query: Presto query to run\n",
      "        :type query: str\n",
      "        :param query_context: Context in which query need to be run\n",
      "        :type query_context: dict\n",
      "        :param result_configuration: Dict with path to store results in and config related to encryption\n",
      "        :type result_configuration: dict\n",
      "        :param client_request_token: Unique token created by user to avoid multiple executions of same query\n",
      "        :type client_request_token: str\n",
      "        :return: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetch the status of submitted athena query. Returns None or one of valid query states.\n",
      "\n",
      "        :param query_execution_id: Id of submitted athena query\n",
      "        :type query_execution_id: str\n",
      "        :return: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Poll the status of submitted athena query until query state reaches final state.\n",
      "        Returns one of the final states\n",
      "\n",
      "        :param query_execution_id: Id of submitted athena query\n",
      "        :type query_execution_id: str\n",
      "        :param max_tries: Number of times to poll for query state before function exits\n",
      "        :type max_tries: int\n",
      "        :return: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns an SFTP connection object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sleep for the time specified in the exception. If not specified, wait\n",
      "        for 60 seconds.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Call Zendesk API and return results\n",
      "\n",
      "        :param path: The Zendesk API to call\n",
      "        :param query: Query parameters\n",
      "        :param get_all_pages: Accumulate results over all pages before\n",
      "               returning. Due to strict rate limiting, this can often timeout.\n",
      "               Waits for recommended period between tries after a timeout.\n",
      "        :param side_loading: Retrieve related records as part of a single\n",
      "               request. In order to enable side-loading, add an 'include'\n",
      "               query parameter containing a comma-separated list of resources\n",
      "               to load. For more information on side-loading see\n",
      "               https://developer.zendesk.com/rest_api/docs/core/side_loading\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the partition values for a table.\n",
      "\n",
      "        :param database_name: The name of the catalog database where the partitions reside.\n",
      "        :type database_name: str\n",
      "        :param table_name: The name of the partitions' table.\n",
      "        :type table_name: str\n",
      "        :param expression: An expression filtering the partitions to be returned.\n",
      "            Please see official AWS documentation for further information.\n",
      "            https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-partitions.html#aws-glue-api-catalog-partitions-GetPartitions\n",
      "        :type expression: str\n",
      "        :param page_size: pagination size\n",
      "        :type page_size: int\n",
      "        :param max_items: maximum items to return\n",
      "        :type max_items: int\n",
      "        :return: set of partition values where each value is a tuple since\n",
      "            a partition may be composed of multiple columns. For example:\n",
      "            ``{('2018-01-01','1'), ('2018-01-01','2')}``\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the information of the table\n",
      "\n",
      "        :param database_name: Name of hive database (schema) @table belongs to\n",
      "        :type database_name: str\n",
      "        :param table_name: Name of hive table\n",
      "        :type table_name: str\n",
      "        :rtype: dict\n",
      "\n",
      "        >>> hook = AwsGlueCatalogHook()\n",
      "        >>> r = hook.get_table('db', 'table_foo')\n",
      "        >>> r['Name'] = 'table_foo'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the physical location of the table\n",
      "\n",
      "        :param database_name: Name of hive database (schema) @table belongs to\n",
      "        :type database_name: str\n",
      "        :param table_name: Name of hive table\n",
      "        :type table_name: str\n",
      "        :return: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return status of a cluster\n",
      "\n",
      "        :param cluster_identifier: unique identifier of a cluster\n",
      "        :type cluster_identifier: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a cluster and optionally create a snapshot\n",
      "\n",
      "        :param cluster_identifier: unique identifier of a cluster\n",
      "        :type cluster_identifier: str\n",
      "        :param skip_final_cluster_snapshot: determines cluster snapshot creation\n",
      "        :type skip_final_cluster_snapshot: bool\n",
      "        :param final_cluster_snapshot_identifier: name of final cluster snapshot\n",
      "        :type final_cluster_snapshot_identifier: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets a list of snapshots for a cluster\n",
      "\n",
      "        :param cluster_identifier: unique identifier of a cluster\n",
      "        :type cluster_identifier: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Restores a cluster from its snapshot\n",
      "\n",
      "        :param cluster_identifier: unique identifier of a cluster\n",
      "        :type cluster_identifier: str\n",
      "        :param snapshot_identifier: unique identifier for a snapshot of a cluster\n",
      "        :type snapshot_identifier: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a snapshot of a cluster\n",
      "\n",
      "        :param snapshot_identifier: unique identifier for a snapshot of a cluster\n",
      "        :type snapshot_identifier: str\n",
      "        :param cluster_identifier: unique identifier of a cluster\n",
      "        :type cluster_identifier: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SlackAPIOperator calls will not fail even if the call is not unsuccessful.\n",
      "        It should not prevent a DAG from completing in success\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a job flow using the config from the EMR connection.\n",
      "        Keys of the json extra hash may have the arguments of the boto3\n",
      "        run_job_flow method.\n",
      "        Overrides for this config may be passed as the job_flow_overrides.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Will test the filepath result and test if its size is at least self.filesize\n",
      "\n",
      "        :param result: a list of dicts returned by Snakebite ls\n",
      "        :param size: the file size in MB a file should be at least to trigger True\n",
      "        :return: (bool) depending on the matching criteria\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Will filter if instructed to do so the result to remove matching criteria\n",
      "\n",
      "        :param result: list of dicts returned by Snakebite ls\n",
      "        :type result: list[dict]\n",
      "        :param ignored_ext: list of ignored extensions\n",
      "        :type ignored_ext: list\n",
      "        :param ignore_copying: shall we ignore ?\n",
      "        :type ignore_copying: bool\n",
      "        :return: list of dicts which were not removed\n",
      "        :rtype: list[dict]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed by task_instance at runtime\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get pool by a given name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a pool with a given parameters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete pool by a given name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts a python dictionary to the proto supplied\n",
      "\n",
      "        :param py_dict: The dictionary to convert\n",
      "        :type py_dict: dict\n",
      "        :param proto: The proto object to merge with dictionary\n",
      "        :type proto: protobuf\n",
      "        :return: A parsed python dictionary in provided proto format\n",
      "        :raises:\n",
      "            ParseError: On JSON parsing problems.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given an operation, continuously fetches the status from Google Cloud until either\n",
      "        completion or an error occurring\n",
      "\n",
      "        :param operation: The Operation to wait for\n",
      "        :type operation: google.cloud.container_V1.gapic.enums.Operation\n",
      "        :param project_id: Google Cloud Platform project ID\n",
      "        :type project_id: str\n",
      "        :return: A new, updated operation fetched from Google Cloud\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetches the operation from Google Cloud\n",
      "\n",
      "        :param operation_name: Name of operation to fetch\n",
      "        :type operation_name: str\n",
      "        :param project_id: Google Cloud Platform project ID\n",
      "        :type project_id: str\n",
      "        :return: The new, updated operation from Google Cloud\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Append labels to provided Cluster Protobuf\n",
      "\n",
      "        Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current\n",
      "         airflow version string follows semantic versioning spec: x.y.z).\n",
      "\n",
      "        :param cluster_proto: The proto to append resource_label airflow\n",
      "            version to\n",
      "        :type cluster_proto: google.cloud.container_v1.types.Cluster\n",
      "        :param key: The key label\n",
      "        :type key: str\n",
      "        :param val:\n",
      "        :type val: str\n",
      "        :return: The cluster proto updated with new label\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a cluster, consisting of the specified number and type of Google Compute\n",
      "        Engine instances.\n",
      "\n",
      "        :param cluster: A Cluster protobuf or dict. If dict is provided, it must\n",
      "            be of the same form as the protobuf message\n",
      "            :class:`google.cloud.container_v1.types.Cluster`\n",
      "        :type cluster: dict or google.cloud.container_v1.types.Cluster\n",
      "        :param project_id: Google Cloud Platform project ID\n",
      "        :type project_id: str\n",
      "        :param retry: A retry object (``google.api_core.retry.Retry``) used to\n",
      "            retry requests.\n",
      "            If None is specified, requests will not be retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: The amount of time, in seconds, to wait for the request to\n",
      "            complete. Note that if retry is specified, the timeout applies to each\n",
      "            individual attempt.\n",
      "        :type timeout: float\n",
      "        :return: The full url to the new, or existing, cluster\n",
      "        :raises:\n",
      "            ParseError: On JSON parsing problems when trying to convert dict\n",
      "            AirflowException: cluster is not dict type nor Cluster proto type\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets details of specified cluster\n",
      "\n",
      "        :param name: The name of the cluster to retrieve\n",
      "        :type name: str\n",
      "        :param project_id: Google Cloud Platform project ID\n",
      "        :type project_id: str\n",
      "        :param retry: A retry object used to retry requests. If None is specified,\n",
      "            requests will not be retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: The amount of time, in seconds, to wait for the request to\n",
      "            complete. Note that if retry is specified, the timeout applies to each\n",
      "            individual attempt.\n",
      "        :type timeout: float\n",
      "        :return: google.cloud.container_v1.types.Cluster\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given a Discord http_conn_id, return the default webhook endpoint or override if a\n",
      "        webhook_endpoint is manually supplied.\n",
      "\n",
      "        :param http_conn_id: The provided connection ID\n",
      "        :param webhook_endpoint: The manually provided webhook endpoint\n",
      "        :return: Webhook endpoint (str) to use\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Construct the Discord JSON payload. All relevant parameters are combined here\n",
      "        to a valid Discord JSON payload.\n",
      "\n",
      "        :return: Discord payload (str) to send\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute the Discord webhook call\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Encrypts a plaintext message using Google Cloud KMS.\n",
      "\n",
      "        :param key_name: The Resource Name for the key (or key version)\n",
      "                         to be used for encyption. Of the form\n",
      "                         ``projects/*/locations/*/keyRings/*/cryptoKeys/**``\n",
      "        :type key_name: str\n",
      "        :param plaintext: The message to be encrypted.\n",
      "        :type plaintext: bytes\n",
      "        :param authenticated_data: Optional additional authenticated data that\n",
      "                                   must also be provided to decrypt the message.\n",
      "        :type authenticated_data: bytes\n",
      "        :return: The base 64 encoded ciphertext of the original message.\n",
      "        :rtype: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Imports table from remote location to target dir. Arguments are\n",
      "        copies of direct sqoop command line arguments\n",
      "\n",
      "        :param table: Table to read\n",
      "        :param target_dir: HDFS destination dir\n",
      "        :param append: Append data to an existing dataset in HDFS\n",
      "        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\".\n",
      "            Imports data to into the specified format. Defaults to text.\n",
      "        :param columns: <col,col,col…> Columns to import from table\n",
      "        :param split_by: Column of the table used to split work units\n",
      "        :param where: WHERE clause to use during import\n",
      "        :param direct: Use direct connector if exists for the database\n",
      "        :param driver: Manually specify JDBC driver class to use\n",
      "        :param extra_import_options: Extra import options to pass as dict.\n",
      "            If a key doesn't have a value, just pass an empty string to it.\n",
      "            Don't include prefix of -- for sqoop options.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Imports a specific query from the rdbms to hdfs\n",
      "\n",
      "        :param query: Free format query to run\n",
      "        :param target_dir: HDFS destination dir\n",
      "        :param append: Append data to an existing dataset in HDFS\n",
      "        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\"\n",
      "            Imports data to hdfs into the specified format. Defaults to text.\n",
      "        :param split_by: Column of the table used to split work units\n",
      "        :param direct: Use direct import fast path\n",
      "        :param driver: Manually specify JDBC driver class to use\n",
      "        :param extra_import_options: Extra import options to pass as dict.\n",
      "            If a key doesn't have a value, just pass an empty string to it.\n",
      "            Don't include prefix of -- for sqoop options.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exports Hive table to remote location. Arguments are copies of direct\n",
      "        sqoop command line Arguments\n",
      "\n",
      "        :param table: Table remote destination\n",
      "        :param export_dir: Hive table to export\n",
      "        :param input_null_string: The string to be interpreted as null for\n",
      "            string columns\n",
      "        :param input_null_non_string: The string to be interpreted as null\n",
      "            for non-string columns\n",
      "        :param staging_table: The table in which data will be staged before\n",
      "            being inserted into the destination table\n",
      "        :param clear_staging_table: Indicate that any data present in the\n",
      "            staging table can be deleted\n",
      "        :param enclosed_by: Sets a required field enclosing character\n",
      "        :param escaped_by: Sets the escape character\n",
      "        :param input_fields_terminated_by: Sets the field separator character\n",
      "        :param input_lines_terminated_by: Sets the end-of-line character\n",
      "        :param input_optionally_enclosed_by: Sets a field enclosing character\n",
      "        :param batch: Use batch mode for underlying statement execution\n",
      "        :param relaxed_isolation: Transaction isolation to read uncommitted\n",
      "            for the mappers\n",
      "        :param extra_export_options: Extra export options to pass as dict.\n",
      "            If a key doesn't have a value, just pass an empty string to it.\n",
      "            Don't include prefix of -- for sqoop options.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Cloud Text to Speech.\n",
      "\n",
      "        :return: Google Cloud Text to Speech client object.\n",
      "        :rtype: google.cloud.texttospeech_v1.TextToSpeechClient\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthesizes text input\n",
      "\n",
      "        :param input_data: text input to be synthesized. See more:\n",
      "            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesisInput\n",
      "        :type input_data: dict or google.cloud.texttospeech_v1.types.SynthesisInput\n",
      "        :param voice: configuration of voice to be used in synthesis. See more:\n",
      "            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.VoiceSelectionParams\n",
      "        :type voice: dict or google.cloud.texttospeech_v1.types.VoiceSelectionParams\n",
      "        :param audio_config: configuration of the synthesized audio. See more:\n",
      "            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.AudioConfig\n",
      "        :type audio_config: dict or google.cloud.texttospeech_v1.types.AudioConfig\n",
      "        :return: SynthesizeSpeechResponse See more:\n",
      "            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesizeSpeechResponse\n",
      "        :rtype: object\n",
      "        :param retry: (Optional) A retry object used to retry requests. If None is specified,\n",
      "                requests will not be retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.\n",
      "            Note that if retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Close and upload local log file to remote storage S3.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "When using git to retrieve the DAGs, use the GitSync Init Container\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Defines any necessary environment variables for the pod executor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Defines any necessary secrets for the pod executor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Defines the security context\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get link to qubole command result page.\n",
      "\n",
      "        :param operator: operator\n",
      "        :param dttm: datetime\n",
      "        :return: url link\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Heartbeats update the job's entry in the database with a timestamp\n",
      "        for the latest_heartbeat and allows for the job to be killed\n",
      "        externally. This allows at the system level to monitor what is\n",
      "        actually active.\n",
      "\n",
      "        For instance, an old heartbeat for SchedulerJob would mean something\n",
      "        is wrong.\n",
      "\n",
      "        This also allows for any job to be killed externally, regardless\n",
      "        of who is running it or on which machine it is running.\n",
      "\n",
      "        Note that if your heartbeat is set to 60 seconds and you call this\n",
      "        method after 10 seconds of processing since the last heartbeat, it\n",
      "        will sleep 50 seconds to complete the 60 seconds and keep a steady\n",
      "        heart rate. If you go over 60 seconds before calling it, it won't\n",
      "        sleep at all.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Launch a process to process the given file.\n",
      "\n",
      "        :param result_queue: the queue to use for passing back the result\n",
      "        :type result_queue: multiprocessing.Queue\n",
      "        :param file_path: the file to process\n",
      "        :type file_path: unicode\n",
      "        :param pickle_dags: whether to pickle the DAGs found in the file and\n",
      "            save them to the DB\n",
      "        :type pickle_dags: bool\n",
      "        :param dag_id_white_list: if specified, only examine DAG ID's that are\n",
      "            in this list\n",
      "        :type dag_id_white_list: list[unicode]\n",
      "        :param thread_name: the name to use for the process that is launched\n",
      "        :type thread_name: unicode\n",
      "        :return: the process that was launched\n",
      "        :rtype: multiprocessing.Process\n",
      "        :param zombies: zombie task instances to kill\n",
      "        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Launch the process and start processing the DAG.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if the process launched to process this file is done.\n",
      "\n",
      "        :return: whether the process is finished running\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method to clean up processor_agent to avoid leaving orphan processes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "For the DAGs in the given DagBag, record any associated import errors and clears\n",
      "        errors for files that no longer have them. These are usually displayed through the\n",
      "        Airflow UI so that users know that there are issues parsing DAGs.\n",
      "\n",
      "        :param session: session for ORM operations\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "        :param dagbag: DagBag containing DAGs with import errors\n",
      "        :type dagbag: airflow.models.DagBag\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This method schedules the tasks for a single DAG by looking at the\n",
      "        active DAG runs and adding task instances that should run to the\n",
      "        queue.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "For all DAG IDs in the SimpleDagBag, look for task instances in the\n",
      "        old_states and set them to new_state if the corresponding DagRun\n",
      "        does not exist or exists but is not in the running state. This\n",
      "        normally should not happen, but it can if the state of DagRuns are\n",
      "        changed manually.\n",
      "\n",
      "        :param old_states: examine TaskInstances in this state\n",
      "        :type old_state: list[airflow.utils.state.State]\n",
      "        :param new_state: set TaskInstances to this state\n",
      "        :type new_state: airflow.utils.state.State\n",
      "        :param simple_dag_bag: TaskInstances associated with DAGs in the\n",
      "            simple_dag_bag and with states in the old_state will be examined\n",
      "        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the concurrency maps.\n",
      "\n",
      "        :param states: List of states to query for\n",
      "        :type states: list[airflow.utils.state.State]\n",
      "        :return: A map from (dag_id, task_id) to # of task instances and\n",
      "         a map from (dag_id, task_id) to # of task instances in the given state list\n",
      "        :rtype: dict[tuple[str, str], int]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Changes the state of task instances in the list with one of the given states\n",
      "        to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.\n",
      "\n",
      "        :param task_instances: TaskInstances to change the state of\n",
      "        :type task_instances: list[airflow.models.TaskInstance]\n",
      "        :param acceptable_states: Filters the TaskInstances updated to be in these states\n",
      "        :type acceptable_states: Iterable[State]\n",
      "        :rtype: list[airflow.utils.dag_processing.SimpleTaskInstance]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Takes task_instances, which should have been set to queued, and enqueues them\n",
      "        with the executor.\n",
      "\n",
      "        :param simple_task_instances: TaskInstances to enqueue\n",
      "        :type simple_task_instances: list[SimpleTaskInstance]\n",
      "        :param simple_dag_bag: Should contains all of the task_instances' dags\n",
      "        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempts to execute TaskInstances that should be executed by the scheduler.\n",
      "\n",
      "        There are three steps:\n",
      "        1. Pick TIs by priority with the constraint that they are in the expected states\n",
      "        and that we do exceed max_active_runs or pool limits.\n",
      "        2. Change the state for the TIs above atomically.\n",
      "        3. Enqueue the TIs in the executor.\n",
      "\n",
      "        :param simple_dag_bag: TaskInstances associated with DAGs in the\n",
      "            simple_dag_bag will be fetched from the DB and executed\n",
      "        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag\n",
      "        :param states: Execute TaskInstances in these states\n",
      "        :type states: tuple[airflow.utils.state.State]\n",
      "        :return: Number of task instance with state changed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If there are tasks left over in the executor,\n",
      "        we set them back to SCHEDULED to avoid creating hanging tasks.\n",
      "\n",
      "        :param session: session for ORM operations\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Respond to executor events.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Process a Python file containing Airflow DAGs.\n",
      "\n",
      "        This includes:\n",
      "\n",
      "        1. Execute the file and look for DAG objects in the namespace.\n",
      "        2. Pickle the DAG and save it to the DB (if necessary).\n",
      "        3. For each DAG, see what tasks should run and create appropriate task\n",
      "        instances in the DB.\n",
      "        4. Record any errors importing the file into ORM\n",
      "        5. Kill (in ORM) any task instances belonging to the DAGs that haven't\n",
      "        issued a heartbeat in a while.\n",
      "\n",
      "        Returns a list of SimpleDag objects that represent the DAGs found in\n",
      "        the file\n",
      "\n",
      "        :param file_path: the path to the Python file that should be executed\n",
      "        :type file_path: unicode\n",
      "        :param zombies: zombie task instances to kill.\n",
      "        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]\n",
      "        :param pickle_dags: whether serialize the DAGs found in the file and\n",
      "            save them to the db\n",
      "        :type pickle_dags: bool\n",
      "        :return: a list of SimpleDags made from the Dags found in the file\n",
      "        :rtype: list[airflow.utils.dag_processing.SimpleDagBag]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the counters per state of the tasks that were running. Can re-add\n",
      "        to tasks to run in case required.\n",
      "\n",
      "        :param ti_status: the internal status of the backfill job tasks\n",
      "        :type ti_status: BackfillJob._DagRunTaskStatus\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if the executor agrees with the state of task instances\n",
      "        that are running\n",
      "\n",
      "        :param running: dict of key, task to verify\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a dag run for the given run date, which will be matched to an existing\n",
      "        dag run if available or create a new dag run otherwise. If the max_active_runs\n",
      "        limit is reached, this function will return None.\n",
      "\n",
      "        :param run_date: the execution date for the dag run\n",
      "        :type run_date: datetime.datetime\n",
      "        :param session: the database session object\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "        :return: a DagRun in state RUNNING or None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a map of task instance key to task instance object for the tasks to\n",
      "        run in the given dag run.\n",
      "\n",
      "        :param dag_run: the dag run to get the tasks from\n",
      "        :type dag_run: airflow.models.DagRun\n",
      "        :param session: the database session object\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Computes the dag runs and their respective task instances for\n",
      "        the given run dates and executes the task instances.\n",
      "        Returns a list of execution dates of the dag runs that were executed.\n",
      "\n",
      "        :param run_dates: Execution dates for dag runs\n",
      "        :type run_dates: list\n",
      "        :param ti_status: internal BackfillJob status structure to tis track progress\n",
      "        :type ti_status: BackfillJob._DagRunTaskStatus\n",
      "        :param executor: the executor to use, it must be previously started\n",
      "        :type executor: BaseExecutor\n",
      "        :param pickle_id: numeric id of the pickled dag, None if not pickled\n",
      "        :type pickle_id: int\n",
      "        :param start_date: backfill start date\n",
      "        :type start_date: datetime.datetime\n",
      "        :param session: the current session object\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Go through the dag_runs and update the state based on the task_instance state.\n",
      "        Then set DAG runs that are not finished to failed.\n",
      "\n",
      "        :param dag_runs: DAG runs\n",
      "        :param session: session\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initializes all components required to run a dag for a specified date range and\n",
      "        calls helper method to execute the tasks.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Self destruct task if state has been moved away from running externally\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Provides a client for interacting with the Cloud Spanner API.\n",
      "\n",
      "        :param project_id: The ID of the  GCP project.\n",
      "        :type project_id: str\n",
      "        :return: google.cloud.spanner_v1.client.Client\n",
      "        :rtype: object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets information about a particular instance.\n",
      "\n",
      "        :param project_id: Optional, The ID of the  GCP project that owns the Cloud Spanner\n",
      "            database.  If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :param instance_id: The ID of the Cloud Spanner instance.\n",
      "        :type instance_id: str\n",
      "        :return: google.cloud.spanner_v1.instance.Instance\n",
      "        :rtype: object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Invokes a method on a given instance by applying a specified Callable.\n",
      "\n",
      "        :param project_id: The ID of the  GCP project that owns the Cloud Spanner\n",
      "            database.\n",
      "        :type project_id: str\n",
      "        :param instance_id: The ID of the instance.\n",
      "        :type instance_id: str\n",
      "        :param configuration_name: Name of the instance configuration defining how the\n",
      "            instance will be created. Required for instances which do not yet exist.\n",
      "        :type configuration_name: str\n",
      "        :param node_count: (Optional) Number of nodes allocated to the instance.\n",
      "        :type node_count: int\n",
      "        :param display_name: (Optional) The display name for the instance in the Cloud\n",
      "            Console UI. (Must be between 4 and 30 characters.) If this value is not set\n",
      "            in the constructor, will fall back to the instance ID.\n",
      "        :type display_name: str\n",
      "        :param func: Method of the instance to be called.\n",
      "        :type func: Callable\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new Cloud Spanner instance.\n",
      "\n",
      "        :param instance_id: The ID of the Cloud Spanner instance.\n",
      "        :type instance_id: str\n",
      "        :param configuration_name: The name of the instance configuration defining how the\n",
      "            instance will be created. Possible configuration values can be retrieved via\n",
      "            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list\n",
      "        :type configuration_name: str\n",
      "        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner\n",
      "            instance.\n",
      "        :type node_count: int\n",
      "        :param display_name: (Optional) The display name for the instance in the GCP\n",
      "            Console. Must be between 4 and 30 characters.  If this value is not set in\n",
      "            the constructor, the name falls back to the instance ID.\n",
      "        :type display_name: str\n",
      "        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n",
      "            database. If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates an existing Cloud Spanner instance.\n",
      "\n",
      "        :param instance_id: The ID of the Cloud Spanner instance.\n",
      "        :type instance_id: str\n",
      "        :param configuration_name: The name of the instance configuration defining how the\n",
      "            instance will be created. Possible configuration values can be retrieved via\n",
      "            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list\n",
      "        :type configuration_name: str\n",
      "        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner\n",
      "            instance.\n",
      "        :type node_count: int\n",
      "        :param display_name: (Optional) The display name for the instance in the GCP\n",
      "            Console. Must be between 4 and 30 characters. If this value is not set in\n",
      "            the constructor, the name falls back to the instance ID.\n",
      "        :type display_name: str\n",
      "        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n",
      "            database. If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an existing Cloud Spanner instance.\n",
      "\n",
      "        :param instance_id: The ID of the Cloud Spanner instance.\n",
      "        :type instance_id: str\n",
      "        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner\n",
      "            database. If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves a database in Cloud Spanner. If the database does not exist\n",
      "        in the specified instance, it returns None.\n",
      "\n",
      "        :param instance_id: The ID of the Cloud Spanner instance.\n",
      "        :type instance_id: str\n",
      "        :param database_id: The ID of the database in Cloud Spanner.\n",
      "        :type database_id: str\n",
      "        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n",
      "            database. If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: Database object or None if database does not exist\n",
      "        :rtype: google.cloud.spanner_v1.database.Database or None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new database in Cloud Spanner.\n",
      "\n",
      "        :type project_id: str\n",
      "        :param instance_id: The ID of the Cloud Spanner instance.\n",
      "        :type instance_id: str\n",
      "        :param database_id: The ID of the database to create in Cloud Spanner.\n",
      "        :type database_id: str\n",
      "        :param ddl_statements: The string list containing DDL for the new database.\n",
      "        :type ddl_statements: list[str]\n",
      "        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n",
      "            database. If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates DDL of a database in Cloud Spanner.\n",
      "\n",
      "        :type project_id: str\n",
      "        :param instance_id: The ID of the Cloud Spanner instance.\n",
      "        :type instance_id: str\n",
      "        :param database_id: The ID of the database in Cloud Spanner.\n",
      "        :type database_id: str\n",
      "        :param ddl_statements: The string list containing DDL for the new database.\n",
      "        :type ddl_statements: list[str]\n",
      "        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner\n",
      "            database. If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :param operation_id: (Optional) The unique per database operation ID that can be\n",
      "            specified to implement idempotency check.\n",
      "        :type operation_id: str\n",
      "        :return: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Drops a database in Cloud Spanner.\n",
      "\n",
      "        :type project_id: str\n",
      "        :param instance_id: The ID of the Cloud Spanner instance.\n",
      "        :type instance_id: str\n",
      "        :param database_id: The ID of the database in Cloud Spanner.\n",
      "        :type database_id: str\n",
      "        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n",
      "            database. If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :return: True if everything succeeded\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pokes for a mail attachment on the mail server.\n",
      "\n",
      "        :param context: The context that is being provided when poking.\n",
      "        :type context: dict\n",
      "        :return: True if attachment with the given name is present and False if not.\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates additional_properties parameter based on language_hints, web_detection_params and\n",
      "    additional_properties parameters specified by the user\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a cassandra Session object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a table exists in Cassandra\n",
      "\n",
      "        :param table: Target Cassandra table.\n",
      "                      Use dot notation to target a specific keyspace.\n",
      "        :type table: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a record exists in Cassandra\n",
      "\n",
      "        :param table: Target Cassandra table.\n",
      "                      Use dot notation to target a specific keyspace.\n",
      "        :type table: str\n",
      "        :param keys: The keys and their values to check the existence.\n",
      "        :type keys: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Construct the command to poll the driver status.\n",
      "\n",
      "        :return: full command to be executed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Remote Popen to execute the spark-submit job\n",
      "\n",
      "        :param application: Submitted application, jar or py file\n",
      "        :type application: str\n",
      "        :param kwargs: extra arguments to Popen (see subprocess.Popen)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processes the log files and extracts useful information out of it.\n",
      "\n",
      "        If the deploy-mode is 'client', log the output of the submit command as those\n",
      "        are the output logs of the Spark worker directly.\n",
      "\n",
      "        Remark: If the driver needs to be tracked for its status, the log-level of the\n",
      "        spark deploy needs to be at least INFO (log4j.logger.org.apache.spark.deploy=INFO)\n",
      "\n",
      "        :param itr: An iterator which iterates over the input of the subprocess\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parses the logs of the spark driver status query process\n",
      "\n",
      "        :param itr: An iterator which iterates over the input of the subprocess\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the task runner that can be used to run the given job.\n",
      "\n",
      "    :param local_task_job: The LocalTaskJob associated with the TaskInstance\n",
      "        that needs to be executed.\n",
      "    :type local_task_job: airflow.jobs.LocalTaskJob\n",
      "    :return: The task runner to use to run the task.\n",
      "    :rtype: airflow.task.task_runner.base_task_runner.BaseTaskRunner\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Try to use a waiter from the below pull request\n",
      "\n",
      "            * https://github.com/boto/botocore/pull/1307\n",
      "\n",
      "        If the waiter is not available apply a exponential backoff\n",
      "\n",
      "            * docs.aws.amazon.com/general/latest/gr/api-retries.html\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries mysql and returns a cursor to the results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Configure a csv writer with the file_handle and write schema\n",
      "        as headers for the new file.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Takes a cursor, and writes the BigQuery schema in .json format for the\n",
      "        results to a local file system.\n",
      "\n",
      "        :return: A dictionary where key is a filename to be used as an object\n",
      "            name in GCS, and values are file handles to local files that\n",
      "            contains the BigQuery schema fields in .json format.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a dict of column name and column type based on self.schema if not None.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper function that maps from MySQL fields to BigQuery fields. Used\n",
      "        when a schema_filename is set.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute sqoop job\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Saves the lineage to XCom and if configured to do so sends it\n",
      "    to the backend.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the extra property by deserializing json.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a set of dates as a list based on a start, end and delta, delta\n",
      "    can be something that can be added to `datetime.datetime`\n",
      "    or a cron expression as a `str`\n",
      "\n",
      "    :Example::\n",
      "\n",
      "        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta=timedelta(1))\n",
      "            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n",
      "            datetime.datetime(2016, 1, 3, 0, 0)]\n",
      "        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta='0 0 * * *')\n",
      "            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n",
      "            datetime.datetime(2016, 1, 3, 0, 0)]\n",
      "        date_range(datetime(2016, 1, 1), datetime(2016, 3, 3), delta=\"0 0 0 * *\")\n",
      "            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 2, 1, 0, 0),\n",
      "            datetime.datetime(2016, 3, 1, 0, 0)]\n",
      "\n",
      "    :param start_date: anchor date to start the series from\n",
      "    :type start_date: datetime.datetime\n",
      "    :param end_date: right boundary for the date range\n",
      "    :type end_date: datetime.datetime\n",
      "    :param num: alternatively to end_date, you can specify the number of\n",
      "        number of entries you want in the range. This number can be negative,\n",
      "        output will always be sorted regardless\n",
      "    :type num: int\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert an array of time durations in seconds to the specified time unit.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a datetime object representing `n` days ago. By default the time is\n",
      "    set to midnight.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initialize the role with the permissions and related view-menus.\n",
      "\n",
      "        :param role_name:\n",
      "        :param role_vms:\n",
      "        :param role_perms:\n",
      "        :return:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete the given Role\n",
      "\n",
      "        :param role_name: the name of a role in the ab_role table\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get all the roles associated with the user.\n",
      "\n",
      "        :param user: the ab_user in FAB model.\n",
      "        :return: a list of roles associated with the user.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a set of tuples with the perm name and view menu name\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Whether the user has this role name\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Whether the user has this perm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "FAB leaves faulty permissions that need to be cleaned up\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add the new permission , view_menu to ab_permission_view_role if not exists.\n",
      "        It will add the related entry to ab_permission\n",
      "        and ab_view_menu two meta tables as well.\n",
      "\n",
      "        :param permission_name: Name of the permission.\n",
      "        :type permission_name: str\n",
      "        :param view_menu_name: Name of the view-menu\n",
      "        :type view_menu_name: str\n",
      "        :return:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Admin should have all the permission-views.\n",
      "        Add the missing ones to the table for admin.\n",
      "\n",
      "        :return: None.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Set the access policy on the given DAG's ViewModel.\n",
      "\n",
      "        :param dag_id: the ID of the DAG whose permissions should be updated\n",
      "        :type dag_id: string\n",
      "        :param access_control: a dict where each key is a rolename and\n",
      "            each value is a set() of permission names (e.g.,\n",
      "            {'can_dag_read'}\n",
      "        :type access_control: dict\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create perm-vm if not exist and insert into FAB security model for all-dags.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deferred load of Fernet key.\n",
      "\n",
      "    This function could fail either because Cryptography is not installed\n",
      "    or because the Fernet key is invalid.\n",
      "\n",
      "    :return: Fernet object\n",
      "    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks for existence of the partition in the AWS Glue Catalog table\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the AwsGlueCatalogHook\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check for message on subscribed queue and write to xcom the message with key ``messages``\n",
      "\n",
      "        :param context: the context object\n",
      "        :type context: dict\n",
      "        :return: ``True`` if message is available or ``False``\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a snakebite HDFSClient object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Establishes a connection depending on the security mode set via config or environment variable.\n",
      "\n",
      "        :return: a hdfscli InsecureClient or KerberosClient object.\n",
      "        :rtype: hdfs.InsecureClient or hdfs.ext.kerberos.KerberosClient\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check for the existence of a path in HDFS by querying FileStatus.\n",
      "\n",
      "        :param hdfs_path: The path to check.\n",
      "        :type hdfs_path: str\n",
      "        :return: True if the path exists and False if not.\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "r\"\"\"\n",
      "        Uploads a file to HDFS.\n",
      "\n",
      "        :param source: Local path to file or folder.\n",
      "            If it's a folder, all the files inside of it will be uploaded.\n",
      "            .. note:: This implies that folders empty of files will not be created remotely.\n",
      "\n",
      "        :type source: str\n",
      "        :param destination: PTarget HDFS path.\n",
      "            If it already exists and is a directory, files will be uploaded inside.\n",
      "        :type destination: str\n",
      "        :param overwrite: Overwrite any existing file or directory.\n",
      "        :type overwrite: bool\n",
      "        :param parallelism: Number of threads to use for parallelization.\n",
      "            A value of `0` (or negative) uses as many threads as there are files.\n",
      "        :type parallelism: int\n",
      "        :param \\**kwargs: Keyword arguments forwarded to :meth:`hdfs.client.Client.upload`.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Establish a connection to pinot broker through pinot dbqpi.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the connection uri for pinot broker.\n",
      "\n",
      "        e.g: http://localhost:9000/pql\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert native python ``datetime.date`` object  to a format supported by the API\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert native python ``datetime.time`` object  to a format supported by the API\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a Redis connection.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executes the sql and returns a pandas dataframe\n",
      "\n",
      "        :param sql: the sql statement to be executed (str) or a list of\n",
      "            sql statements to execute\n",
      "        :type sql: str or list\n",
      "        :param parameters: The parameters to render the SQL query with.\n",
      "        :type parameters: mapping or iterable\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Runs a command or a list of commands. Pass a list of sql\n",
      "        statements to the sql parameter to get them to execute\n",
      "        sequentially\n",
      "\n",
      "        :param sql: the sql statement to be executed (str) or a list of\n",
      "            sql statements to execute\n",
      "        :type sql: str or list\n",
      "        :param autocommit: What to set the connection's autocommit setting to\n",
      "            before executing the query.\n",
      "        :type autocommit: bool\n",
      "        :param parameters: The parameters to render the SQL query with.\n",
      "        :type parameters: mapping or iterable\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets the autocommit flag on the connection\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A generic way to insert a set of tuples into a table,\n",
      "        a new transaction is created every commit_every rows\n",
      "\n",
      "        :param table: Name of the target table\n",
      "        :type table: str\n",
      "        :param rows: The rows to insert into the table\n",
      "        :type rows: iterable of tuples\n",
      "        :param target_fields: The names of the columns to fill in the table\n",
      "        :type target_fields: iterable of strings\n",
      "        :param commit_every: The maximum number of rows to insert in one\n",
      "            transaction. Set to 0 to insert all rows in one transaction.\n",
      "        :type commit_every: int\n",
      "        :param replace: Whether to replace instead of insert\n",
      "        :type replace: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the SQL literal of the cell as a string.\n",
      "\n",
      "        :param cell: The cell to insert into the table\n",
      "        :type cell: object\n",
      "        :param conn: The database connection\n",
      "        :type conn: connection object\n",
      "        :return: The serialized cell\n",
      "        :rtype: str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "An endpoint helping check the health status of the Airflow instance,\n",
      "        including metadatabase and scheduler.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A restful endpoint that returns external links for a given Operator\n",
      "\n",
      "        It queries the operator that sent the request for the links it wishes\n",
      "        to provide for a given external link name.\n",
      "\n",
      "        API: GET\n",
      "        Args: dag_id: The id of the dag containing the task in question\n",
      "              task_id: The id of the task in question\n",
      "              execution_date: The date of execution of the task\n",
      "              link_name: The name of the link reference to find the actual URL for\n",
      "\n",
      "        Returns:\n",
      "            200: {url: <url of link>, error: None} - returned when there was no problem\n",
      "                finding the URL\n",
      "            404: {url: None, error: <error message>} - returned when the operator does\n",
      "                not return a URL\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens a connection to the cloudant service and closes it automatically if used as context manager.\n",
      "\n",
      "        .. note::\n",
      "            In the connection form:\n",
      "            - 'host' equals the 'Account' (optional)\n",
      "            - 'login' equals the 'Username (or API Key)' (required)\n",
      "            - 'password' equals the 'Password' (required)\n",
      "\n",
      "        :return: an authorized cloudant session context manager object.\n",
      "        :rtype: cloudant\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Call the SlackWebhookHook to post the provided Slack message\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the Credentials object for Google API\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns an authorized HTTP object to be used to build a Google cloud\n",
      "        service hook connection.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Function decorator that intercepts HTTP Errors and raises AirflowException\n",
      "        with more informative message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decorator that provides fallback for Google Cloud Platform project id. If\n",
      "        the project is None it will be replaced with the project_id from the\n",
      "        service account the Hook is authenticated with. Project id can be specified\n",
      "        either via project_id kwarg or via first parameter in positional args.\n",
      "\n",
      "        :param func: function to wrap\n",
      "        :return: result of the function call\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A list of states indicating that a task either has not completed\n",
      "        a run or has not even started.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Construct the spark-sql command to execute. Verbose output is enabled\n",
      "        as default.\n",
      "\n",
      "        :param cmd: command to append to the spark-sql command\n",
      "        :type cmd: str\n",
      "        :return: full command to be executed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
      "\n",
      "    See ``ToTensor`` for more details.\n",
      "\n",
      "    Args:\n",
      "        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
      "\n",
      "    Returns:\n",
      "        Tensor: Converted image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Normalize a tensor image with mean and standard deviation.\n",
      "\n",
      "    .. note::\n",
      "        This transform acts out of place by default, i.e., it does not mutates the input tensor.\n",
      "\n",
      "    See :class:`~torchvision.transforms.Normalize` for more details.\n",
      "\n",
      "    Args:\n",
      "        tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
      "        mean (sequence): Sequence of means for each channel.\n",
      "        std (sequence): Sequence of standard deviations for each channel.\n",
      "\n",
      "    Returns:\n",
      "        Tensor: Normalized Tensor image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "r\"\"\"Resize the input PIL Image to the given size.\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): Image to be resized.\n",
      "        size (sequence or int): Desired output size. If size is a sequence like\n",
      "            (h, w), the output size will be matched to this. If size is an int,\n",
      "            the smaller edge of the image will be matched to this number maintaing\n",
      "            the aspect ratio. i.e, if height > width, then image will be rescaled to\n",
      "            :math:`\\left(\\text{size} \\times \\frac{\\text{height}}{\\text{width}}, \\text{size}\\right)`\n",
      "        interpolation (int, optional): Desired interpolation. Default is\n",
      "            ``PIL.Image.BILINEAR``\n",
      "\n",
      "    Returns:\n",
      "        PIL Image: Resized image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "r\"\"\"Pad the given PIL Image on all sides with specified padding mode and fill value.\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): Image to be padded.\n",
      "        padding (int or tuple): Padding on each border. If a single int is provided this\n",
      "            is used to pad all borders. If tuple of length 2 is provided this is the padding\n",
      "            on left/right and top/bottom respectively. If a tuple of length 4 is provided\n",
      "            this is the padding for the left, top, right and bottom borders\n",
      "            respectively.\n",
      "        fill: Pixel fill value for constant fill. Default is 0. If a tuple of\n",
      "            length 3, it is used to fill R, G, B channels respectively.\n",
      "            This value is only used when the padding_mode is constant\n",
      "        padding_mode: Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.\n",
      "\n",
      "            - constant: pads with a constant value, this value is specified with fill\n",
      "\n",
      "            - edge: pads with the last value on the edge of the image\n",
      "\n",
      "            - reflect: pads with reflection of image (without repeating the last value on the edge)\n",
      "\n",
      "                       padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n",
      "                       will result in [3, 2, 1, 2, 3, 4, 3, 2]\n",
      "\n",
      "            - symmetric: pads with reflection of image (repeating the last value on the edge)\n",
      "\n",
      "                         padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n",
      "                         will result in [2, 1, 1, 2, 3, 4, 4, 3]\n",
      "\n",
      "    Returns:\n",
      "        PIL Image: Padded image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Crop the given PIL Image.\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): Image to be cropped.\n",
      "        i (int): i in (i,j) i.e coordinates of the upper left corner.\n",
      "        j (int): j in (i,j) i.e coordinates of the upper left corner.\n",
      "        h (int): Height of the cropped image.\n",
      "        w (int): Width of the cropped image.\n",
      "\n",
      "    Returns:\n",
      "        PIL Image: Cropped image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Crop the given PIL Image and resize it to desired size.\n",
      "\n",
      "    Notably used in :class:`~torchvision.transforms.RandomResizedCrop`.\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): Image to be cropped.\n",
      "        i (int): i in (i,j) i.e coordinates of the upper left corner\n",
      "        j (int): j in (i,j) i.e coordinates of the upper left corner\n",
      "        h (int): Height of the cropped image.\n",
      "        w (int): Width of the cropped image.\n",
      "        size (sequence or int): Desired output size. Same semantics as ``resize``.\n",
      "        interpolation (int, optional): Desired interpolation. Default is\n",
      "            ``PIL.Image.BILINEAR``.\n",
      "    Returns:\n",
      "        PIL Image: Cropped image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Horizontally flip the given PIL Image.\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): Image to be flipped.\n",
      "\n",
      "    Returns:\n",
      "        PIL Image:  Horizontall flipped image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Perform perspective transform of the given PIL Image.\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): Image to be transformed.\n",
      "        coeffs (tuple) : 8-tuple (a, b, c, d, e, f, g, h) which contains the coefficients.\n",
      "                            for a perspective transform.\n",
      "        interpolation: Default- Image.BICUBIC\n",
      "    Returns:\n",
      "        PIL Image:  Perspectively transformed Image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vertically flip the given PIL Image.\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): Image to be flipped.\n",
      "\n",
      "    Returns:\n",
      "        PIL Image:  Vertically flipped image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Crop the given PIL Image into four corners and the central crop.\n",
      "\n",
      "    .. Note::\n",
      "        This transform returns a tuple of images and there may be a\n",
      "        mismatch in the number of inputs and targets your ``Dataset`` returns.\n",
      "\n",
      "    Args:\n",
      "       size (sequence or int): Desired output size of the crop. If size is an\n",
      "           int instead of sequence like (h, w), a square crop (size, size) is\n",
      "           made.\n",
      "\n",
      "    Returns:\n",
      "       tuple: tuple (tl, tr, bl, br, center)\n",
      "                Corresponding top left, top right, bottom left, bottom right and center crop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjust brightness of an Image.\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): PIL Image to be adjusted.\n",
      "        brightness_factor (float):  How much to adjust the brightness. Can be\n",
      "            any non negative number. 0 gives a black image, 1 gives the\n",
      "            original image while 2 increases the brightness by a factor of 2.\n",
      "\n",
      "    Returns:\n",
      "        PIL Image: Brightness adjusted image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjust contrast of an Image.\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): PIL Image to be adjusted.\n",
      "        contrast_factor (float): How much to adjust the contrast. Can be any\n",
      "            non negative number. 0 gives a solid gray image, 1 gives the\n",
      "            original image while 2 increases the contrast by a factor of 2.\n",
      "\n",
      "    Returns:\n",
      "        PIL Image: Contrast adjusted image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjust color saturation of an image.\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): PIL Image to be adjusted.\n",
      "        saturation_factor (float):  How much to adjust the saturation. 0 will\n",
      "            give a black and white image, 1 will give the original image while\n",
      "            2 will enhance the saturation by a factor of 2.\n",
      "\n",
      "    Returns:\n",
      "        PIL Image: Saturation adjusted image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjust hue of an image.\n",
      "\n",
      "    The image hue is adjusted by converting the image to HSV and\n",
      "    cyclically shifting the intensities in the hue channel (H).\n",
      "    The image is then converted back to original image mode.\n",
      "\n",
      "    `hue_factor` is the amount of shift in H channel and must be in the\n",
      "    interval `[-0.5, 0.5]`.\n",
      "\n",
      "    See `Hue`_ for more details.\n",
      "\n",
      "    .. _Hue: https://en.wikipedia.org/wiki/Hue\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): PIL Image to be adjusted.\n",
      "        hue_factor (float):  How much to shift the hue channel. Should be in\n",
      "            [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in\n",
      "            HSV space in positive and negative direction respectively.\n",
      "            0 means no shift. Therefore, both -0.5 and 0.5 will give an image\n",
      "            with complementary colors while 0 gives the original image.\n",
      "\n",
      "    Returns:\n",
      "        PIL Image: Hue adjusted image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "r\"\"\"Perform gamma correction on an image.\n",
      "\n",
      "    Also known as Power Law Transform. Intensities in RGB mode are adjusted\n",
      "    based on the following equation:\n",
      "\n",
      "    .. math::\n",
      "        I_{\\text{out}} = 255 \\times \\text{gain} \\times \\left(\\frac{I_{\\text{in}}}{255}\\right)^{\\gamma}\n",
      "\n",
      "    See `Gamma Correction`_ for more details.\n",
      "\n",
      "    .. _Gamma Correction: https://en.wikipedia.org/wiki/Gamma_correction\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): PIL Image to be adjusted.\n",
      "        gamma (float): Non negative real number, same as :math:`\\gamma` in the equation.\n",
      "            gamma larger than 1 make the shadows darker,\n",
      "            while gamma smaller than 1 make dark regions lighter.\n",
      "        gain (float): The constant multiplier.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rotate the image by angle.\n",
      "\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): PIL Image to be rotated.\n",
      "        angle (float or int): In degrees degrees counter clockwise order.\n",
      "        resample (``PIL.Image.NEAREST`` or ``PIL.Image.BILINEAR`` or ``PIL.Image.BICUBIC``, optional):\n",
      "            An optional resampling filter. See `filters`_ for more information.\n",
      "            If omitted, or if the image has mode \"1\" or \"P\", it is set to ``PIL.Image.NEAREST``.\n",
      "        expand (bool, optional): Optional expansion flag.\n",
      "            If true, expands the output image to make it large enough to hold the entire rotated image.\n",
      "            If false or omitted, make the output image the same size as the input image.\n",
      "            Note that the expand flag assumes rotation around the center and no translation.\n",
      "        center (2-tuple, optional): Optional center of rotation.\n",
      "            Origin is the upper left corner.\n",
      "            Default is the center of the image.\n",
      "\n",
      "    .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Apply affine transformation on the image keeping image center invariant\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): PIL Image to be rotated.\n",
      "        angle (float or int): rotation angle in degrees between -180 and 180, clockwise direction.\n",
      "        translate (list or tuple of integers): horizontal and vertical translations (post-rotation translation)\n",
      "        scale (float): overall scale\n",
      "        shear (float): shear angle value in degrees between -180 to 180, clockwise direction.\n",
      "        resample (``PIL.Image.NEAREST`` or ``PIL.Image.BILINEAR`` or ``PIL.Image.BICUBIC``, optional):\n",
      "            An optional resampling filter.\n",
      "            See `filters`_ for more information.\n",
      "            If omitted, or if the image has mode \"1\" or \"P\", it is set to ``PIL.Image.NEAREST``.\n",
      "        fillcolor (int): Optional fill color for the area outside the transform in the output image. (Pillow>=5.0.0)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert image to grayscale version of image.\n",
      "\n",
      "    Args:\n",
      "        img (PIL Image): Image to be converted to grayscale.\n",
      "\n",
      "    Returns:\n",
      "        PIL Image: Grayscale version of the image.\n",
      "            if num_output_channels = 1 : returned image is single channel\n",
      "\n",
      "            if num_output_channels = 3 : returned image is 3 channel with r = g = b\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Save a given Tensor into an image file.\n",
      "\n",
      "    Args:\n",
      "        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n",
      "            saves the tensor as a grid of images by calling ``make_grid``.\n",
      "        **kwargs: Other arguments are documented in ``make_grid``.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Finds the class folders in a dataset.\n",
      "\n",
      "        Args:\n",
      "            dir (string): Root directory path.\n",
      "\n",
      "        Returns:\n",
      "            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
      "\n",
      "        Ensures:\n",
      "            No class is a subdirectory of another.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a Tensor containing the patches\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a Tensor containing the list of labels\n",
      "       Read the file and keep only the ID of the 3D point.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a Tensor containing the ground truth matches\n",
      "       Read the file and keep only 3D point ID.\n",
      "       Matches are represented with a 1, non matches with a 0.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Computes the accuracy over the k top predictions for the specified values of k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This function disables printing when not in master process\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download a file from a url and place it in root.\n",
      "\n",
      "    Args:\n",
      "        url (str): URL to download file from\n",
      "        root (str): Directory to place downloaded file in\n",
      "        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n",
      "        md5 (str, optional): MD5 checksum of the download. If None, do not check\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List all directories at a given root\n",
      "\n",
      "    Args:\n",
      "        root (str): Path to directory whose folders need to be listed\n",
      "        prefix (bool, optional): If true, prepends the path to each result, otherwise\n",
      "            only returns the name of the directories found\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List all files ending with a suffix at a given root\n",
      "\n",
      "    Args:\n",
      "        root (str): Path to directory whose folders need to be listed\n",
      "        suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png').\n",
      "            It uses the Python \"str.endswith\" method and is passed directly\n",
      "        prefix (bool, optional): If true, prepends the path to each result, otherwise\n",
      "            only returns the name of the files found\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download a Google Drive file from  and place it in root.\n",
      "\n",
      "    Args:\n",
      "        file_id (str): id of file to be downloaded\n",
      "        root (str): Directory to place downloaded file in\n",
      "        filename (str, optional): Name to save the file under. If None, use the id of the file.\n",
      "        md5 (str, optional): MD5 checksum of the download. If None, do not check\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get parameters for ``crop`` for a random crop.\n",
      "\n",
      "        Args:\n",
      "            img (PIL Image): Image to be cropped.\n",
      "            output_size (tuple): Expected output size of the crop.\n",
      "\n",
      "        Returns:\n",
      "            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get parameters for ``perspective`` for a random perspective transform.\n",
      "\n",
      "        Args:\n",
      "            width : width of the image.\n",
      "            height : height of the image.\n",
      "\n",
      "        Returns:\n",
      "            List containing [top-left, top-right, bottom-right, bottom-left] of the orignal image,\n",
      "            List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get parameters for ``crop`` for a random sized crop.\n",
      "\n",
      "        Args:\n",
      "            img (PIL Image): Image to be cropped.\n",
      "            scale (tuple): range of size of the origin size cropped\n",
      "            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n",
      "\n",
      "        Returns:\n",
      "            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n",
      "                sized crop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a randomized transform to be applied on image.\n",
      "\n",
      "        Arguments are same as that of __init__.\n",
      "\n",
      "        Returns:\n",
      "            Transform which randomly adjusts brightness, contrast and\n",
      "            saturation in a random order.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get parameters for affine transformation\n",
      "\n",
      "        Returns:\n",
      "            sequence: params to be passed to the affine transformation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download and extract the tarball, and download each individual photo.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download the MNIST data if it doesn't exist in processed_folder already.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download the EMNIST data if it doesn't exist in processed_folder already.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add a specific enqueue time to the message.\n",
      "\n",
      "        :param schedule_time: The scheduled time to enqueue the message.\n",
      "        :type schedule_time: ~datetime.datetime\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Defer the message.\n",
      "\n",
      "        This message will remain in the queue but must be received\n",
      "        specifically by its sequence number in order to be processed.\n",
      "\n",
      "        :raises: ~azure.servicebus.common.errors.MessageAlreadySettled if the message has been settled.\n",
      "        :raises: ~azure.servicebus.common.errors.MessageLockExpired if message lock has already expired.\n",
      "        :raises: ~azure.servicebus.common.errors.SessionLockExpired if session lock has already expired.\n",
      "        :raises: ~azure.servicebus.common.errors.MessageSettleFailed if message settle operation fails.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gives the sas-url to download the configurations for vpn-sites in a\n",
      "        resource group.\n",
      "\n",
      "        :param resource_group_name: The resource group name.\n",
      "        :type resource_group_name: str\n",
      "        :param virtual_wan_name: The name of the VirtualWAN for which\n",
      "         configuration of all vpn-sites is needed.\n",
      "        :type virtual_wan_name: str\n",
      "        :param vpn_sites: List of resource-ids of the vpn-sites for which\n",
      "         config is to be downloaded.\n",
      "        :type vpn_sites:\n",
      "         list[~azure.mgmt.network.v2018_04_01.models.SubResource]\n",
      "        :param output_blob_sas_url: The sas-url to download the configurations\n",
      "         for vpn-sites\n",
      "        :type output_blob_sas_url: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises:\n",
      "         :class:`ErrorException<azure.mgmt.network.v2018_04_01.models.ErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Guess Python Autorest options based on the spec path.\n",
      "\n",
      "    Expected path:\n",
      "    specification/compute/resource-manager/readme.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates a running PowerShell command with more data.\n",
      "\n",
      "        :param resource_group_name: The resource group name uniquely\n",
      "         identifies the resource group within the user subscriptionId.\n",
      "        :type resource_group_name: str\n",
      "        :param node_name: The node name (256 characters maximum).\n",
      "        :type node_name: str\n",
      "        :param session: The sessionId from the user.\n",
      "        :type session: str\n",
      "        :param pssession: The PowerShell sessionId from the user.\n",
      "        :type pssession: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns\n",
      "         PowerShellCommandResults or\n",
      "         ClientRawResponse<PowerShellCommandResults> if raw==True\n",
      "        :rtype:\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.servermanager.models.PowerShellCommandResults]\n",
      "         or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.servermanager.models.PowerShellCommandResults]]\n",
      "        :raises:\n",
      "         :class:`ErrorException<azure.mgmt.servermanager.models.ErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the managed application definition.\n",
      "\n",
      "        :param application_definition_id: The fully qualified ID of the\n",
      "         managed application definition, including the managed application name\n",
      "         and the managed application definition resource type. Use the format,\n",
      "         /subscriptions/{guid}/resourceGroups/{resource-group-name}/Microsoft.Solutions/applicationDefinitions/{applicationDefinition-name}\n",
      "        :type application_definition_id: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.mgmt.resource.managedapplications.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new managed application definition.\n",
      "\n",
      "        :param application_definition_id: The fully qualified ID of the\n",
      "         managed application definition, including the managed application name\n",
      "         and the managed application definition resource type. Use the format,\n",
      "         /subscriptions/{guid}/resourceGroups/{resource-group-name}/Microsoft.Solutions/applicationDefinitions/{applicationDefinition-name}\n",
      "        :type application_definition_id: str\n",
      "        :param parameters: Parameters supplied to the create or update a\n",
      "         managed application definition.\n",
      "        :type parameters:\n",
      "         ~azure.mgmt.resource.managedapplications.models.ApplicationDefinition\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns ApplicationDefinition\n",
      "         or ClientRawResponse<ApplicationDefinition> if raw==True\n",
      "        :rtype:\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.resource.managedapplications.models.ApplicationDefinition]\n",
      "         or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.resource.managedapplications.models.ApplicationDefinition]]\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.mgmt.resource.managedapplications.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the target uri for the request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create connection for the request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends request to cloud service server and return the response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executes script actions on the specified HDInsight cluster.\n",
      "\n",
      "        :param resource_group_name: The name of the resource group.\n",
      "        :type resource_group_name: str\n",
      "        :param cluster_name: The name of the cluster.\n",
      "        :type cluster_name: str\n",
      "        :param persist_on_success: Gets or sets if the scripts needs to be\n",
      "         persisted.\n",
      "        :type persist_on_success: bool\n",
      "        :param script_actions: The list of run time script actions.\n",
      "        :type script_actions:\n",
      "         list[~azure.mgmt.hdinsight.models.RuntimeScriptAction]\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.mgmt.hdinsight.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check the availability of a Front Door resource name.\n",
      "\n",
      "        :param name: The resource name to validate.\n",
      "        :type name: str\n",
      "        :param type: The type of the resource whose name is to be validated.\n",
      "         Possible values include: 'Microsoft.Network/frontDoors',\n",
      "         'Microsoft.Network/frontDoors/frontendEndpoints'\n",
      "        :type type: str or ~azure.mgmt.frontdoor.models.ResourceType\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: CheckNameAvailabilityOutput or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.mgmt.frontdoor.models.CheckNameAvailabilityOutput or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.mgmt.frontdoor.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Permanently deletes the specified vault. aka Purges the deleted Azure\n",
      "        key vault.\n",
      "\n",
      "        :param vault_name: The name of the soft-deleted vault.\n",
      "        :type vault_name: str\n",
      "        :param location: The location of the soft-deleted vault.\n",
      "        :type location: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the URI for the authorization server if present, otherwise empty string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Extracts the host authority from the given URI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a CLI profile class.\n",
      "\n",
      "    .. versionadded:: 1.1.6\n",
      "\n",
      "    :return: A CLI Profile\n",
      "    :rtype: azure.cli.core._profile.Profile\n",
      "    :raises: ImportError if azure-cli-core package is not available\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return Credentials and default SubscriptionID of current loaded profile of the CLI.\n",
      "\n",
      "    Credentials will be the \"az login\" command:\n",
      "    https://docs.microsoft.com/cli/azure/authenticate-azure-cli\n",
      "\n",
      "    Default subscription ID is either the only one you have, or you can define it:\n",
      "    https://docs.microsoft.com/cli/azure/manage-azure-subscriptions-azure-cli\n",
      "\n",
      "    .. versionadded:: 1.1.6\n",
      "\n",
      "    :param str resource: The alternative resource for credentials if not ARM (GraphRBac, etc.)\n",
      "    :param bool with_tenant: If True, return a three-tuple with last as tenant ID\n",
      "    :return: tuple of Credentials and SubscriptionID (and tenant ID if with_tenant)\n",
      "    :rtype: tuple\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets predictions for a given utterance, in the form of intents and\n",
      "        entities. The current maximum query size is 500 characters.\n",
      "\n",
      "        :param app_id: The LUIS application ID (Guid).\n",
      "        :type app_id: str\n",
      "        :param query: The utterance to predict.\n",
      "        :type query: str\n",
      "        :param timezone_offset: The timezone offset for the location of the\n",
      "         request.\n",
      "        :type timezone_offset: float\n",
      "        :param verbose: If true, return all intents instead of just the top\n",
      "         scoring intent.\n",
      "        :type verbose: bool\n",
      "        :param staging: Use the staging endpoint slot.\n",
      "        :type staging: bool\n",
      "        :param spell_check: Enable spell checking.\n",
      "        :type spell_check: bool\n",
      "        :param bing_spell_check_subscription_key: The subscription key to use\n",
      "         when enabling Bing spell check\n",
      "        :type bing_spell_check_subscription_key: str\n",
      "        :param log: Log query (default is true)\n",
      "        :type log: bool\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: LuisResult or ClientRawResponse if raw=true\n",
      "        :rtype:\n",
      "         ~azure.cognitiveservices.language.luis.runtime.models.LuisResult or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`APIErrorException<azure.cognitiveservices.language.luis.runtime.models.APIErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check Name Availability for global uniqueness.\n",
      "\n",
      "        :param location: The location in which uniqueness will be verified.\n",
      "        :type location: str\n",
      "        :param name: Resource Name To Verify\n",
      "        :type name: str\n",
      "        :param type: Fully qualified resource type which includes provider\n",
      "         namespace\n",
      "        :type type: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: CheckNameAvailabilityResponse or ClientRawResponse if\n",
      "         raw=true\n",
      "        :rtype: ~azure.mgmt.mixedreality.models.CheckNameAvailabilityResponse\n",
      "         or ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.mgmt.mixedreality.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens the request.\n",
      "\n",
      "        method:\n",
      "            the request VERB 'GET', 'POST', etc.\n",
      "        url:\n",
      "            the url to connect\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets up the timeout for the request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets the request header.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets back all response headers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends the request body.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets status of response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets status text of response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets response body as a SAFEARRAY and converts the SAFEARRAY to str.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets client certificate for the request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Connects to host and sends the request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends the headers of request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends request body.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the response and generates the _Response object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "simplified an id to be more friendly for us people\n",
      "----------------------------------------------------------------------------------------------------\n",
      "converts a Python name into a serializable name\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Verify whether two faces belong to a same person. Compares a face Id\n",
      "        with a Person Id.\n",
      "\n",
      "        :param face_id: FaceId of the face, comes from Face - Detect\n",
      "        :type face_id: str\n",
      "        :param person_id: Specify a certain person in a person group or a\n",
      "         large person group. personId is created in PersonGroup Person - Create\n",
      "         or LargePersonGroup Person - Create.\n",
      "        :type person_id: str\n",
      "        :param person_group_id: Using existing personGroupId and personId for\n",
      "         fast loading a specified person. personGroupId is created in\n",
      "         PersonGroup - Create. Parameter personGroupId and largePersonGroupId\n",
      "         should not be provided at the same time.\n",
      "        :type person_group_id: str\n",
      "        :param large_person_group_id: Using existing largePersonGroupId and\n",
      "         personId for fast loading a specified person. largePersonGroupId is\n",
      "         created in LargePersonGroup - Create. Parameter personGroupId and\n",
      "         largePersonGroupId should not be provided at the same time.\n",
      "        :type large_person_group_id: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: VerifyResult or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.cognitiveservices.vision.face.models.VerifyResult or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`APIErrorException<azure.cognitiveservices.vision.face.models.APIErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a job to the specified account.\n",
      "\n",
      "        The Batch service supports two ways to control the work done as part of\n",
      "        a job. In the first approach, the user specifies a Job Manager task.\n",
      "        The Batch service launches this task when it is ready to start the job.\n",
      "        The Job Manager task controls all other tasks that run under this job,\n",
      "        by using the Task APIs. In the second approach, the user directly\n",
      "        controls the execution of tasks under an active job, by using the Task\n",
      "        APIs. Also note: when naming jobs, avoid including sensitive\n",
      "        information such as user names or secret project names. This\n",
      "        information may appear in telemetry logs accessible to Microsoft\n",
      "        Support engineers.\n",
      "\n",
      "        :param job: The job to be added.\n",
      "        :type job: ~azure.batch.models.JobAddParameter\n",
      "        :param job_add_options: Additional parameters for the operation\n",
      "        :type job_add_options: ~azure.batch.models.JobAddOptions\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: None or ClientRawResponse if raw=true\n",
      "        :rtype: None or ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`BatchErrorException<azure.batch.models.BatchErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "get properties from entry xml\n",
      "----------------------------------------------------------------------------------------------------\n",
      "descends through a hierarchy of nodes returning the list of children\n",
      "        at the inner most level.  Only returns children who share a common parent,\n",
      "        not cousins.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Recursively searches from the parent to the child,\n",
      "        gathering all the applicable namespaces along the way\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts xml response to service bus namespace\n",
      "\n",
      "        The xml format for namespace:\n",
      "<entry>\n",
      "<id>uuid:00000000-0000-0000-0000-000000000000;id=0000000</id>\n",
      "<title type=\"text\">myunittests</title>\n",
      "<updated>2012-08-22T16:48:10Z</updated>\n",
      "<content type=\"application/xml\">\n",
      "    <NamespaceDescription\n",
      "        xmlns=\"http://schemas.microsoft.com/netservices/2010/10/servicebus/connect\"\n",
      "        xmlns:i=\"http://www.w3.org/2001/XMLSchema-instance\">\n",
      "    <Name>myunittests</Name>\n",
      "    <Region>West US</Region>\n",
      "    <DefaultKey>0000000000000000000000000000000000000000000=</DefaultKey>\n",
      "    <Status>Active</Status>\n",
      "    <CreatedAt>2012-08-22T16:48:10.217Z</CreatedAt>\n",
      "    <AcsManagementEndpoint>https://myunittests-sb.accesscontrol.windows.net/</AcsManagementEndpoint>\n",
      "    <ServiceBusEndpoint>https://myunittests.servicebus.windows.net/</ServiceBusEndpoint>\n",
      "    <ConnectionString>Endpoint=sb://myunittests.servicebus.windows.net/;SharedSecretIssuer=owner;SharedSecretValue=0000000000000000000000000000000000000000000=</ConnectionString>\n",
      "    <SubscriptionId>00000000000000000000000000000000</SubscriptionId>\n",
      "    <Enabled>true</Enabled>\n",
      "    </NamespaceDescription>\n",
      "</content>\n",
      "</entry>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts xml response to service bus region\n",
      "\n",
      "        The xml format for region:\n",
      "<entry>\n",
      "<id>uuid:157c311f-081f-4b4a-a0ba-a8f990ffd2a3;id=1756759</id>\n",
      "<title type=\"text\"></title>\n",
      "<updated>2013-04-10T18:25:29Z</updated>\n",
      "<content type=\"application/xml\">\n",
      "    <RegionCodeDescription\n",
      "        xmlns=\"http://schemas.microsoft.com/netservices/2010/10/servicebus/connect\"\n",
      "        xmlns:i=\"http://www.w3.org/2001/XMLSchema-instance\">\n",
      "    <Code>East Asia</Code>\n",
      "    <FullName>East Asia</FullName>\n",
      "    </RegionCodeDescription>\n",
      "</content>\n",
      "</entry>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts xml response to service bus namespace availability\n",
      "\n",
      "        The xml format:\n",
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<entry xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "    <id>uuid:9fc7c652-1856-47ab-8d74-cd31502ea8e6;id=3683292</id>\n",
      "    <title type=\"text\"></title>\n",
      "    <updated>2013-04-16T03:03:37Z</updated>\n",
      "    <content type=\"application/xml\">\n",
      "        <NamespaceAvailability\n",
      "            xmlns=\"http://schemas.microsoft.com/netservices/2010/10/servicebus/connect\"\n",
      "            xmlns:i=\"http://www.w3.org/2001/XMLSchema-instance\">\n",
      "            <Result>false</Result>\n",
      "        </NamespaceAvailability>\n",
      "    </content>\n",
      "</entry>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts xml response to service bus metrics objects\n",
      "\n",
      "        The xml format for MetricProperties\n",
      "<entry>\n",
      "    <id>https://sbgm.windows.net/Metrics(\\'listeners.active\\')</id>\n",
      "    <title/>\n",
      "    <updated>2014-10-09T11:56:50Z</updated>\n",
      "    <author>\n",
      "        <name/>\n",
      "    </author>\n",
      "    <content type=\"application/xml\">\n",
      "        <m:properties>\n",
      "            <d:Name>listeners.active</d:Name>\n",
      "            <d:PrimaryAggregation>Average</d:PrimaryAggregation>\n",
      "            <d:Unit>Count</d:Unit>\n",
      "            <d:DisplayName>Active listeners</d:DisplayName>\n",
      "        </m:properties>\n",
      "    </content>\n",
      "</entry>\n",
      "\n",
      "        The xml format for MetricValues\n",
      "    <entry>\n",
      "        <id>https://sbgm.windows.net/MetricValues(datetime\\'2014-10-02T00:00:00Z\\')</id>\n",
      "        <title/>\n",
      "        <updated>2014-10-09T18:38:28Z</updated>\n",
      "        <author>\n",
      "            <name/>\n",
      "        </author>\n",
      "        <content type=\"application/xml\">\n",
      "            <m:properties>\n",
      "                <d:Timestamp m:type=\"Edm.DateTime\">2014-10-02T00:00:00Z</d:Timestamp>\n",
      "                <d:Min m:type=\"Edm.Int64\">-118</d:Min>\n",
      "                <d:Max m:type=\"Edm.Int64\">15</d:Max>\n",
      "                <d:Average m:type=\"Edm.Single\">-78.44444</d:Average>\n",
      "                <d:Total m:type=\"Edm.Int64\">0</d:Total>\n",
      "            </m:properties>\n",
      "        </content>\n",
      "    </entry>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Replaces the runbook draft content.\n",
      "\n",
      "        :param resource_group_name: Name of an Azure Resource group.\n",
      "        :type resource_group_name: str\n",
      "        :param automation_account_name: The name of the automation account.\n",
      "        :type automation_account_name: str\n",
      "        :param runbook_name: The runbook name.\n",
      "        :type runbook_name: str\n",
      "        :param runbook_content: The runbook draft content.\n",
      "        :type runbook_content: Generator\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns object or\n",
      "         ClientRawResponse<object> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[Generator]\n",
      "         or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[Generator]]\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get domain name recommendations based on keywords.\n",
      "\n",
      "        Get domain name recommendations based on keywords.\n",
      "\n",
      "        :param keywords: Keywords to be used for generating domain\n",
      "         recommendations.\n",
      "        :type keywords: str\n",
      "        :param max_domain_recommendations: Maximum number of recommendations.\n",
      "        :type max_domain_recommendations: int\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: An iterator like instance of NameIdentifier\n",
      "        :rtype:\n",
      "         ~azure.mgmt.web.models.NameIdentifierPaged[~azure.mgmt.web.models.NameIdentifier]\n",
      "        :raises:\n",
      "         :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Asynchronous operation to modify a knowledgebase.\n",
      "\n",
      "        :param kb_id: Knowledgebase id.\n",
      "        :type kb_id: str\n",
      "        :param update_kb: Post body of the request.\n",
      "        :type update_kb:\n",
      "         ~azure.cognitiveservices.knowledge.qnamaker.models.UpdateKbOperationDTO\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: Operation or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.cognitiveservices.knowledge.qnamaker.models.Operation\n",
      "         or ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.cognitiveservices.knowledge.qnamaker.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets a collection that contains the object IDs of the groups of which\n",
      "        the user is a member.\n",
      "\n",
      "        :param object_id: The object ID of the user for which to get group\n",
      "         membership.\n",
      "        :type object_id: str\n",
      "        :param security_enabled_only: If true, only membership in\n",
      "         security-enabled groups should be checked. Otherwise, membership in\n",
      "         all groups should be checked.\n",
      "        :type security_enabled_only: bool\n",
      "        :param additional_properties: Unmatched properties from the message\n",
      "         are deserialized this collection\n",
      "        :type additional_properties: dict[str, object]\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: An iterator like instance of str\n",
      "        :rtype: ~azure.graphrbac.models.StrPaged[str]\n",
      "        :raises:\n",
      "         :class:`GraphErrorException<azure.graphrbac.models.GraphErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Will clone the given PR branch and vuild the package with the given name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Import data into Redis cache.\n",
      "\n",
      "        :param resource_group_name: The name of the resource group.\n",
      "        :type resource_group_name: str\n",
      "        :param name: The name of the Redis cache.\n",
      "        :type name: str\n",
      "        :param files: files to import.\n",
      "        :type files: list[str]\n",
      "        :param format: File format.\n",
      "        :type format: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Publish runbook draft.\n",
      "\n",
      "        :param resource_group_name: Name of an Azure Resource group.\n",
      "        :type resource_group_name: str\n",
      "        :param automation_account_name: The name of the automation account.\n",
      "        :type automation_account_name: str\n",
      "        :param runbook_name: The parameters supplied to the publish runbook\n",
      "         operation.\n",
      "        :type runbook_name: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Renew the message lock.\n",
      "\n",
      "        This will maintain the lock on the message to ensure\n",
      "        it is not returned to the queue to be reprocessed. In order to complete (or otherwise settle)\n",
      "        the message, the lock must be maintained. Messages received via ReceiveAndDelete mode are not\n",
      "        locked, and therefore cannot be renewed. This operation can also be performed as an asynchronous\n",
      "        background task by registering the message with an `azure.servicebus.aio.AutoLockRenew` instance.\n",
      "        This operation is only available for non-sessionful messages.\n",
      "\n",
      "        :raises: TypeError if the message is sessionful.\n",
      "        :raises: ~azure.servicebus.common.errors.MessageLockExpired is message lock has already expired.\n",
      "        :raises: ~azure.servicebus.common.errors.SessionLockExpired if session lock has already expired.\n",
      "        :raises: ~azure.servicebus.common.errors.MessageAlreadySettled is message has already been settled.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Replace alterations data.\n",
      "\n",
      "        :param word_alterations: Collection of word alterations.\n",
      "        :type word_alterations:\n",
      "         list[~azure.cognitiveservices.knowledge.qnamaker.models.AlterationsDTO]\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: None or ClientRawResponse if raw=true\n",
      "        :rtype: None or ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.cognitiveservices.knowledge.qnamaker.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds the specified value as a new version of the specified secret\n",
      "        resource.\n",
      "\n",
      "        Creates a new value of the specified secret resource. The name of the\n",
      "        value is typically the version identifier. Once created the value\n",
      "        cannot be changed.\n",
      "\n",
      "        :param secret_resource_name: The name of the secret resource.\n",
      "        :type secret_resource_name: str\n",
      "        :param secret_value_resource_name: The name of the secret resource\n",
      "         value which is typically the version identifier for the value.\n",
      "        :type secret_value_resource_name: str\n",
      "        :param name: Version identifier of the secret value.\n",
      "        :type name: str\n",
      "        :param value: The actual value of the secret.\n",
      "        :type value: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: SecretValueResourceDescription or ClientRawResponse if\n",
      "         raw=true\n",
      "        :rtype: ~azure.servicefabric.models.SecretValueResourceDescription or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns system properties for the specified storage account.\n",
      "\n",
      "        service_name:\n",
      "            Name of the storage service account.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the primary and secondary access keys for the specified\n",
      "        storage account.\n",
      "\n",
      "        service_name:\n",
      "            Name of the storage service account.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Regenerates the primary or secondary access key for the specified\n",
      "        storage account.\n",
      "\n",
      "        service_name:\n",
      "            Name of the storage service account.\n",
      "        key_type:\n",
      "            Specifies which key to regenerate. Valid values are:\n",
      "            Primary, Secondary\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new storage account in Windows Azure.\n",
      "\n",
      "        service_name:\n",
      "            A name for the storage account that is unique within Windows Azure.\n",
      "            Storage account names must be between 3 and 24 characters in length\n",
      "            and use numbers and lower-case letters only.\n",
      "        description:\n",
      "            A description for the storage account. The description may be up\n",
      "            to 1024 characters in length.\n",
      "        label:\n",
      "            A name for the storage account. The name may be up to 100\n",
      "            characters in length. The name can be used to identify the storage\n",
      "            account for your tracking purposes.\n",
      "        affinity_group:\n",
      "            The name of an existing affinity group in the specified\n",
      "            subscription. You can specify either a location or affinity_group,\n",
      "            but not both.\n",
      "        location:\n",
      "            The location where the storage account is created. You can specify\n",
      "            either a location or affinity_group, but not both.\n",
      "        geo_replication_enabled:\n",
      "            Deprecated. Replaced by the account_type parameter.\n",
      "        extended_properties:\n",
      "            Dictionary containing name/value pairs of storage account\n",
      "            properties. You can have a maximum of 50 extended property\n",
      "            name/value pairs. The maximum length of the Name element is 64\n",
      "            characters, only alphanumeric characters and underscores are valid\n",
      "            in the Name, and the name must start with a letter. The value has\n",
      "            a maximum length of 255 characters.\n",
      "        account_type:\n",
      "            Specifies whether the account supports locally-redundant storage,\n",
      "            geo-redundant storage, zone-redundant storage, or read access\n",
      "            geo-redundant storage.\n",
      "            Possible values are:\n",
      "                Standard_LRS, Standard_ZRS, Standard_GRS, Standard_RAGRS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the label, the description, and enables or disables the\n",
      "        geo-replication status for a storage account in Windows Azure.\n",
      "\n",
      "        service_name:\n",
      "            Name of the storage service account.\n",
      "        description:\n",
      "            A description for the storage account. The description may be up\n",
      "            to 1024 characters in length.\n",
      "        label:\n",
      "            A name for the storage account. The name may be up to 100\n",
      "            characters in length. The name can be used to identify the storage\n",
      "            account for your tracking purposes.\n",
      "        geo_replication_enabled:\n",
      "            Deprecated. Replaced by the account_type parameter.\n",
      "        extended_properties:\n",
      "            Dictionary containing name/value pairs of storage account\n",
      "            properties. You can have a maximum of 50 extended property\n",
      "            name/value pairs. The maximum length of the Name element is 64\n",
      "            characters, only alphanumeric characters and underscores are valid\n",
      "            in the Name, and the name must start with a letter. The value has\n",
      "            a maximum length of 255 characters.\n",
      "        account_type:\n",
      "            Specifies whether the account supports locally-redundant storage,\n",
      "            geo-redundant storage, zone-redundant storage, or read access\n",
      "            geo-redundant storage.\n",
      "            Possible values are:\n",
      "                Standard_LRS, Standard_ZRS, Standard_GRS, Standard_RAGRS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified storage account from Windows Azure.\n",
      "\n",
      "        service_name:\n",
      "            Name of the storage service account.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks to see if the specified storage account name is available, or\n",
      "        if it has already been taken.\n",
      "\n",
      "        service_name:\n",
      "            Name of the storage service account.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves system properties for the specified hosted service. These\n",
      "        properties include the service name and service type; the name of the\n",
      "        affinity group to which the service belongs, or its location if it is\n",
      "        not part of an affinity group; and optionally, information on the\n",
      "        service's deployments.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        embed_detail:\n",
      "            When True, the management service returns properties for all\n",
      "            deployments of the service, as well as for the service itself.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new hosted service in Windows Azure.\n",
      "\n",
      "        service_name:\n",
      "            A name for the hosted service that is unique within Windows Azure.\n",
      "            This name is the DNS prefix name and can be used to access the\n",
      "            hosted service.\n",
      "        label:\n",
      "            A name for the hosted service. The name can be up to 100 characters\n",
      "            in length. The name can be used to identify the storage account for\n",
      "            your tracking purposes.\n",
      "        description:\n",
      "            A description for the hosted service. The description can be up to\n",
      "            1024 characters in length.\n",
      "        location:\n",
      "            The location where the hosted service will be created. You can\n",
      "            specify either a location or affinity_group, but not both.\n",
      "        affinity_group:\n",
      "            The name of an existing affinity group associated with this\n",
      "            subscription. This name is a GUID and can be retrieved by examining\n",
      "            the name element of the response body returned by\n",
      "            list_affinity_groups. You can specify either a location or\n",
      "            affinity_group, but not both.\n",
      "        extended_properties:\n",
      "            Dictionary containing name/value pairs of storage account\n",
      "            properties. You can have a maximum of 50 extended property\n",
      "            name/value pairs. The maximum length of the Name element is 64\n",
      "            characters, only alphanumeric characters and underscores are valid\n",
      "            in the Name, and the name must start with a letter. The value has\n",
      "            a maximum length of 255 characters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified hosted service from Windows Azure.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        complete:\n",
      "            True if all OS/data disks and the source blobs for the disks should\n",
      "            also be deleted from storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uploads a new service package and creates a new deployment on staging\n",
      "        or production.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        deployment_slot:\n",
      "            The environment to which the hosted service is deployed. Valid\n",
      "            values are: staging, production\n",
      "        name:\n",
      "            The name for the deployment. The deployment name must be unique\n",
      "            among other deployments for the hosted service.\n",
      "        package_url:\n",
      "            A URL that refers to the location of the service package in the\n",
      "            Blob service. The service package can be located either in a\n",
      "            storage account beneath the same subscription or a Shared Access\n",
      "            Signature (SAS) URI from any storage account.\n",
      "        label:\n",
      "            A name for the hosted service. The name can be up to 100 characters\n",
      "            in length. It is recommended that the label be unique within the\n",
      "            subscription. The name can be used to identify the hosted service\n",
      "            for your tracking purposes.\n",
      "        configuration:\n",
      "            The base-64 encoded service configuration file for the deployment.\n",
      "        start_deployment:\n",
      "            Indicates whether to start the deployment immediately after it is\n",
      "            created. If false, the service model is still deployed to the\n",
      "            virtual machines but the code is not run immediately. Instead, the\n",
      "            service is Suspended until you call Update Deployment Status and\n",
      "            set the status to Running, at which time the service will be\n",
      "            started. A deployed service still incurs charges, even if it is\n",
      "            suspended.\n",
      "        treat_warnings_as_error:\n",
      "            Indicates whether to treat package validation warnings as errors.\n",
      "            If set to true, the Created Deployment operation fails if there\n",
      "            are validation warnings on the service package.\n",
      "        extended_properties:\n",
      "            Dictionary containing name/value pairs of storage account\n",
      "            properties. You can have a maximum of 50 extended property\n",
      "            name/value pairs. The maximum length of the Name element is 64\n",
      "            characters, only alphanumeric characters and underscores are valid\n",
      "            in the Name, and the name must start with a letter. The value has\n",
      "            a maximum length of 255 characters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified deployment.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initiates a virtual IP swap between the staging and production\n",
      "        deployment environments for a service. If the service is currently\n",
      "        running in the staging environment, it will be swapped to the\n",
      "        production environment. If it is running in the production\n",
      "        environment, it will be swapped to staging.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        production:\n",
      "            The name of the production deployment.\n",
      "        source_deployment:\n",
      "            The name of the source deployment.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initiates a change to the deployment configuration.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        configuration:\n",
      "            The base-64 encoded service configuration file for the deployment.\n",
      "        treat_warnings_as_error:\n",
      "            Indicates whether to treat package validation warnings as errors.\n",
      "            If set to true, the Created Deployment operation fails if there\n",
      "            are validation warnings on the service package.\n",
      "        mode:\n",
      "            If set to Manual, WalkUpgradeDomain must be called to apply the\n",
      "            update. If set to Auto, the Windows Azure platform will\n",
      "            automatically apply the update To each upgrade domain for the\n",
      "            service. Possible values are: Auto, Manual\n",
      "        extended_properties:\n",
      "            Dictionary containing name/value pairs of storage account\n",
      "            properties. You can have a maximum of 50 extended property\n",
      "            name/value pairs. The maximum length of the Name element is 64\n",
      "            characters, only alphanumeric characters and underscores are valid\n",
      "            in the Name, and the name must start with a letter. The value has\n",
      "            a maximum length of 255 characters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initiates a change in deployment status.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        status:\n",
      "            The change to initiate to the deployment status. Possible values\n",
      "            include:\n",
      "                Running, Suspended\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initiates an upgrade.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        mode:\n",
      "            If set to Manual, WalkUpgradeDomain must be called to apply the\n",
      "            update. If set to Auto, the Windows Azure platform will\n",
      "            automatically apply the update To each upgrade domain for the\n",
      "            service. Possible values are: Auto, Manual\n",
      "        package_url:\n",
      "            A URL that refers to the location of the service package in the\n",
      "            Blob service. The service package can be located either in a\n",
      "            storage account beneath the same subscription or a Shared Access\n",
      "            Signature (SAS) URI from any storage account.\n",
      "        configuration:\n",
      "            The base-64 encoded service configuration file for the deployment.\n",
      "        label:\n",
      "            A name for the hosted service. The name can be up to 100 characters\n",
      "            in length. It is recommended that the label be unique within the\n",
      "            subscription. The name can be used to identify the hosted service\n",
      "            for your tracking purposes.\n",
      "        force:\n",
      "            Specifies whether the rollback should proceed even when it will\n",
      "            cause local data to be lost from some role instances. True if the\n",
      "            rollback should proceed; otherwise false if the rollback should\n",
      "            fail.\n",
      "        role_to_upgrade:\n",
      "            The name of the specific role to upgrade.\n",
      "        extended_properties:\n",
      "            Dictionary containing name/value pairs of storage account\n",
      "            properties. You can have a maximum of 50 extended property\n",
      "            name/value pairs. The maximum length of the Name element is 64\n",
      "            characters, only alphanumeric characters and underscores are valid\n",
      "            in the Name, and the name must start with a letter. The value has\n",
      "            a maximum length of 255 characters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Specifies the next upgrade domain to be walked during manual in-place\n",
      "        upgrade or configuration change.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        upgrade_domain:\n",
      "            An integer value that identifies the upgrade domain to walk.\n",
      "            Upgrade domains are identified with a zero-based index: the first\n",
      "            upgrade domain has an ID of 0, the second has an ID of 1, and so on.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Requests a reboot of a role instance that is running in a deployment.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_instance_name:\n",
      "            The name of the role instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reinstalls the operating system on instances of web roles or worker\n",
      "        roles and initializes the storage resources that are used by them. If\n",
      "        you do not want to initialize storage resources, you can use\n",
      "        reimage_role_instance.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_instance_names:\n",
      "            List of role instance names.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks to see if the specified hosted service name is available, or if\n",
      "        it has already been taken.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists all of the service certificates associated with the specified\n",
      "        hosted service.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the public data for the specified X.509 certificate associated\n",
      "        with a hosted service.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        thumbalgorithm:\n",
      "            The algorithm for the certificate's thumbprint.\n",
      "        thumbprint:\n",
      "            The hexadecimal representation of the thumbprint.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a certificate to a hosted service.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        data:\n",
      "            The base-64 encoded form of the pfx/cer file.\n",
      "        certificate_format:\n",
      "            The service certificate format.\n",
      "        password:\n",
      "            The certificate password. Default to None when using cer format.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a service certificate from the certificate store of a hosted\n",
      "        service.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        thumbalgorithm:\n",
      "            The algorithm for the certificate's thumbprint.\n",
      "        thumbprint:\n",
      "            The hexadecimal representation of the thumbprint.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Get Management Certificate operation retrieves information about\n",
      "        the management certificate with the specified thumbprint. Management\n",
      "        certificates, which are also known as subscription certificates,\n",
      "        authenticate clients attempting to connect to resources associated\n",
      "        with your Windows Azure subscription.\n",
      "\n",
      "        thumbprint:\n",
      "            The thumbprint value of the certificate.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Add Management Certificate operation adds a certificate to the\n",
      "        list of management certificates. Management certificates, which are\n",
      "        also known as subscription certificates, authenticate clients\n",
      "        attempting to connect to resources associated with your Windows Azure\n",
      "        subscription.\n",
      "\n",
      "        public_key:\n",
      "            A base64 representation of the management certificate public key.\n",
      "        thumbprint:\n",
      "            The thumb print that uniquely identifies the management\n",
      "            certificate.\n",
      "        data:\n",
      "            The certificate's raw data in base-64 encoded .cer format.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Delete Management Certificate operation deletes a certificate from\n",
      "        the list of management certificates. Management certificates, which\n",
      "        are also known as subscription certificates, authenticate clients\n",
      "        attempting to connect to resources associated with your Windows Azure\n",
      "        subscription.\n",
      "\n",
      "        thumbprint:\n",
      "            The thumb print that uniquely identifies the management\n",
      "            certificate.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the system properties associated with the specified affinity\n",
      "        group.\n",
      "\n",
      "        affinity_group_name:\n",
      "            The name of the affinity group.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new affinity group for the specified subscription.\n",
      "\n",
      "        name:\n",
      "            A name for the affinity group that is unique to the subscription.\n",
      "        label:\n",
      "            A name for the affinity group. The name can be up to 100 characters\n",
      "            in length.\n",
      "        location:\n",
      "            The data center location where the affinity group will be created.\n",
      "            To list available locations, use the list_location function.\n",
      "        description:\n",
      "            A description for the affinity group. The description can be up to\n",
      "            1024 characters in length.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an affinity group in the specified subscription.\n",
      "\n",
      "        affinity_group_name:\n",
      "            The name of the affinity group.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List subscription operations.\n",
      "\n",
      "        start_time: Required. An ISO8601 date.\n",
      "        end_time: Required. An ISO8601 date.\n",
      "        object_id_filter: Optional. Returns subscription operations only for the specified object type and object ID\n",
      "        operation_result_filter: Optional. Returns subscription operations only for the specified result status, either Succeeded, Failed, or InProgress.\n",
      "        continuation_token: Optional.\n",
      "        More information at:\n",
      "        https://msdn.microsoft.com/en-us/library/azure/gg715318.aspx\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reserves an IPv4 address for the specified subscription.\n",
      "\n",
      "        name:\n",
      "            Required. Specifies the name for the reserved IP address.\n",
      "        label:\n",
      "            Optional. Specifies a label for the reserved IP address. The label\n",
      "            can be up to 100 characters long and can be used for your tracking\n",
      "            purposes.\n",
      "        location:\n",
      "            Required. Specifies the location of the reserved IP address. This\n",
      "            should be the same location that is assigned to the cloud service\n",
      "            containing the deployment that will use the reserved IP address.\n",
      "            To see the available locations, you can use list_locations.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a reserved IP address from the specified subscription.\n",
      "\n",
      "        name:\n",
      "            Required. Name of the reserved IP address.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Associate an existing reservedIP to a deployment.\n",
      "\n",
      "        name:\n",
      "            Required. Name of the reserved IP address.\n",
      "\n",
      "        service_name:\n",
      "            Required. Name of the hosted service.\n",
      "\n",
      "        deployment_name:\n",
      "            Required. Name of the deployment.\n",
      "\n",
      "        virtual_ip_name:\n",
      "            Optional. Name of the VirtualIP in case of multi Vip tenant.\n",
      "            If this value is not specified default virtualIP is used\n",
      "            for this operation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Disassociate an existing reservedIP from the given deployment.\n",
      "\n",
      "        name:\n",
      "            Required. Name of the reserved IP address.\n",
      "\n",
      "        service_name:\n",
      "            Required. Name of the hosted service.\n",
      "\n",
      "        deployment_name:\n",
      "            Required. Name of the deployment.\n",
      "\n",
      "        virtual_ip_name:\n",
      "            Optional. Name of the VirtualIP in case of multi Vip tenant.\n",
      "            If this value is not specified default virtualIP is used\n",
      "            for this operation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves information about the specified reserved IP address.\n",
      "\n",
      "        name:\n",
      "            Required. Name of the reserved IP address.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the specified virtual machine.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Provisions a virtual machine based on the supplied configuration.\n",
      "\n",
      "        service_name:\n",
      "            Name of the hosted service.\n",
      "        deployment_name:\n",
      "            The name for the deployment. The deployment name must be unique\n",
      "            among other deployments for the hosted service.\n",
      "        deployment_slot:\n",
      "            The environment to which the hosted service is deployed. Valid\n",
      "            values are: staging, production\n",
      "        label:\n",
      "            Specifies an identifier for the deployment. The label can be up to\n",
      "            100 characters long. The label can be used for tracking purposes.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "        system_config:\n",
      "            Contains the metadata required to provision a virtual machine from\n",
      "            a Windows or Linux OS image.  Use an instance of\n",
      "            WindowsConfigurationSet or LinuxConfigurationSet.\n",
      "        os_virtual_hard_disk:\n",
      "            Contains the parameters Windows Azure uses to create the operating\n",
      "            system disk for the virtual machine. If you are creating a Virtual\n",
      "            Machine by using a VM Image, this parameter is not used.\n",
      "        network_config:\n",
      "            Encapsulates the metadata required to create the virtual network\n",
      "            configuration for a virtual machine. If you do not include a\n",
      "            network configuration set you will not be able to access the VM\n",
      "            through VIPs over the internet. If your virtual machine belongs to\n",
      "            a virtual network you can not specify which subnet address space\n",
      "            it resides under. Use an instance of ConfigurationSet.\n",
      "        availability_set_name:\n",
      "            Specifies the name of an availability set to which to add the\n",
      "            virtual machine. This value controls the virtual machine\n",
      "            allocation in the Windows Azure environment. Virtual machines\n",
      "            specified in the same availability set are allocated to different\n",
      "            nodes to maximize availability.\n",
      "        data_virtual_hard_disks:\n",
      "            Contains the parameters Windows Azure uses to create a data disk\n",
      "            for a virtual machine.\n",
      "        role_size:\n",
      "            The size of the virtual machine to allocate. The default value is\n",
      "            Small. Possible values are: ExtraSmall,Small,Medium,Large,\n",
      "            ExtraLarge,A5,A6,A7,A8,A9,Basic_A0,Basic_A1,Basic_A2,Basic_A3,\n",
      "            Basic_A4,Standard_D1,Standard_D2,Standard_D3,Standard_D4,\n",
      "            Standard_D11,Standard_D12,Standard_D13,Standard_D14,Standard_G1,\n",
      "            Standard_G2,Sandard_G3,Standard_G4,Standard_G5. The specified\n",
      "            value must be compatible with the disk selected in the \n",
      "            OSVirtualHardDisk values.\n",
      "        role_type:\n",
      "            The type of the role for the virtual machine. The only supported\n",
      "            value is PersistentVMRole.\n",
      "        virtual_network_name:\n",
      "            Specifies the name of an existing virtual network to which the\n",
      "            deployment will belong.\n",
      "        resource_extension_references:\n",
      "            Optional. Contains a collection of resource extensions that are to\n",
      "            be installed on the Virtual Machine. This element is used if\n",
      "            provision_guest_agent is set to True. Use an iterable of instances\n",
      "            of ResourceExtensionReference.\n",
      "        provision_guest_agent:\n",
      "            Optional. Indicates whether the VM Agent is installed on the\n",
      "            Virtual Machine. To run a resource extension in a Virtual Machine,\n",
      "            this service must be installed.\n",
      "        vm_image_name:\n",
      "            Optional. Specifies the name of the VM Image that is to be used to\n",
      "            create the Virtual Machine. If this is specified, the\n",
      "            system_config and network_config parameters are not used.\n",
      "        media_location:\n",
      "            Optional. Required if the Virtual Machine is being created from a\n",
      "            published VM Image. Specifies the location of the VHD file that is\n",
      "            created when VMImageName specifies a published VM Image.\n",
      "        dns_servers:\n",
      "            Optional. List of DNS servers (use DnsServer class) to associate\n",
      "            with the Virtual Machine.\n",
      "        reserved_ip_name:\n",
      "            Optional. Specifies the name of a reserved IP address that is to be\n",
      "            assigned to the deployment. You must run create_reserved_ip_address\n",
      "            before you can assign the address to the deployment using this\n",
      "            element.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a virtual machine to an existing deployment.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "        system_config:\n",
      "            Contains the metadata required to provision a virtual machine from\n",
      "            a Windows or Linux OS image.  Use an instance of\n",
      "            WindowsConfigurationSet or LinuxConfigurationSet.\n",
      "        os_virtual_hard_disk:\n",
      "            Contains the parameters Windows Azure uses to create the operating\n",
      "            system disk for the virtual machine. If you are creating a Virtual\n",
      "            Machine by using a VM Image, this parameter is not used.\n",
      "        network_config:\n",
      "            Encapsulates the metadata required to create the virtual network\n",
      "            configuration for a virtual machine. If you do not include a\n",
      "            network configuration set you will not be able to access the VM\n",
      "            through VIPs over the internet. If your virtual machine belongs to\n",
      "            a virtual network you can not specify which subnet address space\n",
      "            it resides under.\n",
      "        availability_set_name:\n",
      "            Specifies the name of an availability set to which to add the\n",
      "            virtual machine. This value controls the virtual machine allocation\n",
      "            in the Windows Azure environment. Virtual machines specified in the\n",
      "            same availability set are allocated to different nodes to maximize\n",
      "            availability.\n",
      "        data_virtual_hard_disks:\n",
      "            Contains the parameters Windows Azure uses to create a data disk\n",
      "            for a virtual machine.\n",
      "        role_size:\n",
      "            The size of the virtual machine to allocate. The default value is\n",
      "            Small. Possible values are: ExtraSmall, Small, Medium, Large,\n",
      "            ExtraLarge. The specified value must be compatible with the disk\n",
      "            selected in the OSVirtualHardDisk values.\n",
      "        role_type:\n",
      "            The type of the role for the virtual machine. The only supported\n",
      "            value is PersistentVMRole.\n",
      "        resource_extension_references:\n",
      "            Optional. Contains a collection of resource extensions that are to\n",
      "            be installed on the Virtual Machine. This element is used if\n",
      "            provision_guest_agent is set to True.\n",
      "        provision_guest_agent:\n",
      "            Optional. Indicates whether the VM Agent is installed on the\n",
      "            Virtual Machine. To run a resource extension in a Virtual Machine,\n",
      "            this service must be installed.\n",
      "        vm_image_name:\n",
      "            Optional. Specifies the name of the VM Image that is to be used to\n",
      "            create the Virtual Machine. If this is specified, the\n",
      "            system_config and network_config parameters are not used.\n",
      "        media_location:\n",
      "            Optional. Required if the Virtual Machine is being created from a\n",
      "            published VM Image. Specifies the location of the VHD file that is\n",
      "            created when VMImageName specifies a published VM Image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the specified virtual machine.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "        os_virtual_hard_disk:\n",
      "            Contains the parameters Windows Azure uses to create the operating\n",
      "            system disk for the virtual machine.\n",
      "        network_config:\n",
      "            Encapsulates the metadata required to create the virtual network\n",
      "            configuration for a virtual machine. If you do not include a\n",
      "            network configuration set you will not be able to access the VM\n",
      "            through VIPs over the internet. If your virtual machine belongs to\n",
      "            a virtual network you can not specify which subnet address space\n",
      "            it resides under.\n",
      "        availability_set_name:\n",
      "            Specifies the name of an availability set to which to add the\n",
      "            virtual machine. This value controls the virtual machine allocation\n",
      "            in the Windows Azure environment. Virtual machines specified in the\n",
      "            same availability set are allocated to different nodes to maximize\n",
      "            availability.\n",
      "        data_virtual_hard_disks:\n",
      "            Contains the parameters Windows Azure uses to create a data disk\n",
      "            for a virtual machine.\n",
      "        role_size:\n",
      "            The size of the virtual machine to allocate. The default value is\n",
      "            Small. Possible values are: ExtraSmall, Small, Medium, Large,\n",
      "            ExtraLarge. The specified value must be compatible with the disk\n",
      "            selected in the OSVirtualHardDisk values.\n",
      "        role_type:\n",
      "            The type of the role for the virtual machine. The only supported\n",
      "            value is PersistentVMRole.\n",
      "        resource_extension_references:\n",
      "            Optional. Contains a collection of resource extensions that are to\n",
      "            be installed on the Virtual Machine. This element is used if\n",
      "            provision_guest_agent is set to True.\n",
      "        provision_guest_agent:\n",
      "            Optional. Indicates whether the VM Agent is installed on the\n",
      "            Virtual Machine. To run a resource extension in a Virtual Machine,\n",
      "            this service must be installed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified virtual machine.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "        complete:\n",
      "            True if all OS/data disks and the source blobs for the disks should\n",
      "            also be deleted from storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Capture Role operation captures a virtual machine image to your\n",
      "        image gallery. From the captured image, you can create additional\n",
      "        customized virtual machines.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "        post_capture_action:\n",
      "            Specifies the action after capture operation completes. Possible\n",
      "            values are: Delete, Reprovision.\n",
      "        target_image_name:\n",
      "            Specifies the image name of the captured virtual machine.\n",
      "        target_image_label:\n",
      "            Specifies the friendly name of the captured virtual machine.\n",
      "        provisioning_configuration:\n",
      "            Use an instance of WindowsConfigurationSet or LinuxConfigurationSet.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts the specified virtual machine.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts the specified virtual machines.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_names:\n",
      "            The names of the roles, as an enumerable of strings.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Restarts the specified virtual machine.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shuts down the specified virtual machine.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "        post_shutdown_action:\n",
      "            Specifies how the Virtual Machine should be shut down. Values are:\n",
      "                Stopped\n",
      "                    Shuts down the Virtual Machine but retains the compute\n",
      "                    resources. You will continue to be billed for the resources\n",
      "                    that the stopped machine uses.\n",
      "                StoppedDeallocated\n",
      "                    Shuts down the Virtual Machine and releases the compute\n",
      "                    resources. You are not billed for the compute resources that\n",
      "                    this Virtual Machine uses. If a static Virtual Network IP\n",
      "                    address is assigned to the Virtual Machine, it is reserved.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shuts down the specified virtual machines.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_names:\n",
      "            The names of the roles, as an enumerable of strings.\n",
      "        post_shutdown_action:\n",
      "            Specifies how the Virtual Machine should be shut down. Values are:\n",
      "                Stopped\n",
      "                    Shuts down the Virtual Machine but retains the compute\n",
      "                    resources. You will continue to be billed for the resources\n",
      "                    that the stopped machine uses.\n",
      "                StoppedDeallocated\n",
      "                    Shuts down the Virtual Machine and releases the compute\n",
      "                    resources. You are not billed for the compute resources that\n",
      "                    this Virtual Machine uses. If a static Virtual Network IP\n",
      "                    address is assigned to the Virtual Machine, it is reserved.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a DNS server definition to an existing deployment.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        dns_server_name:\n",
      "            Specifies the name of the DNS server.\n",
      "        address:\n",
      "            Specifies the IP address of the DNS server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the ip address of a DNS server.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        dns_server_name:\n",
      "            Specifies the name of the DNS server.\n",
      "        address:\n",
      "            Specifies the IP address of the DNS server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a DNS server from a deployment.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        dns_server_name:\n",
      "            Name of the DNS server that you want to delete.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists the versions of a resource extension that are available to add\n",
      "        to a Virtual Machine.\n",
      "\n",
      "        publisher_name:\n",
      "            Name of the resource extension publisher.\n",
      "        extension_name:\n",
      "            Name of the resource extension.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Replicate a VM image to multiple target locations. This operation\n",
      "        is only for publishers. You have to be registered as image publisher\n",
      "        with Microsoft Azure to be able to call this.\n",
      "\n",
      "        vm_image_name:\n",
      "            Specifies the name of the VM Image that is to be used for\n",
      "            replication\n",
      "        regions:\n",
      "            Specified a list of regions to replicate the image to\n",
      "            Note: The regions in the request body are not additive. If a VM\n",
      "            Image has already been replicated to Regions A, B, and C, and\n",
      "            a request is made to replicate to Regions A and D, the VM\n",
      "            Image will remain in Region A, will be replicated in Region D,\n",
      "            and will be unreplicated from Regions B and C\n",
      "        offer:\n",
      "            Specifies the publisher defined name of the offer. The allowed\n",
      "            characters are uppercase or lowercase letters, digit,\n",
      "            hypen(-), period (.).The maximum allowed length is 64 characters.\n",
      "        sku:\n",
      "            Specifies the publisher defined name of the Sku. The allowed\n",
      "            characters are uppercase or lowercase letters, digit,\n",
      "            hypen(-), period (.). The maximum allowed length is 64 characters.\n",
      "        version:\n",
      "            Specifies the publisher defined version of the image.\n",
      "            The allowed characters are digit and period.\n",
      "            Format: <MajorVersion>.<MinorVersion>.<Patch>\n",
      "            Example: '1.0.0' or '1.1.0' The 3 version number to\n",
      "            follow standard of most of the RPs. See http://semver.org\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Unreplicate a VM image from all regions This operation\n",
      "        is only for publishers. You have to be registered as image publisher\n",
      "        with Microsoft Azure to be able to call this\n",
      "\n",
      "        vm_image_name:\n",
      "            Specifies the name of the VM Image that is to be used for\n",
      "            unreplication. The VM Image Name should be the user VM Image,\n",
      "            not the published name of the VM Image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Share an already replicated OS image. This operation is only for\n",
      "        publishers. You have to be registered as image publisher with Windows\n",
      "        Azure to be able to call this.\n",
      "\n",
      "        vm_image_name:\n",
      "            The name of the virtual machine image to share\n",
      "        permission:\n",
      "            The sharing permission: public, msdn, or private\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a VM Image in the image repository that is associated with the\n",
      "        specified subscription using a specified set of virtual hard disks.\n",
      "\n",
      "        vm_image:\n",
      "            An instance of VMImage class.\n",
      "        vm_image.name: Required. Specifies the name of the image.\n",
      "        vm_image.label: Required. Specifies an identifier for the image.\n",
      "        vm_image.description: Optional. Specifies the description of the image.\n",
      "        vm_image.os_disk_configuration:\n",
      "            Required. Specifies configuration information for the operating \n",
      "            system disk that is associated with the image.\n",
      "        vm_image.os_disk_configuration.host_caching:\n",
      "            Optional. Specifies the caching behavior of the operating system disk.\n",
      "            Possible values are: None, ReadOnly, ReadWrite \n",
      "        vm_image.os_disk_configuration.os_state:\n",
      "            Required. Specifies the state of the operating system in the image.\n",
      "            Possible values are: Generalized, Specialized\n",
      "            A Virtual Machine that is fully configured and running contains a\n",
      "            Specialized operating system. A Virtual Machine on which the\n",
      "            Sysprep command has been run with the generalize option contains a\n",
      "            Generalized operating system.\n",
      "        vm_image.os_disk_configuration.os:\n",
      "            Required. Specifies the operating system type of the image.\n",
      "        vm_image.os_disk_configuration.media_link:\n",
      "            Required. Specifies the location of the blob in Windows Azure\n",
      "            storage. The blob location belongs to a storage account in the\n",
      "            subscription specified by the <subscription-id> value in the\n",
      "            operation call.\n",
      "        vm_image.data_disk_configurations:\n",
      "            Optional. Specifies configuration information for the data disks\n",
      "            that are associated with the image. A VM Image might not have data\n",
      "            disks associated with it.\n",
      "        vm_image.data_disk_configurations[].host_caching:\n",
      "            Optional. Specifies the caching behavior of the data disk.\n",
      "            Possible values are: None, ReadOnly, ReadWrite \n",
      "        vm_image.data_disk_configurations[].lun:\n",
      "            Optional if the lun for the disk is 0. Specifies the Logical Unit\n",
      "            Number (LUN) for the data disk.\n",
      "        vm_image.data_disk_configurations[].media_link:\n",
      "            Required. Specifies the location of the blob in Windows Azure\n",
      "            storage. The blob location belongs to a storage account in the\n",
      "            subscription specified by the <subscription-id> value in the\n",
      "            operation call.\n",
      "        vm_image.data_disk_configurations[].logical_size_in_gb:\n",
      "            Required. Specifies the size, in GB, of the data disk.\n",
      "        vm_image.language: Optional. Specifies the language of the image.\n",
      "        vm_image.image_family:\n",
      "            Optional. Specifies a value that can be used to group VM Images.\n",
      "        vm_image.recommended_vm_size:\n",
      "            Optional. Specifies the size to use for the Virtual Machine that\n",
      "            is created from the VM Image.\n",
      "        vm_image.eula:\n",
      "            Optional. Specifies the End User License Agreement that is\n",
      "            associated with the image. The value for this element is a string,\n",
      "            but it is recommended that the value be a URL that points to a EULA.\n",
      "        vm_image.icon_uri:\n",
      "            Optional. Specifies the URI to the icon that is displayed for the\n",
      "            image in the Management Portal.\n",
      "        vm_image.small_icon_uri:\n",
      "            Optional. Specifies the URI to the small icon that is displayed for\n",
      "            the image in the Management Portal.\n",
      "        vm_image.privacy_uri:\n",
      "            Optional. Specifies the URI that points to a document that contains\n",
      "            the privacy policy related to the image.\n",
      "        vm_image.published_date:\n",
      "            Optional. Specifies the date when the image was added to the image\n",
      "            repository.\n",
      "        vm_image.show_in_gui:\n",
      "            Optional. Indicates whether the VM Images should be listed in the\n",
      "            portal.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified VM Image from the image repository that is\n",
      "        associated with the specified subscription.\n",
      "\n",
      "        vm_image_name:\n",
      "            The name of the image.\n",
      "        delete_vhd:\n",
      "            Deletes the underlying vhd blob in Azure storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves a list of the VM Images from the image repository that is\n",
      "        associated with the specified subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates a VM Image in the image repository that is associated with the\n",
      "        specified subscription.\n",
      "\n",
      "        vm_image_name:\n",
      "            Name of image to update.\n",
      "        vm_image:\n",
      "            An instance of VMImage class.\n",
      "        vm_image.label: Optional. Specifies an identifier for the image.\n",
      "        vm_image.os_disk_configuration:\n",
      "            Required. Specifies configuration information for the operating \n",
      "            system disk that is associated with the image.\n",
      "        vm_image.os_disk_configuration.host_caching:\n",
      "            Optional. Specifies the caching behavior of the operating system disk.\n",
      "            Possible values are: None, ReadOnly, ReadWrite \n",
      "        vm_image.data_disk_configurations:\n",
      "            Optional. Specifies configuration information for the data disks\n",
      "            that are associated with the image. A VM Image might not have data\n",
      "            disks associated with it.\n",
      "        vm_image.data_disk_configurations[].name:\n",
      "            Required. Specifies the name of the data disk.\n",
      "        vm_image.data_disk_configurations[].host_caching:\n",
      "            Optional. Specifies the caching behavior of the data disk.\n",
      "            Possible values are: None, ReadOnly, ReadWrite \n",
      "        vm_image.data_disk_configurations[].lun:\n",
      "            Optional if the lun for the disk is 0. Specifies the Logical Unit\n",
      "            Number (LUN) for the data disk.\n",
      "        vm_image.description: Optional. Specifies the description of the image.\n",
      "        vm_image.language: Optional. Specifies the language of the image.\n",
      "        vm_image.image_family:\n",
      "            Optional. Specifies a value that can be used to group VM Images.\n",
      "        vm_image.recommended_vm_size:\n",
      "            Optional. Specifies the size to use for the Virtual Machine that\n",
      "            is created from the VM Image.\n",
      "        vm_image.eula:\n",
      "            Optional. Specifies the End User License Agreement that is\n",
      "            associated with the image. The value for this element is a string,\n",
      "            but it is recommended that the value be a URL that points to a EULA.\n",
      "        vm_image.icon_uri:\n",
      "            Optional. Specifies the URI to the icon that is displayed for the\n",
      "            image in the Management Portal.\n",
      "        vm_image.small_icon_uri:\n",
      "            Optional. Specifies the URI to the small icon that is displayed for\n",
      "            the image in the Management Portal.\n",
      "        vm_image.privacy_uri:\n",
      "            Optional. Specifies the URI that points to a document that contains\n",
      "            the privacy policy related to the image.\n",
      "        vm_image.published_date:\n",
      "            Optional. Specifies the date when the image was added to the image\n",
      "            repository.\n",
      "        vm_image.show_in_gui:\n",
      "            Optional. Indicates whether the VM Images should be listed in the\n",
      "            portal.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds an OS image that is currently stored in a storage account in your\n",
      "        subscription to the image repository.\n",
      "\n",
      "        label:\n",
      "            Specifies the friendly name of the image.\n",
      "        media_link:\n",
      "            Specifies the location of the blob in Windows Azure blob store\n",
      "            where the media for the image is located. The blob location must\n",
      "            belong to a storage account in the subscription specified by the\n",
      "            <subscription-id> value in the operation call. Example:\n",
      "            http://example.blob.core.windows.net/disks/mydisk.vhd\n",
      "        name:\n",
      "            Specifies a name for the OS image that Windows Azure uses to\n",
      "            identify the image when creating one or more virtual machines.\n",
      "        os:\n",
      "            The operating system type of the OS image. Possible values are:\n",
      "            Linux, Windows\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates an OS image that in your image repository.\n",
      "\n",
      "        image_name:\n",
      "            The name of the image to update.\n",
      "        label:\n",
      "            Specifies the friendly name of the image to be updated. You cannot\n",
      "            use this operation to update images provided by the Windows Azure\n",
      "            platform.\n",
      "        media_link:\n",
      "            Specifies the location of the blob in Windows Azure blob store\n",
      "            where the media for the image is located. The blob location must\n",
      "            belong to a storage account in the subscription specified by the\n",
      "            <subscription-id> value in the operation call. Example:\n",
      "            http://example.blob.core.windows.net/disks/mydisk.vhd\n",
      "        name:\n",
      "            Specifies a name for the OS image that Windows Azure uses to\n",
      "            identify the image when creating one or more VM Roles.\n",
      "        os:\n",
      "            The operating system type of the OS image. Possible values are:\n",
      "            Linux, Windows\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates metadata elements from a given OS image reference.\n",
      "\n",
      "        image_name:\n",
      "            The name of the image to update.\n",
      "        os_image:\n",
      "            An instance of OSImage class.\n",
      "        os_image.label: Optional. Specifies an identifier for the image.\n",
      "        os_image.description: Optional. Specifies the description of the image.\n",
      "        os_image.language: Optional. Specifies the language of the image.\n",
      "        os_image.image_family:\n",
      "            Optional. Specifies a value that can be used to group VM Images.\n",
      "        os_image.recommended_vm_size:\n",
      "            Optional. Specifies the size to use for the Virtual Machine that\n",
      "            is created from the VM Image.\n",
      "        os_image.eula:\n",
      "            Optional. Specifies the End User License Agreement that is\n",
      "            associated with the image. The value for this element is a string,\n",
      "            but it is recommended that the value be a URL that points to a EULA.\n",
      "        os_image.icon_uri:\n",
      "            Optional. Specifies the URI to the icon that is displayed for the\n",
      "            image in the Management Portal.\n",
      "        os_image.small_icon_uri:\n",
      "            Optional. Specifies the URI to the small icon that is displayed for\n",
      "            the image in the Management Portal.\n",
      "        os_image.privacy_uri:\n",
      "            Optional. Specifies the URI that points to a document that contains\n",
      "            the privacy policy related to the image.\n",
      "        os_image.published_date:\n",
      "            Optional. Specifies the date when the image was added to the image\n",
      "            repository.\n",
      "        os.image.media_link:\n",
      "            Required: Specifies the location of the blob in Windows Azure\n",
      "            blob store where the media for the image is located. The blob\n",
      "            location must belong to a storage account in the subscription\n",
      "            specified by the <subscription-id> value in the operation call.\n",
      "            Example:\n",
      "            http://example.blob.core.windows.net/disks/mydisk.vhd\n",
      "        os_image.name:\n",
      "            Specifies a name for the OS image that Windows Azure uses to\n",
      "            identify the image when creating one or more VM Roles.\n",
      "        os_image.os:\n",
      "            The operating system type of the OS image. Possible values are:\n",
      "            Linux, Windows\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified OS image from your image repository.\n",
      "\n",
      "        image_name:\n",
      "            The name of the image.\n",
      "        delete_vhd:\n",
      "            Deletes the underlying vhd blob in Azure storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the specified data disk from a virtual machine.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "        lun:\n",
      "            The Logical Unit Number (LUN) for the disk.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a data disk to a virtual machine.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "        lun:\n",
      "            Specifies the Logical Unit Number (LUN) for the disk. The LUN\n",
      "            specifies the slot in which the data drive appears when mounted\n",
      "            for usage by the virtual machine. Valid LUN values are 0 through 15.\n",
      "        host_caching:\n",
      "            Specifies the platform caching behavior of data disk blob for\n",
      "            read/write efficiency. The default vault is ReadOnly. Possible\n",
      "            values are: None, ReadOnly, ReadWrite\n",
      "        media_link:\n",
      "            Specifies the location of the blob in Windows Azure blob store\n",
      "            where the media for the disk is located. The blob location must\n",
      "            belong to the storage account in the subscription specified by the\n",
      "            <subscription-id> value in the operation call. Example:\n",
      "            http://example.blob.core.windows.net/disks/mydisk.vhd\n",
      "        disk_label:\n",
      "            Specifies the description of the data disk. When you attach a disk,\n",
      "            either by directly referencing a media using the MediaLink element\n",
      "            or specifying the target disk size, you can use the DiskLabel\n",
      "            element to customize the name property of the target data disk.\n",
      "        disk_name:\n",
      "            Specifies the name of the disk. Windows Azure uses the specified\n",
      "            disk to create the data disk for the machine and populates this\n",
      "            field with the disk name.\n",
      "        logical_disk_size_in_gb:\n",
      "            Specifies the size, in GB, of an empty disk to be attached to the\n",
      "            role. The disk can be created as part of disk attach or create VM\n",
      "            role call by specifying the value for this property. Windows Azure\n",
      "            creates the empty disk based on size preference and attaches the\n",
      "            newly created disk to the Role.\n",
      "        source_media_link:\n",
      "            Specifies the location of a blob in account storage which is\n",
      "            mounted as a data disk when the virtual machine is created.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the specified data disk attached to the specified virtual\n",
      "        machine.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "        lun:\n",
      "            Specifies the Logical Unit Number (LUN) for the disk. The LUN\n",
      "            specifies the slot in which the data drive appears when mounted\n",
      "            for usage by the virtual machine. Valid LUN values are 0 through\n",
      "            15.\n",
      "        host_caching:\n",
      "            Specifies the platform caching behavior of data disk blob for\n",
      "            read/write efficiency. The default vault is ReadOnly. Possible\n",
      "            values are: None, ReadOnly, ReadWrite\n",
      "        media_link:\n",
      "            Specifies the location of the blob in Windows Azure blob store\n",
      "            where the media for the disk is located. The blob location must\n",
      "            belong to the storage account in the subscription specified by\n",
      "            the <subscription-id> value in the operation call. Example:\n",
      "            http://example.blob.core.windows.net/disks/mydisk.vhd\n",
      "        updated_lun:\n",
      "            Specifies the Logical Unit Number (LUN) for the disk. The LUN\n",
      "            specifies the slot in which the data drive appears when mounted\n",
      "            for usage by the virtual machine. Valid LUN values are 0 through 15.\n",
      "        disk_label:\n",
      "            Specifies the description of the data disk. When you attach a disk,\n",
      "            either by directly referencing a media using the MediaLink element\n",
      "            or specifying the target disk size, you can use the DiskLabel\n",
      "            element to customize the name property of the target data disk.\n",
      "        disk_name:\n",
      "            Specifies the name of the disk. Windows Azure uses the specified\n",
      "            disk to create the data disk for the machine and populates this\n",
      "            field with the disk name.\n",
      "        logical_disk_size_in_gb:\n",
      "            Specifies the size, in GB, of an empty disk to be attached to the\n",
      "            role. The disk can be created as part of disk attach or create VM\n",
      "            role call by specifying the value for this property. Windows Azure\n",
      "            creates the empty disk based on size preference and attaches the\n",
      "            newly created disk to the Role.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Removes the specified data disk from a virtual machine.\n",
      "\n",
      "        service_name:\n",
      "            The name of the service.\n",
      "        deployment_name:\n",
      "            The name of the deployment.\n",
      "        role_name:\n",
      "            The name of the role.\n",
      "        lun:\n",
      "            The Logical Unit Number (LUN) for the disk.\n",
      "        delete_vhd:\n",
      "            Deletes the underlying vhd blob in Azure storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a disk to the user image repository. The disk can be an OS disk\n",
      "        or a data disk.\n",
      "\n",
      "        has_operating_system:\n",
      "            Deprecated.\n",
      "        label:\n",
      "            Specifies the description of the disk.\n",
      "        media_link:\n",
      "            Specifies the location of the blob in Windows Azure blob store\n",
      "            where the media for the disk is located. The blob location must\n",
      "            belong to the storage account in the current subscription specified\n",
      "            by the <subscription-id> value in the operation call. Example:\n",
      "            http://example.blob.core.windows.net/disks/mydisk.vhd\n",
      "        name:\n",
      "            Specifies a name for the disk. Windows Azure uses the name to\n",
      "            identify the disk when creating virtual machines from the disk.\n",
      "        os:\n",
      "            The OS type of the disk. Possible values are: Linux, Windows\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates an existing disk in your image repository.\n",
      "\n",
      "        disk_name:\n",
      "            The name of the disk to update.\n",
      "        has_operating_system:\n",
      "            Deprecated.\n",
      "        label:\n",
      "            Specifies the description of the disk.\n",
      "        media_link:\n",
      "            Deprecated.\n",
      "        name:\n",
      "            Deprecated.\n",
      "        os:\n",
      "            Deprecated.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified data or operating system disk from your image\n",
      "        repository.\n",
      "\n",
      "        disk_name:\n",
      "            The name of the disk to delete.\n",
      "        delete_vhd:\n",
      "            Deletes the underlying vhd blob in Azure storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Summarizes policy states for the resources under the management group.\n",
      "\n",
      "        :param management_group_name: Management group name.\n",
      "        :type management_group_name: str\n",
      "        :param query_options: Additional parameters for the operation\n",
      "        :type query_options: ~azure.mgmt.policyinsights.models.QueryOptions\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: SummarizeResults or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.mgmt.policyinsights.models.SummarizeResults or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`QueryFailureException<azure.mgmt.policyinsights.models.QueryFailureException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This is a temporary patch pending a fix in uAMQP.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Receive a batch of messages at once.\n",
      "\n",
      "        This approach it optimal if you wish to process multiple messages simultaneously. Note that the\n",
      "        number of messages retrieved in a single batch will be dependent on\n",
      "        whether `prefetch` was set for the receiver. This call will prioritize returning\n",
      "        quickly over meeting a specified batch size, and so will return as soon as at least\n",
      "        one message is received and there is a gap in incoming messages regardless\n",
      "        of the specified batch size.\n",
      "\n",
      "        :param max_batch_size: Maximum number of messages in the batch. Actual number\n",
      "         returned will depend on prefetch size and incoming stream rate.\n",
      "        :type max_batch_size: int\n",
      "        :param timeout: The time to wait in seconds for the first message to arrive.\n",
      "         If no messages arrive, and no timeout is specified, this call will not return\n",
      "         until the connection is closed. If specified, an no messages arrive within the\n",
      "         timeout period, an empty list will be returned.\n",
      "        :rtype: list[~azure.servicebus.common.message.Message]\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/test_examples.py\n",
      "                :start-after: [START fetch_next_messages]\n",
      "                :end-before: [END fetch_next_messages]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Get the messages in batch from the receiver\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Renew the session lock.\n",
      "\n",
      "        This operation must be performed periodically in order to retain a lock on the\n",
      "        session to continue message processing.\n",
      "        Once the lock is lost the connection will be closed. This operation can\n",
      "        also be performed as a threaded background task by registering the session\n",
      "        with an `azure.servicebus.AutoLockRenew` instance.\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/test_examples.py\n",
      "                :start-after: [START renew_lock]\n",
      "                :end-before: [END renew_lock]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Renew the session lock before it expires\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create or update a VM scale set.\n",
      "\n",
      "        :param resource_group_name: The name of the resource group.\n",
      "        :type resource_group_name: str\n",
      "        :param vm_scale_set_name: The name of the VM scale set to create or\n",
      "         update.\n",
      "        :type vm_scale_set_name: str\n",
      "        :param parameters: The scale set object.\n",
      "        :type parameters:\n",
      "         ~azure.mgmt.compute.v2019_03_01.models.VirtualMachineScaleSet\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns VirtualMachineScaleSet\n",
      "         or ClientRawResponse<VirtualMachineScaleSet> if raw==True\n",
      "        :rtype:\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.compute.v2019_03_01.models.VirtualMachineScaleSet]\n",
      "         or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.compute.v2019_03_01.models.VirtualMachineScaleSet]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts SinglePlacementGroup property to false for a existing virtual\n",
      "        machine scale set.\n",
      "\n",
      "        :param resource_group_name: The name of the resource group.\n",
      "        :type resource_group_name: str\n",
      "        :param vm_scale_set_name: The name of the virtual machine scale set to\n",
      "         create or update.\n",
      "        :type vm_scale_set_name: str\n",
      "        :param active_placement_group_id: Id of the placement group in which\n",
      "         you want future virtual machine instances to be placed. To query\n",
      "         placement group Id, please use Virtual Machine Scale Set VMs - Get\n",
      "         API. If not provided, the platform will choose one with maximum number\n",
      "         of virtual machine instances.\n",
      "        :type active_placement_group_id: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: None or ClientRawResponse if raw=true\n",
      "        :rtype: None or ~msrest.pipeline.ClientRawResponse\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Detect profanity and match against custom and shared blacklists.\n",
      "\n",
      "        Detects profanity in more than 100 languages and match against custom\n",
      "        and shared blacklists.\n",
      "\n",
      "        :param text_content_type: The content type. Possible values include:\n",
      "         'text/plain', 'text/html', 'text/xml', 'text/markdown'\n",
      "        :type text_content_type: str\n",
      "        :param text_content: Content to screen.\n",
      "        :type text_content: Generator\n",
      "        :param language: Language of the text.\n",
      "        :type language: str\n",
      "        :param autocorrect: Autocorrect text.\n",
      "        :type autocorrect: bool\n",
      "        :param pii: Detect personal identifiable information.\n",
      "        :type pii: bool\n",
      "        :param list_id: The list Id.\n",
      "        :type list_id: str\n",
      "        :param classify: Classify input.\n",
      "        :type classify: bool\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param callback: When specified, will be called with each chunk of\n",
      "         data that is streamed. The callback should take two arguments, the\n",
      "         bytes of the current chunk of data and the response object. If the\n",
      "         data is uploading, response will be None.\n",
      "        :type callback: Callable[Bytes, response=None]\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: Screen or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.cognitiveservices.vision.contentmoderator.models.Screen\n",
      "         or ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`APIErrorException<azure.cognitiveservices.vision.contentmoderator.models.APIErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new key, stores it, then returns key parameters and\n",
      "        attributes to the client.\n",
      "\n",
      "        The create key operation can be used to create any key type in Azure\n",
      "        Key Vault. If the named key already exists, Azure Key Vault creates a\n",
      "        new version of the key. It requires the keys/create permission.\n",
      "\n",
      "        :param vault_base_url: The vault name, for example\n",
      "         https://myvault.vault.azure.net.\n",
      "        :type vault_base_url: str\n",
      "        :param key_name: The name for the new key. The system will generate\n",
      "         the version name for the new key.\n",
      "        :type key_name: str\n",
      "        :param kty: The type of key to create. For valid values, see\n",
      "         JsonWebKeyType. Possible values include: 'EC', 'EC-HSM', 'RSA',\n",
      "         'RSA-HSM', 'oct'\n",
      "        :type kty: str or ~azure.keyvault.v2016_10_01.models.JsonWebKeyType\n",
      "        :param key_size: The key size in bits. For example: 2048, 3072, or\n",
      "         4096 for RSA.\n",
      "        :type key_size: int\n",
      "        :param key_ops:\n",
      "        :type key_ops: list[str or\n",
      "         ~azure.keyvault.v2016_10_01.models.JsonWebKeyOperation]\n",
      "        :param key_attributes:\n",
      "        :type key_attributes: ~azure.keyvault.v2016_10_01.models.KeyAttributes\n",
      "        :param tags: Application specific metadata in the form of key-value\n",
      "         pairs.\n",
      "        :type tags: dict[str, str]\n",
      "        :param curve: Elliptic curve name. For valid values, see\n",
      "         JsonWebKeyCurveName. Possible values include: 'P-256', 'P-384',\n",
      "         'P-521', 'SECP256K1'\n",
      "        :type curve: str or\n",
      "         ~azure.keyvault.v2016_10_01.models.JsonWebKeyCurveName\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: KeyBundle or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.keyvault.v2016_10_01.models.KeyBundle or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`KeyVaultErrorException<azure.keyvault.v2016_10_01.models.KeyVaultErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Imports an externally created key, stores it, and returns key\n",
      "        parameters and attributes to the client.\n",
      "\n",
      "        The import key operation may be used to import any key type into an\n",
      "        Azure Key Vault. If the named key already exists, Azure Key Vault\n",
      "        creates a new version of the key. This operation requires the\n",
      "        keys/import permission.\n",
      "\n",
      "        :param vault_base_url: The vault name, for example\n",
      "         https://myvault.vault.azure.net.\n",
      "        :type vault_base_url: str\n",
      "        :param key_name: Name for the imported key.\n",
      "        :type key_name: str\n",
      "        :param key: The Json web key\n",
      "        :type key: ~azure.keyvault.v2016_10_01.models.JsonWebKey\n",
      "        :param hsm: Whether to import as a hardware key (HSM) or software key.\n",
      "        :type hsm: bool\n",
      "        :param key_attributes: The key management attributes.\n",
      "        :type key_attributes: ~azure.keyvault.v2016_10_01.models.KeyAttributes\n",
      "        :param tags: Application specific metadata in the form of key-value\n",
      "         pairs.\n",
      "        :type tags: dict[str, str]\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: KeyBundle or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.keyvault.v2016_10_01.models.KeyBundle or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`KeyVaultErrorException<azure.keyvault.v2016_10_01.models.KeyVaultErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The update key operation changes specified attributes of a stored key\n",
      "        and can be applied to any key type and key version stored in Azure Key\n",
      "        Vault.\n",
      "\n",
      "        In order to perform this operation, the key must already exist in the\n",
      "        Key Vault. Note: The cryptographic material of a key itself cannot be\n",
      "        changed. This operation requires the keys/update permission.\n",
      "\n",
      "        :param vault_base_url: The vault name, for example\n",
      "         https://myvault.vault.azure.net.\n",
      "        :type vault_base_url: str\n",
      "        :param key_name: The name of key to update.\n",
      "        :type key_name: str\n",
      "        :param key_version: The version of the key to update.\n",
      "        :type key_version: str\n",
      "        :param key_ops: Json web key operations. For more information on\n",
      "         possible key operations, see JsonWebKeyOperation.\n",
      "        :type key_ops: list[str or\n",
      "         ~azure.keyvault.v2016_10_01.models.JsonWebKeyOperation]\n",
      "        :param key_attributes:\n",
      "        :type key_attributes: ~azure.keyvault.v2016_10_01.models.KeyAttributes\n",
      "        :param tags: Application specific metadata in the form of key-value\n",
      "         pairs.\n",
      "        :type tags: dict[str, str]\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: KeyBundle or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.keyvault.v2016_10_01.models.KeyBundle or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`KeyVaultErrorException<azure.keyvault.v2016_10_01.models.KeyVaultErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets a secret in a specified key vault.\n",
      "\n",
      "        The SET operation adds a secret to the Azure Key Vault. If the named\n",
      "        secret already exists, Azure Key Vault creates a new version of that\n",
      "        secret. This operation requires the secrets/set permission.\n",
      "\n",
      "        :param vault_base_url: The vault name, for example\n",
      "         https://myvault.vault.azure.net.\n",
      "        :type vault_base_url: str\n",
      "        :param secret_name: The name of the secret.\n",
      "        :type secret_name: str\n",
      "        :param value: The value of the secret.\n",
      "        :type value: str\n",
      "        :param tags: Application specific metadata in the form of key-value\n",
      "         pairs.\n",
      "        :type tags: dict[str, str]\n",
      "        :param content_type: Type of the secret value such as a password.\n",
      "        :type content_type: str\n",
      "        :param secret_attributes: The secret management attributes.\n",
      "        :type secret_attributes:\n",
      "         ~azure.keyvault.v2016_10_01.models.SecretAttributes\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: SecretBundle or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.keyvault.v2016_10_01.models.SecretBundle or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`KeyVaultErrorException<azure.keyvault.v2016_10_01.models.KeyVaultErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets the specified certificate issuer.\n",
      "\n",
      "        The SetCertificateIssuer operation adds or updates the specified\n",
      "        certificate issuer. This operation requires the certificates/setissuers\n",
      "        permission.\n",
      "\n",
      "        :param vault_base_url: The vault name, for example\n",
      "         https://myvault.vault.azure.net.\n",
      "        :type vault_base_url: str\n",
      "        :param issuer_name: The name of the issuer.\n",
      "        :type issuer_name: str\n",
      "        :param provider: The issuer provider.\n",
      "        :type provider: str\n",
      "        :param credentials: The credentials to be used for the issuer.\n",
      "        :type credentials:\n",
      "         ~azure.keyvault.v2016_10_01.models.IssuerCredentials\n",
      "        :param organization_details: Details of the organization as provided\n",
      "         to the issuer.\n",
      "        :type organization_details:\n",
      "         ~azure.keyvault.v2016_10_01.models.OrganizationDetails\n",
      "        :param attributes: Attributes of the issuer object.\n",
      "        :type attributes: ~azure.keyvault.v2016_10_01.models.IssuerAttributes\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: IssuerBundle or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.keyvault.v2016_10_01.models.IssuerBundle or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`KeyVaultErrorException<azure.keyvault.v2016_10_01.models.KeyVaultErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a Service Bus client from a connection string.\n",
      "\n",
      "        :param conn_str: The connection string.\n",
      "        :type conn_str: str\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START create_async_servicebus_client_connstr]\n",
      "                :end-before: [END create_async_servicebus_client_connstr]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Create a ServiceBusClient via a connection string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get an async client for a subscription entity.\n",
      "\n",
      "        :param topic_name: The name of the topic.\n",
      "        :type topic_name: str\n",
      "        :param subscription_name: The name of the subscription.\n",
      "        :type subscription_name: str\n",
      "        :rtype: ~azure.servicebus.aio.async_client.SubscriptionClient\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the subscription is not found.\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START get_async_subscription_client]\n",
      "                :end-before: [END get_async_subscription_client]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Get a TopicClient for the specified topic.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get an async client for all subscription entities in the topic.\n",
      "\n",
      "        :param topic_name: The topic to list subscriptions for.\n",
      "        :type topic_name: str\n",
      "        :rtype: list[~azure.servicebus.aio.async_client.SubscriptionClient]\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the topic is not found.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send one or more messages to the current entity.\n",
      "\n",
      "        This operation will open a single-use connection, send the supplied messages, and close\n",
      "        connection. If the entity requires sessions, a session ID must be either\n",
      "        provided here, or set on each outgoing message.\n",
      "\n",
      "        :param messages: One or more messages to be sent.\n",
      "        :type messages: ~azure.servicebus.aio.async_message.Message or\n",
      "         list[~azure.servicebus.aio.async_message.Message]\n",
      "        :param message_timeout: The period in seconds during which the Message must be\n",
      "         sent. If the send is not completed in this time it will return a failure result.\n",
      "        :type message_timeout: int\n",
      "        :param session: An optional session ID. If supplied this session ID will be\n",
      "         applied to every outgoing message sent with this Sender.\n",
      "         If an individual message already has a session ID, that will be\n",
      "         used instead. If no session ID is supplied here, nor set on an outgoing\n",
      "         message, a ValueError will be raised if the entity is sessionful.\n",
      "        :type session: str or ~uuid.Guid\n",
      "        :raises: ~azure.servicebus.common.errors.MessageSendFailed\n",
      "        :returns: A list of the send results of all the messages. Each\n",
      "         send result is a tuple with two values. The first is a boolean, indicating `True`\n",
      "         if the message sent, or `False` if it failed. The second is an error if the message\n",
      "         failed, otherwise it will be `None`.\n",
      "        :rtype: list[tuple[bool, ~azure.servicebus.common.errors.MessageSendFailed]]\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START queue_client_send]\n",
      "                :end-before: [END queue_client_send]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Send a single message.\n",
      "\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START queue_client_send_multiple]\n",
      "                :end-before: [END queue_client_send_multiple]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Send multiple messages.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a Sender for the Service Bus endpoint.\n",
      "\n",
      "        A Sender represents a single open connection within which multiple send operations can be made.\n",
      "\n",
      "        :param message_timeout: The period in seconds during which messages sent with\n",
      "         this Sender must be sent. If the send is not completed in this time it will fail.\n",
      "        :type message_timeout: int\n",
      "        :param session: An optional session ID. If supplied this session ID will be\n",
      "         applied to every outgoing message sent with this Sender.\n",
      "         If an individual message already has a session ID, that will be\n",
      "         used instead. If no session ID is supplied here, nor set on an outgoing\n",
      "         message, a ValueError will be raised if the entity is sessionful.\n",
      "        :type session: str or ~uuid.Guid\n",
      "        :returns: A Sender instance with an unopened connection.\n",
      "        :rtype: ~azure.servicebus.aio.async_send_handler.Sender\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START open_close_sender_context]\n",
      "                :end-before: [END open_close_sender_context]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Send multiple messages with a Sender.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a Receiver for the Service Bus endpoint.\n",
      "\n",
      "        A Receiver represents a single open connection with which multiple receive operations can be made.\n",
      "\n",
      "        :param session: A specific session from which to receive. This must be specified for a\n",
      "         sessionful entity, otherwise it must be None. In order to receive the next available\n",
      "         session, set this to NEXT_AVAILABLE.\n",
      "        :type session: str or ~azure.servicebus.common.constants.NEXT_AVAILABLE\n",
      "        :param prefetch: The maximum number of messages to cache with each request to the service.\n",
      "         The default value is 0, meaning messages will be received from the service and processed\n",
      "         one at a time. Increasing this value will improve message throughput performance but increase\n",
      "         the chance that messages will expire while they are cached if they're not processed fast enough.\n",
      "        :type prefetch: int\n",
      "        :param mode: The mode with which messages will be retrieved from the entity. The two options\n",
      "         are PeekLock and ReceiveAndDelete. Messages received with PeekLock must be settled within a given\n",
      "         lock period before they will be removed from the queue. Messages received with ReceiveAndDelete\n",
      "         will be immediately removed from the queue, and cannot be subsequently rejected or re-received if\n",
      "         the client fails to process the message. The default mode is PeekLock.\n",
      "        :type mode: ~azure.servicebus.common.constants.ReceiveSettleMode\n",
      "        :param idle_timeout: The timeout in seconds between received messages after which the receiver will\n",
      "         automatically shutdown. The default value is 0, meaning no timeout.\n",
      "        :type idle_timeout: int\n",
      "        :returns: A Receiver instance with an unopened connection.\n",
      "        :rtype: ~azure.servicebus.aio.async_receive_handler.Receiver\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START open_close_receiver_context]\n",
      "                :end-before: [END open_close_receiver_context]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Receive messages with a Receiver.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a Receiver for the deadletter endpoint of the entity.\n",
      "\n",
      "        A Receiver represents a single open connection with which multiple receive operations can be made.\n",
      "\n",
      "        :param transfer_deadletter: Whether to connect to the transfer deadletter queue, or the standard\n",
      "         deadletter queue. Default is False, using the standard deadletter endpoint.\n",
      "        :type transfer_deadletter: bool\n",
      "        :param prefetch: The maximum number of messages to cache with each request to the service.\n",
      "         The default value is 0, meaning messages will be received from the service and processed\n",
      "         one at a time. Increasing this value will improve message throughput performance but increase\n",
      "         the change that messages will expire while they are cached if they're not processed fast enough.\n",
      "        :type prefetch: int\n",
      "        :param mode: The mode with which messages will be retrieved from the entity. The two options\n",
      "         are PeekLock and ReceiveAndDelete. Messages received with PeekLock must be settled within a given\n",
      "         lock period before they will be removed from the queue. Messages received with ReceiveAndDelete\n",
      "         will be immediately removed from the queue, and cannot be subsequently rejected or re-received if\n",
      "         the client fails to process the message. The default mode is PeekLock.\n",
      "        :type mode: ~azure.servicebus.common.constants.ReceiveSettleMode\n",
      "        :param idle_timeout: The timeout in seconds between received messages after which the receiver will\n",
      "         automatically shutdown. The default value is 0, meaning no timeout.\n",
      "        :type idle_timeout: int\n",
      "        :returns: A Receiver instance with an unopened Connection.\n",
      "        :rtype: ~azure.servicebus.aio.async_receive_handler.Receiver\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START receiver_deadletter_messages]\n",
      "                :end-before: [END receiver_deadletter_messages]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Receive dead-lettered messages.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Extracts request id from response header.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Performs a GET request and returns the response.\n",
      "\n",
      "        path:\n",
      "            Path to the resource.\n",
      "            Ex: '/<subscription-id>/services/hostedservices/<service-name>'\n",
      "        x_ms_version:\n",
      "            If specified, this is used for the x-ms-version header.\n",
      "            Otherwise, self.x_ms_version is used.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Performs a PUT request and returns the response.\n",
      "\n",
      "        path:\n",
      "            Path to the resource.\n",
      "            Ex: '/<subscription-id>/services/hostedservices/<service-name>'\n",
      "        body:\n",
      "            Body for the PUT request.\n",
      "        x_ms_version:\n",
      "            If specified, this is used for the x-ms-version header.\n",
      "            Otherwise, self.x_ms_version is used.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Waits for an asynchronous operation to complete.\n",
      "\n",
      "        This calls get_operation_status in a loop and returns when the expected\n",
      "        status is reached. The result of get_operation_status is returned. By\n",
      "        default, an exception is raised on timeout or error status.\n",
      "\n",
      "        request_id:\n",
      "            The request ID for the request you wish to track.\n",
      "        wait_for_status:\n",
      "            Status to wait for. Default is 'Succeeded'.\n",
      "        timeout:\n",
      "            Total timeout in seconds. Default is 30s.\n",
      "        sleep_interval:\n",
      "            Sleep time in seconds for each iteration. Default is 5s.\n",
      "        progress_callback:\n",
      "            Callback for each iteration.\n",
      "            Default prints '.'.\n",
      "            Set it to None for no progress notification.\n",
      "        success_callback:\n",
      "            Callback on success. Default prints newline.\n",
      "            Set it to None for no success notification.\n",
      "        failure_callback:\n",
      "            Callback on failure. Default prints newline+error details then\n",
      "            raises exception.\n",
      "            Set it to None for no failure notification.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the status of the specified operation. After calling an\n",
      "        asynchronous operation, you can call Get Operation Status to determine\n",
      "        whether the operation has succeeded, failed, or is still in progress.\n",
      "\n",
      "        request_id:\n",
      "            The request ID for the request you wish to track.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add additional headers for management.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Assumed called on Travis, to prepare a package to be deployed\n",
      "\n",
      "    This method prints on stdout for Travis.\n",
      "    Return is obj to pass to sys.exit() directly\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List certificates in a specified key vault.\n",
      "\n",
      "        The GetCertificates operation returns the set of certificates resources\n",
      "        in the specified key vault. This operation requires the\n",
      "        certificates/list permission.\n",
      "\n",
      "        :param vault_base_url: The vault name, for example\n",
      "         https://myvault.vault.azure.net.\n",
      "        :type vault_base_url: str\n",
      "        :param maxresults: Maximum number of results to return in a page. If\n",
      "         not specified the service will return up to 25 results.\n",
      "        :type maxresults: int\n",
      "        :param include_pending: Specifies whether to include certificates\n",
      "         which are not completely provisioned.\n",
      "        :type include_pending: bool\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: An iterator like instance of CertificateItem\n",
      "        :rtype:\n",
      "         ~azure.keyvault.v7_0.models.CertificateItemPaged[~azure.keyvault.v7_0.models.CertificateItem]\n",
      "        :raises:\n",
      "         :class:`KeyVaultErrorException<azure.keyvault.v7_0.models.KeyVaultErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get list of available service bus regions.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List the service bus namespaces defined on the account.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get details about a specific namespace.\n",
      "\n",
      "        name:\n",
      "            Name of the service bus namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a new service bus namespace.\n",
      "\n",
      "        name:\n",
      "            Name of the service bus namespace to create.\n",
      "        region:\n",
      "            Region to create the namespace in.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a service bus namespace.\n",
      "\n",
      "        name:\n",
      "            Name of the service bus namespace to delete.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks to see if the specified service bus namespace is available, or\n",
      "        if it has already been taken.\n",
      "\n",
      "        name:\n",
      "            Name of the service bus namespace to validate.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the topics in the service namespace.\n",
      "\n",
      "        name:\n",
      "            Name of the service bus namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the notification hubs in the service namespace.\n",
      "\n",
      "        name:\n",
      "            Name of the service bus namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the relays in the service namespace.\n",
      "\n",
      "        name:\n",
      "            Name of the service bus namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This operation gets rollup data for Service Bus metrics queue.\n",
      "        Rollup data includes the time granularity for the telemetry aggregation as well as\n",
      "        the retention settings for each time granularity.\n",
      "\n",
      "        name:\n",
      "            Name of the service bus namespace.\n",
      "        queue_name:\n",
      "            Name of the service bus queue in this namespace.\n",
      "        metric:\n",
      "            name of a supported metric\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This operation gets rollup data for Service Bus metrics topic.\n",
      "        Rollup data includes the time granularity for the telemetry aggregation as well as\n",
      "        the retention settings for each time granularity.\n",
      "\n",
      "        name:\n",
      "            Name of the service bus namespace.\n",
      "        topic_name:\n",
      "            Name of the service bus queue in this namespace.\n",
      "        metric:\n",
      "            name of a supported metric\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This operation gets rollup data for Service Bus metrics notification hub.\n",
      "        Rollup data includes the time granularity for the telemetry aggregation as well as\n",
      "        the retention settings for each time granularity.\n",
      "\n",
      "        name:\n",
      "            Name of the service bus namespace.\n",
      "        hub_name:\n",
      "            Name of the service bus notification hub in this namespace.\n",
      "        metric:\n",
      "            name of a supported metric\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This operation gets rollup data for Service Bus metrics relay.\n",
      "        Rollup data includes the time granularity for the telemetry aggregation as well as\n",
      "        the retention settings for each time granularity.\n",
      "\n",
      "        name:\n",
      "            Name of the service bus namespace.\n",
      "        relay_name:\n",
      "            Name of the service bus relay in this namespace.\n",
      "        metric:\n",
      "            name of a supported metric\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a virtual environment in a directory.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a venv with these packages in a temp dir and yielf the env.\n",
      "\n",
      "    packages should be an iterable of pip version instructio (e.g. package~=1.2.3)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a new Azure SQL Database server.\n",
      "\n",
      "        admin_login:\n",
      "            The administrator login name for the new server.\n",
      "        admin_password:\n",
      "            The administrator login password for the new server.\n",
      "        location:\n",
      "            The region to deploy the new server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reset the administrator password for a server.\n",
      "\n",
      "        server_name:\n",
      "            Name of the server to change the password.\n",
      "        admin_password:\n",
      "            The new administrator password for the server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets quotas for an Azure SQL Database Server.\n",
      "\n",
      "        server_name:\n",
      "            Name of the server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the event logs for an Azure SQL Database Server.\n",
      "\n",
      "        server_name:\n",
      "            Name of the server to retrieve the event logs from.\n",
      "        start_date:\n",
      "            The starting date and time of the events to retrieve in UTC format,\n",
      "            for example '2011-09-28 16:05:00'.\n",
      "        interval_size_in_minutes:\n",
      "            Size of the event logs to retrieve (in minutes).\n",
      "            Valid values are: 5, 60, or 1440.\n",
      "        event_types:\n",
      "            The event type of the log entries you want to retrieve.\n",
      "            Valid values are: \n",
      "                - connection_successful\n",
      "                - connection_failed\n",
      "                - connection_terminated\n",
      "                - deadlock\n",
      "                - throttling\n",
      "                - throttling_long_transaction\n",
      "            To return all event types pass in an empty string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates an Azure SQL Database server firewall rule.\n",
      "\n",
      "        server_name:\n",
      "            Name of the server to set the firewall rule on. \n",
      "        name:\n",
      "            The name of the new firewall rule.\n",
      "        start_ip_address:\n",
      "            The lowest IP address in the range of the server-level firewall\n",
      "            setting. IP addresses equal to or greater than this can attempt to\n",
      "            connect to the server. The lowest possible IP address is 0.0.0.0.\n",
      "        end_ip_address:\n",
      "            The highest IP address in the range of the server-level firewall\n",
      "            setting. IP addresses equal to or less than this can attempt to\n",
      "            connect to the server. The highest possible IP address is\n",
      "            255.255.255.255.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Update a firewall rule for an Azure SQL Database server.\n",
      "\n",
      "        server_name:\n",
      "            Name of the server to set the firewall rule on. \n",
      "        name:\n",
      "            The name of the firewall rule to update.\n",
      "        start_ip_address:\n",
      "            The lowest IP address in the range of the server-level firewall\n",
      "            setting. IP addresses equal to or greater than this can attempt to\n",
      "            connect to the server. The lowest possible IP address is 0.0.0.0.\n",
      "        end_ip_address:\n",
      "            The highest IP address in the range of the server-level firewall\n",
      "            setting. IP addresses equal to or less than this can attempt to\n",
      "            connect to the server. The highest possible IP address is\n",
      "            255.255.255.255.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an Azure SQL Database server firewall rule.\n",
      "\n",
      "        server_name:\n",
      "            Name of the server with the firewall rule you want to delete.\n",
      "        name:\n",
      "            Name of the firewall rule you want to delete.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the set of firewall rules for an Azure SQL Database Server.\n",
      "\n",
      "        server_name:\n",
      "            Name of the server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the service level objectives for an Azure SQL Database server.\n",
      "\n",
      "        server_name:\n",
      "            Name of the server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new Azure SQL Database.\n",
      "\n",
      "        server_name:\n",
      "            Name of the server to contain the new database.\n",
      "        name:\n",
      "            Required. The name for the new database. See Naming Requirements\n",
      "            in Azure SQL Database General Guidelines and Limitations and\n",
      "            Database Identifiers for more information.\n",
      "        service_objective_id:\n",
      "            Required. The GUID corresponding to the performance level for\n",
      "            Edition. See List Service Level Objectives for current values.\n",
      "        edition:\n",
      "            Optional. The Service Tier (Edition) for the new database. If\n",
      "            omitted, the default is Web. Valid values are Web, Business,\n",
      "            Basic, Standard, and Premium. See Azure SQL Database Service Tiers\n",
      "            (Editions) and Web and Business Edition Sunset FAQ for more\n",
      "            information.\n",
      "        collation_name:\n",
      "            Optional. The database collation. This can be any collation\n",
      "            supported by SQL. If omitted, the default collation is used. See\n",
      "            SQL Server Collation Support in Azure SQL Database General\n",
      "            Guidelines and Limitations for more information.\n",
      "        max_size_bytes:\n",
      "            Optional. Sets the maximum size, in bytes, for the database. This\n",
      "            value must be within the range of allowed values for Edition. If\n",
      "            omitted, the default value for the edition is used. See Azure SQL\n",
      "            Database Service Tiers (Editions) for current maximum databases\n",
      "            sizes. Convert MB or GB values to bytes.\n",
      "            1 MB = 1048576 bytes. 1 GB = 1073741824 bytes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates existing database details.\n",
      "\n",
      "        server_name:\n",
      "            Name of the server to contain the new database.\n",
      "        name:\n",
      "            Required. The name for the new database. See Naming Requirements\n",
      "            in Azure SQL Database General Guidelines and Limitations and\n",
      "            Database Identifiers for more information.\n",
      "        new_database_name:\n",
      "            Optional. The new name for the new database.\n",
      "        service_objective_id:\n",
      "            Optional. The new service level to apply to the database. For more\n",
      "            information about service levels, see Azure SQL Database Service\n",
      "            Tiers and Performance Levels. Use List Service Level Objectives to\n",
      "            get the correct ID for the desired service objective.\n",
      "        edition:\n",
      "            Optional. The new edition for the new database.\n",
      "        max_size_bytes:\n",
      "            Optional. The new size of the database in bytes. For information on\n",
      "            available sizes for each edition, see Azure SQL Database Service\n",
      "            Tiers (Editions).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an Azure SQL Database.\n",
      "\n",
      "        server_name:\n",
      "            Name of the server where the database is located.\n",
      "        name:\n",
      "            Name of the database to delete.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List the SQL databases defined on the specified server name\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets all legal agreements that user needs to accept before purchasing a\n",
      "        domain.\n",
      "\n",
      "        Gets all legal agreements that user needs to accept before purchasing a\n",
      "        domain.\n",
      "\n",
      "        :param name: Name of the top-level domain.\n",
      "        :type name: str\n",
      "        :param include_privacy: If <code>true</code>, then the list of\n",
      "         agreements will include agreements for domain privacy as well;\n",
      "         otherwise, <code>false</code>.\n",
      "        :type include_privacy: bool\n",
      "        :param for_transfer: If <code>true</code>, then the list of agreements\n",
      "         will include agreements for domain transfer as well; otherwise,\n",
      "         <code>false</code>.\n",
      "        :type for_transfer: bool\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: An iterator like instance of TldLegalAgreement\n",
      "        :rtype:\n",
      "         ~azure.mgmt.web.models.TldLegalAgreementPaged[~azure.mgmt.web.models.TldLegalAgreement]\n",
      "        :raises:\n",
      "         :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Close down the handler connection.\n",
      "\n",
      "        If the handler has already closed,\n",
      "        this operation will do nothing. An optional exception can be passed in to\n",
      "        indicate that the handler was shutdown due to error.\n",
      "        It is recommended to open a handler within a context manager as\n",
      "        opposed to calling the method directly.\n",
      "\n",
      "        .. note:: This operation is not thread-safe.\n",
      "\n",
      "        :param exception: An optional exception if the handler is closing\n",
      "         due to an error.\n",
      "        :type exception: Exception\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START open_close_sender_directly]\n",
      "                :end-before: [END open_close_sender_directly]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Explicitly open and close a Sender.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Close down the receiver connection.\n",
      "\n",
      "        If the receiver has already closed, this operation will do nothing. An optional\n",
      "        exception can be passed in to indicate that the handler was shutdown due to error.\n",
      "        It is recommended to open a handler within a context manager as\n",
      "        opposed to calling the method directly.\n",
      "        The receiver will be implicitly closed on completion of the message iterator,\n",
      "        however this method will need to be called explicitly if the message iterator is not run\n",
      "        to completion.\n",
      "\n",
      "        .. note:: This operation is not thread-safe.\n",
      "\n",
      "        :param exception: An optional exception if the handler is closing\n",
      "         due to an error.\n",
      "        :type exception: Exception\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START open_close_receiver_directly]\n",
      "                :end-before: [END open_close_receiver_directly]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Iterate then explicitly close a Receiver.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the session state.\n",
      "\n",
      "        Returns None if no state has been set.\n",
      "\n",
      "        :rtype: str\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START set_session_state]\n",
      "                :end-before: [END set_session_state]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Getting and setting the state of a session.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Set the session state.\n",
      "\n",
      "        :param state: The state value.\n",
      "        :type state: str or bytes or bytearray\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START set_session_state]\n",
      "                :end-before: [END set_session_state]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Getting and setting the state of a session.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Receive messages that have previously been deferred.\n",
      "\n",
      "        This operation can only receive deferred messages from the current session.\n",
      "        When receiving deferred messages from a partitioned entity, all of the supplied\n",
      "        sequence numbers must be messages from the same partition.\n",
      "\n",
      "        :param sequence_numbers: A list of the sequence numbers of messages that have been\n",
      "         deferred.\n",
      "        :type sequence_numbers: list[int]\n",
      "        :param mode: The receive mode, default value is PeekLock.\n",
      "        :type mode: ~azure.servicebus.common.constants.ReceiveSettleMode\n",
      "        :rtype: list[~azure.servicebus.aio.async_message.DeferredMessage]\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START receiver_defer_session_messages]\n",
      "                :end-before: [END receiver_defer_session_messages]\n",
      "                :language: python\n",
      "                :dedent: 8\n",
      "                :caption: Defer messages, then retrieve them by sequence number.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Merges two `Reservation`s.\n",
      "\n",
      "        Merge the specified `Reservation`s into a new `Reservation`. The two\n",
      "        `Reservation`s being merged must have same properties.\n",
      "\n",
      "        :param reservation_order_id: Order Id of the reservation\n",
      "        :type reservation_order_id: str\n",
      "        :param sources: Format of the resource id should be\n",
      "         /providers/Microsoft.Capacity/reservationOrders/{reservationOrderId}/reservations/{reservationId}\n",
      "        :type sources: list[str]\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns list or\n",
      "         ClientRawResponse<list> if raw==True\n",
      "        :rtype:\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[list[~azure.mgmt.reservations.models.ReservationResponse]]\n",
      "         or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[list[~azure.mgmt.reservations.models.ReservationResponse]]]\n",
      "        :raises:\n",
      "         :class:`ErrorException<azure.mgmt.reservations.models.ErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Verifies that the challenge is a Bearer challenge and returns the key=value pairs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Purges data in an Log Analytics workspace by a set of user-defined\n",
      "        filters.\n",
      "\n",
      "        :param resource_group_name: The name of the resource group to get. The\n",
      "         name is case insensitive.\n",
      "        :type resource_group_name: str\n",
      "        :param workspace_name: Log Analytics workspace name\n",
      "        :type workspace_name: str\n",
      "        :param table: Table from which to purge data.\n",
      "        :type table: str\n",
      "        :param filters: The set of columns and filters (queries) to run over\n",
      "         them to purge the resulting data.\n",
      "        :type filters:\n",
      "         list[~azure.mgmt.loganalytics.models.WorkspacePurgeBodyFilters]\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns object or\n",
      "         ClientRawResponse<object> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[object] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[object]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Handle connection and service errors.\n",
      "\n",
      "    Called internally when an event has failed to send so we\n",
      "    can parse the error to determine whether we should attempt\n",
      "    to retry sending the event again.\n",
      "    Returns the action to take according to error type.\n",
      "\n",
      "    :param error: The error received in the send attempt.\n",
      "    :type error: Exception\n",
      "    :rtype: ~uamqp.errors.ErrorAction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new queue. Once created, this queue's resource manifest is\n",
      "        immutable.\n",
      "\n",
      "        queue_name:\n",
      "            Name of the queue to create.\n",
      "        queue:\n",
      "            Queue object to create.\n",
      "        fail_on_exist:\n",
      "            Specify whether to throw an exception when the queue exists.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an existing queue. This operation will also remove all\n",
      "        associated state including messages in the queue.\n",
      "\n",
      "        queue_name:\n",
      "            Name of the queue to delete.\n",
      "        fail_not_exist:\n",
      "            Specify whether to throw an exception if the queue doesn't exist.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves an existing queue.\n",
      "\n",
      "        queue_name:\n",
      "            Name of the queue.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new topic. Once created, this topic resource manifest is\n",
      "        immutable.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic to create.\n",
      "        topic:\n",
      "            Topic object to create.\n",
      "        fail_on_exist:\n",
      "            Specify whether to throw an exception when the topic exists.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the description for the specified topic.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new rule. Once created, this rule's resource manifest is\n",
      "        immutable.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic.\n",
      "        subscription_name:\n",
      "            Name of the subscription.\n",
      "        rule_name:\n",
      "            Name of the rule.\n",
      "        fail_on_exist:\n",
      "            Specify whether to throw an exception when the rule exists.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the description for the specified rule.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic.\n",
      "        subscription_name:\n",
      "            Name of the subscription.\n",
      "        rule_name:\n",
      "            Name of the rule.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the rules that exist under the specified subscription.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic.\n",
      "        subscription_name:\n",
      "            Name of the subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new subscription. Once created, this subscription resource\n",
      "        manifest is immutable.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic.\n",
      "        subscription_name:\n",
      "            Name of the subscription.\n",
      "        fail_on_exist:\n",
      "            Specify whether throw exception when subscription exists.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets an existing subscription.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic.\n",
      "        subscription_name:\n",
      "            Name of the subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the subscriptions in the specified topic.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Enqueues a message into the specified topic. The limit to the number\n",
      "        of messages which may be present in the topic is governed by the\n",
      "        message size in MaxTopicSizeInBytes. If this message causes the topic\n",
      "        to exceed its quota, a quota exceeded error is returned and the\n",
      "        message will be rejected.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic.\n",
      "        message:\n",
      "            Message object containing message body and properties.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Unlock a message for processing by other receivers on a given\n",
      "        subscription. This operation deletes the lock object, causing the\n",
      "        message to be unlocked. A message must have first been locked by a\n",
      "        receiver before this operation is called.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic.\n",
      "        subscription_name:\n",
      "            Name of the subscription.\n",
      "        sequence_number:\n",
      "            The sequence number of the message to be unlocked as returned in\n",
      "            BrokerProperties['SequenceNumber'] by the Peek Message operation.\n",
      "        lock_token:\n",
      "            The ID of the lock as returned by the Peek Message operation in\n",
      "            BrokerProperties['LockToken']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends a batch of messages into the specified queue. The limit to the number of\n",
      "        messages which may be present in the topic is governed by the message\n",
      "        size the MaxTopicSizeInMegaBytes. If this message will cause the queue\n",
      "        to exceed its quota, a quota exceeded error is returned and the\n",
      "        message will be rejected.\n",
      "\n",
      "        queue_name:\n",
      "            Name of the queue.\n",
      "        messages:\n",
      "            List of message objects containing message body and properties.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Unlocks a message for processing by other receivers on a given\n",
      "        queue. This operation deletes the lock object, causing the\n",
      "        message to be unlocked. A message must have first been locked by a\n",
      "        receiver before this operation is called.\n",
      "\n",
      "        queue_name:\n",
      "            Name of the queue.\n",
      "        sequence_number:\n",
      "            The sequence number of the message to be unlocked as returned in\n",
      "            BrokerProperties['SequenceNumber'] by the Peek Message operation.\n",
      "        lock_token:\n",
      "            The ID of the lock as returned by the Peek Message operation in\n",
      "            BrokerProperties['LockToken']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Receive a message from a queue for processing.\n",
      "\n",
      "        queue_name:\n",
      "            Name of the queue.\n",
      "        peek_lock:\n",
      "            Optional. True to retrieve and lock the message. False to read and\n",
      "            delete the message. Default is True (lock).\n",
      "        timeout:\n",
      "            Optional. The timeout parameter is expressed in seconds.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Receive a message from a subscription for processing.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic.\n",
      "        subscription_name:\n",
      "            Name of the subscription.\n",
      "        peek_lock:\n",
      "            Optional. True to retrieve and lock the message. False to read and\n",
      "            delete the message. Default is True (lock).\n",
      "        timeout:\n",
      "            Optional. The timeout parameter is expressed in seconds.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new Event Hub.\n",
      "\n",
      "        hub_name:\n",
      "            Name of event hub.\n",
      "        hub:\n",
      "            Optional. Event hub properties. Instance of EventHub class.\n",
      "        hub.message_retention_in_days:\n",
      "            Number of days to retain the events for this Event Hub.\n",
      "        hub.status:\n",
      "            Status of the Event Hub (enabled or disabled).\n",
      "        hub.user_metadata:\n",
      "            User metadata.\n",
      "        hub.partition_count:\n",
      "            Number of shards on the Event Hub.\n",
      "        fail_on_exist:\n",
      "            Specify whether to throw an exception when the event hub exists.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates an Event Hub.\n",
      "\n",
      "        hub_name:\n",
      "            Name of event hub.\n",
      "        hub:\n",
      "            Optional. Event hub properties. Instance of EventHub class.\n",
      "        hub.message_retention_in_days:\n",
      "            Number of days to retain the events for this Event Hub.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves an existing event hub.\n",
      "\n",
      "        hub_name:\n",
      "            Name of the event hub.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends a new message event to an Event Hub.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add additional headers for Service Bus.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "return the signed string with token.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if token expires or not.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns token for the request.\n",
      "\n",
      "        host:\n",
      "            the Service Bus service request.\n",
      "        path:\n",
      "            the Service Bus service request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "pulls the query string out of the URI and moves it into\n",
      "        the query portion of the request object.  If there are already\n",
      "        query parameters on the request the parameters in the URI will\n",
      "        appear after the existing parameters\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reset Service Principal Profile of a managed cluster.\n",
      "\n",
      "        Update the service principal Profile for a managed cluster.\n",
      "\n",
      "        :param resource_group_name: The name of the resource group.\n",
      "        :type resource_group_name: str\n",
      "        :param resource_name: The name of the managed cluster resource.\n",
      "        :type resource_name: str\n",
      "        :param client_id: The ID for the service principal.\n",
      "        :type client_id: str\n",
      "        :param secret: The secret password associated with the service\n",
      "         principal in plain text.\n",
      "        :type secret: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes itself if find queue name or topic name and subscription\n",
      "        name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Unlocks itself if find queue name or topic name and subscription\n",
      "        name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Renew lock on itself if find queue name or topic name and subscription\n",
      "        name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "add addtional headers to request for message request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "return the current message as expected by batch body format\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the health of a Service Fabric cluster.\n",
      "\n",
      "        Use EventsHealthStateFilter to filter the collection of health events\n",
      "        reported on the cluster based on the health state.\n",
      "        Similarly, use NodesHealthStateFilter and ApplicationsHealthStateFilter\n",
      "        to filter the collection of nodes and applications returned based on\n",
      "        their aggregated health state.\n",
      "\n",
      "        :param nodes_health_state_filter: Allows filtering of the node health\n",
      "         state objects returned in the result of cluster health query\n",
      "         based on their health state. The possible values for this parameter\n",
      "         include integer value of one of the\n",
      "         following health states. Only nodes that match the filter are\n",
      "         returned. All nodes are used to evaluate the aggregated health state.\n",
      "         If not specified, all entries are returned.\n",
      "         The state values are flag-based enumeration, so the value could be a\n",
      "         combination of these values obtained using bitwise 'OR' operator.\n",
      "         For example, if the provided value is 6 then health state of nodes\n",
      "         with HealthState value of OK (2) and Warning (4) are returned.\n",
      "         - Default - Default value. Matches any HealthState. The value is zero.\n",
      "         - None - Filter that doesn't match any HealthState value. Used in\n",
      "         order to return no results on a given collection of states. The value\n",
      "         is 1.\n",
      "         - Ok - Filter that matches input with HealthState value Ok. The value\n",
      "         is 2.\n",
      "         - Warning - Filter that matches input with HealthState value Warning.\n",
      "         The value is 4.\n",
      "         - Error - Filter that matches input with HealthState value Error. The\n",
      "         value is 8.\n",
      "         - All - Filter that matches input with any HealthState value. The\n",
      "         value is 65535.\n",
      "        :type nodes_health_state_filter: int\n",
      "        :param applications_health_state_filter: Allows filtering of the\n",
      "         application health state objects returned in the result of cluster\n",
      "         health\n",
      "         query based on their health state.\n",
      "         The possible values for this parameter include integer value obtained\n",
      "         from members or bitwise operations\n",
      "         on members of HealthStateFilter enumeration. Only applications that\n",
      "         match the filter are returned.\n",
      "         All applications are used to evaluate the aggregated health state. If\n",
      "         not specified, all entries are returned.\n",
      "         The state values are flag-based enumeration, so the value could be a\n",
      "         combination of these values obtained using bitwise 'OR' operator.\n",
      "         For example, if the provided value is 6 then health state of\n",
      "         applications with HealthState value of OK (2) and Warning (4) are\n",
      "         returned.\n",
      "         - Default - Default value. Matches any HealthState. The value is zero.\n",
      "         - None - Filter that doesn't match any HealthState value. Used in\n",
      "         order to return no results on a given collection of states. The value\n",
      "         is 1.\n",
      "         - Ok - Filter that matches input with HealthState value Ok. The value\n",
      "         is 2.\n",
      "         - Warning - Filter that matches input with HealthState value Warning.\n",
      "         The value is 4.\n",
      "         - Error - Filter that matches input with HealthState value Error. The\n",
      "         value is 8.\n",
      "         - All - Filter that matches input with any HealthState value. The\n",
      "         value is 65535.\n",
      "        :type applications_health_state_filter: int\n",
      "        :param events_health_state_filter: Allows filtering the collection of\n",
      "         HealthEvent objects returned based on health state.\n",
      "         The possible values for this parameter include integer value of one of\n",
      "         the following health states.\n",
      "         Only events that match the filter are returned. All events are used to\n",
      "         evaluate the aggregated health state.\n",
      "         If not specified, all entries are returned. The state values are\n",
      "         flag-based enumeration, so the value could be a combination of these\n",
      "         values, obtained using the bitwise 'OR' operator. For example, If the\n",
      "         provided value is 6 then all of the events with HealthState value of\n",
      "         OK (2) and Warning (4) are returned.\n",
      "         - Default - Default value. Matches any HealthState. The value is zero.\n",
      "         - None - Filter that doesn't match any HealthState value. Used in\n",
      "         order to return no results on a given collection of states. The value\n",
      "         is 1.\n",
      "         - Ok - Filter that matches input with HealthState value Ok. The value\n",
      "         is 2.\n",
      "         - Warning - Filter that matches input with HealthState value Warning.\n",
      "         The value is 4.\n",
      "         - Error - Filter that matches input with HealthState value Error. The\n",
      "         value is 8.\n",
      "         - All - Filter that matches input with any HealthState value. The\n",
      "         value is 65535.\n",
      "        :type events_health_state_filter: int\n",
      "        :param exclude_health_statistics: Indicates whether the health\n",
      "         statistics should be returned as part of the query result. False by\n",
      "         default.\n",
      "         The statistics show the number of children entities in health state\n",
      "         Ok, Warning, and Error.\n",
      "        :type exclude_health_statistics: bool\n",
      "        :param include_system_application_health_statistics: Indicates whether\n",
      "         the health statistics should include the fabric:/System application\n",
      "         health statistics. False by default.\n",
      "         If IncludeSystemApplicationHealthStatistics is set to true, the health\n",
      "         statistics include the entities that belong to the fabric:/System\n",
      "         application.\n",
      "         Otherwise, the query result includes health statistics only for user\n",
      "         applications.\n",
      "         The health statistics must be included in the query result for this\n",
      "         parameter to be applied.\n",
      "        :type include_system_application_health_statistics: bool\n",
      "        :param timeout: The server timeout for performing the operation in\n",
      "         seconds. This timeout specifies the time duration that the client is\n",
      "         willing to wait for the requested operation to complete. The default\n",
      "         value for this parameter is 60 seconds.\n",
      "        :type timeout: long\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: ClusterHealth or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.servicefabric.models.ClusterHealth or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the health of a Service Fabric cluster using the specified policy.\n",
      "\n",
      "        Use EventsHealthStateFilter to filter the collection of health events\n",
      "        reported on the cluster based on the health state.\n",
      "        Similarly, use NodesHealthStateFilter and ApplicationsHealthStateFilter\n",
      "        to filter the collection of nodes and applications returned based on\n",
      "        their aggregated health state.\n",
      "        Use ClusterHealthPolicies to override the health policies used to\n",
      "        evaluate the health.\n",
      "\n",
      "        :param nodes_health_state_filter: Allows filtering of the node health\n",
      "         state objects returned in the result of cluster health query\n",
      "         based on their health state. The possible values for this parameter\n",
      "         include integer value of one of the\n",
      "         following health states. Only nodes that match the filter are\n",
      "         returned. All nodes are used to evaluate the aggregated health state.\n",
      "         If not specified, all entries are returned.\n",
      "         The state values are flag-based enumeration, so the value could be a\n",
      "         combination of these values obtained using bitwise 'OR' operator.\n",
      "         For example, if the provided value is 6 then health state of nodes\n",
      "         with HealthState value of OK (2) and Warning (4) are returned.\n",
      "         - Default - Default value. Matches any HealthState. The value is zero.\n",
      "         - None - Filter that doesn't match any HealthState value. Used in\n",
      "         order to return no results on a given collection of states. The value\n",
      "         is 1.\n",
      "         - Ok - Filter that matches input with HealthState value Ok. The value\n",
      "         is 2.\n",
      "         - Warning - Filter that matches input with HealthState value Warning.\n",
      "         The value is 4.\n",
      "         - Error - Filter that matches input with HealthState value Error. The\n",
      "         value is 8.\n",
      "         - All - Filter that matches input with any HealthState value. The\n",
      "         value is 65535.\n",
      "        :type nodes_health_state_filter: int\n",
      "        :param applications_health_state_filter: Allows filtering of the\n",
      "         application health state objects returned in the result of cluster\n",
      "         health\n",
      "         query based on their health state.\n",
      "         The possible values for this parameter include integer value obtained\n",
      "         from members or bitwise operations\n",
      "         on members of HealthStateFilter enumeration. Only applications that\n",
      "         match the filter are returned.\n",
      "         All applications are used to evaluate the aggregated health state. If\n",
      "         not specified, all entries are returned.\n",
      "         The state values are flag-based enumeration, so the value could be a\n",
      "         combination of these values obtained using bitwise 'OR' operator.\n",
      "         For example, if the provided value is 6 then health state of\n",
      "         applications with HealthState value of OK (2) and Warning (4) are\n",
      "         returned.\n",
      "         - Default - Default value. Matches any HealthState. The value is zero.\n",
      "         - None - Filter that doesn't match any HealthState value. Used in\n",
      "         order to return no results on a given collection of states. The value\n",
      "         is 1.\n",
      "         - Ok - Filter that matches input with HealthState value Ok. The value\n",
      "         is 2.\n",
      "         - Warning - Filter that matches input with HealthState value Warning.\n",
      "         The value is 4.\n",
      "         - Error - Filter that matches input with HealthState value Error. The\n",
      "         value is 8.\n",
      "         - All - Filter that matches input with any HealthState value. The\n",
      "         value is 65535.\n",
      "        :type applications_health_state_filter: int\n",
      "        :param events_health_state_filter: Allows filtering the collection of\n",
      "         HealthEvent objects returned based on health state.\n",
      "         The possible values for this parameter include integer value of one of\n",
      "         the following health states.\n",
      "         Only events that match the filter are returned. All events are used to\n",
      "         evaluate the aggregated health state.\n",
      "         If not specified, all entries are returned. The state values are\n",
      "         flag-based enumeration, so the value could be a combination of these\n",
      "         values, obtained using the bitwise 'OR' operator. For example, If the\n",
      "         provided value is 6 then all of the events with HealthState value of\n",
      "         OK (2) and Warning (4) are returned.\n",
      "         - Default - Default value. Matches any HealthState. The value is zero.\n",
      "         - None - Filter that doesn't match any HealthState value. Used in\n",
      "         order to return no results on a given collection of states. The value\n",
      "         is 1.\n",
      "         - Ok - Filter that matches input with HealthState value Ok. The value\n",
      "         is 2.\n",
      "         - Warning - Filter that matches input with HealthState value Warning.\n",
      "         The value is 4.\n",
      "         - Error - Filter that matches input with HealthState value Error. The\n",
      "         value is 8.\n",
      "         - All - Filter that matches input with any HealthState value. The\n",
      "         value is 65535.\n",
      "        :type events_health_state_filter: int\n",
      "        :param exclude_health_statistics: Indicates whether the health\n",
      "         statistics should be returned as part of the query result. False by\n",
      "         default.\n",
      "         The statistics show the number of children entities in health state\n",
      "         Ok, Warning, and Error.\n",
      "        :type exclude_health_statistics: bool\n",
      "        :param include_system_application_health_statistics: Indicates whether\n",
      "         the health statistics should include the fabric:/System application\n",
      "         health statistics. False by default.\n",
      "         If IncludeSystemApplicationHealthStatistics is set to true, the health\n",
      "         statistics include the entities that belong to the fabric:/System\n",
      "         application.\n",
      "         Otherwise, the query result includes health statistics only for user\n",
      "         applications.\n",
      "         The health statistics must be included in the query result for this\n",
      "         parameter to be applied.\n",
      "        :type include_system_application_health_statistics: bool\n",
      "        :param timeout: The server timeout for performing the operation in\n",
      "         seconds. This timeout specifies the time duration that the client is\n",
      "         willing to wait for the requested operation to complete. The default\n",
      "         value for this parameter is 60 seconds.\n",
      "        :type timeout: long\n",
      "        :param application_health_policy_map: Defines a map that contains\n",
      "         specific application health policies for different applications.\n",
      "         Each entry specifies as key the application name and as value an\n",
      "         ApplicationHealthPolicy used to evaluate the application health.\n",
      "         If an application is not specified in the map, the application health\n",
      "         evaluation uses the ApplicationHealthPolicy found in its application\n",
      "         manifest or the default application health policy (if no health policy\n",
      "         is defined in the manifest).\n",
      "         The map is empty by default.\n",
      "        :type application_health_policy_map:\n",
      "         list[~azure.servicefabric.models.ApplicationHealthPolicyMapItem]\n",
      "        :param cluster_health_policy: Defines a health policy used to evaluate\n",
      "         the health of the cluster or of a cluster node.\n",
      "        :type cluster_health_policy:\n",
      "         ~azure.servicefabric.models.ClusterHealthPolicy\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: ClusterHealth or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.servicefabric.models.ClusterHealth or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Removes or unregisters a Service Fabric application type from the\n",
      "        cluster.\n",
      "\n",
      "        This operation can only be performed if all application instances of\n",
      "        the application type have been deleted. Once the application type is\n",
      "        unregistered, no new application instances can be created for this\n",
      "        particular application type.\n",
      "\n",
      "        :param application_type_name: The name of the application type.\n",
      "        :type application_type_name: str\n",
      "        :param application_type_version: The version of the application type\n",
      "         as defined in the application manifest.\n",
      "        :type application_type_version: str\n",
      "        :param timeout: The server timeout for performing the operation in\n",
      "         seconds. This timeout specifies the time duration that the client is\n",
      "         willing to wait for the requested operation to complete. The default\n",
      "         value for this parameter is 60 seconds.\n",
      "        :type timeout: long\n",
      "        :param async_parameter: The flag indicating whether or not unprovision\n",
      "         should occur asynchronously. When set to true, the unprovision\n",
      "         operation returns when the request is accepted by the system, and the\n",
      "         unprovision operation continues without any timeout limit. The default\n",
      "         value is false. However, we recommend setting it to true for large\n",
      "         application packages that were provisioned.\n",
      "        :type async_parameter: bool\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: None or ClientRawResponse if raw=true\n",
      "        :rtype: None or ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets a list of repair tasks matching the given filters.\n",
      "\n",
      "        This API supports the Service Fabric platform; it is not meant to be\n",
      "        used directly from your code.\n",
      "\n",
      "        :param task_id_filter: The repair task ID prefix to be matched.\n",
      "        :type task_id_filter: str\n",
      "        :param state_filter: A bitwise-OR of the following values, specifying\n",
      "         which task states should be included in the result list.\n",
      "         - 1 - Created\n",
      "         - 2 - Claimed\n",
      "         - 4 - Preparing\n",
      "         - 8 - Approved\n",
      "         - 16 - Executing\n",
      "         - 32 - Restoring\n",
      "         - 64 - Completed\n",
      "        :type state_filter: int\n",
      "        :param executor_filter: The name of the repair executor whose claimed\n",
      "         tasks should be included in the list.\n",
      "        :type executor_filter: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: list or ClientRawResponse if raw=true\n",
      "        :rtype: list[~azure.servicefabric.models.RepairTask] or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Submits a property batch.\n",
      "\n",
      "        Submits a batch of property operations. Either all or none of the\n",
      "        operations will be committed.\n",
      "\n",
      "        :param name_id: The Service Fabric name, without the 'fabric:' URI\n",
      "         scheme.\n",
      "        :type name_id: str\n",
      "        :param timeout: The server timeout for performing the operation in\n",
      "         seconds. This timeout specifies the time duration that the client is\n",
      "         willing to wait for the requested operation to complete. The default\n",
      "         value for this parameter is 60 seconds.\n",
      "        :type timeout: long\n",
      "        :param operations: A list of the property batch operations to be\n",
      "         executed.\n",
      "        :type operations:\n",
      "         list[~azure.servicefabric.models.PropertyBatchOperation]\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: PropertyBatchInfo or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.servicefabric.models.PropertyBatchInfo or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Simple error handler for azure.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Start capturing network packets for the site.\n",
      "\n",
      "        Start capturing network packets for the site.\n",
      "\n",
      "        :param resource_group_name: Name of the resource group to which the\n",
      "         resource belongs.\n",
      "        :type resource_group_name: str\n",
      "        :param name: The name of the web app.\n",
      "        :type name: str\n",
      "        :param duration_in_seconds: The duration to keep capturing in seconds.\n",
      "        :type duration_in_seconds: int\n",
      "        :param max_frame_length: The maximum frame length in bytes (Optional).\n",
      "        :type max_frame_length: int\n",
      "        :param sas_url: The Blob URL to store capture file.\n",
      "        :type sas_url: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns list or\n",
      "         ClientRawResponse<list> if raw==True\n",
      "        :rtype:\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[list[~azure.mgmt.web.models.NetworkTrace]]\n",
      "         or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[list[~azure.mgmt.web.models.NetworkTrace]]]\n",
      "        :raises:\n",
      "         :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the difference in configuration settings between two web app slots.\n",
      "\n",
      "        Get the difference in configuration settings between two web app slots.\n",
      "\n",
      "        :param resource_group_name: Name of the resource group to which the\n",
      "         resource belongs.\n",
      "        :type resource_group_name: str\n",
      "        :param name: Name of the app.\n",
      "        :type name: str\n",
      "        :param slot: Name of the source slot. If a slot is not specified, the\n",
      "         production slot is used as the source slot.\n",
      "        :type slot: str\n",
      "        :param target_slot: Destination deployment slot during swap operation.\n",
      "        :type target_slot: str\n",
      "        :param preserve_vnet: <code>true</code> to preserve Virtual Network to\n",
      "         the slot during swap; otherwise, <code>false</code>.\n",
      "        :type preserve_vnet: bool\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: An iterator like instance of SlotDifference\n",
      "        :rtype:\n",
      "         ~azure.mgmt.web.models.SlotDifferencePaged[~azure.mgmt.web.models.SlotDifference]\n",
      "        :raises:\n",
      "         :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Swaps two deployment slots of an app.\n",
      "\n",
      "        Swaps two deployment slots of an app.\n",
      "\n",
      "        :param resource_group_name: Name of the resource group to which the\n",
      "         resource belongs.\n",
      "        :type resource_group_name: str\n",
      "        :param name: Name of the app.\n",
      "        :type name: str\n",
      "        :param slot: Name of the source slot. If a slot is not specified, the\n",
      "         production slot is used as the source slot.\n",
      "        :type slot: str\n",
      "        :param target_slot: Destination deployment slot during swap operation.\n",
      "        :type target_slot: str\n",
      "        :param preserve_vnet: <code>true</code> to preserve Virtual Network to\n",
      "         the slot during swap; otherwise, <code>false</code>.\n",
      "        :type preserve_vnet: bool\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute OData query.\n",
      "\n",
      "        Executes an OData query for events.\n",
      "\n",
      "        :param app_id: ID of the application. This is Application ID from the\n",
      "         API Access settings blade in the Azure portal.\n",
      "        :type app_id: str\n",
      "        :param event_type: The type of events to query; either a standard\n",
      "         event type (`traces`, `customEvents`, `pageViews`, `requests`,\n",
      "         `dependencies`, `exceptions`, `availabilityResults`) or `$all` to\n",
      "         query across all event types. Possible values include: '$all',\n",
      "         'traces', 'customEvents', 'pageViews', 'browserTimings', 'requests',\n",
      "         'dependencies', 'exceptions', 'availabilityResults',\n",
      "         'performanceCounters', 'customMetrics'\n",
      "        :type event_type: str or ~azure.applicationinsights.models.EventType\n",
      "        :param timespan: Optional. The timespan over which to retrieve events.\n",
      "         This is an ISO8601 time period value.  This timespan is applied in\n",
      "         addition to any that are specified in the Odata expression.\n",
      "        :type timespan: str\n",
      "        :param filter: An expression used to filter the returned events\n",
      "        :type filter: str\n",
      "        :param search: A free-text search expression to match for whether a\n",
      "         particular event should be returned\n",
      "        :type search: str\n",
      "        :param orderby: A comma-separated list of properties with \\\\\"asc\\\\\"\n",
      "         (the default) or \\\\\"desc\\\\\" to control the order of returned events\n",
      "        :type orderby: str\n",
      "        :param select: Limits the properties to just those requested on each\n",
      "         returned event\n",
      "        :type select: str\n",
      "        :param skip: The number of items to skip over before returning events\n",
      "        :type skip: int\n",
      "        :param top: The number of events to return\n",
      "        :type top: int\n",
      "        :param format: Format for the returned events\n",
      "        :type format: str\n",
      "        :param count: Request a count of matching items included with the\n",
      "         returned events\n",
      "        :type count: bool\n",
      "        :param apply: An expression used for aggregation over returned events\n",
      "        :type apply: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: EventsResults or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.applicationinsights.models.EventsResults or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.applicationinsights.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add a face to a large face list. The input face is specified as an\n",
      "        image with a targetFace rectangle. It returns a persistedFaceId\n",
      "        representing the added face, and persistedFaceId will not expire.\n",
      "\n",
      "        :param large_face_list_id: Id referencing a particular large face\n",
      "         list.\n",
      "        :type large_face_list_id: str\n",
      "        :param image: An image stream.\n",
      "        :type image: Generator\n",
      "        :param user_data: User-specified data about the face for any purpose.\n",
      "         The maximum length is 1KB.\n",
      "        :type user_data: str\n",
      "        :param target_face: A face rectangle to specify the target face to be\n",
      "         added to a person in the format of \"targetFace=left,top,width,height\".\n",
      "         E.g. \"targetFace=10,10,100,100\". If there is more than one face in the\n",
      "         image, targetFace is required to specify which face to add. No\n",
      "         targetFace means there is only one face detected in the entire image.\n",
      "        :type target_face: list[int]\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param callback: When specified, will be called with each chunk of\n",
      "         data that is streamed. The callback should take two arguments, the\n",
      "         bytes of the current chunk of data and the response object. If the\n",
      "         data is uploading, response will be None.\n",
      "        :type callback: Callable[Bytes, response=None]\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: PersistedFace or ClientRawResponse if raw=true\n",
      "        :rtype: ~azure.cognitiveservices.vision.face.models.PersistedFace or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`APIErrorException<azure.cognitiveservices.vision.face.models.APIErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reset auth_attempted on redirects.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates Migration configuration and starts migration of entities from\n",
      "        Standard to Premium namespace.\n",
      "\n",
      "        :param resource_group_name: Name of the Resource group within the\n",
      "         Azure subscription.\n",
      "        :type resource_group_name: str\n",
      "        :param namespace_name: The namespace name\n",
      "        :type namespace_name: str\n",
      "        :param target_namespace: Existing premium Namespace ARM Id name which\n",
      "         has no entities, will be used for migration\n",
      "        :type target_namespace: str\n",
      "        :param post_migration_name: Name to access Standard Namespace after\n",
      "         migration\n",
      "        :type post_migration_name: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns\n",
      "         MigrationConfigProperties or\n",
      "         ClientRawResponse<MigrationConfigProperties> if raw==True\n",
      "        :rtype:\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.servicebus.models.MigrationConfigProperties]\n",
      "         or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.servicebus.models.MigrationConfigProperties]]\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.mgmt.servicebus.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Publishes a batch of events to an Azure Event Grid topic.\n",
      "\n",
      "        :param topic_hostname: The host name of the topic, e.g.\n",
      "         topic1.westus2-1.eventgrid.azure.net\n",
      "        :type topic_hostname: str\n",
      "        :param events: An array of events to be published to Event Grid.\n",
      "        :type events: list[~azure.eventgrid.models.EventGridEvent]\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: None or ClientRawResponse if raw=true\n",
      "        :rtype: None or ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`HttpOperationError<msrest.exceptions.HttpOperationError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Moves resources from one resource group to another resource group.\n",
      "\n",
      "        The resources to move must be in the same source resource group. The\n",
      "        target resource group may be in a different subscription. When moving\n",
      "        resources, both the source group and the target group are locked for\n",
      "        the duration of the operation. Write and delete operations are blocked\n",
      "        on the groups until the move completes. .\n",
      "\n",
      "        :param source_resource_group_name: The name of the resource group\n",
      "         containing the resources to move.\n",
      "        :type source_resource_group_name: str\n",
      "        :param resources: The IDs of the resources.\n",
      "        :type resources: list[str]\n",
      "        :param target_resource_group: The target resource group.\n",
      "        :type target_resource_group: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Define a new default profile.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries policy tracked resources under the management group.\n",
      "\n",
      "        :param management_group_name: Management group name.\n",
      "        :type management_group_name: str\n",
      "        :param query_options: Additional parameters for the operation\n",
      "        :type query_options: ~azure.mgmt.policyinsights.models.QueryOptions\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: An iterator like instance of PolicyTrackedResource\n",
      "        :rtype:\n",
      "         ~azure.mgmt.policyinsights.models.PolicyTrackedResourcePaged[~azure.mgmt.policyinsights.models.PolicyTrackedResource]\n",
      "        :raises:\n",
      "         :class:`QueryFailureException<azure.mgmt.policyinsights.models.QueryFailureException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a queue entity.\n",
      "\n",
      "        :param queue_name: The name of the new queue.\n",
      "        :type queue_name: str\n",
      "        :param lock_duration: The lock durection in seconds for each message in the queue.\n",
      "        :type lock_duration: int\n",
      "        :param max_size_in_megabytes: The max size to allow the queue to grow to.\n",
      "        :type max_size_in_megabytes: int\n",
      "        :param requires_duplicate_detection: Whether the queue will require every message with\n",
      "         a specified time frame to have a unique ID. Non-unique messages will be discarded.\n",
      "         Default value is False.\n",
      "        :type requires_duplicate_detection: bool\n",
      "        :param requires_session: Whether the queue will be sessionful, and therefore require all\n",
      "         message to have a Session ID and be received by a sessionful receiver.\n",
      "         Default value is False.\n",
      "        :type requires_session: bool\n",
      "        :param default_message_time_to_live: The length of time a message will remain in the queue\n",
      "         before it is either discarded or moved to the dead letter queue.\n",
      "        :type default_message_time_to_live: ~datetime.timedelta\n",
      "        :param dead_lettering_on_message_expiration: Whether to move expired messages to the\n",
      "         dead letter queue. Default value is False.\n",
      "        :type dead_lettering_on_message_expiration: bool\n",
      "        :param duplicate_detection_history_time_window: The period within which all incoming messages\n",
      "         must have a unique message ID.\n",
      "        :type duplicate_detection_history_time_window: ~datetime.timedelta\n",
      "        :param max_delivery_count: The maximum number of times a message will attempt to be delivered\n",
      "         before it is moved to the dead letter queue.\n",
      "        :type max_delivery_count: int\n",
      "        :param enable_batched_operations:\n",
      "        :type: enable_batched_operations: bool\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n",
      "        :raises: ~azure.common.AzureConflictHttpError if a queue of the same name already exists.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a queue entity.\n",
      "\n",
      "        :param queue_name: The name of the queue to delete.\n",
      "        :type queue_name: str\n",
      "        :param fail_not_exist: Whether to raise an exception if the named queue is not\n",
      "         found. If set to True, a ServiceBusResourceNotFound will be raised.\n",
      "         Default value is False.\n",
      "        :type fail_not_exist: bool\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namesapce is not found.\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the queue is not found\n",
      "         and `fail_not_exist` is set to True.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a topic entity.\n",
      "\n",
      "        :param topic_name: The name of the new topic.\n",
      "        :type topic_name: str\n",
      "        :param max_size_in_megabytes: The max size to allow the topic to grow to.\n",
      "        :type max_size_in_megabytes: int\n",
      "        :param requires_duplicate_detection: Whether the topic will require every message with\n",
      "         a specified time frame to have a unique ID. Non-unique messages will be discarded.\n",
      "         Default value is False.\n",
      "        :type requires_duplicate_detection: bool\n",
      "        :param default_message_time_to_live: The length of time a message will remain in the topic\n",
      "         before it is either discarded or moved to the dead letter queue.\n",
      "        :type default_message_time_to_live: ~datetime.timedelta\n",
      "        :param duplicate_detection_history_time_window: The period within which all incoming messages\n",
      "         must have a unique message ID.\n",
      "        :type duplicate_detection_history_time_window: ~datetime.timedelta\n",
      "        :param enable_batched_operations:\n",
      "        :type: enable_batched_operations: bool\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n",
      "        :raises: ~azure.common.AzureConflictHttpError if a topic of the same name already exists.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a topic entity.\n",
      "\n",
      "        :param topic_name: The name of the topic to delete.\n",
      "        :type topic_name: str\n",
      "        :param fail_not_exist: Whether to raise an exception if the named topic is not\n",
      "         found. If set to True, a ServiceBusResourceNotFound will be raised.\n",
      "         Default value is False.\n",
      "        :type fail_not_exist: bool\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namesapce is not found.\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the topic is not found\n",
      "         and `fail_not_exist` is set to True.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a subscription entity.\n",
      "\n",
      "        :param topic_name: The name of the topic under which to create the subscription.\n",
      "        :param subscription_name: The name of the new subscription.\n",
      "        :type subscription_name: str\n",
      "        :param lock_duration: The lock durection in seconds for each message in the subscription.\n",
      "        :type lock_duration: int\n",
      "        :param requires_session: Whether the subscription will be sessionful, and therefore require all\n",
      "         message to have a Session ID and be received by a sessionful receiver.\n",
      "         Default value is False.\n",
      "        :type requires_session: bool\n",
      "        :param default_message_time_to_live: The length of time a message will remain in the subscription\n",
      "         before it is either discarded or moved to the dead letter queue.\n",
      "        :type default_message_time_to_live: ~datetime.timedelta\n",
      "        :param dead_lettering_on_message_expiration: Whether to move expired messages to the\n",
      "         dead letter queue. Default value is False.\n",
      "        :type dead_lettering_on_message_expiration: bool\n",
      "        :param dead_lettering_on_filter_evaluation_exceptions: Whether to move messages that error on\n",
      "         filtering into the dead letter queue. Default is False, and the messages will be discarded.\n",
      "        :type dead_lettering_on_filter_evaluation_exceptions: bool\n",
      "        :param max_delivery_count: The maximum number of times a message will attempt to be delivered\n",
      "         before it is moved to the dead letter queue.\n",
      "        :type max_delivery_count: int\n",
      "        :param enable_batched_operations:\n",
      "        :type: enable_batched_operations: bool\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n",
      "        :raises: ~azure.common.AzureConflictHttpError if a queue of the same name already exists.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a Client from a Service Bus connection string.\n",
      "\n",
      "        :param conn_str: The connection string.\n",
      "        :type conn_str: str\n",
      "        :param name: The name of the entity, if the 'EntityName' property is\n",
      "         not included in the connection string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Perform an operation to update the properties of the entity.\n",
      "\n",
      "        :returns: The properties of the entity as a dictionary.\n",
      "        :rtype: dict[str, Any]\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the entity does not exist.\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the endpoint cannot be reached.\n",
      "        :raises: ~azure.common.AzureHTTPError if the credentials are invalid.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Whether the receivers lock on a particular session has expired.\n",
      "\n",
      "        :rtype: bool\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a session for a node.\n",
      "\n",
      "        :param resource_group_name: The resource group name uniquely\n",
      "         identifies the resource group within the user subscriptionId.\n",
      "        :type resource_group_name: str\n",
      "        :param node_name: The node name (256 characters maximum).\n",
      "        :type node_name: str\n",
      "        :param session: The sessionId from the user.\n",
      "        :type session: str\n",
      "        :param user_name: Encrypted User name to be used to connect to node.\n",
      "        :type user_name: str\n",
      "        :param password: Encrypted Password associated with user name.\n",
      "        :type password: str\n",
      "        :param retention_period: Session retention period. Possible values\n",
      "         include: 'Session', 'Persistent'\n",
      "        :type retention_period: str or\n",
      "         ~azure.mgmt.servermanager.models.RetentionPeriod\n",
      "        :param credential_data_format: Credential data format. Possible values\n",
      "         include: 'RsaEncrypted'\n",
      "        :type credential_data_format: str or\n",
      "         ~azure.mgmt.servermanager.models.CredentialDataFormat\n",
      "        :param encryption_certificate_thumbprint: Encryption certificate\n",
      "         thumbprint.\n",
      "        :type encryption_certificate_thumbprint: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns SessionResource or\n",
      "         ClientRawResponse<SessionResource> if raw==True\n",
      "        :rtype:\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.servermanager.models.SessionResource]\n",
      "         or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.servermanager.models.SessionResource]]\n",
      "        :raises:\n",
      "         :class:`ErrorException<azure.mgmt.servermanager.models.ErrorException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates an Azure subscription.\n",
      "\n",
      "        :param billing_account_name: The name of the commerce root billing\n",
      "         account.\n",
      "        :type billing_account_name: str\n",
      "        :param invoice_section_name: The name of the invoice section.\n",
      "        :type invoice_section_name: str\n",
      "        :param body: The subscription creation parameters.\n",
      "        :type body:\n",
      "         ~azure.mgmt.subscription.models.SubscriptionCreationParameters\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns\n",
      "         SubscriptionCreationResult or\n",
      "         ClientRawResponse<SubscriptionCreationResult> if raw==True\n",
      "        :rtype:\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.subscription.models.SubscriptionCreationResult]\n",
      "         or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.subscription.models.SubscriptionCreationResult]]\n",
      "        :raises:\n",
      "         :class:`ErrorResponseException<azure.mgmt.subscription.models.ErrorResponseException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Export logs that show Api requests made by this subscription in the\n",
      "        given time window to show throttling activities.\n",
      "\n",
      "        :param parameters: Parameters supplied to the LogAnalytics\n",
      "         getRequestRateByInterval Api.\n",
      "        :type parameters:\n",
      "         ~azure.mgmt.compute.v2018_04_01.models.RequestRateByIntervalInput\n",
      "        :param location: The location upon which virtual-machine-sizes is\n",
      "         queried.\n",
      "        :type location: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns\n",
      "         LogAnalyticsOperationResult or\n",
      "         ClientRawResponse<LogAnalyticsOperationResult> if raw==True\n",
      "        :rtype:\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.compute.v2018_04_01.models.LogAnalyticsOperationResult]\n",
      "         or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.compute.v2018_04_01.models.LogAnalyticsOperationResult]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scan output for exceptions\n",
      "\n",
      "    If there is an output from an add task collection call add it to the results.\n",
      "\n",
      "    :param results_queue: Queue containing results of attempted add_collection's\n",
      "    :type results_queue: collections.deque\n",
      "    :return: list of TaskAddResults\n",
      "    :rtype: list[~TaskAddResult]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a chunk of tasks to the job\n",
      "\n",
      "        Retry chunk if body exceeds the maximum request size and retry tasks\n",
      "        if failed due to server errors.\n",
      "\n",
      "        :param results_queue: Queue to place the return value of the request\n",
      "        :type results_queue: collections.deque\n",
      "        :param chunk_tasks_to_add: Chunk of at most 100 tasks with retry details\n",
      "        :type chunk_tasks_to_add: list[~TrackedCloudTask]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Main method for worker to run\n",
      "\n",
      "        Pops a chunk of tasks off the collection of pending tasks to be added and submits them to be added.\n",
      "\n",
      "        :param collections.deque results_queue: Queue for worker to output results to\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Will build the actual config for Jinja2, based on SDK config.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resets the user password on an environment This operation can take a\n",
      "        while to complete.\n",
      "\n",
      "        :param user_name: The name of the user.\n",
      "        :type user_name: str\n",
      "        :param reset_password_payload: Represents the payload for resetting\n",
      "         passwords.\n",
      "        :type reset_password_payload:\n",
      "         ~azure.mgmt.labservices.models.ResetPasswordPayload\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts an environment by starting all resources inside the environment.\n",
      "        This operation can take a while to complete.\n",
      "\n",
      "        :param user_name: The name of the user.\n",
      "        :type user_name: str\n",
      "        :param environment_id: The resourceId of the environment\n",
      "        :type environment_id: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create message from response.\n",
      "\n",
      "    response:\n",
      "        response from Service Bus cloud server.\n",
      "    service_instance:\n",
      "        the Service Bus client.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts entry element to rule object.\n",
      "\n",
      "    The format of xml for rule:\n",
      "<entry xmlns='http://www.w3.org/2005/Atom'>\n",
      "<content type='application/xml'>\n",
      "<RuleDescription\n",
      "    xmlns:i=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
      "    xmlns=\"http://schemas.microsoft.com/netservices/2010/10/servicebus/connect\">\n",
      "    <Filter i:type=\"SqlFilterExpression\">\n",
      "        <SqlExpression>MyProperty='XYZ'</SqlExpression>\n",
      "    </Filter>\n",
      "    <Action i:type=\"SqlFilterAction\">\n",
      "        <SqlExpression>set MyProperty2 = 'ABC'</SqlExpression>\n",
      "    </Action>\n",
      "</RuleDescription>\n",
      "</content>\n",
      "</entry>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts entry element to queue object.\n",
      "\n",
      "    The format of xml response for queue:\n",
      "<QueueDescription\n",
      "    xmlns=\\\"http://schemas.microsoft.com/netservices/2010/10/servicebus/connect\\\">\n",
      "    <MaxSizeInBytes>10000</MaxSizeInBytes>\n",
      "    <DefaultMessageTimeToLive>PT5M</DefaultMessageTimeToLive>\n",
      "    <LockDuration>PT2M</LockDuration>\n",
      "    <RequiresGroupedReceives>False</RequiresGroupedReceives>\n",
      "    <SupportsDuplicateDetection>False</SupportsDuplicateDetection>\n",
      "    ...\n",
      "</QueueDescription>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts entry element to topic\n",
      "\n",
      "    The xml format for topic:\n",
      "<entry xmlns='http://www.w3.org/2005/Atom'>\n",
      "    <content type='application/xml'>\n",
      "    <TopicDescription\n",
      "        xmlns:i=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
      "        xmlns=\"http://schemas.microsoft.com/netservices/2010/10/servicebus/connect\">\n",
      "        <DefaultMessageTimeToLive>P10675199DT2H48M5.4775807S</DefaultMessageTimeToLive>\n",
      "        <MaxSizeInMegabytes>1024</MaxSizeInMegabytes>\n",
      "        <RequiresDuplicateDetection>false</RequiresDuplicateDetection>\n",
      "        <DuplicateDetectionHistoryTimeWindow>P7D</DuplicateDetectionHistoryTimeWindow>\n",
      "        <DeadLetteringOnFilterEvaluationExceptions>true</DeadLetteringOnFilterEvaluationExceptions>\n",
      "    </TopicDescription>\n",
      "    </content>\n",
      "</entry>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts entry element to subscription\n",
      "\n",
      "    The xml format for subscription:\n",
      "<entry xmlns='http://www.w3.org/2005/Atom'>\n",
      "    <content type='application/xml'>\n",
      "    <SubscriptionDescription\n",
      "        xmlns:i=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
      "        xmlns=\"http://schemas.microsoft.com/netservices/2010/10/servicebus/connect\">\n",
      "        <LockDuration>PT5M</LockDuration>\n",
      "        <RequiresSession>false</RequiresSession>\n",
      "        <DefaultMessageTimeToLive>P10675199DT2H48M5.4775807S</DefaultMessageTimeToLive>\n",
      "        <DeadLetteringOnMessageExpiration>false</DeadLetteringOnMessageExpiration>\n",
      "        <DeadLetteringOnFilterEvaluationExceptions>true</DeadLetteringOnFilterEvaluationExceptions>\n",
      "    </SubscriptionDescription>\n",
      "    </content>\n",
      "</entry>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new certificate inside the specified account.\n",
      "\n",
      "        :param resource_group_name: The name of the resource group that\n",
      "         contains the Batch account.\n",
      "        :type resource_group_name: str\n",
      "        :param account_name: The name of the Batch account.\n",
      "        :type account_name: str\n",
      "        :param certificate_name: The identifier for the certificate. This must\n",
      "         be made up of algorithm and thumbprint separated by a dash, and must\n",
      "         match the certificate data in the request. For example SHA1-a3d1c5.\n",
      "        :type certificate_name: str\n",
      "        :param parameters: Additional parameters for certificate creation.\n",
      "        :type parameters:\n",
      "         ~azure.mgmt.batch.models.CertificateCreateOrUpdateParameters\n",
      "        :param if_match: The entity state (ETag) version of the certificate to\n",
      "         update. A value of \"*\" can be used to apply the operation only if the\n",
      "         certificate already exists. If omitted, this operation will always be\n",
      "         applied.\n",
      "        :type if_match: str\n",
      "        :param if_none_match: Set to '*' to allow a new certificate to be\n",
      "         created, but to prevent updating an existing certificate. Other values\n",
      "         will be ignored.\n",
      "        :type if_none_match: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :return: An instance of AzureOperationPoller that returns Certificate\n",
      "         or ClientRawResponse if raw=true\n",
      "        :rtype:\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.batch.models.Certificate]\n",
      "         or ~msrest.pipeline.ClientRawResponse\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified certificate.\n",
      "\n",
      "        :param resource_group_name: The name of the resource group that\n",
      "         contains the Batch account.\n",
      "        :type resource_group_name: str\n",
      "        :param account_name: The name of the Batch account.\n",
      "        :type account_name: str\n",
      "        :param certificate_name: The identifier for the certificate. This must\n",
      "         be made up of algorithm and thumbprint separated by a dash, and must\n",
      "         match the certificate data in the request. For example SHA1-a3d1c5.\n",
      "        :type certificate_name: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :return: An instance of AzureOperationPoller that returns None or\n",
      "         ClientRawResponse if raw=true\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrest.pipeline.ClientRawResponse\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a SDK client initialized with current CLI credentials, CLI default subscription and CLI default cloud.\n",
      "\n",
      "    This method will fill automatically the following client parameters:\n",
      "    - credentials\n",
      "    - subscription_id\n",
      "    - base_url\n",
      "\n",
      "    Parameters provided in kwargs will override CLI parameters and be passed directly to the client.\n",
      "\n",
      "    :Example:\n",
      "\n",
      "    .. code:: python\n",
      "\n",
      "        from azure.common.client_factory import get_client_from_cli_profile\n",
      "        from azure.mgmt.compute import ComputeManagementClient\n",
      "        client = get_client_from_cli_profile(ComputeManagementClient)\n",
      "\n",
      "    .. versionadded:: 1.1.6\n",
      "\n",
      "    :param client_class: A SDK client class\n",
      "    :return: An instantiated client\n",
      "    :raises: ImportError if azure-cli-core package is not available\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a SDK client initialized with a JSON auth dict.\n",
      "\n",
      "    The easiest way to obtain this content is to call the following CLI commands:\n",
      "\n",
      "    .. code:: bash\n",
      "\n",
      "        az ad sp create-for-rbac --sdk-auth\n",
      "\n",
      "    This method will fill automatically the following client parameters:\n",
      "    - credentials\n",
      "    - subscription_id\n",
      "    - base_url\n",
      "    - tenant_id\n",
      "\n",
      "    Parameters provided in kwargs will override parameters and be passed directly to the client.\n",
      "\n",
      "    :Example:\n",
      "\n",
      "    .. code:: python\n",
      "\n",
      "        from azure.common.client_factory import get_client_from_auth_file\n",
      "        from azure.mgmt.compute import ComputeManagementClient\n",
      "        config_dict = {\n",
      "            \"clientId\": \"ad735158-65ca-11e7-ba4d-ecb1d756380e\",\n",
      "            \"clientSecret\": \"b70bb224-65ca-11e7-810c-ecb1d756380e\",\n",
      "            \"subscriptionId\": \"bfc42d3a-65ca-11e7-95cf-ecb1d756380e\",\n",
      "            \"tenantId\": \"c81da1d8-65ca-11e7-b1d1-ecb1d756380e\",\n",
      "            \"activeDirectoryEndpointUrl\": \"https://login.microsoftonline.com\",\n",
      "            \"resourceManagerEndpointUrl\": \"https://management.azure.com/\",\n",
      "            \"activeDirectoryGraphResourceId\": \"https://graph.windows.net/\",\n",
      "            \"sqlManagementEndpointUrl\": \"https://management.core.windows.net:8443/\",\n",
      "            \"galleryEndpointUrl\": \"https://gallery.azure.com/\",\n",
      "            \"managementEndpointUrl\": \"https://management.core.windows.net/\"\n",
      "        }\n",
      "        client = get_client_from_json_dict(ComputeManagementClient, config_dict)\n",
      "\n",
      "    .. versionadded:: 1.1.7\n",
      "\n",
      "    :param client_class: A SDK client class\n",
      "    :param dict config_dict: A config dict.\n",
      "    :return: An instantiated client\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a SDK client initialized with auth file.\n",
      "\n",
      "    The easiest way to obtain this file is to call the following CLI commands:\n",
      "\n",
      "    .. code:: bash\n",
      "\n",
      "        az ad sp create-for-rbac --sdk-auth\n",
      "\n",
      "    You can specific the file path directly, or fill the environment variable AZURE_AUTH_LOCATION.\n",
      "    File must be UTF-8.\n",
      "\n",
      "    This method will fill automatically the following client parameters:\n",
      "    - credentials\n",
      "    - subscription_id\n",
      "    - base_url\n",
      "\n",
      "    Parameters provided in kwargs will override parameters and be passed directly to the client.\n",
      "\n",
      "    :Example:\n",
      "\n",
      "    .. code:: python\n",
      "\n",
      "        from azure.common.client_factory import get_client_from_auth_file\n",
      "        from azure.mgmt.compute import ComputeManagementClient\n",
      "        client = get_client_from_auth_file(ComputeManagementClient)\n",
      "\n",
      "    Example of file:\n",
      "\n",
      "    .. code:: json\n",
      "\n",
      "        {\n",
      "            \"clientId\": \"ad735158-65ca-11e7-ba4d-ecb1d756380e\",\n",
      "            \"clientSecret\": \"b70bb224-65ca-11e7-810c-ecb1d756380e\",\n",
      "            \"subscriptionId\": \"bfc42d3a-65ca-11e7-95cf-ecb1d756380e\",\n",
      "            \"tenantId\": \"c81da1d8-65ca-11e7-b1d1-ecb1d756380e\",\n",
      "            \"activeDirectoryEndpointUrl\": \"https://login.microsoftonline.com\",\n",
      "            \"resourceManagerEndpointUrl\": \"https://management.azure.com/\",\n",
      "            \"activeDirectoryGraphResourceId\": \"https://graph.windows.net/\",\n",
      "            \"sqlManagementEndpointUrl\": \"https://management.core.windows.net:8443/\",\n",
      "            \"galleryEndpointUrl\": \"https://gallery.azure.com/\",\n",
      "            \"managementEndpointUrl\": \"https://management.core.windows.net/\"\n",
      "        }\n",
      "\n",
      "    .. versionadded:: 1.1.7\n",
      "\n",
      "    :param client_class: A SDK client class\n",
      "    :param str auth_path: Path to the file.\n",
      "    :return: An instantiated client\n",
      "    :raises: KeyError if AZURE_AUTH_LOCATION is not an environment variable and no path is provided\n",
      "    :raises: FileNotFoundError if provided file path does not exists\n",
      "    :raises: json.JSONDecodeError if provided file is not JSON valid\n",
      "    :raises: UnicodeDecodeError if file is not UTF8 compliant\n",
      "----------------------------------------------------------------------------------------------------\n",
      "resp_body is the XML we received\n",
      "        resp_type is a string, such as Containers,\n",
      "        return_type is the type we're constructing, such as ContainerEnumResults\n",
      "        item_type is the type object of the item to be created, such as Container\n",
      "\n",
      "        This function then returns a ContainerEnumResults object with the\n",
      "        containers member populated with the results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "get properties from element tree element\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete the Provisioning Service Certificate.\n",
      "\n",
      "        Deletes the specified certificate assosciated with the Provisioning\n",
      "        Service.\n",
      "\n",
      "        :param resource_group_name: Resource group identifier.\n",
      "        :type resource_group_name: str\n",
      "        :param if_match: ETag of the certificate\n",
      "        :type if_match: str\n",
      "        :param provisioning_service_name: The name of the provisioning\n",
      "         service.\n",
      "        :type provisioning_service_name: str\n",
      "        :param certificate_name: This is a mandatory field, and is the logical\n",
      "         name of the certificate that the provisioning service will access by.\n",
      "        :type certificate_name: str\n",
      "        :param certificatename: This is optional, and it is the Common Name of\n",
      "         the certificate.\n",
      "        :type certificatename: str\n",
      "        :param certificateraw_bytes: Raw data within the certificate.\n",
      "        :type certificateraw_bytes: bytearray\n",
      "        :param certificateis_verified: Indicates if certificate has been\n",
      "         verified by owner of the private key.\n",
      "        :type certificateis_verified: bool\n",
      "        :param certificatepurpose: A description that mentions the purpose of\n",
      "         the certificate. Possible values include: 'clientAuthentication',\n",
      "         'serverAuthentication'\n",
      "        :type certificatepurpose: str or\n",
      "         ~azure.mgmt.iothubprovisioningservices.models.CertificatePurpose\n",
      "        :param certificatecreated: Time the certificate is created.\n",
      "        :type certificatecreated: datetime\n",
      "        :param certificatelast_updated: Time the certificate is last updated.\n",
      "        :type certificatelast_updated: datetime\n",
      "        :param certificatehas_private_key: Indicates if the certificate\n",
      "         contains a private key.\n",
      "        :type certificatehas_private_key: bool\n",
      "        :param certificatenonce: Random number generated to indicate Proof of\n",
      "         Possession.\n",
      "        :type certificatenonce: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: returns the direct response alongside the\n",
      "         deserialized response\n",
      "        :param operation_config: :ref:`Operation configuration\n",
      "         overrides<msrest:optionsforoperations>`.\n",
      "        :return: None or ClientRawResponse if raw=true\n",
      "        :rtype: None or ~msrest.pipeline.ClientRawResponse\n",
      "        :raises:\n",
      "         :class:`ErrorDetailsException<azure.mgmt.iothubprovisioningservices.models.ErrorDetailsException>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a client for a queue entity.\n",
      "\n",
      "        :param queue_name: The name of the queue.\n",
      "        :type queue_name: str\n",
      "        :rtype: ~azure.servicebus.servicebus_client.QueueClient\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the queue is not found.\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/test_examples.py\n",
      "                :start-after: [START get_queue_client]\n",
      "                :end-before: [END get_queue_client]\n",
      "                :language: python\n",
      "                :dedent: 8\n",
      "                :caption: Get the specific queue client from Service Bus client\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get clients for all queue entities in the namespace.\n",
      "\n",
      "        :rtype: list[~azure.servicebus.servicebus_client.QueueClient]\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/test_examples.py\n",
      "                :start-after: [START list_queues]\n",
      "                :end-before: [END list_queues]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: List the queues from Service Bus client\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a client for a topic entity.\n",
      "\n",
      "        :param topic_name: The name of the topic.\n",
      "        :type topic_name: str\n",
      "        :rtype: ~azure.servicebus.servicebus_client.TopicClient\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the topic is not found.\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/test_examples.py\n",
      "                :start-after: [START get_topic_client]\n",
      "                :end-before: [END get_topic_client]\n",
      "                :language: python\n",
      "                :dedent: 8\n",
      "                :caption: Get the specific topic client from Service Bus client\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a client for all topic entities in the namespace.\n",
      "\n",
      "        :rtype: list[~azure.servicebus.servicebus_client.TopicClient]\n",
      "        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/test_examples.py\n",
      "                :start-after: [START list_topics]\n",
      "                :end-before: [END list_topics]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: List the topics from Service Bus client\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Receive messages by sequence number that have been previously deferred.\n",
      "\n",
      "        When receiving deferred messages from a partitioned entity, all of the supplied\n",
      "        sequence numbers must be messages from the same partition.\n",
      "\n",
      "        :param sequence_numbers: A list of the sequence numbers of messages that have been\n",
      "         deferred.\n",
      "        :type sequence_numbers: list[int]\n",
      "        :param mode: The mode with which messages will be retrieved from the entity. The two options\n",
      "         are PeekLock and ReceiveAndDelete. Messages received with PeekLock must be settled within a given\n",
      "         lock period before they will be removed from the queue. Messages received with ReceiveAndDelete\n",
      "         will be immediately removed from the queue, and cannot be subsequently rejected or re-received if\n",
      "         the client fails to process the message. The default mode is PeekLock.\n",
      "        :type mode: ~azure.servicebus.common.constants.ReceiveSettleMode\n",
      "        :rtype: list[~azure.servicebus.common.message.Message]\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/test_examples.py\n",
      "                :start-after: [START receive_deferred_messages_service_bus]\n",
      "                :end-before: [END receive_deferred_messages_service_bus]\n",
      "                :language: python\n",
      "                :dedent: 8\n",
      "                :caption: Get the messages which were deferred using their sequence numbers\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Settle messages that have been previously deferred.\n",
      "\n",
      "        :param settlement: How the messages are to be settled. This must be a string\n",
      "         of one of the following values: 'completed', 'suspended', 'abandoned'.\n",
      "        :type settlement: str\n",
      "        :param messages: A list of deferred messages to be settled.\n",
      "        :type messages: list[~azure.servicebus.common.message.DeferredMessage]\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/test_examples.py\n",
      "                :start-after: [START settle_deferred_messages_service_bus]\n",
      "                :end-before: [END settle_deferred_messages_service_bus]\n",
      "                :language: python\n",
      "                :dedent: 8\n",
      "                :caption: Settle deferred messages.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List the web sites defined on this webspace.\n",
      "\n",
      "        webspace_name:\n",
      "            The name of the webspace.\n",
      "        website_name:\n",
      "            The name of the website.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a website.\n",
      "\n",
      "        webspace_name:\n",
      "            The name of the webspace.\n",
      "        website_name:\n",
      "            The name of the website.\n",
      "        geo_region:\n",
      "            The geographical region of the webspace that will be created.\n",
      "        host_names:\n",
      "            An array of fully qualified domain names for website. Only one\n",
      "            hostname can be specified in the azurewebsites.net domain.\n",
      "            The hostname should match the name of the website. Custom domains\n",
      "            can only be specified for Shared or Standard websites.\n",
      "        plan:\n",
      "            This value must be 'VirtualDedicatedPlan'.\n",
      "        compute_mode:\n",
      "            This value should be 'Shared' for the Free or Paid Shared\n",
      "            offerings, or 'Dedicated' for the Standard offering. The default\n",
      "            value is 'Shared'. If you set it to 'Dedicated', you must specify\n",
      "            a value for the server_farm parameter.\n",
      "        server_farm:\n",
      "            The name of the Server Farm associated with this website. This is\n",
      "            a required value for Standard mode.\n",
      "        site_mode:\n",
      "            Can be None, 'Limited' or 'Basic'. This value is 'Limited' for the\n",
      "            Free offering, and 'Basic' for the Paid Shared offering. Standard\n",
      "            mode does not use the site_mode parameter; it uses the compute_mode\n",
      "            parameter.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a website.\n",
      "\n",
      "        webspace_name:\n",
      "            The name of the webspace.\n",
      "        website_name:\n",
      "            The name of the website.\n",
      "        delete_empty_server_farm:\n",
      "            If the site being deleted is the last web site in a server farm,\n",
      "            you can delete the server farm by setting this to True.\n",
      "        delete_metrics:\n",
      "            To also delete the metrics for the site that you are deleting, you\n",
      "            can set this to True.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Update a web site.\n",
      "\n",
      "        webspace_name:\n",
      "            The name of the webspace.\n",
      "        website_name:\n",
      "            The name of the website.\n",
      "        state:\n",
      "            The wanted state ('Running' or 'Stopped' accepted)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Restart a web site.\n",
      "\n",
      "        webspace_name:\n",
      "            The name of the webspace.\n",
      "        website_name:\n",
      "            The name of the website.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get historical usage metrics.\n",
      "\n",
      "        webspace_name:\n",
      "            The name of the webspace.\n",
      "        website_name:\n",
      "            The name of the website.\n",
      "        metrics:\n",
      "            Optional. List of metrics name. Otherwise, all metrics returned.\n",
      "        start_time:\n",
      "            Optional. An ISO8601 date. Otherwise, current hour is used.\n",
      "        end_time:\n",
      "            Optional. An ISO8601 date. Otherwise, current time is used.\n",
      "        time_grain:\n",
      "            Optional. A rollup name, as P1D. OTherwise, default rollup for the metrics is used.\n",
      "        More information and metrics name at:\n",
      "        http://msdn.microsoft.com/en-us/library/azure/dn166964.aspx\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get metric definitions of metrics available of this web site.\n",
      "\n",
      "        webspace_name:\n",
      "            The name of the webspace.\n",
      "        website_name:\n",
      "            The name of the website.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a site's publish profile as a string\n",
      "\n",
      "        webspace_name:\n",
      "            The name of the webspace.\n",
      "        website_name:\n",
      "            The name of the website.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a site's publish profile as an object\n",
      "\n",
      "        webspace_name:\n",
      "            The name of the webspace.\n",
      "        website_name:\n",
      "            The name of the website.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the policies for the specified container registry.\n",
      "\n",
      "        :param resource_group_name: The name of the resource group to which\n",
      "         the container registry belongs.\n",
      "        :type resource_group_name: str\n",
      "        :param registry_name: The name of the container registry.\n",
      "        :type registry_name: str\n",
      "        :param quarantine_policy: An object that represents quarantine policy\n",
      "         for a container registry.\n",
      "        :type quarantine_policy:\n",
      "         ~azure.mgmt.containerregistry.v2018_02_01_preview.models.QuarantinePolicy\n",
      "        :param trust_policy: An object that represents content trust policy\n",
      "         for a container registry.\n",
      "        :type trust_policy:\n",
      "         ~azure.mgmt.containerregistry.v2018_02_01_preview.models.TrustPolicy\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns RegistryPolicies or\n",
      "         ClientRawResponse<RegistryPolicies> if raw==True\n",
      "        :rtype:\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.containerregistry.v2018_02_01_preview.models.RegistryPolicies]\n",
      "         or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.containerregistry.v2018_02_01_preview.models.RegistryPolicies]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Create Cloud Service request creates a new cloud service. When job\n",
      "        collections are created, they are hosted within a cloud service.\n",
      "        A cloud service groups job collections together in a given region.\n",
      "        Once a cloud service has been created, job collections can then be\n",
      "        created and contained within it.\n",
      "\n",
      "        cloud_service_id:\n",
      "            The cloud service id\n",
      "        label:\n",
      "            The name of the cloud service.\n",
      "        description:\n",
      "            The description of the cloud service.\n",
      "        geo_region:\n",
      "            The geographical region of the webspace that will be created.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Check Name Availability operation checks if a new job collection with\n",
      "        the given name may be created, or if it is unavailable. The result of the\n",
      "        operation is a Boolean true or false.\n",
      "\n",
      "        cloud_service_id:\n",
      "            The cloud service id\n",
      "        job_collection_id:\n",
      "            The name of the job_collection_id.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Get Job Collection operation gets the details of a job collection\n",
      "\n",
      "        cloud_service_id:\n",
      "            The cloud service id\n",
      "        job_collection_id:\n",
      "            Name of the hosted service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Completes the restore operation on a managed database.\n",
      "\n",
      "        :param location_name: The name of the region where the resource is\n",
      "         located.\n",
      "        :type location_name: str\n",
      "        :param operation_id: Management operation id that this request tries\n",
      "         to complete.\n",
      "        :type operation_id: str\n",
      "        :param last_backup_name: The last backup name to apply\n",
      "        :type last_backup_name: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cancel one or more messages that have previsouly been scheduled and are still pending.\n",
      "\n",
      "        :param sequence_numbers: The seqeuence numbers of the scheduled messages.\n",
      "        :type sequence_numbers: int\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START cancel_schedule_messages]\n",
      "                :end-before: [END cancel_schedule_messages]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Schedule messages.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wait until all pending messages have been sent.\n",
      "\n",
      "        :returns: A list of the send results of all the pending messages. Each\n",
      "         send result is a tuple with two values. The first is a boolean, indicating `True`\n",
      "         if the message sent, or `False` if it failed. The second is an error if the message\n",
      "         failed, otherwise it will be `None`.\n",
      "        :rtype: list[tuple[bool, ~azure.servicebus.common.errors.MessageSendFailed]]\n",
      "\n",
      "        Example:\n",
      "            .. literalinclude:: ../examples/async_examples/test_examples_async.py\n",
      "                :start-after: [START queue_sender_messages]\n",
      "                :end-before: [END queue_sender_messages]\n",
      "                :language: python\n",
      "                :dedent: 4\n",
      "                :caption: Schedule messages.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reconnect the handler.\n",
      "\n",
      "        If the handler was disconnected from the service with\n",
      "        a retryable error - attempt to reconnect.\n",
      "        This method will be called automatically for most retryable errors.\n",
      "        Also attempts to re-queue any messages that were pending before the reconnect.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Writes a certificate file to the specified location.  This can then be used \n",
      "    to instantiate ServiceManagementService.  Returns the subscription ID.\n",
      "\n",
      "    publish_settings_path: \n",
      "        Path to subscription file downloaded from \n",
      "        http://go.microsoft.com/fwlink/?LinkID=301775\n",
      "    path_to_write_certificate:\n",
      "        Path to write the certificate file.\n",
      "    subscription_id:\n",
      "        (optional)  Provide a subscription id here if you wish to use a \n",
      "        specific subscription under the publish settings file.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load any stored cookies for the plugin that have not expired.\n",
      "\n",
      "        :return: list of the restored cookie names\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the width of the string it would be when displayed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Drops Characters by unicode not by bytes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clears out the previous line and prints a new one.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Formats the file size into a human readable format.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Formats elapsed seconds into a human readable format.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a status line with appropriate size.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Progress an iterator and updates a pretty status line to the terminal.\n",
      "\n",
      "    The status line contains:\n",
      "     - Amount of data read from the iterator\n",
      "     - Time elapsed\n",
      "     - Average speed, based on the last few seconds.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yield the segment number and when it will be available\n",
      "        There are two cases for segment number generation, static and dynamic.\n",
      "\n",
      "        In the case of static stream, the segment number starts at the startNumber and counts\n",
      "        up to the number of segments that are represented by the periods duration.\n",
      "\n",
      "        In the case of dynamic streams, the segments should appear at the specified time\n",
      "        in the simplest case the segment number is based on the time since the availabilityStartTime\n",
      "        :return:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Segments are yielded when they are available\n",
      "\n",
      "        Segments appear on a time line, for dynamic content they are only available at a certain time\n",
      "        and sometimes for a limited time. For static content they are all available at the same time.\n",
      "\n",
      "        :param kwargs: extra args to pass to the segment template\n",
      "        :return: yields Segments\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pauses the thread for a specified time.\n",
      "\n",
      "        Returns False if interrupted by another thread and True if the\n",
      "        time runs out normally.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a segment to the download pool and write queue.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Puts a value into a queue but aborts if this thread is closed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns any parameters needed for Akamai HD player verification.\n",
      "\n",
      "        Algorithm originally documented by KSV, source:\n",
      "        http://stream-recorder.com/forum/showpost.php?p=43761&postcount=13\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given an HTTP response from the sessino endpoint, extract the nonce, so we can \"sign\" requests with it.\n",
      "        We don't really sign the requests in the traditional sense of a nonce, we just incude them in the auth requests.\n",
      "\n",
      "        :param http_result: HTTP response from the bbc session endpoint.\n",
      "        :type http_result: requests.Response\n",
      "        :return: nonce to \"sign\" url requests with\n",
      "        :rtype: string\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Find the Video Packet ID in the HTML for the provided URL\n",
      "\n",
      "        :param url: URL to download, if res is not provided.\n",
      "        :param res: Provide a cached version of the HTTP response to search\n",
      "        :type url: string\n",
      "        :type res: requests.Response\n",
      "        :return: Video Packet ID for a Programme in iPlayer\n",
      "        :rtype: string\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wrapper around json.loads.\n",
      "\n",
      "    Wraps errors in custom exception with a snippet of the data in the message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wrapper around ElementTree.fromstring with some extras.\n",
      "\n",
      "    Provides these extra features:\n",
      "     - Handles incorrectly encoded XML\n",
      "     - Allows stripping namespace information\n",
      "     - Wraps errors in custom exception with a snippet of the data in the message\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses a query string into a dict.\n",
      "\n",
      "    Unlike parse_qs and parse_qsl, duplicate keys are not preserved in\n",
      "    favor of a simpler return value.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Search for a key in a nested dict, or list of nested dicts, and return the values.\n",
      "\n",
      "    :param data: dict/list to search\n",
      "    :param key: key to find\n",
      "    :return: matches for key\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spawn the process defined in `cmd`\n",
      "\n",
      "        parameters is converted to options the short and long option prefixes\n",
      "        if a list is given as the value, the parameter is repeated with each\n",
      "        value\n",
      "\n",
      "        If timeout is set the spawn will block until the process returns or\n",
      "        the timeout expires.\n",
      "\n",
      "        :param parameters: optional parameters\n",
      "        :param arguments: positional arguments\n",
      "        :param stderr: where to redirect stderr to\n",
      "        :param timeout: timeout for short lived process\n",
      "        :param long_option_prefix: option prefix, default -\n",
      "        :param short_option_prefix: long option prefix, default --\n",
      "        :return: spawned process\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Brute force regex based HTML tag parser. This is a rough-and-ready searcher to find HTML tags when\n",
      "    standards compliance is not required. Will find tags that are commented out, or inside script tag etc.\n",
      "\n",
      "    :param html: HTML page\n",
      "    :param tag: tag name to find\n",
      "    :return: generator with Tags\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempt to parse a DASH manifest file and return its streams\n",
      "\n",
      "        :param session: Streamlink session instance\n",
      "        :param url_or_manifest: URL of the manifest file or an XML manifest string\n",
      "        :return: a dict of name -> DASHStream instances\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Determine which Unicode encoding the JSON text sample is encoded with\n",
      "\n",
      "        RFC4627 (http://www.ietf.org/rfc/rfc4627.txt) suggests that the encoding of JSON text can be determined\n",
      "        by checking the pattern of NULL bytes in first 4 octets of the text.\n",
      "        :param sample: a sample of at least 4 bytes of the JSON text\n",
      "        :return: the most likely encoding of the JSON text\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses JSON from a response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses XML from a response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses a semi-colon delimited list of cookies.\n",
      "\n",
      "        Example: foo=bar;baz=qux\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses a semi-colon delimited list of headers.\n",
      "\n",
      "        Example: foo=bar;baz=qux\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses a semi-colon delimited list of query parameters.\n",
      "\n",
      "        Example: foo=bar;baz=qux\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the message for this LogRecord.\n",
      "\n",
      "        Return the message for this LogRecord after merging any user-supplied\n",
      "        arguments with the message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A factory method which can be overridden in subclasses to create\n",
      "        specialized LogRecords.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempt a login to LiveEdu.tv\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a plugin from the same directory as the calling plugin.\n",
      "\n",
      "    The path used is extracted from the last call in module scope,\n",
      "    therefore this must be called only from module level in the\n",
      "    originating plugin or the correct plugin path will not be found.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Update or remove keys from a query string in a URL\n",
      "\n",
      "    :param url: URL to update\n",
      "    :param qsd: dict of keys to update, a None value leaves it unchanged\n",
      "    :param remove: list of keys to remove, or \"*\" to remove all\n",
      "                   note: updated keys are never removed, even if unchanged\n",
      "    :return: updated URL\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reads FLV tags from fd or buf and returns them with adjusted\n",
      "           timestamps.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Find all the arguments required by name\n",
      "\n",
      "        :param name: name of the argument the find the dependencies\n",
      "\n",
      "        :return: list of dependant arguments\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if file already exists and ask the user if it should\n",
      "    be overwritten if it does.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decides where to write the stream.\n",
      "\n",
      "    Depending on arguments it can be one of these:\n",
      "     - The stdout pipe\n",
      "     - A subprocess' stdin pipe\n",
      "     - A named pipe that the subprocess reads from\n",
      "     - A regular file\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a HTTP server listening on a given host and port.\n",
      "\n",
      "    If host is empty, listen on all available interfaces, and if port is 0,\n",
      "    listen on a random high port.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Repeatedly accept HTTP connections on a server.\n",
      "\n",
      "    Forever if the serving externally, or while a player is running if it is not\n",
      "    empty.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Continuously output the stream over HTTP.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prepares a filename to be passed to the player.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens a stream and reads 8192 bytes from it.\n",
      "\n",
      "    This is useful to check if a stream actually has data\n",
      "    before opening the output.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Open stream, create output and finally write the stream to output.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reads data from stream and then writes it to the output.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decides what to do with the selected stream.\n",
      "\n",
      "    Depending on arguments it can be one of these:\n",
      "     - Output internal command-line\n",
      "     - Output JSON represenation\n",
      "     - Continuously output the stream over HTTP\n",
      "     - Output stream data to selected output\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetches streams using correct parameters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempts to fetch streams repeatedly\n",
      "       until some are returned or limit hit.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the real stream name of a synonym.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Formats a dict of streams.\n",
      "\n",
      "    Filters out synonyms and displays them next to\n",
      "    the stream they point to.\n",
      "\n",
      "    Streams are sorted according to their quality\n",
      "    (based on plugin.stream_weight).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The URL handler.\n",
      "\n",
      "    Attempts to resolve the URL to a plugin and then attempts\n",
      "    to fetch a list of available streams.\n",
      "\n",
      "    Proceeds to handle stream if user specified a valid one,\n",
      "    otherwise output list of valid streams.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Outputs a list of all plugins Streamlink has loaded.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens a web browser to allow the user to grant Streamlink\n",
      "       access to their Twitch account.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempts to load plugins from a list of directories.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses arguments.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Console setup.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets the global HTTP settings, such as proxy and headers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads any additional plugins.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets Streamlink options.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Show current installed versions\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Try to find a stream_id\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fallback if no stream_id was found before\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets general options used by plugins and streams originating\n",
      "        from this session object.\n",
      "\n",
      "        :param key: key of the option\n",
      "        :param value: value to set the option to\n",
      "\n",
      "\n",
      "        **Available options**:\n",
      "\n",
      "        ======================== =========================================\n",
      "        hds-live-edge            ( float) Specify the time live HDS\n",
      "                                 streams will start from the edge of\n",
      "                                 stream, default: ``10.0``\n",
      "\n",
      "        hds-segment-attempts     (int) How many attempts should be done\n",
      "                                 to download each HDS segment, default: ``3``\n",
      "\n",
      "        hds-segment-threads      (int) The size of the thread pool used\n",
      "                                 to download segments, default: ``1``\n",
      "\n",
      "        hds-segment-timeout      (float) HDS segment connect and read\n",
      "                                 timeout, default: ``10.0``\n",
      "\n",
      "        hds-timeout              (float) Timeout for reading data from\n",
      "                                 HDS streams, default: ``60.0``\n",
      "\n",
      "        hls-live-edge            (int) How many segments from the end\n",
      "                                 to start live streams on, default: ``3``\n",
      "\n",
      "        hls-segment-attempts     (int) How many attempts should be done\n",
      "                                 to download each HLS segment, default: ``3``\n",
      "\n",
      "        hls-segment-threads      (int) The size of the thread pool used\n",
      "                                 to download segments, default: ``1``\n",
      "\n",
      "        hls-segment-timeout      (float) HLS segment connect and read\n",
      "                                 timeout, default: ``10.0``\n",
      "\n",
      "        hls-timeout              (float) Timeout for reading data from\n",
      "                                 HLS streams, default: ``60.0``\n",
      "\n",
      "        http-proxy               (str) Specify a HTTP proxy to use for\n",
      "                                 all HTTP requests\n",
      "\n",
      "        https-proxy              (str) Specify a HTTPS proxy to use for\n",
      "                                 all HTTPS requests\n",
      "\n",
      "        http-cookies             (dict or str) A dict or a semi-colon (;)\n",
      "                                 delimited str of cookies to add to each\n",
      "                                 HTTP request, e.g. ``foo=bar;baz=qux``\n",
      "\n",
      "        http-headers             (dict or str) A dict or semi-colon (;)\n",
      "                                 delimited str of headers to add to each\n",
      "                                 HTTP request, e.g. ``foo=bar;baz=qux``\n",
      "\n",
      "        http-query-params        (dict or str) A dict or a ampersand (&)\n",
      "                                 delimited string of query parameters to\n",
      "                                 add to each HTTP request,\n",
      "                                 e.g. ``foo=bar&baz=qux``\n",
      "\n",
      "        http-trust-env           (bool) Trust HTTP settings set in the\n",
      "                                 environment, such as environment\n",
      "                                 variables (HTTP_PROXY, etc) and\n",
      "                                 ~/.netrc authentication\n",
      "\n",
      "        http-ssl-verify          (bool) Verify SSL certificates,\n",
      "                                 default: ``True``\n",
      "\n",
      "        http-ssl-cert            (str or tuple) SSL certificate to use,\n",
      "                                 can be either a .pem file (str) or a\n",
      "                                 .crt/.key pair (tuple)\n",
      "\n",
      "        http-timeout             (float) General timeout used by all HTTP\n",
      "                                 requests except the ones covered by\n",
      "                                 other options, default: ``20.0``\n",
      "\n",
      "        http-stream-timeout      (float) Timeout for reading data from\n",
      "                                 HTTP streams, default: ``60.0``\n",
      "\n",
      "        subprocess-errorlog      (bool) Log errors from subprocesses to\n",
      "                                 a file located in the temp directory\n",
      "\n",
      "        subprocess-errorlog-path (str) Log errors from subprocesses to\n",
      "                                 a specific file\n",
      "\n",
      "        ringbuffer-size          (int) The size of the internal ring\n",
      "                                 buffer used by most stream types,\n",
      "                                 default: ``16777216`` (16MB)\n",
      "\n",
      "        rtmp-proxy               (str) Specify a proxy (SOCKS) that RTMP\n",
      "                                 streams will use\n",
      "\n",
      "        rtmp-rtmpdump            (str) Specify the location of the\n",
      "                                 rtmpdump executable used by RTMP streams,\n",
      "                                 e.g. ``/usr/local/bin/rtmpdump``\n",
      "\n",
      "        rtmp-timeout             (float) Timeout for reading data from\n",
      "                                 RTMP streams, default: ``60.0``\n",
      "\n",
      "        ffmpeg-ffmpeg            (str) Specify the location of the\n",
      "                                 ffmpeg executable use by Muxing streams\n",
      "                                 e.g. ``/usr/local/bin/ffmpeg``\n",
      "\n",
      "        ffmpeg-verbose           (bool) Log stderr from ffmpeg to the\n",
      "                                 console\n",
      "\n",
      "        ffmpeg-verbose-path      (str) Specify the location of the\n",
      "                                 ffmpeg stderr log file\n",
      "\n",
      "        ffmpeg-video-transcode   (str) The codec to use if transcoding\n",
      "                                 video when muxing with ffmpeg\n",
      "                                 e.g. ``h264``\n",
      "\n",
      "        ffmpeg-audio-transcode   (str) The codec to use if transcoding\n",
      "                                 audio when muxing with ffmpeg\n",
      "                                 e.g. ``aac``\n",
      "\n",
      "        stream-segment-attempts  (int) How many attempts should be done\n",
      "                                 to download each segment, default: ``3``.\n",
      "                                 General option used by streams not\n",
      "                                 covered by other options.\n",
      "\n",
      "        stream-segment-threads   (int) The size of the thread pool used\n",
      "                                 to download segments, default: ``1``.\n",
      "                                 General option used by streams not\n",
      "                                 covered by other options.\n",
      "\n",
      "        stream-segment-timeout   (float) Segment connect and read\n",
      "                                 timeout, default: ``10.0``.\n",
      "                                 General option used by streams not\n",
      "                                 covered by other options.\n",
      "\n",
      "        stream-timeout           (float) Timeout for reading data from\n",
      "                                 stream, default: ``60.0``.\n",
      "                                 General option used by streams not\n",
      "                                 covered by other options.\n",
      "\n",
      "        locale                   (str) Locale setting, in the RFC 1766 format\n",
      "                                 eg. en_US or es_ES\n",
      "                                 default: ``system locale``.\n",
      "\n",
      "        user-input-requester     (UserInputRequester) instance of UserInputRequester\n",
      "                                 to collect input from the user at runtime. Must be\n",
      "                                 set before the plugins are loaded.\n",
      "                                 default: ``UserInputRequester``.\n",
      "        ======================== =========================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns current value of specified option.\n",
      "\n",
      "        :param key: key of the option\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets plugin specific options used by plugins originating\n",
      "        from this session object.\n",
      "\n",
      "        :param plugin: name of the plugin\n",
      "        :param key: key of the option\n",
      "        :param value: value to set the option to\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns current value of plugin specific option.\n",
      "\n",
      "        :param plugin: name of the plugin\n",
      "        :param key: key of the option\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempts to find a plugin that can use this URL.\n",
      "\n",
      "        The default protocol (http) will be prefixed to the URL if\n",
      "        not specified.\n",
      "\n",
      "        Raises :exc:`NoPluginError` on failure.\n",
      "\n",
      "        :param url: a URL to match against loaded plugins\n",
      "        :param follow_redirect: follow redirects\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempt to load plugins from the path specified.\n",
      "\n",
      "        :param path: full path to a directory where to look for plugins\n",
      "----------------------------------------------------------------------------------------------------\n",
      "converts a timestamp to seconds\n",
      "\n",
      "      - hours:minutes:seconds to seconds\n",
      "      - minutes:seconds to seconds\n",
      "      - 11h22m33s to seconds\n",
      "      - 11h to seconds\n",
      "      - 20h15m to seconds\n",
      "      - seconds to seconds\n",
      "\n",
      "    :param value: hh:mm:ss ; 00h00m00s ; seconds\n",
      "    :return: seconds\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if the string value starts with another string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if the string value ends with another string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if the string value contains another string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a named attribute from an object.\n",
      "\n",
      "    When a default argument is given, it is returned when the attribute\n",
      "    doesn't exist.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Filters out unwanted items using the specified function.\n",
      "\n",
      "    Supports both dicts and sequences, key/value pairs are\n",
      "    expanded when applied to a dict.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Apply function to each value inside the sequence or dict.\n",
      "\n",
      "    Supports both dicts and sequences, key/value pairs are\n",
      "    expanded when applied to a dict.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses an URL and validates its attributes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Find a XML element via xpath.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Find a list of XML elements via xpath.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Finds embedded player url in HTTP response.\n",
      "\n",
      "    :param response: Response object.\n",
      "    :returns: Player url (str).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempts to parse a M3U8 playlist from a string of data.\n",
      "\n",
      "    If specified, *base_uri* is the base URI that relative URIs will\n",
      "    be joined together with, otherwise relative URIs will be as is.\n",
      "\n",
      "    If specified, *parser* can be a M3U8Parser subclass to be used\n",
      "    to parse the data.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if the current player supports adding a title\n",
      "\n",
      "        :param cmd: command to test\n",
      "        :return: name of the player|None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Logs in to Steam\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the stream_id contained in the HTML.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns a nested list of different stream options.\n",
      "\n",
      "        Each entry in the list will contain a stream_url and stream_quality_name\n",
      "        for each stream occurrence that was found in the JS.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "login and update cached cookies\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a key-function mapping.\n",
      "\n",
      "        The return value from the function should be either\n",
      "          - A tuple containing a name and stream\n",
      "          - A iterator of tuples containing a name and stream\n",
      "\n",
      "        Any extra arguments will be passed to the function.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Makes a call against the api.\n",
      "\n",
      "        :param entrypoint: API method to call.\n",
      "        :param params: parameters to include in the request data.\n",
      "        :param schema: schema to use to validate the data\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts a session against Crunchyroll's server.\n",
      "            Is recommended that you call this method before making any other calls\n",
      "            to make sure you have a valid session against the server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the data for a certain media item.\n",
      "\n",
      "            :param media_id: id that identifies the media item to be accessed.\n",
      "            :param fields: list of the media\"s field to be returned. By default the\n",
      "            API returns some fields, but others are not returned unless they are\n",
      "            explicity asked for. I have no real documentation on the fields, but\n",
      "            they all seem to start with the \"media.\" prefix (e.g. media.name,\n",
      "            media.stream_data).\n",
      "            :param schema: validation schema to use\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new CrunchyrollAPI object, initiates it's session and\n",
      "        tries to authenticate it either by using saved credentials or the\n",
      "        user's username and password.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read the frame images from a directory and join them as a video\n",
      "\n",
      "    Args:\n",
      "        frame_dir (str): The directory containing video frames.\n",
      "        video_file (str): Output filename.\n",
      "        fps (float): FPS of the output video.\n",
      "        fourcc (str): Fourcc of the output video, this should be compatible\n",
      "            with the output file type.\n",
      "        filename_tmpl (str): Filename template with the index as the variable.\n",
      "        start (int): Starting frame index.\n",
      "        end (int): Ending frame index.\n",
      "        show_progress (bool): Whether to show a progress bar.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read the next frame.\n",
      "\n",
      "        If the next frame have been decoded before and in the cache, then\n",
      "        return it directly, otherwise decode, cache and return it.\n",
      "\n",
      "        Returns:\n",
      "            ndarray or None: Return the frame if successful, otherwise None.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get frame by index.\n",
      "\n",
      "        Args:\n",
      "            frame_id (int): Index of the expected frame, 0-based.\n",
      "\n",
      "        Returns:\n",
      "            ndarray or None: Return the frame if successful, otherwise None.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a video to frame images\n",
      "\n",
      "        Args:\n",
      "            frame_dir (str): Output directory to store all the frame images.\n",
      "            file_start (int): Filenames will start from the specified number.\n",
      "            filename_tmpl (str): Filename template with the index as the\n",
      "                placeholder.\n",
      "            start (int): The starting frame index.\n",
      "            max_num (int): Maximum number of frames to be written.\n",
      "            show_progress (bool): Whether to show a progress bar.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Track the progress of tasks execution with a progress bar.\n",
      "\n",
      "    Tasks are done with a simple for-loop.\n",
      "\n",
      "    Args:\n",
      "        func (callable): The function to be applied to each task.\n",
      "        tasks (list or tuple[Iterable, int]): A list of tasks or\n",
      "            (tasks, total num).\n",
      "        bar_width (int): Width of progress bar.\n",
      "\n",
      "    Returns:\n",
      "        list: The task results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Track the progress of parallel task execution with a progress bar.\n",
      "\n",
      "    The built-in :mod:`multiprocessing` module is used for process pools and\n",
      "    tasks are done with :func:`Pool.map` or :func:`Pool.imap_unordered`.\n",
      "\n",
      "    Args:\n",
      "        func (callable): The function to be applied to each task.\n",
      "        tasks (list or tuple[Iterable, int]): A list of tasks or\n",
      "            (tasks, total num).\n",
      "        nproc (int): Process (worker) number.\n",
      "        initializer (None or callable): Refer to :class:`multiprocessing.Pool`\n",
      "            for details.\n",
      "        initargs (None or tuple): Refer to :class:`multiprocessing.Pool` for\n",
      "            details.\n",
      "        chunksize (int): Refer to :class:`multiprocessing.Pool` for details.\n",
      "        bar_width (int): Width of progress bar.\n",
      "        skip_first (bool): Whether to skip the first sample for each worker\n",
      "            when estimating fps, since the initialization step may takes\n",
      "            longer.\n",
      "        keep_order (bool): If True, :func:`Pool.imap` is used, otherwise\n",
      "            :func:`Pool.imap_unordered` is used.\n",
      "\n",
      "    Returns:\n",
      "        list: The task results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Flip an image horizontally or vertically.\n",
      "\n",
      "    Args:\n",
      "        img (ndarray): Image to be flipped.\n",
      "        direction (str): The flip direction, either \"horizontal\" or \"vertical\".\n",
      "\n",
      "    Returns:\n",
      "        ndarray: The flipped image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rotate an image.\n",
      "\n",
      "    Args:\n",
      "        img (ndarray): Image to be rotated.\n",
      "        angle (float): Rotation angle in degrees, positive values mean\n",
      "            clockwise rotation.\n",
      "        center (tuple): Center of the rotation in the source image, by default\n",
      "            it is the center of the image.\n",
      "        scale (float): Isotropic scale factor.\n",
      "        border_value (int): Border value.\n",
      "        auto_bound (bool): Whether to adjust the image size to cover the whole\n",
      "            rotated image.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: The rotated image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clip bboxes to fit the image shape.\n",
      "\n",
      "    Args:\n",
      "        bboxes (ndarray): Shape (..., 4*k)\n",
      "        img_shape (tuple): (height, width) of the image.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: Clipped bboxes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scaling bboxes w.r.t the box center.\n",
      "\n",
      "    Args:\n",
      "        bboxes (ndarray): Shape(..., 4).\n",
      "        scale (float): Scaling factor.\n",
      "        clip_shape (tuple, optional): If specified, bboxes that exceed the\n",
      "            boundary will be clipped according to the given shape (h, w).\n",
      "\n",
      "    Returns:\n",
      "        ndarray: Scaled bboxes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Crop image patches.\n",
      "\n",
      "    3 steps: scale the bboxes -> clip bboxes -> crop and pad.\n",
      "\n",
      "    Args:\n",
      "        img (ndarray): Image to be cropped.\n",
      "        bboxes (ndarray): Shape (k, 4) or (4, ), location of cropped bboxes.\n",
      "        scale (float, optional): Scale ratio of bboxes, the default value\n",
      "            1.0 means no padding.\n",
      "        pad_fill (number or list): Value to be filled for padding, None for\n",
      "            no padding.\n",
      "\n",
      "    Returns:\n",
      "        list or ndarray: The cropped image patches.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pad an image to a certain shape.\n",
      "\n",
      "    Args:\n",
      "        img (ndarray): Image to be padded.\n",
      "        shape (tuple): Expected padding shape.\n",
      "        pad_val (number or sequence): Values to be filled in padding areas.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: The padded image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pad an image to ensure each edge to be multiple to some number.\n",
      "\n",
      "    Args:\n",
      "        img (ndarray): Image to be padded.\n",
      "        divisor (int): Padded image edges will be multiple to divisor.\n",
      "        pad_val (number or sequence): Same as :func:`impad`.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: The padded image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rescale a size by a ratio.\n",
      "\n",
      "    Args:\n",
      "        size (tuple): w, h.\n",
      "        scale (float): Scaling factor.\n",
      "\n",
      "    Returns:\n",
      "        tuple[int]: scaled size.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resize image to a given size.\n",
      "\n",
      "    Args:\n",
      "        img (ndarray): The input image.\n",
      "        size (tuple): Target (w, h).\n",
      "        return_scale (bool): Whether to return `w_scale` and `h_scale`.\n",
      "        interpolation (str): Interpolation method, accepted values are\n",
      "            \"nearest\", \"bilinear\", \"bicubic\", \"area\", \"lanczos\".\n",
      "\n",
      "    Returns:\n",
      "        tuple or ndarray: (`resized_img`, `w_scale`, `h_scale`) or\n",
      "            `resized_img`.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resize image to the same size of a given image.\n",
      "\n",
      "    Args:\n",
      "        img (ndarray): The input image.\n",
      "        dst_img (ndarray): The target image.\n",
      "        return_scale (bool): Whether to return `w_scale` and `h_scale`.\n",
      "        interpolation (str): Same as :func:`resize`.\n",
      "\n",
      "    Returns:\n",
      "        tuple or ndarray: (`resized_img`, `w_scale`, `h_scale`) or\n",
      "            `resized_img`.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resize image while keeping the aspect ratio.\n",
      "\n",
      "    Args:\n",
      "        img (ndarray): The input image.\n",
      "        scale (float or tuple[int]): The scaling factor or maximum size.\n",
      "            If it is a float number, then the image will be rescaled by this\n",
      "            factor, else if it is a tuple of 2 integers, then the image will\n",
      "            be rescaled as large as possible within the scale.\n",
      "        return_scale (bool): Whether to return the scaling factor besides the\n",
      "            rescaled image.\n",
      "        interpolation (str): Same as :func:`resize`.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: The rescaled image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Register a handler for some file extensions.\n",
      "\n",
      "    Args:\n",
      "        handler (:obj:`BaseFileHandler`): Handler to be registered.\n",
      "        file_formats (str or list[str]): File formats to be handled by this\n",
      "            handler.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get priority value.\n",
      "\n",
      "    Args:\n",
      "        priority (int or str or :obj:`Priority`): Priority.\n",
      "\n",
      "    Returns:\n",
      "        int: The priority value.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dequantize an array.\n",
      "\n",
      "    Args:\n",
      "        arr (ndarray): Input array.\n",
      "        min_val (scalar): Minimum value to be clipped.\n",
      "        max_val (scalar): Maximum value to be clipped.\n",
      "        levels (int): Quantization levels.\n",
      "        dtype (np.type): The type of the dequantized array.\n",
      "\n",
      "    Returns:\n",
      "        tuple: Dequantized array.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Show an image.\n",
      "\n",
      "    Args:\n",
      "        img (str or ndarray): The image to be displayed.\n",
      "        win_name (str): The window name.\n",
      "        wait_time (int): Value of waitKey param.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Draw bboxes on an image.\n",
      "\n",
      "    Args:\n",
      "        img (str or ndarray): The image to be displayed.\n",
      "        bboxes (list or ndarray): A list of ndarray of shape (k, 4).\n",
      "        colors (list[str or tuple or Color]): A list of colors.\n",
      "        top_k (int): Plot the first k bboxes only if set positive.\n",
      "        thickness (int): Thickness of lines.\n",
      "        show (bool): Whether to show the image.\n",
      "        win_name (str): The window name.\n",
      "        wait_time (int): Value of waitKey param.\n",
      "        out_file (str, optional): The filename to write the image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read an optical flow map.\n",
      "\n",
      "    Args:\n",
      "        flow_or_path (ndarray or str): A flow map or filepath.\n",
      "        quantize (bool): whether to read quantized pair, if set to True,\n",
      "            remaining args will be passed to :func:`dequantize_flow`.\n",
      "        concat_axis (int): The axis that dx and dy are concatenated,\n",
      "            can be either 0 or 1. Ignored if quantize is False.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: Optical flow represented as a (h, w, 2) numpy array\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Write optical flow to file.\n",
      "\n",
      "    If the flow is not quantized, it will be saved as a .flo file losslessly,\n",
      "    otherwise a jpeg image which is lossy but of much smaller size. (dx and dy\n",
      "    will be concatenated horizontally into a single image if quantize is True.)\n",
      "\n",
      "    Args:\n",
      "        flow (ndarray): (h, w, 2) array of optical flow.\n",
      "        filename (str): Output filepath.\n",
      "        quantize (bool): Whether to quantize the flow and save it to 2 jpeg\n",
      "            images. If set to True, remaining args will be passed to\n",
      "            :func:`quantize_flow`.\n",
      "        concat_axis (int): The axis that dx and dy are concatenated,\n",
      "            can be either 0 or 1. Ignored if quantize is False.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Recover from quantized flow.\n",
      "\n",
      "    Args:\n",
      "        dx (ndarray): Quantized dx.\n",
      "        dy (ndarray): Quantized dy.\n",
      "        max_val (float): Maximum value used when quantizing.\n",
      "        denorm (bool): Whether to multiply flow values with width/height.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: Dequantized flow.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load state_dict to a module.\n",
      "\n",
      "    This method is modified from :meth:`torch.nn.Module.load_state_dict`.\n",
      "    Default value for ``strict`` is set to ``False`` and the message for\n",
      "    param mismatch will be shown even if strict is False.\n",
      "\n",
      "    Args:\n",
      "        module (Module): Module that receives the state_dict.\n",
      "        state_dict (OrderedDict): Weights.\n",
      "        strict (bool): whether to strictly enforce that the keys\n",
      "            in :attr:`state_dict` match the keys returned by this module's\n",
      "            :meth:`~torch.nn.Module.state_dict` function. Default: ``False``.\n",
      "        logger (:obj:`logging.Logger`, optional): Logger to log the error\n",
      "            message. If not specified, print function will be used.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load checkpoint from a file or URI.\n",
      "\n",
      "    Args:\n",
      "        model (Module): Module to load checkpoint.\n",
      "        filename (str): Either a filepath or URL or modelzoo://xxxxxxx.\n",
      "        map_location (str): Same as :func:`torch.load`.\n",
      "        strict (bool): Whether to allow different params for the model and\n",
      "            checkpoint.\n",
      "        logger (:mod:`logging.Logger` or None): The logger for error message.\n",
      "\n",
      "    Returns:\n",
      "        dict or OrderedDict: The loaded checkpoint.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Copy a model state_dict to cpu.\n",
      "\n",
      "    Args:\n",
      "        state_dict (OrderedDict): Model weights on GPU.\n",
      "\n",
      "    Returns:\n",
      "        OrderedDict: Model weights on GPU.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Save checkpoint to file.\n",
      "\n",
      "    The checkpoint will have 3 fields: ``meta``, ``state_dict`` and\n",
      "    ``optimizer``. By default ``meta`` will contain version and time info.\n",
      "\n",
      "    Args:\n",
      "        model (Module): Module whose params are to be saved.\n",
      "        filename (str): Checkpoint filename.\n",
      "        optimizer (:obj:`Optimizer`, optional): Optimizer to be saved.\n",
      "        meta (dict, optional): Metadata to be saved in checkpoint.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Init the optimizer.\n",
      "\n",
      "        Args:\n",
      "            optimizer (dict or :obj:`~torch.optim.Optimizer`): Either an\n",
      "                optimizer object or a dict used for constructing the optimizer.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`~torch.optim.Optimizer`: An optimizer object.\n",
      "\n",
      "        Examples:\n",
      "            >>> optimizer = dict(type='SGD', lr=0.01, momentum=0.9)\n",
      "            >>> type(runner.init_optimizer(optimizer))\n",
      "            <class 'torch.optim.sgd.SGD'>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Init the logger.\n",
      "\n",
      "        Args:\n",
      "            log_dir(str, optional): Log file directory. If not specified, no\n",
      "                log file will be used.\n",
      "            level (int or str): See the built-in python logging module.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`~logging.Logger`: Python logger.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get current learning rates.\n",
      "\n",
      "        Returns:\n",
      "            list: Current learning rate of all param groups.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Register a hook into the hook list.\n",
      "\n",
      "        Args:\n",
      "            hook (:obj:`Hook`): The hook to be registered.\n",
      "            priority (int or str or :obj:`Priority`): Hook priority.\n",
      "                Lower value means higher priority.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Start running.\n",
      "\n",
      "        Args:\n",
      "            data_loaders (list[:obj:`DataLoader`]): Dataloaders for training\n",
      "                and validation.\n",
      "            workflow (list[tuple]): A list of (phase, epochs) to specify the\n",
      "                running order and epochs. E.g, [('train', 2), ('val', 1)] means\n",
      "                running 2 epochs for training and 1 epoch for validation,\n",
      "                iteratively.\n",
      "            max_epochs (int): Total training epochs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Register default hooks for training.\n",
      "\n",
      "        Default hooks include:\n",
      "\n",
      "        - LrUpdaterHook\n",
      "        - OptimizerStepperHook\n",
      "        - CheckpointSaverHook\n",
      "        - IterTimerHook\n",
      "        - LoggerHook(s)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a video with ffmpeg.\n",
      "\n",
      "    This provides a general api to ffmpeg, the executed command is::\n",
      "\n",
      "        `ffmpeg -y <pre_options> -i <in_file> <options> <out_file>`\n",
      "\n",
      "    Options(kwargs) are mapped to ffmpeg commands with the following rules:\n",
      "\n",
      "    - key=val: \"-key val\"\n",
      "    - key=True: \"-key\"\n",
      "    - key=False: \"\"\n",
      "\n",
      "    Args:\n",
      "        in_file (str): Input video filename.\n",
      "        out_file (str): Output video filename.\n",
      "        pre_options (str): Options appears before \"-i <in_file>\".\n",
      "        print_cmd (bool): Whether to print the final ffmpeg command.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resize a video.\n",
      "\n",
      "    Args:\n",
      "        in_file (str): Input video filename.\n",
      "        out_file (str): Output video filename.\n",
      "        size (tuple): Expected size (w, h), eg, (320, 240) or (320, -1).\n",
      "        ratio (tuple or float): Expected resize ratio, (2, 0.5) means\n",
      "            (w*2, h*0.5).\n",
      "        keep_ar (bool): Whether to keep original aspect ratio.\n",
      "        log_level (str): Logging level of ffmpeg.\n",
      "        print_cmd (bool): Whether to print the final ffmpeg command.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cut a clip from a video.\n",
      "\n",
      "    Args:\n",
      "        in_file (str): Input video filename.\n",
      "        out_file (str): Output video filename.\n",
      "        start (None or float): Start time (in seconds).\n",
      "        end (None or float): End time (in seconds).\n",
      "        vcodec (None or str): Output video codec, None for unchanged.\n",
      "        acodec (None or str): Output audio codec, None for unchanged.\n",
      "        log_level (str): Logging level of ffmpeg.\n",
      "        print_cmd (bool): Whether to print the final ffmpeg command.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Concatenate multiple videos into a single one.\n",
      "\n",
      "    Args:\n",
      "        video_list (list): A list of video filenames\n",
      "        out_file (str): Output video filename\n",
      "        vcodec (None or str): Output video codec, None for unchanged\n",
      "        acodec (None or str): Output audio codec, None for unchanged\n",
      "        log_level (str): Logging level of ffmpeg.\n",
      "        print_cmd (bool): Whether to print the final ffmpeg command.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load a text file and parse the content as a list of strings.\n",
      "\n",
      "    Args:\n",
      "        filename (str): Filename.\n",
      "        prefix (str): The prefix to be inserted to the begining of each item.\n",
      "        offset (int): The offset of lines.\n",
      "        max_num (int): The maximum number of lines to be read,\n",
      "            zeros and negatives mean no limitation.\n",
      "\n",
      "    Returns:\n",
      "        list[str]: A list of strings.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load a text file and parse the content as a dict.\n",
      "\n",
      "    Each line of the text file will be two or more columns splited by\n",
      "    whitespaces or tabs. The first column will be parsed as dict keys, and\n",
      "    the following columns will be parsed as dict values.\n",
      "\n",
      "    Args:\n",
      "        filename(str): Filename.\n",
      "        key_type(type): Type of the dict's keys. str is user by default and\n",
      "            type conversion will be performed if specified.\n",
      "\n",
      "    Returns:\n",
      "        dict: The parsed contents.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "3x3 convolution with padding\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initialize an object from dict.\n",
      "\n",
      "    The dict must contain the key \"type\", which indicates the object type, it\n",
      "    can be either a string or type, such as \"list\" or ``list``. Remaining\n",
      "    fields are treated as the arguments for constructing the object.\n",
      "\n",
      "    Args:\n",
      "        info (dict): Object types and arguments.\n",
      "        parent (:class:`module`): Module which may containing expected object\n",
      "            classes.\n",
      "        default_args (dict, optional): Default arguments for initializing the\n",
      "            object.\n",
      "\n",
      "    Returns:\n",
      "        any type: Object built from the dict.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read an image.\n",
      "\n",
      "    Args:\n",
      "        img_or_path (ndarray or str): Either a numpy array or image path.\n",
      "            If it is a numpy array (loaded image), then it will be returned\n",
      "            as is.\n",
      "        flag (str): Flags specifying the color type of a loaded image,\n",
      "            candidates are `color`, `grayscale` and `unchanged`.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: Loaded image array.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read an image from bytes.\n",
      "\n",
      "    Args:\n",
      "        content (bytes): Image bytes got from files or other streams.\n",
      "        flag (str): Same as :func:`imread`.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: Loaded image array.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Write image to file\n",
      "\n",
      "    Args:\n",
      "        img (ndarray): Image array to be written.\n",
      "        file_path (str): Image file path.\n",
      "        params (None or list): Same as opencv's :func:`imwrite` interface.\n",
      "        auto_mkdir (bool): If the parent folder of `file_path` does not exist,\n",
      "            whether to create it automatically.\n",
      "\n",
      "    Returns:\n",
      "        bool: Successful or not.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a BGR image to grayscale image.\n",
      "\n",
      "    Args:\n",
      "        img (ndarray): The input image.\n",
      "        keepdim (bool): If False (by default), then return the grayscale image\n",
      "            with 2 dims, otherwise 3 dims.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: The converted grayscale image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a grayscale image to BGR image.\n",
      "\n",
      "    Args:\n",
      "        img (ndarray or str): The input image.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: The converted BGR image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cast elements of an iterable object into some type.\n",
      "\n",
      "    Args:\n",
      "        inputs (Iterable): The input object.\n",
      "        dst_type (type): Destination type.\n",
      "        return_type (type, optional): If specified, the output object will be\n",
      "            converted to this type, otherwise an iterator.\n",
      "\n",
      "    Returns:\n",
      "        iterator or specified type: The converted object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check whether it is a sequence of some type.\n",
      "\n",
      "    Args:\n",
      "        seq (Sequence): The sequence to be checked.\n",
      "        expected_type (type): Expected type of sequence items.\n",
      "        seq_type (type, optional): Expected sequence type.\n",
      "\n",
      "    Returns:\n",
      "        bool: Whether the sequence is valid.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Slice a list into several sub lists by a list of given length.\n",
      "\n",
      "    Args:\n",
      "        in_list (list): The list to be sliced.\n",
      "        lens(int or list): The expected length of each out list.\n",
      "\n",
      "    Returns:\n",
      "        list: A list of sliced list.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A decorator factory to check if prerequisites are satisfied.\n",
      "\n",
      "    Args:\n",
      "        prerequisites (str of list[str]): Prerequisites to be checked.\n",
      "        checker (callable): The checker method that returns True if a\n",
      "            prerequisite is meet, False otherwise.\n",
      "        msg_tmpl (str): The message template with two variables.\n",
      "\n",
      "    Returns:\n",
      "        decorator: A specific decorator.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Average latest n values or all values\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scatters tensor across multiple GPUs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert various input to color tuples.\n",
      "\n",
      "    Args:\n",
      "        color (:obj:`Color`/str/tuple/int/ndarray): Color inputs\n",
      "\n",
      "    Returns:\n",
      "        tuple[int]: A tuple of 3 integers indicating BGR channels.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add check points in a single line.\n",
      "\n",
      "    This method is suitable for running a task on a list of items. A timer will\n",
      "    be registered when the method is called for the first time.\n",
      "\n",
      "    :Example:\n",
      "\n",
      "    >>> import time\n",
      "    >>> import mmcv\n",
      "    >>> for i in range(1, 6):\n",
      "    >>>     # simulate a code block\n",
      "    >>>     time.sleep(i)\n",
      "    >>>     mmcv.check_time('task1')\n",
      "    2.000\n",
      "    3.000\n",
      "    4.000\n",
      "    5.000\n",
      "\n",
      "    Args:\n",
      "        timer_id (str): Timer identifier.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Start the timer.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Total time since the timer is started.\n",
      "\n",
      "        Returns (float): Time in seconds.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time since the last checking.\n",
      "\n",
      "        Either :func:`since_start` or :func:`since_last_check` is a checking\n",
      "        operation.\n",
      "\n",
      "        Returns (float): Time in seconds.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Show optical flow.\n",
      "\n",
      "    Args:\n",
      "        flow (ndarray or str): The optical flow to be displayed.\n",
      "        win_name (str): The window name.\n",
      "        wait_time (int): Value of waitKey param.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert flow map to RGB image.\n",
      "\n",
      "    Args:\n",
      "        flow (ndarray): Array of optical flow.\n",
      "        color_wheel (ndarray or None): Color wheel used to map flow field to\n",
      "            RGB colorspace. Default color wheel will be used if not specified.\n",
      "        unknown_thr (str): Values above this threshold will be marked as\n",
      "            unknown and thus ignored.\n",
      "\n",
      "    Returns:\n",
      "        ndarray: RGB image that can be visualized.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Build a color wheel.\n",
      "\n",
      "    Args:\n",
      "        bins(list or tuple, optional): Specify the number of bins for each\n",
      "            color range, corresponding to six ranges: red -> yellow,\n",
      "            yellow -> green, green -> cyan, cyan -> blue, blue -> magenta,\n",
      "            magenta -> red. [15, 6, 4, 11, 13, 6] is used for default\n",
      "            (see Middlebury).\n",
      "\n",
      "    Returns:\n",
      "        ndarray: Color wheel of shape (total_bins, 3).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Computes the precision@k for the specified values of k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scatter inputs to target gpus.\n",
      "\n",
      "    The only difference from original :func:`scatter` is to add support for\n",
      "    :type:`~mmcv.parallel.DataContainer`.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scatter with support for kwargs dictionary\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This captures a common pattern of fanning out a single value to N steps,\n",
      "    where each step has similar structure. The strict requirement here is that each step\n",
      "    must provide an output named the parameters parallel_step_output.\n",
      "\n",
      "    This takes those steps and then uses a join node to coalesce them so that downstream\n",
      "    steps can depend on a single output.\n",
      "\n",
      "    Currently the join step just does a passthrough with no computation. It remains\n",
      "    to be seen if there should be any work or verification done in this step, especially\n",
      "    in multi-process environments that require persistence between steps.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ensures argument obj is a native Python dictionary, raises an exception if not, and otherwise\n",
      "    returns obj.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ensures argument obj is either a dictionary or None; if the latter, instantiates an empty\n",
      "    dictionary.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Callback receives a stream of event_records\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Record a stream of event records to json\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read a config file and instantiate the RCParser.\n",
      "\n",
      "        Create new :class:`configparser.ConfigParser` for the given **path**\n",
      "        and instantiate the :class:`RCParser` with the ConfigParser as\n",
      "        :attr:`config` attribute.\n",
      "\n",
      "        If the **path** doesn't exist, raise :exc:`ConfigFileError`.\n",
      "        Otherwise return a new :class:`RCParser` instance.\n",
      "\n",
      "        :param path:\n",
      "            Optional path to the config file to parse.\n",
      "            If not given, use ``'~/.pypirc'``.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get config dictionary for the given repository.\n",
      "\n",
      "        If the repository section is not found in the config file,\n",
      "        return ``None``.  If the file is invalid, raise\n",
      "        :exc:`configparser.Error`.\n",
      "\n",
      "        Otherwise return a dictionary with:\n",
      "\n",
      "        * ``'repository'`` -- the repository URL\n",
      "        * ``'username'`` -- username for authentication\n",
      "        * ``'password'`` -- password for authentication\n",
      "\n",
      "        :param repository:\n",
      "            Name or URL of the repository to find in the ``.pypirc`` file.\n",
      "            The repository section must be defined in the config file.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This recursive descent thing formats a config dict for GraphQL.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a pipeline by name. Only constructs that pipeline and caches it.\n",
      "\n",
      "        Args:\n",
      "            name (str): Name of the pipeline to retriever\n",
      "\n",
      "        Returns:\n",
      "            PipelineDefinition: Instance of PipelineDefinition with that name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return all pipelines as a list\n",
      "\n",
      "        Returns:\n",
      "            List[PipelineDefinition]:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This function polls the process until it returns a valid\n",
      "    item or returns PROCESS_DEAD_AND_QUEUE_EMPTY if it is in\n",
      "    a state where the process has terminated and the queue is empty\n",
      "\n",
      "    Warning: if the child process is in an infinite loop. This will\n",
      "    also infinitely loop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute pipeline using message queue as a transport\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Waits until all there are no processes enqueued.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The schema for configuration data that describes the type, optionality, defaults, and description.\n",
      "\n",
      "    Args:\n",
      "        dagster_type (DagsterType):\n",
      "            A ``DagsterType`` describing the schema of this field, ie `Dict({'example': Field(String)})`\n",
      "        default_value (Any):\n",
      "            A default value to use that respects the schema provided via dagster_type\n",
      "        is_optional (bool): Whether the presence of this field is optional\n",
      "        despcription (str):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Builds the execution plan.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Here we build a new ExecutionPlan from a pipeline definition and the environment config.\n",
      "\n",
      "        To do this, we iterate through the pipeline's solids in topological order, and hand off the\n",
      "        execution steps for each solid to a companion _PlanBuilder object.\n",
      "\n",
      "        Once we've processed the entire pipeline, we invoke _PlanBuilder.build() to construct the\n",
      "        ExecutionPlan object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Build a pipeline which is a subset of another pipeline.\n",
      "    Only includes the solids which are in solid_names.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the solid named \"name\". Throws if it does not exist.\n",
      "\n",
      "        Args:\n",
      "            name (str): Name of solid\n",
      "\n",
      "        Returns:\n",
      "            SolidDefinition: SolidDefinition with correct name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the shell commands we'll use to actually build and publish a package to PyPI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tags all submodules for a new release.\n",
      "\n",
      "    Ensures that git tags, as well as the version.py files in each submodule, agree and that the\n",
      "    new version is strictly greater than the current version. Will fail if the new version\n",
      "    is not an increment (following PEP 440). Creates a new git tag and commit.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a context definition from a pre-existing context. This can be useful\n",
      "        in testing contexts where you may want to create a context manually and then\n",
      "        pass it into a one-off PipelineDefinition\n",
      "\n",
      "        Args:\n",
      "            context (ExecutionContext): The context that will provided to the pipeline.\n",
      "        Returns:\n",
      "            PipelineContextDefinition: The passthrough context definition.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A decorator for annotating a function that can take the selected properties\n",
      "    from a ``config_value`` in to an instance of a custom type.\n",
      "\n",
      "    Args:\n",
      "        config_cls (Selector)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A decorator for a annotating a function that can take the selected properties\n",
      "    of a ``config_value`` and an instance of a custom type and materialize it.\n",
      "\n",
      "    Args:\n",
      "        config_cls (Selector):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Automagically wrap a block of text.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download an object from s3.\n",
      "\n",
      "    Args:\n",
      "        info (ExpectationExecutionInfo): Must expose a boto3 S3 client as its `s3` resource.\n",
      "\n",
      "    Returns:\n",
      "        str:\n",
      "            The path to the downloaded object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a file to s3.\n",
      "\n",
      "    Args:\n",
      "        info (ExpectationExecutionInfo): Must expose a boto3 S3 client as its `s3` resource.\n",
      "\n",
      "    Returns:\n",
      "        (str, str):\n",
      "            The bucket and key to which the file was uploaded.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wraps the execution of user-space code in an error boundary. This places a uniform\n",
      "    policy around an user code invoked by the framework. This ensures that all user\n",
      "    errors are wrapped in the DagsterUserCodeExecutionError, and that the original stack\n",
      "    trace of the user error is preserved, so that it can be reported without confusing\n",
      "    framework code in the stack trace, if a tool author wishes to do so. This has\n",
      "    been especially help in a notebooking context.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The missing mkdir -p functionality in os.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wraps the output of a user provided function that may yield or return a value and\n",
      "    returns a generator that asserts it only yields a single value.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In the event of pipeline initialization failure, we want to be able to log the failure\n",
      "    without a dependency on the ExecutionContext to initialize DagsterLog\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Whether the solid execution was successful\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Whether the solid execution was skipped\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return dictionary of transformed results, with keys being output names.\n",
      "        Returns None if execution result isn't a success.\n",
      "\n",
      "        Reconstructs the pipeline context to materialize values.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns transformed value either for DEFAULT_OUTPUT or for the output\n",
      "        given as output_name. Returns None if execution result isn't a success.\n",
      "\n",
      "        Reconstructs the pipeline context to materialize value.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Returns the failing step's data that happened during this solid's execution, if any\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A permissive dict will permit the user to partially specify the permitted fields. Any fields\n",
      "    that are specified and passed in will be type checked. Other fields will be allowed, but\n",
      "    will be ignored by the type checker.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Datasets must be of form \"project.dataset\" or \"dataset\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tables must be of form \"project.dataset.table\" or \"dataset.table\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute the user-specified transform for the solid. Wrap in an error boundary and do\n",
      "    all relevant logging and metrics tracking\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Takes a python cls and creates a type for it in the Dagster domain.\n",
      "\n",
      "    Args:\n",
      "        existing_type (cls)\n",
      "            The python type you want to project in to the Dagster type system.\n",
      "        name (Optional[str]):\n",
      "        description (Optiona[str]):\n",
      "        input_schema (Optional[InputSchema]):\n",
      "            An instance of a class that inherits from :py:class:`InputSchema` that\n",
      "            can map config data to a value of this type.\n",
      "\n",
      "        output_schema (Optiona[OutputSchema]):\n",
      "            An instance of a class that inherits from :py:class:`OutputSchema` that\n",
      "            can map config data to persisting values of this type.\n",
      "\n",
      "        serialization_strategy (Optional[SerializationStrategy]):\n",
      "            The default behavior for how to serialize this value for\n",
      "            persisting between execution steps.\n",
      "\n",
      "        storage_plugins (Optional[Dict[RunStorageMode, TypeStoragePlugin]]):\n",
      "            Storage type specific overrides for the serialization strategy.\n",
      "            This allows for storage specific optimzations such as effecient\n",
      "            distributed storage on S3.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A decorator for creating a resource. The decorated function will be used as the \n",
      "    resource_fn in a ResourceDefinition.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Events API v2 enables you to add PagerDuty's advanced event and incident management\n",
      "        functionality to any system that can make an outbound HTTP connection.\n",
      "\n",
      "        Arguments:\n",
      "            summary {string} -- A high-level, text summary message of the event. Will be used to\n",
      "                                construct an alert's description.\n",
      "\n",
      "                                Example: \"PING OK - Packet loss = 0%, RTA = 1.41 ms\" \"Host\n",
      "                                         'acme-andromeda-sv1-c40 :: 179.21.24.50' is DOWN\"\n",
      "\n",
      "            source {string} -- Specific human-readable unique identifier, such as a hostname, for\n",
      "                               the system having the problem.\n",
      "\n",
      "                               Examples:\n",
      "                               \"prod05.theseus.acme-widgets.com\"\n",
      "                               \"171.26.23.22\"\n",
      "                               \"aws:elasticache:us-east-1:852511987:cluster/api-stats-prod-003\"\n",
      "                               \"9c09acd49a25\"\n",
      "\n",
      "            severity {string} -- How impacted the affected system is. Displayed to users in lists\n",
      "                                 and influences the priority of any created incidents. Must be one\n",
      "                                 of {info, warning, error, critical}\n",
      "\n",
      "        Keyword Arguments:\n",
      "            event_action {str} -- There are three types of events that PagerDuty recognizes, and\n",
      "                                  are used to represent different types of activity in your\n",
      "                                  monitored systems. (default: 'trigger')\n",
      "                * trigger: When PagerDuty receives a trigger event, it will either open a new alert,\n",
      "                           or add a new trigger log entry to an existing alert, depending on the\n",
      "                           provided dedup_key. Your monitoring tools should send PagerDuty a trigger\n",
      "                           when a new problem has been detected. You may send additional triggers\n",
      "                           when a previously detected problem has occurred again.\n",
      "\n",
      "                * acknowledge: acknowledge events cause the referenced incident to enter the\n",
      "                               acknowledged state. While an incident is acknowledged, it won't\n",
      "                               generate any additional notifications, even if it receives new\n",
      "                               trigger events. Your monitoring tools should send PagerDuty an\n",
      "                               acknowledge event when they know someone is presently working on the\n",
      "                               problem.\n",
      "\n",
      "                * resolve: resolve events cause the referenced incident to enter the resolved state.\n",
      "                           Once an incident is resolved, it won't generate any additional\n",
      "                           notifications. New trigger events with the same dedup_key as a resolved\n",
      "                           incident won't re-open the incident. Instead, a new incident will be\n",
      "                           created. Your monitoring tools should send PagerDuty a resolve event when\n",
      "                           the problem that caused the initial trigger event has been fixed.\n",
      "\n",
      "            dedup_key {string} -- Deduplication key for correlating triggers and resolves. The\n",
      "                                  maximum permitted length of this property is 255 characters.\n",
      "\n",
      "            timestamp {string} -- Timestamp (ISO 8601). When the upstream system detected / created\n",
      "                                  the event. This is useful if a system batches or holds events\n",
      "                                  before sending them to PagerDuty.\n",
      "\n",
      "                                  Optional - Will be auto-generated by PagerDuty if not provided.\n",
      "\n",
      "                                  Example:\n",
      "                                  2015-07-17T08:42:58.315+0000\n",
      "\n",
      "            component {string} -- The part or component of the affected system that is broken.\n",
      "\n",
      "                                  Examples:\n",
      "                                  \"keepalive\"\n",
      "                                  \"webping\"\n",
      "                                  \"mysql\"\n",
      "                                  \"wqueue\"\n",
      "\n",
      "            group {string} -- A cluster or grouping of sources. For example, sources\n",
      "                              “prod-datapipe-02” and “prod-datapipe-03” might both be part of\n",
      "                              “prod-datapipe”\n",
      "\n",
      "                              Examples:\n",
      "                              \"prod-datapipe\"\n",
      "                              \"www\"\n",
      "                              \"web_stack\"\n",
      "\n",
      "            event_class {string} -- The class/type of the event.\n",
      "\n",
      "                                    Examples:\n",
      "                                    \"High CPU\"\n",
      "                                    \"Latency\"\n",
      "                                    \"500 Error\"\n",
      "\n",
      "            custom_details {Dict[str, str]} -- Additional details about the event and affected\n",
      "                                               system.\n",
      "\n",
      "                                               Example:\n",
      "                                               {\"ping time\": \"1500ms\", \"load avg\": 0.75 }\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Groups execution steps by solid, in topological order of the solids.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for el in df['docstring'].to_list():\n",
    "    print(el)\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove from documentation string\n",
    "# Args, :return, :param, LINKS, >>>, :type, :rtype, Parameters, * , The return type is\n",
    "# Swap \\s+ with ' '\n",
    "# Then, remove empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary(docstring):\n",
    "    patterns_to_remove = [\n",
    "        r':param.*$',  # Remove parameter descriptions\n",
    "        r':return.*$',  # Remove return value descriptions\n",
    "        r':type.*$',   # Remove type descriptions\n",
    "        r':rtype.*$',  # Remove return type descriptions\n",
    "        r'Args:.*$',  # Remove \"Args:\" section\n",
    "        r'Parameters.*$',  # Remove \"Parameters:\" section\n",
    "        r'>>>.*$',  # Remove Python console examples\n",
    "        r'\\* .*$',  # Remove bullet points\n",
    "        r'The return type is.*$',  # Remove return type statements\n",
    "        r'https?://.*$',  # Remove URLs\n",
    "        r':?Examples?.*$',  # Remove \"Example\" section\n",
    "        r'Returns.*$',  # Remove \"Returns\" section\n",
    "        r'\\.\\.\\.?.*$',  # Remove ellipses\n",
    "        r'NOTE.*',  # Remove \"NOTE\" section\n",
    "        r'See Also.*$',  # Remove \"See Also\" section\n",
    "        r'Links.*$',  # Remove \"Links\" section\n",
    "        r'Raises.*$',  # Remove \"Raises\" section\n",
    "        r'\\s\\w+:.*$',  # Remove any word before : and after\n",
    "    ]\n",
    "    # Combine the patterns into a single regular expression\n",
    "    pattern = re.compile(r'({})'.format('|'.join(patterns_to_remove)), re.MULTILINE | re.DOTALL)\n",
    "    # Remove the patterns from the docstring\n",
    "    summary = pattern.sub('', docstring).strip()\n",
    "    summary = re.sub('\\s+', ' ', summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987/987 [00:00<00:00, 28692.87it/s]\n",
      "/tmp/ipykernel_961993/108663722.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['method_summary'] = df['docstring'].progress_apply(extract_summary)\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['method_summary'] = df['docstring'].progress_apply(extract_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str->list Convert XML to URL List. From Biligrab.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Downloads Dailymotion videos by URL.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Downloads Sina videos by URL.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Format text with color or other effects into ANSI escaped string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Print a log message to standard error.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Print an error log message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "What a Terrible Failure!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Detect operating system.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "str->None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "str->dict Information for CKPlayer API content.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Splicing URLs according to video ID to get video details\n",
      "----------------------------------------------------------------------------------------------------\n",
      "str->list of str Give you the real URLs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts a string to a valid filename.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Downloads CBS videos by URL.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Override the original one Ugly ugly dirty hack\n",
      "----------------------------------------------------------------------------------------------------\n",
      "str, str, str, bool, bool ->None Download Acfun video by vid. Call Acfun API, decide which site to use, and pass the job to its extractor.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scans through a string for substrings matched some patterns.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses the query string of a URL and returns the value of a parameter.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the content of a URL via sending a HTTP GET request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Post the content of a URL via sending a HTTP POST request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses host name and port number from a string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "str->str\n",
      "----------------------------------------------------------------------------------------------------\n",
      "JSON, int, int, int->str Get a proper title with courseid+topicID+partID.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "int->None Download a WHOLE course. Reuse the API call to save time.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "int, int, int->None Download ONE PART of the course.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a task is either queued or running in this executor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "returns aws_access_key_id, aws_secret_access_key from extra intended to be used by external import and export statements\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetches a field from extras, and returns it. This is some Airflow magic. The grpc hook type adds custom UI elements to the hook page, which allow admins to specify scopes, credential pem files, etc. They get formatted as shown below.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executes SQL using psycopg2 copy_expert method. Necessary to execute COPY command without access to a superuser.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dumps a database table into a tab-delimited file\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uploads the file to Google cloud storage\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the max partition for a table.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Runs forever, monitoring the child processes of @gunicorn_master_proc and restarting workers occasionally. Each iteration of the loop traverses one edge of this state transition diagram, where each state (node) represents [ num_ready_workers_running / num_workers_running ]. We expect most time to be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size. The horizontal transition at ? happens after the new worker parses all the dags (so it could take a while!) V ────────────────────────────────────────────────────────────────────────┐ [n / n] ──TTIN──> [ [n, n+bs) / n + bs ] ────?───> [n + bs / n + bs] ──TTOU─┘ ^ ^───────────────┘ │ │ ┌────────────────v └──────┴────── [ [0, n) / n ] <─── start We change the number of workers by sending TTIN and TTOU to the gunicorn master process, which increases and decreases the number of child workers respectively. Gunicorn guarantees that on TTOU workers are terminated gracefully and that the oldest worker is terminated.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Cloud Translate\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Translate a string or list of strings. See\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves a resource containing information about a Cloud SQL instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new Cloud SQL instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates settings of a Cloud SQL instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a Cloud SQL instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves a database resource from a Cloud SQL instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new database inside a Cloud SQL instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates a database resource inside a Cloud SQL instance. This method supports patch semantics. See\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a database from a Cloud SQL instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump or CSV file.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts Cloud SQL Proxy. You have to remember to stop the proxy if you started it!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Stops running proxy. You should stop the proxy after you stop using it.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create connection in the Connection table, according to whether it uses proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the dynamically created connection from the Connection table.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete the dynamically created connection from the Connection table.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieve Cloud SQL Proxy runner. It is used to manage the proxy lifecycle per task.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieve database hook. This is the actual Postgres or MySQL database hook that uses proxy or connects directly to the Google Cloud SQL database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clean up database hook after it was used.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reserve free TCP port to be used by Cloud SQL Proxy\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Replaces invalid MLEngine job_id characters with '_'. This also adds a leading 'z' in case job_id starts with an invalid character.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Extract error code from ftp exception\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Remove any existing DAG runs for the perf test DAGs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Remove any existing task instances for the perf test DAGs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Toggle the pause state of the DAGs in the test.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Print operational metrics for the scheduler test.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Override the scheduler heartbeat to determine when the test is complete\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Invoke Lambda Function\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates Operators needed for model evaluation and returns. It gets prediction over inputs via Cloud ML Engine BatchPrediction API by calling MLEngineBatchPredictionOperator, then summarize and validate the result via Cloud Dataflow using DataFlowPythonOperator. For details and pricing about Batch prediction, please refer to the website\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates the directory specified by path, creating intermediate directories as necessary. If directory already exists, this is a no-op.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A small helper function to convert a string to a numeric value if appropriate\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Make a naive datetime.datetime in a given time zone aware.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Make an aware datetime.datetime naive in a given time zone.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Establish a connection to druid broker.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Performs the request\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks the status code and raise an AirflowException exception on non 2XX or 3XX status codes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Grabs extra options like timeout and actually runs the request, checking for the result\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Contextmanager that will create and teardown a session.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Function decorator that provides a session if it isn't provided. If you want to reuse a session or run the function as part of a database transaction, you pass it to the function, if not this wrapper will create one and close it for you.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clear out the database\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses some DatabaseError to provide a better error message\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a set of records from Presto\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a pandas dataframe from a sql query.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute the statement against Presto. Can be used to create views.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A generic way to insert a set of tuples into a table.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a cosmos db client.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a collection exists in CosmosDB.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new collection in the CosmosDB database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a database exists in CosmosDB.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new database in CosmosDB.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an existing database in CosmosDB.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an existing collection in the CosmosDB database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Insert a list of new documents into an existing collection in the CosmosDB database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete an existing document out of a collection in the CosmosDB database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a document from an existing collection in the CosmosDB database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new function in Cloud Function in the location specified in the body.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates Cloud Functions according to the specified update mask.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uploads zip file with sources.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified Cloud Function.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wrapper around the private _get_dep_statuses method that contains some global checks for all dependencies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses a config file for s3 credentials. Can currently parse boto, s3cmd.conf and AWS SDK config formats\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the underlying `botocore.Credentials` object. This contains the following authentication\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ensure all logging output has been flushed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If the path contains a folder with a .zip suffix, then the folder is treated as a zip archive and path to zip is returned.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Traverse a directory and look for Python files.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Construct a TaskInstance from the database based on the primary key\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Launch DagFileProcessorManager processor and start DAG parsing loop in manager.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send termination signal to DAG parsing processor manager and expect it to terminate all DAG file processors.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method to clean up DAG file processors to avoid leaving orphan processes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Use multiple processes to parse and generate tasks for the DAGs in parallel. By processing them in separate processes, we can get parallelism and isolation from potentially harmful user code.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parse DAG files repeatedly in a standalone loop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parse DAG files in a loop controlled by DagParsingSignal. Actual DAG parsing loop will run once upon receiving one agent heartbeat message and will report done when finished the loop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Refresh file paths from dag dir if we haven't done it for too long.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Occasionally print out stats about how fast the files are getting processed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clears import errors for files that no longer exist.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Print out stats about how files are getting processed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Update this with a new set of paths to DAG definition files.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sleeps until all the processors are done.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This should be periodically called by the manager loop. This method will kick off new processes to process DAG definition files and read the results from the finished processors.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Kill all child processes on exit since we don't want to leave them as orphaned.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens a ssh connection to the remote host.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a transfer job that runs periodically.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the latest state of a long-running operation in Google Storage Transfer Service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists long-running operations in Google Storage Transfer Service that match the specified filter.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates a transfer job that runs periodically.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a transfer job. This is a soft delete. After a transfer job is deleted, the job and all the transfer executions are subject to garbage collection. Transfer jobs become eligible for garbage collection 30 days after soft delete.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cancels an transfer operation in Google Storage Transfer Service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pauses an transfer operation in Google Storage Transfer Service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resumes an transfer operation in Google Storage Transfer Service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Waits until the job reaches the expected state.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Runs command and returns stdout\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Remove an option if it exists in config from a file or default config. If both of config have the same option, this removes the option in both configs unless remove_default=False.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Allocate IDs for incomplete keys.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Begins a new transaction.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Commit a transaction, optionally creating, deleting or modifying some entities.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lookup some entities by key.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Roll back a transaction.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run a query for entities.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the latest state of a long-running operation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the long-running operation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Poll backup operation state until it's completed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Export entities from Cloud Datastore to Cloud Storage for backup.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Import a backup from Cloud Storage to Cloud Datastore.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Publish a message to a topic or an endpoint.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetch the hostname using the callable from the config or using `socket.getfqdn` as a fallback.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Cloud Natural Language service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Finds named entities in the text along with entity types, salience, mentions for each entity, and other properties.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A convenience method that provides all the features that analyzeSentiment, analyzeEntities, and analyzeSyntax provide in one call.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Classifies a document into categories.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets template fields for specific operator class.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A role that allows you to include a list of template fields in the middle of the text. This is especially useful when writing guides describing how to use the operator. The result is a list of fields where each field is shorted in the literal block. Sample\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Properly close pooled database connections\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ensures that certain subfolders of AIRFLOW_HOME are on the classpath\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the returned Celery result from the Airflow task ID provided to the sensor, and returns True if the celery result has been finished execution.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return true if the ticket cache contains \"conf\" information as is found in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the Sun Java Krb5LoginModule in Java6, so we need to take an action to work around it.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Transforms a SQLAlchemy model instance into a dictionary\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Yield successive chunks of a given size from a list of items\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reduce the given list of items by splitting it into chunks of the given size and passing each chunk through the reducer\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given a number of tasks, builds a dependency chain. chain(task_1, task_2, task_3, task_4) is equivalent to task_1.set_downstream(task_2) task_2.set_downstream(task_3) task_3.set_downstream(task_4)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given task instance, try_number, filename_template, return the rendered log filename\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Awaits for Google Cloud Dataproc Operation to complete.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Coerces content or all values of content if it is a dict to a string. The function will throw if content contains non-string or non-numeric types. The reason why we have this function is because the ``self.json`` field must be a dict with only string values. This is because ``render_template`` will fail for numerical values.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Handles the Airflow + Databricks lifecycle logic for a Databricks operator\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run an pig script using the pig cli\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetch and return the state of the given celery task. The scope of this function is global so that it can be called by subprocesses in the pool.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "How many Celery tasks should each worker process send.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "How many Celery tasks should be sent to each worker process.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Like a Python builtin dict object, setdefault returns the current value for a key, and if it isn't there, stores the default value and returns it.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Launches a MLEngine job and wait for it to reach a terminal state.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets a MLEngine job based on the job name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Waits for the Job to reach a terminal state. This method will periodically check the job state until the job reach a terminal state.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates the Version on Google Cloud ML Engine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets a version to be the default. Blocks until finished.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists all available versions of a model. Blocks until finished.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the given version of a model. Blocks until finished.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a Model. Blocks until finished.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets a Model. Blocks until finished.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Write batch items to dynamodb table with provisioned throughout capacity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Integrate plugins to the context.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new instance of the configured executor if none exists and returns it\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new instance of the named executor. In case the executor name is not know in airflow, look for it in the plugins\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Handles error callbacks when using Segment with segment_debug_mode set to True\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Trigger a new dag run for a Dag with an execution date of now unless specified in the data.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete all DB records related to the specified Dag.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get all pools.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a pool.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete pool.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a new container group\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the state and exitcode of a container group\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the messages of a container group\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the tail from logs of a container group\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a container group\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Test if a container group exists\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Function decorator that Looks for an argument named \"default_args\", and fills the unspecified arguments from it. Since python2.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Builds an ingest query for an HDFS TSV load.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check for message on subscribed channels and write to xcom the message with key ``message`` An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The previous DagRun, if there is one\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The previous, SCHEDULED DagRun, if there is one\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Determines the overall state of the DagRun based on the state of its TaskInstances.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Verifies the DagRun by checking for removed tasks or tasks that are not in the database yet. It will set state to removed or add the task if required.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "We need to get the headers in addition to the body answer to get the location from them This function uses jenkins_request method from python-jenkins library with just the return call changed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given a context, this function provides a dictionary of values that can be used to externally reconstruct relations between dags, dag_runs, tasks and task_instances. Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if in_env_var_format is set to True.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This function decides whether or not to Trigger the remote DAG\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends a single datapoint metric to DataDog\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries datadog for a specific metric, potentially with some function applied to it and returns the results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the DAG out of the dictionary, and refreshes it if expired\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fail given zombie tasks, which are tasks that haven't had a heartbeat for too long, in the current DagBag.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds the DAG into the bag, recurses into sub dags. Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given a file path or a folder, this method looks for python modules, imports them and adds them to the dagbag collection. Note that if a ``.airflowignore`` file is found while processing the directory, it will behave much like a ``.gitignore``, ignoring files that match any of the regex patterns specified in the file. **Note**: The patterns in .airflowignore are treated as un-anchored regexes, not shell-like glob patterns.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prints a report around DagBag loading stats\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add or subtract days from a YYYY-MM-DD\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Takes an input string and outputs another string as specified in the output format\n",
      "----------------------------------------------------------------------------------------------------\n",
      "poke matching files in a directory with self.regex\n",
      "----------------------------------------------------------------------------------------------------\n",
      "poke for a non empty directory\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clears a set of task instances, but makes sure the running ones get killed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the try number that this task number will be when it is actually run. If the TI is currently running, this will match the column in the databse, in all othercases this will be incremenetd\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Generates the shell command required to execute this task instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the very latest state from the database, if a session is passed, we use and looking up the state becomes part of the session, otherwise a new session is used.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Forces the task instance's state to FAILED in the database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Refreshes the task instance from the database based on the primary key\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clears all XCom data from the database for the task instance\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks whether the dependents of this task instance have all succeeded. This is meant to be used by wait_for_downstream. This is useful when you do not want to start processing the next schedule of a task until the dependents are done. For instance, if the task DROPs and recreates a table.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get datetime of the next retry if the task instance fails. For exponential backoff, retry_delay is used as base and will be converted to seconds.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks on whether the task instance is in the right state and timeframe to be retried.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Make an XCom available for tasks to pull.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pull XComs that optionally meet certain criteria. The default value for `key` limits the search to XComs that were returned by other tasks (as opposed to those that were pushed manually). To remove this filter, pass key=None (or any desired value). If a single task_id string is provided, the result is the value of the most recent matching XCom from that task_id. If multiple task_ids are provided, a tuple of matching values is returned. None is returned whenever no matches are found.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets the log context.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Close and upload local log file to remote storage Wasb.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Google Compute Engine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts an existing instance defined by project_id, zone and resource_id. Must be called with keyword arguments rather than positional.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets machine type of an instance defined by project_id, zone and resource_id. Must be called with keyword arguments rather than positional.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves instance template by project_id and resource_id. Must be called with keyword arguments rather than positional.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Inserts instance template using body specified Must be called with keyword arguments rather than positional.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves Instance Group Manager by project_id, zone and resource_id. Must be called with keyword arguments rather than positional.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Patches Instance Group Manager with the specified body. Must be called with keyword arguments rather than positional.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Waits for the named operation to complete - checks status of the async call.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if bucket_name exists.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates an Amazon S3 bucket.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks that a prefix exists in a bucket\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists prefixes in a bucket under prefix\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists keys in a bucket under prefix and not containing delimiter\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a key exists in a bucket\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reads a key from S3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reads a key with S3 Select.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks that a key matching a wildcard expression exists in a bucket\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a local file to S3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a string to S3 This is provided as a convenience to drop a string in S3. It uses the boto infrastructure to ship a file to s3.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads bytes to S3 This is provided as a convenience to drop a string in S3. It uses the boto infrastructure to ship a file to s3.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a file object to S3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a copy of an object that is already stored in S3.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries cassandra and returns a cursor to the results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts a user type to RECORD that contains n fields, where n is the number of attributes. Each element in the user type class will be converted to its corresponding data type in BQ.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send an email with html content using sendgrid. To use this\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Cloud Speech.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Recognizes audio input\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Call the SparkSqlHook to run the provided sql query\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load AirflowPlugin subclasses from the entrypoints provided. The entry_point group should be 'airflow.plugins'.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check whether a potential object is a subclass of the AirflowPlugin class.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets tasks instances to skipped from the same dag run.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a AzureDLFileSystem object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if a file exists on Azure Data Lake.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a file to Azure Data Lake.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List files in Azure Data Lake Storage\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run Presto Query on Athena\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncompress gz and bz2 files\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries MSSQL and returns a cursor of results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decorates function to execute function at the same time submitting action_logging but in CLI context. It will call action logger callbacks twice, one for pre-execution and the other one for post-execution. Action logger will be called with below keyword\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Builds metrics dict from function args It assumes that function arguments is from airflow.bin.cli module's function and has Namespace instance where it optionally contains \"dag_id\", \"task_id\", and \"execution_date\".\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create the specified cgroup.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete the specified cgroup.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The purpose of this function is to be robust to improper connections settings provided by users, specifically in the host field. For example -- when users supply ``\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Utility function to perform an API call with retries\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sign into Salesforce, only if we are not already signed in.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Make a query to Salesforce.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the description of an object from Salesforce. This description is the object's schema and some extra metadata that Salesforce stores for each object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a list of all available fields for an object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get all instances of the `object` from Salesforce. For each model, only get the fields specified in fields. All we really do underneath the hood is\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a column of a dataframe to UNIX timestamps if applicable\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Write query results to file. Acceptable formats\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetches PyMongo Client\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetches a mongo collection object for querying. Uses connection schema as DB unless specified.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Replaces many documents in a mongo collection. Uses bulk_write with multiple ReplaceOne operations\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks the mail folder for mails containing attachments with the given name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves mail's attachments in the mail folder by its name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Downloads mail's attachments in the mail folder by its name to the local directory.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets all attachments by name for the mail.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the file including name and payload.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Write batch records to Kinesis Firehose\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Determines whether a task is ready to be rescheduled. Only tasks in NONE state with at least one row in task_reschedule table are handled by this dependency class, otherwise this dependency is considered as passed. This dependency fails if the latest reschedule request's reschedule date is still in future.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send email using backend specified in EMAIL_BACKEND.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send an email with html content\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processes DateTimes from the DB making sure it is always returning UTC. Not using timezone.convert_to_utc as that converts to configured TIMEZONE while the DB might be running with some other setting. We assume UTC datetimes in the database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if a blob exists on Azure Blob Storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if a prefix exists on Azure Blob storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a string to Azure Blob Storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read a file from Azure Blob Storage and return as a string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a file from Azure Blob Storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BACKPORT FROM PYTHON3 FTPLIB. List a directory in a standardized format by using MLSD command (RFC-3659). If path is omitted the current directory is assumed. \"facts\" is a list of strings representing the type of information desired (e.g. [\"type\", \"size\", \"perm\"]). Return a generator object yielding a tuple of two elements for every file found in path. First element is the file name, the second one is a dictionary including a variable number of \"facts\" depending on the server and whether \"facts\" argument has been provided.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Transfers the remote file to a local location. If local_full_path_or_buffer is a string path, the file will be put at that location; if it is a file-like buffer, the file will be written to the buffer but not closed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Transfers a local file to the remote location. If local_full_path_or_buffer is a string path, the file will be read from that location; if it is a file-like buffer, the file will be read from the buffer but not closed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Call the DiscordWebhookHook to post message\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the FileService object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if a directory exists on Azure File Share.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if a file exists on Azure File Share.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the list of directories and files stored on a Azure File Share.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a new directory on a Azure File Share.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a file to Azure File Share.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a string to Azure File Share.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a stream to Azure File Share.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Copies an object from a bucket to another, with renaming if requested. destination_bucket or destination_object can be omitted, in which case source bucket/object is used, but not both.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a file from Google Cloud Storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uploads a local file to Google Cloud Storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks for the existence of a file in Google Cloud Storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if an blob_name is updated in Google Cloud Storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an object from the bucket.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List all objects from the bucket with the give string prefix in name\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the size of a file in Google Cloud Storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the CRC32c checksum of an object in Google Cloud Storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the MD5 hash of an object in Google Cloud Storage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new bucket. Google Cloud Storage uses a flat namespace, so you can't create a bucket with a name that is already in use.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Composes a list of existing object into a new object in the same storage bucket_name Currently it only supports up to 32 objects that can be concatenated in a single operation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tar the local file or directory and upload to s3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Extract the S3 operations from the configuration and execute them.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if an S3 URL exists\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Establish an AWS connection for retrieving logs during training\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a training job\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a tuning job\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a transform job\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create an endpoint\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the training job info associated with job_name and print CloudWatch logs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check status of a SageMaker job\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Display the logs for a given training job, optionally tailing them until the job is complete.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute the python dataflow job.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified Cloud Bigtable instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates new instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates the specified Cloud Bigtable table.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified table in Cloud Bigtable.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates number of nodes in the specified Cloud Bigtable cluster.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This function creates the command list from available information\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This function prepares a list of hiveconf params from a dictionary of key value pairs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a pandas DataFrame into hive. Hive data types will be inferred if not passed but column names will not be sanitized.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a local file into Hive Note that the table generated in Hive uses ``STORED AS textfile`` which isn't the most efficient serialization format. If a large amount of data is loaded and/or if the tables gets queried considerably, you may want to use this operator only to stage the data into a temporary table before loading it into its final destination using a ``HiveOperator``.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks whether a partition with a given name exists\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if table exists\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get results of the provided hql in target schema.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute hql in target schema and write results to a csv file.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a set of records from a Hive query.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a pandas dataframe from a Hive query\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Cloud Vision.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get Dingding endpoint for sending message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send Dingding message\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method that binds parameters to a SQL query.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method that escapes parameters to a SQL query.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method that casts a BigQuery row to the appropriate data types. This is useful because BigQuery returns all fields as strings.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "function to check expected type and raise error if type is not correct\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks for the existence of a table in Google BigQuery.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new, empty table in the dataset. To create a view, which is defined by a SQL query, parse a dictionary to 'view' kwarg\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Patch information in an existing table. It only updates fileds that are provided in the request object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cancel all started queries that have not yet completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete an existing table from the dataset; If the table does not exist, return an error unless ignore_if_missing is set to True.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "creates a new, empty table in the dataset; If the table already exists, update the existing table. Since BigQuery does not natively allow table upserts, this is not an atomic operation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Grant authorized view access of a dataset to a view table. If this view has already been granted access to the dataset, do nothing. This method is not atomic. Running it may clobber a simultaneous update.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Method returns dataset_resource if dataset exist and raised 404 error if dataset does not exist\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Method returns full list of BigQuery datasets in the current project\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Method to stream data into BigQuery one record at a time without needing to run a load job\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executes a BigQuery query, and returns the job ID.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute a BigQuery query multiple times with different parameters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method for fetchone, which returns the next row from a buffer. If the buffer is empty, attempts to paginate through the result set for the next page, and load it into the buffer.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries Postgres and returns a cursor to the results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create all the intermediate directories in a remote host\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create queue using connection object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send message to the queue\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run the task command.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A callback that should be called when this is done running.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parse options and process commands\n",
      "----------------------------------------------------------------------------------------------------\n",
      "generate HTML header content\n",
      "----------------------------------------------------------------------------------------------------\n",
      "generate HTML div\n",
      "----------------------------------------------------------------------------------------------------\n",
      "generate javascript code for the chart\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create X-axis\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create Y-axis\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decorator to log user actions\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decorator to make a view compressed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a dag run from this dag including the tasks associated with this dag.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Publish the message to SQS queue\n",
      "----------------------------------------------------------------------------------------------------\n",
      "returns a json response from a json serializable python object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens the given file. If the path contains a folder with a .zip suffix, then the folder is treated as a zip archive, opening the file inside the archive.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Used by cache to get a unique key per URL\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Performs video annotation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get Opsgenie api_key for creating alert\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Overwrite HttpHook get_conn because this hook just needs base_url and headers, and does not need generic params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute the Opsgenie Alert call\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Construct the Opsgenie JSON payload. All relevant parameters are combined here to a valid Opsgenie JSON payload.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Call the OpsgenieAlertHook to post message\n",
      "----------------------------------------------------------------------------------------------------\n",
      "check if aws conn exists already or create one and return it\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run Presto query on athena with provided config and return submitted query_execution_id\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetch the status of submitted athena query.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Poll the status of submitted athena query until query state reaches final state.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sleep for the time specified in the exception. If not specified, wait for 60 seconds.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Call Zendesk API and return results\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the partition values for a table.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the information of the table\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the physical location of the table\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return status of a cluster\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a cluster and optionally create a snapshot\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets a list of snapshots for a cluster\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Restores a cluster from its snapshot\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a snapshot of a cluster\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SlackAPIOperator calls will not fail even if the call is not unsuccessful. It should not prevent a DAG from completing in success\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a job flow using the config from the EMR connection. Keys of the json extra hash may have the arguments of the boto3 run_job_flow method. Overrides for this config may be passed as the job_flow_overrides.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Will test the filepath result and test if its size is at least self.filesize\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Will filter if instructed to do so the result to remove matching criteria\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed by task_instance at runtime\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get pool by a given name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a pool with a given parameters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete pool by a given name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts a python dictionary to the proto supplied\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given an operation, continuously fetches the status from Google Cloud until either completion or an error occurring\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetches the operation from Google Cloud\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Append labels to provided Cluster Protobuf Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current airflow version string follows semantic versioning\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a cluster, consisting of the specified number and type of Google Compute Engine instances.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets details of specified cluster\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given a Discord http_conn_id, return the default webhook endpoint or override if a webhook_endpoint is manually supplied.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Construct the Discord JSON payload. All relevant parameters are combined here to a valid Discord JSON payload.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute the Discord webhook call\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Encrypts a plaintext message using Google Cloud KMS.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Imports table from remote location to target dir. Arguments are copies of direct sqoop command line arguments\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Imports a specific query from the rdbms to hdfs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exports Hive table to remote location. Arguments are copies of direct sqoop command line Arguments\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves connection to Cloud Text to Speech.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthesizes text input\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Close and upload local log file to remote storage S3.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "When using git to retrieve the DAGs, use the GitSync Init Container\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Defines any necessary environment variables for the pod executor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Defines any necessary secrets for the pod executor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Defines the security context\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get link to qubole command result page.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Heartbeats update the job's entry in the database with a timestamp for the latest_heartbeat and allows for the job to be killed externally. This allows at the system level to monitor what is actually active. For instance, an old heartbeat for SchedulerJob would mean something is wrong. This also allows for any job to be killed externally, regardless of who is running it or on which machine it is running. Note that if your heartbeat is set to 60 seconds and you call this method after 10 seconds of processing since the last heartbeat, it will sleep 50 seconds to complete the 60 seconds and keep a steady heart rate. If you go over 60 seconds before calling it, it won't sleep at all.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Launch a process to process the given file.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Launch the process and start processing the DAG.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if the process launched to process this file is done.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper method to clean up processor_agent to avoid leaving orphan processes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "For the DAGs in the given DagBag, record any associated import errors and clears errors for files that no longer have them. These are usually displayed through the Airflow UI so that users know that there are issues parsing DAGs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This method schedules the tasks for a single DAG by looking at the active DAG runs and adding task instances that should run to the queue.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "For all DAG IDs in the SimpleDagBag, look for task instances in the old_states and set them to new_state if the corresponding DagRun does not exist or exists but is not in the running state. This normally should not happen, but it can if the state of DagRuns are changed manually.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the concurrency maps.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Changes the state of task instances in the list with one of the given states to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Takes task_instances, which should have been set to queued, and enqueues them with the executor.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempts to execute TaskInstances that should be executed by the scheduler. There are three\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If there are tasks left over in the executor, we set them back to SCHEDULED to avoid creating hanging tasks.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Respond to executor events.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Process a Python file containing Airflow DAGs. This\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the counters per state of the tasks that were running. Can re-add to tasks to run in case required.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if the executor agrees with the state of task instances that are running\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Computes the dag runs and their respective task instances for the given run dates and executes the task instances.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Go through the dag_runs and update the state based on the task_instance state. Then set DAG runs that are not finished to failed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initializes all components required to run a dag for a specified date range and calls helper method to execute the tasks.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Self destruct task if state has been moved away from running externally\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Provides a client for interacting with the Cloud Spanner API.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets information about a particular instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Invokes a method on a given instance by applying a specified Callable.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new Cloud Spanner instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates an existing Cloud Spanner instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an existing Cloud Spanner instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves a database in Cloud Spanner. If the database does not exist in the specified instance, it returns None.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new database in Cloud Spanner.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates DDL of a database in Cloud Spanner.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Drops a database in Cloud Spanner.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pokes for a mail attachment on the mail server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates additional_properties parameter based on language_hints, web_detection_params and additional_properties parameters specified by the user\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a table exists in Cassandra\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if a record exists in Cassandra\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Construct the command to poll the driver status.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Remote Popen to execute the spark-submit job\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processes the log files and extracts useful information out of it. If the deploy-mode is 'client', log the output of the submit command as those are the output logs of the Spark worker directly.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parses the logs of the spark driver status query process\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the task runner that can be used to run the given job.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Try to use a waiter from the below pull request\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries mysql and returns a cursor to the results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Configure a csv writer with the file_handle and write schema as headers for the new file.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Takes a cursor, and writes the BigQuery schema in .json format for the results to a local file system.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a dict of column name and column type based on self.schema if not None.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Helper function that maps from MySQL fields to BigQuery fields. Used when a schema_filename is set.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute sqoop job\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Saves the lineage to XCom and if configured to do so sends it to the backend.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a set of dates as a list based on a start, end and delta, delta can be something that can be added to `datetime.datetime` or a cron expression as a `str`\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert an array of time durations in seconds to the specified time unit.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a datetime object representing `n` days ago. By default the time is set to midnight.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initialize the role with the permissions and related view-menus.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete the given Role\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get all the roles associated with the user.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Whether the user has this role name\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Whether the user has this perm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "FAB leaves faulty permissions that need to be cleaned up\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add the new permission , view_menu to ab_permission_view_role if not exists. It will add the related entry to ab_permission and ab_view_menu two meta tables as well.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Admin should have all the permission-views. Add the missing ones to the table for admin.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Set the access policy on the given DAG's ViewModel.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create perm-vm if not exist and insert into FAB security model for all-dags.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deferred load of Fernet key. This function could fail either because Cryptography is not installed or because the Fernet key is invalid.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks for existence of the partition in the AWS Glue Catalog table\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the AwsGlueCatalogHook\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check for message on subscribed queue and write to xcom the message with key ``messages``\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Establishes a connection depending on the security mode set via config or environment variable.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check for the existence of a path in HDFS by querying FileStatus.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "r\"\"\" Uploads a file to HDFS.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Establish a connection to pinot broker through pinot dbqpi.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the connection uri for pinot broker. e.g:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert native python ``datetime.date`` object to a format supported by the API\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert native python ``datetime.time`` object to a format supported by the API\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executes the sql and returns a pandas dataframe\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Runs a command or a list of commands. Pass a list of sql statements to the sql parameter to get them to execute sequentially\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets the autocommit flag on the connection\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A generic way to insert a set of tuples into a table, a new transaction is created every commit_every rows\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "An endpoint helping check the health status of the Airflow instance, including metadatabase and scheduler.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A restful endpoint that returns external links for a given Operator It queries the operator that sent the request for the links it wishes to provide for a given external link name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens a connection to the cloudant service and closes it automatically if used as context manager.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Call the SlackWebhookHook to post the provided Slack message\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Function decorator that intercepts HTTP Errors and raises AirflowException with more informative message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decorator that provides fallback for Google Cloud Platform project id. If the project is None it will be replaced with the project_id from the service account the Hook is authenticated with. Project id can be specified either via project_id kwarg or via first parameter in positional args.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A list of states indicating that a task either has not completed a run or has not even started.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Construct the spark-sql command to execute. Verbose output is enabled as default.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor. See ``ToTensor`` for more details.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Normalize a tensor image with mean and standard deviation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "r\"\"\"Resize the input PIL Image to the given size.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "r\"\"\"Pad the given PIL Image on all sides with specified padding mode and fill value.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Crop the given PIL Image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Crop the given PIL Image and resize it to desired size. Notably used in :class:`~torchvision.transforms.RandomResizedCrop`.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Horizontally flip the given PIL Image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Perform perspective transform of the given PIL Image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vertically flip the given PIL Image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Crop the given PIL Image into four corners and the central crop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjust brightness of an Image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjust contrast of an Image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjust color saturation of an image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjust hue of an image. The image hue is adjusted by converting the image to HSV and cyclically shifting the intensities in the hue channel (H). The image is then converted back to original image mode. `hue_factor` is the amount of shift in H channel and must be in the interval `[-0.5, 0.5]`. See `Hue`_ for more details.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "r\"\"\"Perform gamma correction on an image. Also known as Power Law Transform. Intensities in RGB mode are adjusted based on the following\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rotate the image by angle.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Apply affine transformation on the image keeping image center invariant\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert image to grayscale version of image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Save a given Tensor into an image file.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Finds the class folders in a dataset.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a Tensor containing the patches\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a Tensor containing the list of labels Read the file and keep only the ID of the 3D point.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a Tensor containing the ground truth matches Read the file and keep only 3D point ID. Matches are represented with a 1, non matches with a 0.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Computes the accuracy over the k top predictions for the specified values of k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This function disables printing when not in master process\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download a file from a url and place it in root.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List all directories at a given root\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List all files ending with a suffix at a given root\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download a Google Drive file from and place it in root.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get parameters for ``crop`` for a random crop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get parameters for ``perspective`` for a random perspective transform.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get parameters for ``crop`` for a random sized crop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a randomized transform to be applied on image. Arguments are same as that of __init__.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get parameters for affine transformation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download and extract the tarball, and download each individual photo.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download the MNIST data if it doesn't exist in processed_folder already.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download the EMNIST data if it doesn't exist in processed_folder already.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add a specific enqueue time to the message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Defer the message. This message will remain in the queue but must be received specifically by its sequence number in order to be processed. :raises: ~azure.servicebus.common.errors.MessageAlreadySettled if the message has been settled. :raises: ~azure.servicebus.common.errors.MessageLockExpired if message lock has already expired. :raises: ~azure.servicebus.common.errors.SessionLockExpired if session lock has already expired. :raises: ~azure.servicebus.common.errors.MessageSettleFailed if message settle operation fails.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gives the sas-url to download the configurations for vpn-sites in a resource group.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Guess Python Autorest options based on the spec path. Expected\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates a running PowerShell command with more data.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the managed application definition.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new managed application definition.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the target uri for the request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create connection for the request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends request to cloud service server and return the response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executes script actions on the specified HDInsight cluster.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check the availability of a Front Door resource name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Permanently deletes the specified vault. aka Purges the deleted Azure key vault.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Extracts the host authority from the given URI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a CLI profile class.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return Credentials and default SubscriptionID of current loaded profile of the CLI. Credentials will be the \"az login\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets predictions for a given utterance, in the form of intents and entities. The current maximum query size is 500 characters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check Name Availability for global uniqueness.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens the request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets up the timeout for the request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets the request header.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets back all response headers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends the request body.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets status of response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets status text of response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets response body as a SAFEARRAY and converts the SAFEARRAY to str.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets client certificate for the request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Connects to host and sends the request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends the headers of request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends request body.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the response and generates the _Response object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "simplified an id to be more friendly for us people\n",
      "----------------------------------------------------------------------------------------------------\n",
      "converts a Python name into a serializable name\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Verify whether two faces belong to a same person. Compares a face Id with a Person Id.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a job to the specified account. The Batch service supports two ways to control the work done as part of a job. In the first approach, the user specifies a Job Manager task. The Batch service launches this task when it is ready to start the job. The Job Manager task controls all other tasks that run under this job, by using the Task APIs. In the second approach, the user directly controls the execution of tasks under an active job, by using the Task APIs. Also\n",
      "----------------------------------------------------------------------------------------------------\n",
      "get properties from entry xml\n",
      "----------------------------------------------------------------------------------------------------\n",
      "descends through a hierarchy of nodes returning the list of children at the inner most level. Only returns children who share a common parent, not cousins.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Recursively searches from the parent to the child, gathering all the applicable namespaces along the way\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts xml response to service bus namespace The xml format for\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts xml response to service bus region The xml format for\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts xml response to service bus namespace availability The xml\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts xml response to service bus metrics objects The xml format for MetricProperties <entry> <id>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Replaces the runbook draft content.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get domain name recommendations based on keywords. Get domain name recommendations based on keywords.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Asynchronous operation to modify a knowledgebase.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets a collection that contains the object IDs of the groups of which the user is a member.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Will clone the given PR branch and vuild the package with the given name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Import data into Redis cache.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Publish runbook draft.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Renew the message lock. This will maintain the lock on the message to ensure it is not returned to the queue to be reprocessed. In order to complete (or otherwise settle) the message, the lock must be maintained. Messages received via ReceiveAndDelete mode are not locked, and therefore cannot be renewed. This operation can also be performed as an asynchronous background task by registering the message with an `azure.servicebus.aio.AutoLockRenew` instance. This operation is only available for non-sessionful messages. :raises: TypeError if the message is sessionful. :raises: ~azure.servicebus.common.errors.MessageLockExpired is message lock has already expired. :raises: ~azure.servicebus.common.errors.SessionLockExpired if session lock has already expired. :raises: ~azure.servicebus.common.errors.MessageAlreadySettled is message has already been settled.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Replace alterations data.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds the specified value as a new version of the specified secret resource. Creates a new value of the specified secret resource. The name of the value is typically the version identifier. Once created the value cannot be changed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Regenerates the primary or secondary access key for the specified storage account.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new storage account in Windows Azure.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the label, the description, and enables or disables the geo-replication status for a storage account in Windows Azure.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified storage account from Windows Azure.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks to see if the specified storage account name is available, or if it has already been taken.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves system properties for the specified hosted service. These properties include the service name and service type; the name of the affinity group to which the service belongs, or its location if it is not part of an affinity group; and optionally, information on the service's deployments.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new hosted service in Windows Azure.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified hosted service from Windows Azure.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uploads a new service package and creates a new deployment on staging or production.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified deployment.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initiates a virtual IP swap between the staging and production deployment environments for a service. If the service is currently running in the staging environment, it will be swapped to the production environment. If it is running in the production environment, it will be swapped to staging.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initiates a change to the deployment configuration.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initiates a change in deployment status.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initiates an upgrade.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Specifies the next upgrade domain to be walked during manual in-place upgrade or configuration change.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Requests a reboot of a role instance that is running in a deployment.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reinstalls the operating system on instances of web roles or worker roles and initializes the storage resources that are used by them. If you do not want to initialize storage resources, you can use reimage_role_instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks to see if the specified hosted service name is available, or if it has already been taken.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists all of the service certificates associated with the specified hosted service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a certificate to a hosted service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a service certificate from the certificate store of a hosted service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Get Management Certificate operation retrieves information about the management certificate with the specified thumbprint. Management certificates, which are also known as subscription certificates, authenticate clients attempting to connect to resources associated with your Windows Azure subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Add Management Certificate operation adds a certificate to the list of management certificates. Management certificates, which are also known as subscription certificates, authenticate clients attempting to connect to resources associated with your Windows Azure subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Delete Management Certificate operation deletes a certificate from the list of management certificates. Management certificates, which are also known as subscription certificates, authenticate clients attempting to connect to resources associated with your Windows Azure subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new affinity group for the specified subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an affinity group in the specified subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List subscription operations.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reserves an IPv4 address for the specified subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a reserved IP address from the specified subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Associate an existing reservedIP to a deployment.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Disassociate an existing reservedIP from the given deployment.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves information about the specified reserved IP address.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the specified virtual machine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Provisions a virtual machine based on the supplied configuration.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a virtual machine to an existing deployment.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the specified virtual machine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified virtual machine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Capture Role operation captures a virtual machine image to your image gallery. From the captured image, you can create additional customized virtual machines.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts the specified virtual machine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts the specified virtual machines.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Restarts the specified virtual machine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shuts down the specified virtual machine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shuts down the specified virtual machines.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a DNS server definition to an existing deployment.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the ip address of a DNS server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes a DNS server from a deployment.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lists the versions of a resource extension that are available to add to a Virtual Machine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Replicate a VM image to multiple target locations. This operation is only for publishers. You have to be registered as image publisher with Microsoft Azure to be able to call this.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Unreplicate a VM image from all regions This operation is only for publishers. You have to be registered as image publisher with Microsoft Azure to be able to call this\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Share an already replicated OS image. This operation is only for publishers. You have to be registered as image publisher with Windows Azure to be able to call this.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a VM Image in the image repository that is associated with the specified subscription using a specified set of virtual hard disks.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified VM Image from the image repository that is associated with the specified subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves a list of the VM Images from the image repository that is associated with the specified subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates a VM Image in the image repository that is associated with the specified subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds an OS image that is currently stored in a storage account in your subscription to the image repository.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates an OS image that in your image repository.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates metadata elements from a given OS image reference.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified OS image from your image repository.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the specified data disk from a virtual machine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a data disk to a virtual machine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the specified data disk attached to the specified virtual machine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Removes the specified data disk from a virtual machine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a disk to the user image repository. The disk can be an OS disk or a data disk.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates an existing disk in your image repository.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified data or operating system disk from your image repository.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Summarizes policy states for the resources under the management group.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This is a temporary patch pending a fix in uAMQP.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Receive a batch of messages at once. This approach it optimal if you wish to process multiple messages simultaneously. Note that the number of messages retrieved in a single batch will be dependent on whether `prefetch` was set for the receiver. This call will prioritize returning quickly over meeting a specified batch size, and so will return as soon as at least one message is received and there is a gap in incoming messages regardless of the specified batch size.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Renew the session lock. This operation must be performed periodically in order to retain a lock on the session to continue message processing. Once the lock is lost the connection will be closed. This operation can also be performed as a threaded background task by registering the session with an `azure.servicebus.AutoLockRenew` instance.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create or update a VM scale set.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts SinglePlacementGroup property to false for a existing virtual machine scale set.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Detect profanity and match against custom and shared blacklists. Detects profanity in more than 100 languages and match against custom and shared blacklists.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new key, stores it, then returns key parameters and attributes to the client. The create key operation can be used to create any key type in Azure Key Vault. If the named key already exists, Azure Key Vault creates a new version of the key. It requires the keys/create permission.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Imports an externally created key, stores it, and returns key parameters and attributes to the client. The import key operation may be used to import any key type into an Azure Key Vault. If the named key already exists, Azure Key Vault creates a new version of the key. This operation requires the keys/import permission.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The update key operation changes specified attributes of a stored key and can be applied to any key type and key version stored in Azure Key Vault. In order to perform this operation, the key must already exist in the Key Vault.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets a secret in a specified key vault. The SET operation adds a secret to the Azure Key Vault. If the named secret already exists, Azure Key Vault creates a new version of that secret. This operation requires the secrets/set permission.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets the specified certificate issuer. The SetCertificateIssuer operation adds or updates the specified certificate issuer. This operation requires the certificates/setissuers permission.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a Service Bus client from a connection string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get an async client for a subscription entity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get an async client for all subscription entities in the topic.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Send one or more messages to the current entity. This operation will open a single-use connection, send the supplied messages, and close connection. If the entity requires sessions, a session ID must be either provided here, or set on each outgoing message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a Sender for the Service Bus endpoint. A Sender represents a single open connection within which multiple send operations can be made.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a Receiver for the Service Bus endpoint. A Receiver represents a single open connection with which multiple receive operations can be made.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a Receiver for the deadletter endpoint of the entity. A Receiver represents a single open connection with which multiple receive operations can be made.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Extracts request id from response header.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Performs a GET request and returns the response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Performs a PUT request and returns the response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Waits for an asynchronous operation to complete. This calls get_operation_status in a loop and returns when the expected status is reached. The result of get_operation_status is returned. By default, an exception is raised on timeout or error status.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add additional headers for management.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Assumed called on Travis, to prepare a package to be deployed This method prints on stdout for Travis. Return is obj to pass to sys.exit() directly\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List certificates in a specified key vault. The GetCertificates operation returns the set of certificates resources in the specified key vault. This operation requires the certificates/list permission.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get list of available service bus regions.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List the service bus namespaces defined on the account.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get details about a specific namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a new service bus namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a service bus namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks to see if the specified service bus namespace is available, or if it has already been taken.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the topics in the service namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the notification hubs in the service namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the relays in the service namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This operation gets rollup data for Service Bus metrics queue. Rollup data includes the time granularity for the telemetry aggregation as well as the retention settings for each time granularity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This operation gets rollup data for Service Bus metrics topic. Rollup data includes the time granularity for the telemetry aggregation as well as the retention settings for each time granularity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This operation gets rollup data for Service Bus metrics notification hub. Rollup data includes the time granularity for the telemetry aggregation as well as the retention settings for each time granularity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This operation gets rollup data for Service Bus metrics relay. Rollup data includes the time granularity for the telemetry aggregation as well as the retention settings for each time granularity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a virtual environment in a directory.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a venv with these packages in a temp dir and yielf the env. packages should be an iterable of pip version instructio (e.g. package~=1.2.3)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a new Azure SQL Database server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reset the administrator password for a server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets quotas for an Azure SQL Database Server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the event logs for an Azure SQL Database Server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates an Azure SQL Database server firewall rule.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Update a firewall rule for an Azure SQL Database server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an Azure SQL Database server firewall rule.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the set of firewall rules for an Azure SQL Database Server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the service level objectives for an Azure SQL Database server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new Azure SQL Database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates existing database details.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an Azure SQL Database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List the SQL databases defined on the specified server name\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets all legal agreements that user needs to accept before purchasing a domain. Gets all legal agreements that user needs to accept before purchasing a domain.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Close down the handler connection. If the handler has already closed, this operation will do nothing. An optional exception can be passed in to indicate that the handler was shutdown due to error. It is recommended to open a handler within a context manager as opposed to calling the method directly.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Close down the receiver connection. If the receiver has already closed, this operation will do nothing. An optional exception can be passed in to indicate that the handler was shutdown due to error. It is recommended to open a handler within a context manager as opposed to calling the method directly. The receiver will be implicitly closed on completion of the message iterator, however this method will need to be called explicitly if the message iterator is not run to completion.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the session state.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Set the session state.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Receive messages that have previously been deferred. This operation can only receive deferred messages from the current session. When receiving deferred messages from a partitioned entity, all of the supplied sequence numbers must be messages from the same partition.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Merges two `Reservation`s. Merge the specified `Reservation`s into a new `Reservation`. The two `Reservation`s being merged must have same properties.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Verifies that the challenge is a Bearer challenge and returns the key=value pairs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Purges data in an Log Analytics workspace by a set of user-defined filters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Handle connection and service errors. Called internally when an event has failed to send so we can parse the error to determine whether we should attempt to retry sending the event again.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new queue. Once created, this queue's resource manifest is immutable.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes an existing queue. This operation will also remove all associated state including messages in the queue.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves an existing queue.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new topic. Once created, this topic resource manifest is immutable.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the description for the specified topic.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new rule. Once created, this rule's resource manifest is immutable.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the description for the specified rule.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the rules that exist under the specified subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new subscription. Once created, this subscription resource manifest is immutable.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets an existing subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves the subscriptions in the specified topic.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Enqueues a message into the specified topic. The limit to the number of messages which may be present in the topic is governed by the message size in MaxTopicSizeInBytes. If this message causes the topic to exceed its quota, a quota exceeded error is returned and the message will be rejected.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Unlock a message for processing by other receivers on a given subscription. This operation deletes the lock object, causing the message to be unlocked. A message must have first been locked by a receiver before this operation is called.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends a batch of messages into the specified queue. The limit to the number of messages which may be present in the topic is governed by the message size the MaxTopicSizeInMegaBytes. If this message will cause the queue to exceed its quota, a quota exceeded error is returned and the message will be rejected.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Unlocks a message for processing by other receivers on a given queue. This operation deletes the lock object, causing the message to be unlocked. A message must have first been locked by a receiver before this operation is called.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Receive a message from a queue for processing.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Receive a message from a subscription for processing.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new Event Hub.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates an Event Hub.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieves an existing event hub.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sends a new message event to an Event Hub.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add additional headers for Service Bus.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "return the signed string with token.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if token expires or not.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "pulls the query string out of the URI and moves it into the query portion of the request object. If there are already query parameters on the request the parameters in the URI will appear after the existing parameters\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reset Service Principal Profile of a managed cluster. Update the service principal Profile for a managed cluster.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes itself if find queue name or topic name and subscription name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Unlocks itself if find queue name or topic name and subscription name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Renew lock on itself if find queue name or topic name and subscription name.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "add addtional headers to request for message request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "return the current message as expected by batch body format\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the health of a Service Fabric cluster. Use EventsHealthStateFilter to filter the collection of health events reported on the cluster based on the health state. Similarly, use NodesHealthStateFilter and ApplicationsHealthStateFilter to filter the collection of nodes and applications returned based on their aggregated health state.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets the health of a Service Fabric cluster using the specified policy. Use EventsHealthStateFilter to filter the collection of health events reported on the cluster based on the health state. Similarly, use NodesHealthStateFilter and ApplicationsHealthStateFilter to filter the collection of nodes and applications returned based on their aggregated health state. Use ClusterHealthPolicies to override the health policies used to evaluate the health.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Removes or unregisters a Service Fabric application type from the cluster. This operation can only be performed if all application instances of the application type have been deleted. Once the application type is unregistered, no new application instances can be created for this particular application type.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gets a list of repair tasks matching the given filters. This API supports the Service Fabric platform; it is not meant to be used directly from your code.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Submits a property batch. Submits a batch of property operations. Either all or none of the operations will be committed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Simple error handler for azure.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Start capturing network packets for the site. Start capturing network packets for the site.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the difference in configuration settings between two web app slots. Get the difference in configuration settings between two web app slots.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Swaps two deployment slots of an app. Swaps two deployment slots of an app.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute OData query. Executes an OData query for events.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add a face to a large face list. The input face is specified as an image with a targetFace rectangle. It returns a persistedFaceId representing the added face, and persistedFaceId will not expire.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reset auth_attempted on redirects.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates Migration configuration and starts migration of entities from Standard to Premium namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Publishes a batch of events to an Azure Event Grid topic.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Moves resources from one resource group to another resource group. The resources to move must be in the same source resource group. The target resource group may be in a different subscription. When moving resources, both the source group and the target group are locked for the duration of the operation. Write and delete operations are blocked on the groups until the move completes. .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Define a new default profile.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Queries policy tracked resources under the management group.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a queue entity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a queue entity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a topic entity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a topic entity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a subscription entity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a Client from a Service Bus connection string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Perform an operation to update the properties of the entity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Whether the receivers lock on a particular session has expired.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a session for a node.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates an Azure subscription.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Export logs that show Api requests made by this subscription in the given time window to show throttling activities.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scan output for exceptions If there is an output from an add task collection call add it to the results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a chunk of tasks to the job Retry chunk if body exceeds the maximum request size and retry tasks if failed due to server errors.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Main method for worker to run Pops a chunk of tasks off the collection of pending tasks to be added and submits them to be added.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Will build the actual config for Jinja2, based on SDK config.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resets the user password on an environment This operation can take a while to complete.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts an environment by starting all resources inside the environment. This operation can take a while to complete.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create message from response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts entry element to rule object. The format of xml for\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts entry element to queue object. The format of xml response for\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts entry element to topic The xml format for\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Converts entry element to subscription The xml format for\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new certificate inside the specified account.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Deletes the specified certificate.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a SDK client initialized with current CLI credentials, CLI default subscription and CLI default cloud. This method will fill automatically the following client\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a SDK client initialized with a JSON auth dict. The easiest way to obtain this content is to call the following CLI\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return a SDK client initialized with auth file. The easiest way to obtain this file is to call the following CLI\n",
      "----------------------------------------------------------------------------------------------------\n",
      "resp_body is the XML we received resp_type is a string, such as Containers, return_type is the type we're constructing, such as ContainerEnumResults item_type is the type object of the item to be created, such as Container This function then returns a ContainerEnumResults object with the containers member populated with the results.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "get properties from element tree element\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete the Provisioning Service Certificate. Deletes the specified certificate assosciated with the Provisioning Service.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a client for a queue entity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get clients for all queue entities in the namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a client for a topic entity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a client for all topic entities in the namespace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Receive messages by sequence number that have been previously deferred. When receiving deferred messages from a partitioned entity, all of the supplied sequence numbers must be messages from the same partition.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Settle messages that have been previously deferred.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "List the web sites defined on this webspace.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a website.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Delete a website.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Update a web site.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Restart a web site.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get historical usage metrics.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get metric definitions of metrics available of this web site.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a site's publish profile as a string\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a site's publish profile as an object\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Updates the policies for the specified container registry.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Create Cloud Service request creates a new cloud service. When job collections are created, they are hosted within a cloud service. A cloud service groups job collections together in a given region. Once a cloud service has been created, job collections can then be created and contained within it.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Check Name Availability operation checks if a new job collection with the given name may be created, or if it is unavailable. The result of the operation is a Boolean true or false.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Get Job Collection operation gets the details of a job collection\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Completes the restore operation on a managed database.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cancel one or more messages that have previsouly been scheduled and are still pending.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wait until all pending messages have been sent.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reconnect the handler. If the handler was disconnected from the service with a retryable error - attempt to reconnect. This method will be called automatically for most retryable errors. Also attempts to re-queue any messages that were pending before the reconnect.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Writes a certificate file to the specified location. This can then be used to instantiate ServiceManagementService.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load any stored cookies for the plugin that have not expired.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Drops Characters by unicode not by bytes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clears out the previous line and prints a new one.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Formats the file size into a human readable format.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Formats elapsed seconds into a human readable format.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a status line with appropriate size.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Progress an iterator and updates a pretty status line to the terminal. The status line\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yield the segment number and when it will be available There are two cases for segment number generation, static and dynamic. In the case of static stream, the segment number starts at the startNumber and counts up to the number of segments that are represented by the periods duration. In the case of dynamic streams, the segments should appear at the specified time in the simplest case the segment number is based on the time since the availabilityStartTime\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Segments are yielded when they are available Segments appear on a time line, for dynamic content they are only available at a certain time and sometimes for a limited time. For static content they are all available at the same time.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pauses the thread for a specified time.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adds a segment to the download pool and write queue.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Puts a value into a queue but aborts if this thread is closed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Given an HTTP response from the sessino endpoint, extract the nonce, so we can \"sign\" requests with it. We don't really sign the requests in the traditional sense of a nonce, we just incude them in the auth requests.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Find the Video Packet ID in the HTML for the provided URL\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wrapper around json.loads. Wraps errors in custom exception with a snippet of the data in the message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wrapper around ElementTree.fromstring with some extras. Provides these extra\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses a query string into a dict. Unlike parse_qs and parse_qsl, duplicate keys are not preserved in favor of a simpler return value.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Search for a key in a nested dict, or list of nested dicts, and return the values.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spawn the process defined in `cmd` parameters is converted to options the short and long option prefixes if a list is given as the value, the parameter is repeated with each value If timeout is set the spawn will block until the process returns or the timeout expires.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Brute force regex based HTML tag parser. This is a rough-and-ready searcher to find HTML tags when standards compliance is not required. Will find tags that are commented out, or inside script tag etc.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempt to parse a DASH manifest file and return its streams\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Determine which Unicode encoding the JSON text sample is encoded with RFC4627 (\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses JSON from a response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses XML from a response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses a semi-colon delimited list of cookies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses a semi-colon delimited list of headers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses a semi-colon delimited list of query parameters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the message for this LogRecord. Return the message for this LogRecord after merging any user-supplied arguments with the message.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A factory method which can be overridden in subclasses to create specialized LogRecords.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempt a login to LiveEdu.tv\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads a plugin from the same directory as the calling plugin. The path used is extracted from the last call in module scope, therefore this must be called only from module level in the originating plugin or the correct plugin path will not be found.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Update or remove keys from a query string in a URL\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reads FLV tags from fd or buf and returns them with adjusted timestamps.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Find all the arguments required by name\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if file already exists and ask the user if it should be overwritten if it does.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decides where to write the stream. Depending on arguments it can be one of\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a HTTP server listening on a given host and port. If host is empty, listen on all available interfaces, and if port is 0, listen on a random high port.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Repeatedly accept HTTP connections on a server. Forever if the serving externally, or while a player is running if it is not empty.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Continuously output the stream over HTTP.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prepares a filename to be passed to the player.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens a stream and reads 8192 bytes from it. This is useful to check if a stream actually has data before opening the output.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Open stream, create output and finally write the stream to output.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reads data from stream and then writes it to the output.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decides what to do with the selected stream. Depending on arguments it can be one of\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fetches streams using correct parameters.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempts to fetch streams repeatedly until some are returned or limit hit.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Formats a dict of streams. Filters out synonyms and displays them next to the stream they point to. Streams are sorted according to their quality (based on plugin.stream_weight).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The URL handler. Attempts to resolve the URL to a plugin and then attempts to fetch a list of available streams. Proceeds to handle stream if user specified a valid one, otherwise output list of valid streams.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Outputs a list of all plugins Streamlink has loaded.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Opens a web browser to allow the user to grant Streamlink access to their Twitch account.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempts to load plugins from a list of directories.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses arguments.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Console setup.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets the global HTTP settings, such as proxy and headers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loads any additional plugins.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets Streamlink options.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Show current installed versions\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Try to find a stream_id\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fallback if no stream_id was found before\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets general options used by plugins and streams originating from this session object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sets plugin specific options used by plugins originating from this session object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempts to find a plugin that can use this URL. The default protocol (http) will be prefixed to the URL if not specified.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempt to load plugins from the path specified.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "converts a timestamp to seconds -\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if the string value starts with another string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if the string value ends with another string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Checks if the string value contains another string.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a named attribute from an object. When a default argument is given, it is returned when the attribute doesn't exist.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Filters out unwanted items using the specified function. Supports both dicts and sequences, key/value pairs are expanded when applied to a dict.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Apply function to each value inside the sequence or dict. Supports both dicts and sequences, key/value pairs are expanded when applied to a dict.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parses an URL and validates its attributes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Find a XML element via xpath.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Find a list of XML elements via xpath.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Finds embedded player url in HTTP response.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Attempts to parse a M3U8 playlist from a string of data. If specified, *base_uri\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check if the current player supports adding a title\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Logs in to Steam\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "login and update cached cookies\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a key-function mapping. The return value from the function should be either - A tuple containing a name and stream - A iterator of tuples containing a name and stream Any extra arguments will be passed to the function.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Makes a call against the api.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starts a session against Crunchyroll's server. Is recommended that you call this method before making any other calls to make sure you have a valid session against the server.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creates a new CrunchyrollAPI object, initiates it's session and tries to authenticate it either by using saved credentials or the user's username and password.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read the frame images from a directory and join them as a video\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read the next frame. If the next frame have been decoded before and in the cache, then return it directly, otherwise decode, cache and return it.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get frame by index.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a video to frame images\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Track the progress of tasks execution with a progress bar. Tasks are done with a simple for-loop.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Track the progress of parallel task execution with a progress bar. The built-in :mod:`multiprocessing` module is used for process pools and tasks are done with :func:`Pool.map` or :func:`Pool.imap_unordered`.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Flip an image horizontally or vertically.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rotate an image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Clip bboxes to fit the image shape.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scaling bboxes w.r.t the box center.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Crop image patches. 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pad an image to a certain shape.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pad an image to ensure each edge to be multiple to some number.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rescale a size by a ratio.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resize image to a given size.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resize image to the same size of a given image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resize image while keeping the aspect ratio.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Register a handler for some file extensions.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get priority value.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dequantize an array.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Show an image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Draw bboxes on an image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read an optical flow map.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Write optical flow to file. If the flow is not quantized, it will be saved as a .flo file losslessly, otherwise a jpeg image which is lossy but of much smaller size. (dx and dy will be concatenated horizontally into a single image if quantize is True.)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Recover from quantized flow.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load state_dict to a module. This method is modified from :meth:`torch.nn.Module.load_state_dict`. Default value for ``strict`` is set to ``False`` and the message for param mismatch will be shown even if strict is False.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load checkpoint from a file or URI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Copy a model state_dict to cpu.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Save checkpoint to file. The checkpoint will have 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Init the optimizer.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Init the logger.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get current learning rates.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Register a hook into the hook list.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Start running.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Register default hooks for training. Default hooks\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a video with ffmpeg. This provides a general api to ffmpeg, the executed command\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resize a video.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cut a clip from a video.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Concatenate multiple videos into a single one.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load a text file and parse the content as a list of strings.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Load a text file and parse the content as a dict. Each line of the text file will be two or more columns splited by whitespaces or tabs. The first column will be parsed as dict keys, and the following columns will be parsed as dict values.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "3x3 convolution with padding\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initialize an object from dict. The dict must contain the key \"type\", which indicates the object type, it can be either a string or type, such as \"list\" or ``list``. Remaining fields are treated as the arguments for constructing the object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read an image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read an image from bytes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Write image to file\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a BGR image to grayscale image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert a grayscale image to BGR image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cast elements of an iterable object into some type.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Check whether it is a sequence of some type.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Slice a list into several sub lists by a list of given length.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A decorator factory to check if prerequisites are satisfied.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Average latest n values or all values\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scatters tensor across multiple GPUs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert various input to color tuples.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Add check points in a single line. This method is suitable for running a task on a list of items. A timer will be registered when the method is called for the first time.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Start the timer.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Total time since the timer is started.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time since the last checking. Either :func:`since_start` or :func:`since_last_check` is a checking operation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Show optical flow.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Convert flow map to RGB image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Build a color wheel.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Computes the precision@k for the specified values of k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scatter inputs to target gpus. The only difference from original :func:`scatter` is to add support for\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Scatter with support for kwargs dictionary\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This captures a common pattern of fanning out a single value to N steps, where each step has similar structure. The strict requirement here is that each step must provide an output named the parameters parallel_step_output. This takes those steps and then uses a join node to coalesce them so that downstream steps can depend on a single output. Currently the join step just does a passthrough with no computation. It remains to be seen if there should be any work or verification done in this step, especially in multi-process environments that require persistence between steps.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ensures argument obj is a native Python dictionary, raises an exception if not, and otherwise returns obj.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ensures argument obj is either a dictionary or None; if the latter, instantiates an empty dictionary.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Callback receives a stream of event_records\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Record a stream of event records to json\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Read a config file and instantiate the RCParser. Create new :class:`configparser.ConfigParser` for the given **path** and instantiate the :class:`RCParser` with the ConfigParser as :attr:`config` attribute. If the **path*\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get config dictionary for the given repository. If the repository section is not found in the config file, return ``None``. If the file is invalid, raise :exc:`configparser.Error`. Otherwise return a dictionary\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This recursive descent thing formats a config dict for GraphQL.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get a pipeline by name. Only constructs that pipeline and caches it.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return all pipelines as a list\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This function polls the process until it returns a valid item or returns PROCESS_DEAD_AND_QUEUE_EMPTY if it is in a state where the process has terminated and the queue is empty\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute pipeline using message queue as a transport\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Waits until all there are no processes enqueued.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The schema for configuration data that describes the type, optionality, defaults, and description.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Builds the execution plan.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Here we build a new ExecutionPlan from a pipeline definition and the environment config. To do this, we iterate through the pipeline's solids in topological order, and hand off the execution steps for each solid to a companion _PlanBuilder object. Once we've processed the entire pipeline, we invoke _PlanBuilder.build() to construct the ExecutionPlan object.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Build a pipeline which is a subset of another pipeline. Only includes the solids which are in solid_names.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return the solid named \"name\". Throws if it does not exist.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Get the shell commands we'll use to actually build and publish a package to PyPI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tags all submodules for a new release. Ensures that git tags, as well as the version.py files in each submodule, agree and that the new version is strictly greater than the current version. Will fail if the new version is not an increment (following PEP 440). Creates a new git tag and commit.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Create a context definition from a pre-existing context. This can be useful in testing contexts where you may want to create a context manually and then pass it into a one-off PipelineDefinition\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A decorator for annotating a function that can take the selected properties from a ``config_value`` in to an instance of a custom type.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A decorator for a annotating a function that can take the selected properties of a ``config_value`` and an instance of a custom type and materialize it.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Automagically wrap a block of text.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Download an object from s3.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Upload a file to s3.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wraps the execution of user-space code in an error boundary. This places a uniform policy around an user code invoked by the framework. This ensures that all user errors are wrapped in the DagsterUserCodeExecutionError, and that the original stack trace of the user error is preserved, so that it can be reported without confusing framework code in the stack trace, if a tool author wishes to do so. This has been especially help in a notebooking context.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The missing mkdir -p functionality in os.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wraps the output of a user provided function that may yield or return a value and returns a generator that asserts it only yields a single value.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In the event of pipeline initialization failure, we want to be able to log the failure without a dependency on the ExecutionContext to initialize DagsterLog\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Whether the solid execution was successful\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Whether the solid execution was skipped\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Return dictionary of transformed results, with keys being output names.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A permissive dict will permit the user to partially specify the permitted fields. Any fields that are specified and passed in will be type checked. Other fields will be allowed, but will be ignored by the type checker.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Datasets must be of form \"project.dataset\" or \"dataset\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tables must be of form \"project.dataset.table\" or \"dataset.table\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Execute the user-specified transform for the solid. Wrap in an error boundary and do all relevant logging and metrics tracking\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Takes a python cls and creates a type for it in the Dagster domain.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A decorator for creating a resource. The decorated function will be used as the resource_fn in a ResourceDefinition.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Events API v2 enables you to add PagerDuty's advanced event and incident management functionality to any system that can make an outbound HTTP connection.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Groups execution steps by solid, in topological order of the solids.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for el in df['method_summary'].to_list():\n",
    "    print(el)\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>repo</th>\n",
       "      <th>path</th>\n",
       "      <th>func_name</th>\n",
       "      <th>original_string</th>\n",
       "      <th>language</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>sha</th>\n",
       "      <th>url</th>\n",
       "      <th>method_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/executors/base_executor.py</td>\n",
       "      <td>BaseExecutor.has_task</td>\n",
       "      <td>def has_task(self, task_instance):\\n        \"\"...</td>\n",
       "      <td>python</td>\n",
       "      <td>def has_task(self, task_instance):\\n        \"\"...</td>\n",
       "      <td>[def, has_task, (, self, ,, task_instance, ), ...</td>\n",
       "      <td>Checks if a task is either queued or running i...</td>\n",
       "      <td>[Checks, if, a, task, is, either, queued, or, ...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Checks if a task is either queued or running i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/contrib/hooks/snowflake_hook.py</td>\n",
       "      <td>SnowflakeHook._get_aws_credentials</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        \"\"\"\\n...</td>\n",
       "      <td>python</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        \"\"\"\\n...</td>\n",
       "      <td>[def, _get_aws_credentials, (, self, ), :, if,...</td>\n",
       "      <td>returns aws_access_key_id, aws_secret_access_k...</td>\n",
       "      <td>[returns, aws_access_key_id, aws_secret_access...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>returns aws_access_key_id, aws_secret_access_k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/hooks/postgres_hook.py</td>\n",
       "      <td>PostgresHook.copy_expert</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "      <td>python</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "      <td>[def, copy_expert, (, self, ,, sql, ,, filenam...</td>\n",
       "      <td>Executes SQL using psycopg2 copy_expert method...</td>\n",
       "      <td>[Executes, SQL, using, psycopg2, copy_expert, ...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Executes SQL using psycopg2 copy_expert method...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/hooks/postgres_hook.py</td>\n",
       "      <td>PostgresHook.bulk_dump</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "      <td>[def, bulk_dump, (, self, ,, table, ,, tmp_fil...</td>\n",
       "      <td>Dumps a database table into a tab-delimited file</td>\n",
       "      <td>[Dumps, a, database, table, into, a, tab, -, d...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Dumps a database table into a tab-delimited file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/contrib/operators/file_to_gcs.py</td>\n",
       "      <td>FileToGoogleCloudStorageOperator.execute</td>\n",
       "      <td>def execute(self, context):\\n        \"\"\"\\n    ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def execute(self, context):\\n        \"\"\"\\n    ...</td>\n",
       "      <td>[def, execute, (, self, ,, context, ), :, hook...</td>\n",
       "      <td>Uploads the file to Google cloud storage</td>\n",
       "      <td>[Uploads, the, file, to, Google, cloud, storage]</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Uploads the file to Google cloud storage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>2350</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "      <td>flow2rgb</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>python</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>[def, flow2rgb, (, flow, ,, color_wheel, =, No...</td>\n",
       "      <td>Convert flow map to RGB image.\\n\\n    Args:\\n ...</td>\n",
       "      <td>[Convert, flow, map, to, RGB, image, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Convert flow map to RGB image.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>2351</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "      <td>make_color_wheel</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    \"\"\"Build...</td>\n",
       "      <td>python</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    \"\"\"Build...</td>\n",
       "      <td>[def, make_color_wheel, (, bins, =, None, ), :...</td>\n",
       "      <td>Build a color wheel.\\n\\n    Args:\\n        bin...</td>\n",
       "      <td>[Build, a, color, wheel, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Build a color wheel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>2352</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>examples/train_cifar10.py</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>[def, accuracy, (, output, ,, target, ,, topk,...</td>\n",
       "      <td>Computes the precision@k for the specified val...</td>\n",
       "      <td>[Computes, the, precision]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Computes the precision@k for the specified val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>2353</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "      <td>scatter</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>[def, scatter, (, inputs, ,, target_gpus, ,, d...</td>\n",
       "      <td>Scatter inputs to target gpus.\\n\\n    The only...</td>\n",
       "      <td>[Scatter, inputs, to, target, gpus, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Scatter inputs to target gpus. The only differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>2354</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "      <td>scatter_kwargs</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>python</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>[def, scatter_kwargs, (, inputs, ,, kwargs, ,,...</td>\n",
       "      <td>Scatter with support for kwargs dictionary</td>\n",
       "      <td>[Scatter, with, support, for, kwargs, dictionary]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Scatter with support for kwargs dictionary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>846 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             repo                                      path  \\\n",
       "25      25   apache/airflow        airflow/executors/base_executor.py   \n",
       "28      28   apache/airflow   airflow/contrib/hooks/snowflake_hook.py   \n",
       "30      30   apache/airflow            airflow/hooks/postgres_hook.py   \n",
       "31      31   apache/airflow            airflow/hooks/postgres_hook.py   \n",
       "32      32   apache/airflow  airflow/contrib/operators/file_to_gcs.py   \n",
       "...    ...              ...                                       ...   \n",
       "2350  2350  open-mmlab/mmcv             mmcv/visualization/optflow.py   \n",
       "2351  2351  open-mmlab/mmcv             mmcv/visualization/optflow.py   \n",
       "2352  2352  open-mmlab/mmcv                 examples/train_cifar10.py   \n",
       "2353  2353  open-mmlab/mmcv           mmcv/parallel/scatter_gather.py   \n",
       "2354  2354  open-mmlab/mmcv           mmcv/parallel/scatter_gather.py   \n",
       "\n",
       "                                     func_name  \\\n",
       "25                       BaseExecutor.has_task   \n",
       "28          SnowflakeHook._get_aws_credentials   \n",
       "30                    PostgresHook.copy_expert   \n",
       "31                      PostgresHook.bulk_dump   \n",
       "32    FileToGoogleCloudStorageOperator.execute   \n",
       "...                                        ...   \n",
       "2350                                  flow2rgb   \n",
       "2351                          make_color_wheel   \n",
       "2352                                  accuracy   \n",
       "2353                                   scatter   \n",
       "2354                            scatter_kwargs   \n",
       "\n",
       "                                        original_string language  \\\n",
       "25    def has_task(self, task_instance):\\n        \"\"...   python   \n",
       "28    def _get_aws_credentials(self):\\n        \"\"\"\\n...   python   \n",
       "30    def copy_expert(self, sql, filename, open=open...   python   \n",
       "31    def bulk_dump(self, table, tmp_file):\\n       ...   python   \n",
       "32    def execute(self, context):\\n        \"\"\"\\n    ...   python   \n",
       "...                                                 ...      ...   \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...   python   \n",
       "2351  def make_color_wheel(bins=None):\\n    \"\"\"Build...   python   \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...   python   \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...   python   \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...   python   \n",
       "\n",
       "                                                   code  \\\n",
       "25    def has_task(self, task_instance):\\n        \"\"...   \n",
       "28    def _get_aws_credentials(self):\\n        \"\"\"\\n...   \n",
       "30    def copy_expert(self, sql, filename, open=open...   \n",
       "31    def bulk_dump(self, table, tmp_file):\\n       ...   \n",
       "32    def execute(self, context):\\n        \"\"\"\\n    ...   \n",
       "...                                                 ...   \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...   \n",
       "2351  def make_color_wheel(bins=None):\\n    \"\"\"Build...   \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...   \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...   \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...   \n",
       "\n",
       "                                            code_tokens  \\\n",
       "25    [def, has_task, (, self, ,, task_instance, ), ...   \n",
       "28    [def, _get_aws_credentials, (, self, ), :, if,...   \n",
       "30    [def, copy_expert, (, self, ,, sql, ,, filenam...   \n",
       "31    [def, bulk_dump, (, self, ,, table, ,, tmp_fil...   \n",
       "32    [def, execute, (, self, ,, context, ), :, hook...   \n",
       "...                                                 ...   \n",
       "2350  [def, flow2rgb, (, flow, ,, color_wheel, =, No...   \n",
       "2351  [def, make_color_wheel, (, bins, =, None, ), :...   \n",
       "2352  [def, accuracy, (, output, ,, target, ,, topk,...   \n",
       "2353  [def, scatter, (, inputs, ,, target_gpus, ,, d...   \n",
       "2354  [def, scatter_kwargs, (, inputs, ,, kwargs, ,,...   \n",
       "\n",
       "                                              docstring  \\\n",
       "25    Checks if a task is either queued or running i...   \n",
       "28    returns aws_access_key_id, aws_secret_access_k...   \n",
       "30    Executes SQL using psycopg2 copy_expert method...   \n",
       "31     Dumps a database table into a tab-delimited file   \n",
       "32             Uploads the file to Google cloud storage   \n",
       "...                                                 ...   \n",
       "2350  Convert flow map to RGB image.\\n\\n    Args:\\n ...   \n",
       "2351  Build a color wheel.\\n\\n    Args:\\n        bin...   \n",
       "2352  Computes the precision@k for the specified val...   \n",
       "2353  Scatter inputs to target gpus.\\n\\n    The only...   \n",
       "2354         Scatter with support for kwargs dictionary   \n",
       "\n",
       "                                       docstring_tokens  \\\n",
       "25    [Checks, if, a, task, is, either, queued, or, ...   \n",
       "28    [returns, aws_access_key_id, aws_secret_access...   \n",
       "30    [Executes, SQL, using, psycopg2, copy_expert, ...   \n",
       "31    [Dumps, a, database, table, into, a, tab, -, d...   \n",
       "32     [Uploads, the, file, to, Google, cloud, storage]   \n",
       "...                                                 ...   \n",
       "2350            [Convert, flow, map, to, RGB, image, .]   \n",
       "2351                        [Build, a, color, wheel, .]   \n",
       "2352                         [Computes, the, precision]   \n",
       "2353             [Scatter, inputs, to, target, gpus, .]   \n",
       "2354  [Scatter, with, support, for, kwargs, dictionary]   \n",
       "\n",
       "                                           sha  \\\n",
       "25    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "28    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "30    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "31    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "32    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "...                                        ...   \n",
       "2350  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2351  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2352  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2353  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2354  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "\n",
       "                                                    url  \\\n",
       "25    https://github.com/apache/airflow/blob/b69c686...   \n",
       "28    https://github.com/apache/airflow/blob/b69c686...   \n",
       "30    https://github.com/apache/airflow/blob/b69c686...   \n",
       "31    https://github.com/apache/airflow/blob/b69c686...   \n",
       "32    https://github.com/apache/airflow/blob/b69c686...   \n",
       "...                                                 ...   \n",
       "2350  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2351  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2352  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2353  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2354  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "\n",
       "                                         method_summary  \n",
       "25    Checks if a task is either queued or running i...  \n",
       "28    returns aws_access_key_id, aws_secret_access_k...  \n",
       "30    Executes SQL using psycopg2 copy_expert method...  \n",
       "31     Dumps a database table into a tab-delimited file  \n",
       "32             Uploads the file to Google cloud storage  \n",
       "...                                                 ...  \n",
       "2350                     Convert flow map to RGB image.  \n",
       "2351                               Build a color wheel.  \n",
       "2352  Computes the precision@k for the specified val...  \n",
       "2353  Scatter inputs to target gpus. The only differ...  \n",
       "2354         Scatter with support for kwargs dictionary  \n",
       "\n",
       "[846 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df[(10 < df['method_summary'].str.len()) & (df['method_summary'].str.len() < 200)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Azure/azure-sdk-for-python</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>streamlink/streamlink</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         repo  count\n",
       "0  Azure/azure-sdk-for-python    247\n",
       "1              apache/airflow    465\n",
       "2             open-mmlab/mmcv     60\n",
       "3       streamlink/streamlink     74"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by repository_name and count values\n",
    "grouped_df = filtered_df.groupby('repo')['method_summary'].count().reset_index(name='count')\n",
    "# Select those where count > 50\n",
    "grouped_df = grouped_df[grouped_df['count'] > 50]\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>repo</th>\n",
       "      <th>path</th>\n",
       "      <th>func_name</th>\n",
       "      <th>original_string</th>\n",
       "      <th>language</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>sha</th>\n",
       "      <th>url</th>\n",
       "      <th>method_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/executors/base_executor.py</td>\n",
       "      <td>BaseExecutor.has_task</td>\n",
       "      <td>def has_task(self, task_instance):\\n        \"\"...</td>\n",
       "      <td>python</td>\n",
       "      <td>def has_task(self, task_instance):\\n        \"\"...</td>\n",
       "      <td>[def, has_task, (, self, ,, task_instance, ), ...</td>\n",
       "      <td>Checks if a task is either queued or running i...</td>\n",
       "      <td>[Checks, if, a, task, is, either, queued, or, ...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Checks if a task is either queued or running i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/contrib/hooks/snowflake_hook.py</td>\n",
       "      <td>SnowflakeHook._get_aws_credentials</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        \"\"\"\\n...</td>\n",
       "      <td>python</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        \"\"\"\\n...</td>\n",
       "      <td>[def, _get_aws_credentials, (, self, ), :, if,...</td>\n",
       "      <td>returns aws_access_key_id, aws_secret_access_k...</td>\n",
       "      <td>[returns, aws_access_key_id, aws_secret_access...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>returns aws_access_key_id, aws_secret_access_k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/hooks/postgres_hook.py</td>\n",
       "      <td>PostgresHook.copy_expert</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "      <td>python</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "      <td>[def, copy_expert, (, self, ,, sql, ,, filenam...</td>\n",
       "      <td>Executes SQL using psycopg2 copy_expert method...</td>\n",
       "      <td>[Executes, SQL, using, psycopg2, copy_expert, ...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Executes SQL using psycopg2 copy_expert method...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/hooks/postgres_hook.py</td>\n",
       "      <td>PostgresHook.bulk_dump</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "      <td>[def, bulk_dump, (, self, ,, table, ,, tmp_fil...</td>\n",
       "      <td>Dumps a database table into a tab-delimited file</td>\n",
       "      <td>[Dumps, a, database, table, into, a, tab, -, d...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Dumps a database table into a tab-delimited file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/contrib/operators/file_to_gcs.py</td>\n",
       "      <td>FileToGoogleCloudStorageOperator.execute</td>\n",
       "      <td>def execute(self, context):\\n        \"\"\"\\n    ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def execute(self, context):\\n        \"\"\"\\n    ...</td>\n",
       "      <td>[def, execute, (, self, ,, context, ), :, hook...</td>\n",
       "      <td>Uploads the file to Google cloud storage</td>\n",
       "      <td>[Uploads, the, file, to, Google, cloud, storage]</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Uploads the file to Google cloud storage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>2350</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "      <td>flow2rgb</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>python</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>[def, flow2rgb, (, flow, ,, color_wheel, =, No...</td>\n",
       "      <td>Convert flow map to RGB image.\\n\\n    Args:\\n ...</td>\n",
       "      <td>[Convert, flow, map, to, RGB, image, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Convert flow map to RGB image.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>2351</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "      <td>make_color_wheel</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    \"\"\"Build...</td>\n",
       "      <td>python</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    \"\"\"Build...</td>\n",
       "      <td>[def, make_color_wheel, (, bins, =, None, ), :...</td>\n",
       "      <td>Build a color wheel.\\n\\n    Args:\\n        bin...</td>\n",
       "      <td>[Build, a, color, wheel, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Build a color wheel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>2352</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>examples/train_cifar10.py</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>[def, accuracy, (, output, ,, target, ,, topk,...</td>\n",
       "      <td>Computes the precision@k for the specified val...</td>\n",
       "      <td>[Computes, the, precision]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Computes the precision@k for the specified val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>2353</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "      <td>scatter</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>[def, scatter, (, inputs, ,, target_gpus, ,, d...</td>\n",
       "      <td>Scatter inputs to target gpus.\\n\\n    The only...</td>\n",
       "      <td>[Scatter, inputs, to, target, gpus, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Scatter inputs to target gpus. The only differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>2354</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "      <td>scatter_kwargs</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>python</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>[def, scatter_kwargs, (, inputs, ,, kwargs, ,,...</td>\n",
       "      <td>Scatter with support for kwargs dictionary</td>\n",
       "      <td>[Scatter, with, support, for, kwargs, dictionary]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Scatter with support for kwargs dictionary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>846 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             repo                                      path  \\\n",
       "25      25   apache/airflow        airflow/executors/base_executor.py   \n",
       "28      28   apache/airflow   airflow/contrib/hooks/snowflake_hook.py   \n",
       "30      30   apache/airflow            airflow/hooks/postgres_hook.py   \n",
       "31      31   apache/airflow            airflow/hooks/postgres_hook.py   \n",
       "32      32   apache/airflow  airflow/contrib/operators/file_to_gcs.py   \n",
       "...    ...              ...                                       ...   \n",
       "2350  2350  open-mmlab/mmcv             mmcv/visualization/optflow.py   \n",
       "2351  2351  open-mmlab/mmcv             mmcv/visualization/optflow.py   \n",
       "2352  2352  open-mmlab/mmcv                 examples/train_cifar10.py   \n",
       "2353  2353  open-mmlab/mmcv           mmcv/parallel/scatter_gather.py   \n",
       "2354  2354  open-mmlab/mmcv           mmcv/parallel/scatter_gather.py   \n",
       "\n",
       "                                     func_name  \\\n",
       "25                       BaseExecutor.has_task   \n",
       "28          SnowflakeHook._get_aws_credentials   \n",
       "30                    PostgresHook.copy_expert   \n",
       "31                      PostgresHook.bulk_dump   \n",
       "32    FileToGoogleCloudStorageOperator.execute   \n",
       "...                                        ...   \n",
       "2350                                  flow2rgb   \n",
       "2351                          make_color_wheel   \n",
       "2352                                  accuracy   \n",
       "2353                                   scatter   \n",
       "2354                            scatter_kwargs   \n",
       "\n",
       "                                        original_string language  \\\n",
       "25    def has_task(self, task_instance):\\n        \"\"...   python   \n",
       "28    def _get_aws_credentials(self):\\n        \"\"\"\\n...   python   \n",
       "30    def copy_expert(self, sql, filename, open=open...   python   \n",
       "31    def bulk_dump(self, table, tmp_file):\\n       ...   python   \n",
       "32    def execute(self, context):\\n        \"\"\"\\n    ...   python   \n",
       "...                                                 ...      ...   \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...   python   \n",
       "2351  def make_color_wheel(bins=None):\\n    \"\"\"Build...   python   \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...   python   \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...   python   \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...   python   \n",
       "\n",
       "                                                   code  \\\n",
       "25    def has_task(self, task_instance):\\n        \"\"...   \n",
       "28    def _get_aws_credentials(self):\\n        \"\"\"\\n...   \n",
       "30    def copy_expert(self, sql, filename, open=open...   \n",
       "31    def bulk_dump(self, table, tmp_file):\\n       ...   \n",
       "32    def execute(self, context):\\n        \"\"\"\\n    ...   \n",
       "...                                                 ...   \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...   \n",
       "2351  def make_color_wheel(bins=None):\\n    \"\"\"Build...   \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...   \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...   \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...   \n",
       "\n",
       "                                            code_tokens  \\\n",
       "25    [def, has_task, (, self, ,, task_instance, ), ...   \n",
       "28    [def, _get_aws_credentials, (, self, ), :, if,...   \n",
       "30    [def, copy_expert, (, self, ,, sql, ,, filenam...   \n",
       "31    [def, bulk_dump, (, self, ,, table, ,, tmp_fil...   \n",
       "32    [def, execute, (, self, ,, context, ), :, hook...   \n",
       "...                                                 ...   \n",
       "2350  [def, flow2rgb, (, flow, ,, color_wheel, =, No...   \n",
       "2351  [def, make_color_wheel, (, bins, =, None, ), :...   \n",
       "2352  [def, accuracy, (, output, ,, target, ,, topk,...   \n",
       "2353  [def, scatter, (, inputs, ,, target_gpus, ,, d...   \n",
       "2354  [def, scatter_kwargs, (, inputs, ,, kwargs, ,,...   \n",
       "\n",
       "                                              docstring  \\\n",
       "25    Checks if a task is either queued or running i...   \n",
       "28    returns aws_access_key_id, aws_secret_access_k...   \n",
       "30    Executes SQL using psycopg2 copy_expert method...   \n",
       "31     Dumps a database table into a tab-delimited file   \n",
       "32             Uploads the file to Google cloud storage   \n",
       "...                                                 ...   \n",
       "2350  Convert flow map to RGB image.\\n\\n    Args:\\n ...   \n",
       "2351  Build a color wheel.\\n\\n    Args:\\n        bin...   \n",
       "2352  Computes the precision@k for the specified val...   \n",
       "2353  Scatter inputs to target gpus.\\n\\n    The only...   \n",
       "2354         Scatter with support for kwargs dictionary   \n",
       "\n",
       "                                       docstring_tokens  \\\n",
       "25    [Checks, if, a, task, is, either, queued, or, ...   \n",
       "28    [returns, aws_access_key_id, aws_secret_access...   \n",
       "30    [Executes, SQL, using, psycopg2, copy_expert, ...   \n",
       "31    [Dumps, a, database, table, into, a, tab, -, d...   \n",
       "32     [Uploads, the, file, to, Google, cloud, storage]   \n",
       "...                                                 ...   \n",
       "2350            [Convert, flow, map, to, RGB, image, .]   \n",
       "2351                        [Build, a, color, wheel, .]   \n",
       "2352                         [Computes, the, precision]   \n",
       "2353             [Scatter, inputs, to, target, gpus, .]   \n",
       "2354  [Scatter, with, support, for, kwargs, dictionary]   \n",
       "\n",
       "                                           sha  \\\n",
       "25    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "28    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "30    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "31    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "32    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "...                                        ...   \n",
       "2350  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2351  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2352  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2353  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2354  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "\n",
       "                                                    url  \\\n",
       "25    https://github.com/apache/airflow/blob/b69c686...   \n",
       "28    https://github.com/apache/airflow/blob/b69c686...   \n",
       "30    https://github.com/apache/airflow/blob/b69c686...   \n",
       "31    https://github.com/apache/airflow/blob/b69c686...   \n",
       "32    https://github.com/apache/airflow/blob/b69c686...   \n",
       "...                                                 ...   \n",
       "2350  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2351  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2352  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2353  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2354  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "\n",
       "                                         method_summary  \n",
       "25    Checks if a task is either queued or running i...  \n",
       "28    returns aws_access_key_id, aws_secret_access_k...  \n",
       "30    Executes SQL using psycopg2 copy_expert method...  \n",
       "31     Dumps a database table into a tab-delimited file  \n",
       "32             Uploads the file to Google cloud storage  \n",
       "...                                                 ...  \n",
       "2350                     Convert flow map to RGB image.  \n",
       "2351                               Build a color wheel.  \n",
       "2352  Computes the precision@k for the specified val...  \n",
       "2353  Scatter inputs to target gpus. The only differ...  \n",
       "2354         Scatter with support for kwargs dictionary  \n",
       "\n",
       "[846 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df = filtered_df[filtered_df['repo'].isin(grouped_df['repo'])]\n",
    "selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>repo</th>\n",
       "      <th>path</th>\n",
       "      <th>func_name</th>\n",
       "      <th>original_string</th>\n",
       "      <th>language</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>sha</th>\n",
       "      <th>url</th>\n",
       "      <th>method_summary</th>\n",
       "      <th>method_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/executors/base_executor.py</td>\n",
       "      <td>BaseExecutor.has_task</td>\n",
       "      <td>def has_task(self, task_instance):\\n        \"\"...</td>\n",
       "      <td>python</td>\n",
       "      <td>def has_task(self, task_instance):\\n        \"\"...</td>\n",
       "      <td>[def, has_task, (, self, ,, task_instance, ), ...</td>\n",
       "      <td>Checks if a task is either queued or running i...</td>\n",
       "      <td>[Checks, if, a, task, is, either, queued, or, ...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Checks if a task is either queued or running i...</td>\n",
       "      <td>def has_task(self, task_instance):\\n        if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/contrib/hooks/snowflake_hook.py</td>\n",
       "      <td>SnowflakeHook._get_aws_credentials</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        \"\"\"\\n...</td>\n",
       "      <td>python</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        \"\"\"\\n...</td>\n",
       "      <td>[def, _get_aws_credentials, (, self, ), :, if,...</td>\n",
       "      <td>returns aws_access_key_id, aws_secret_access_k...</td>\n",
       "      <td>[returns, aws_access_key_id, aws_secret_access...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>returns aws_access_key_id, aws_secret_access_k...</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        if se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/hooks/postgres_hook.py</td>\n",
       "      <td>PostgresHook.copy_expert</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "      <td>python</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "      <td>[def, copy_expert, (, self, ,, sql, ,, filenam...</td>\n",
       "      <td>Executes SQL using psycopg2 copy_expert method...</td>\n",
       "      <td>[Executes, SQL, using, psycopg2, copy_expert, ...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Executes SQL using psycopg2 copy_expert method...</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/hooks/postgres_hook.py</td>\n",
       "      <td>PostgresHook.bulk_dump</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "      <td>[def, bulk_dump, (, self, ,, table, ,, tmp_fil...</td>\n",
       "      <td>Dumps a database table into a tab-delimited file</td>\n",
       "      <td>[Dumps, a, database, table, into, a, tab, -, d...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Dumps a database table into a tab-delimited file</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/contrib/operators/file_to_gcs.py</td>\n",
       "      <td>FileToGoogleCloudStorageOperator.execute</td>\n",
       "      <td>def execute(self, context):\\n        \"\"\"\\n    ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def execute(self, context):\\n        \"\"\"\\n    ...</td>\n",
       "      <td>[def, execute, (, self, ,, context, ), :, hook...</td>\n",
       "      <td>Uploads the file to Google cloud storage</td>\n",
       "      <td>[Uploads, the, file, to, Google, cloud, storage]</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Uploads the file to Google cloud storage</td>\n",
       "      <td>def execute(self, context):\\n        hook = Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>2350</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "      <td>flow2rgb</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>python</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>[def, flow2rgb, (, flow, ,, color_wheel, =, No...</td>\n",
       "      <td>Convert flow map to RGB image.\\n\\n    Args:\\n ...</td>\n",
       "      <td>[Convert, flow, map, to, RGB, image, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Convert flow map to RGB image.</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>2351</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "      <td>make_color_wheel</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    \"\"\"Build...</td>\n",
       "      <td>python</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    \"\"\"Build...</td>\n",
       "      <td>[def, make_color_wheel, (, bins, =, None, ), :...</td>\n",
       "      <td>Build a color wheel.\\n\\n    Args:\\n        bin...</td>\n",
       "      <td>[Build, a, color, wheel, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Build a color wheel.</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    if bins ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>2352</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>examples/train_cifar10.py</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>[def, accuracy, (, output, ,, target, ,, topk,...</td>\n",
       "      <td>Computes the precision@k for the specified val...</td>\n",
       "      <td>[Computes, the, precision]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Computes the precision@k for the specified val...</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>2353</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "      <td>scatter</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>[def, scatter, (, inputs, ,, target_gpus, ,, d...</td>\n",
       "      <td>Scatter inputs to target gpus.\\n\\n    The only...</td>\n",
       "      <td>[Scatter, inputs, to, target, gpus, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Scatter inputs to target gpus. The only differ...</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>2354</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "      <td>scatter_kwargs</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>python</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>[def, scatter_kwargs, (, inputs, ,, kwargs, ,,...</td>\n",
       "      <td>Scatter with support for kwargs dictionary</td>\n",
       "      <td>[Scatter, with, support, for, kwargs, dictionary]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Scatter with support for kwargs dictionary</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>846 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             repo                                      path  \\\n",
       "25      25   apache/airflow        airflow/executors/base_executor.py   \n",
       "28      28   apache/airflow   airflow/contrib/hooks/snowflake_hook.py   \n",
       "30      30   apache/airflow            airflow/hooks/postgres_hook.py   \n",
       "31      31   apache/airflow            airflow/hooks/postgres_hook.py   \n",
       "32      32   apache/airflow  airflow/contrib/operators/file_to_gcs.py   \n",
       "...    ...              ...                                       ...   \n",
       "2350  2350  open-mmlab/mmcv             mmcv/visualization/optflow.py   \n",
       "2351  2351  open-mmlab/mmcv             mmcv/visualization/optflow.py   \n",
       "2352  2352  open-mmlab/mmcv                 examples/train_cifar10.py   \n",
       "2353  2353  open-mmlab/mmcv           mmcv/parallel/scatter_gather.py   \n",
       "2354  2354  open-mmlab/mmcv           mmcv/parallel/scatter_gather.py   \n",
       "\n",
       "                                     func_name  \\\n",
       "25                       BaseExecutor.has_task   \n",
       "28          SnowflakeHook._get_aws_credentials   \n",
       "30                    PostgresHook.copy_expert   \n",
       "31                      PostgresHook.bulk_dump   \n",
       "32    FileToGoogleCloudStorageOperator.execute   \n",
       "...                                        ...   \n",
       "2350                                  flow2rgb   \n",
       "2351                          make_color_wheel   \n",
       "2352                                  accuracy   \n",
       "2353                                   scatter   \n",
       "2354                            scatter_kwargs   \n",
       "\n",
       "                                        original_string language  \\\n",
       "25    def has_task(self, task_instance):\\n        \"\"...   python   \n",
       "28    def _get_aws_credentials(self):\\n        \"\"\"\\n...   python   \n",
       "30    def copy_expert(self, sql, filename, open=open...   python   \n",
       "31    def bulk_dump(self, table, tmp_file):\\n       ...   python   \n",
       "32    def execute(self, context):\\n        \"\"\"\\n    ...   python   \n",
       "...                                                 ...      ...   \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...   python   \n",
       "2351  def make_color_wheel(bins=None):\\n    \"\"\"Build...   python   \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...   python   \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...   python   \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...   python   \n",
       "\n",
       "                                                   code  \\\n",
       "25    def has_task(self, task_instance):\\n        \"\"...   \n",
       "28    def _get_aws_credentials(self):\\n        \"\"\"\\n...   \n",
       "30    def copy_expert(self, sql, filename, open=open...   \n",
       "31    def bulk_dump(self, table, tmp_file):\\n       ...   \n",
       "32    def execute(self, context):\\n        \"\"\"\\n    ...   \n",
       "...                                                 ...   \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...   \n",
       "2351  def make_color_wheel(bins=None):\\n    \"\"\"Build...   \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...   \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...   \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...   \n",
       "\n",
       "                                            code_tokens  \\\n",
       "25    [def, has_task, (, self, ,, task_instance, ), ...   \n",
       "28    [def, _get_aws_credentials, (, self, ), :, if,...   \n",
       "30    [def, copy_expert, (, self, ,, sql, ,, filenam...   \n",
       "31    [def, bulk_dump, (, self, ,, table, ,, tmp_fil...   \n",
       "32    [def, execute, (, self, ,, context, ), :, hook...   \n",
       "...                                                 ...   \n",
       "2350  [def, flow2rgb, (, flow, ,, color_wheel, =, No...   \n",
       "2351  [def, make_color_wheel, (, bins, =, None, ), :...   \n",
       "2352  [def, accuracy, (, output, ,, target, ,, topk,...   \n",
       "2353  [def, scatter, (, inputs, ,, target_gpus, ,, d...   \n",
       "2354  [def, scatter_kwargs, (, inputs, ,, kwargs, ,,...   \n",
       "\n",
       "                                              docstring  \\\n",
       "25    Checks if a task is either queued or running i...   \n",
       "28    returns aws_access_key_id, aws_secret_access_k...   \n",
       "30    Executes SQL using psycopg2 copy_expert method...   \n",
       "31     Dumps a database table into a tab-delimited file   \n",
       "32             Uploads the file to Google cloud storage   \n",
       "...                                                 ...   \n",
       "2350  Convert flow map to RGB image.\\n\\n    Args:\\n ...   \n",
       "2351  Build a color wheel.\\n\\n    Args:\\n        bin...   \n",
       "2352  Computes the precision@k for the specified val...   \n",
       "2353  Scatter inputs to target gpus.\\n\\n    The only...   \n",
       "2354         Scatter with support for kwargs dictionary   \n",
       "\n",
       "                                       docstring_tokens  \\\n",
       "25    [Checks, if, a, task, is, either, queued, or, ...   \n",
       "28    [returns, aws_access_key_id, aws_secret_access...   \n",
       "30    [Executes, SQL, using, psycopg2, copy_expert, ...   \n",
       "31    [Dumps, a, database, table, into, a, tab, -, d...   \n",
       "32     [Uploads, the, file, to, Google, cloud, storage]   \n",
       "...                                                 ...   \n",
       "2350            [Convert, flow, map, to, RGB, image, .]   \n",
       "2351                        [Build, a, color, wheel, .]   \n",
       "2352                         [Computes, the, precision]   \n",
       "2353             [Scatter, inputs, to, target, gpus, .]   \n",
       "2354  [Scatter, with, support, for, kwargs, dictionary]   \n",
       "\n",
       "                                           sha  \\\n",
       "25    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "28    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "30    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "31    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "32    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "...                                        ...   \n",
       "2350  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2351  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2352  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2353  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2354  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "\n",
       "                                                    url  \\\n",
       "25    https://github.com/apache/airflow/blob/b69c686...   \n",
       "28    https://github.com/apache/airflow/blob/b69c686...   \n",
       "30    https://github.com/apache/airflow/blob/b69c686...   \n",
       "31    https://github.com/apache/airflow/blob/b69c686...   \n",
       "32    https://github.com/apache/airflow/blob/b69c686...   \n",
       "...                                                 ...   \n",
       "2350  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2351  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2352  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2353  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2354  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "\n",
       "                                         method_summary  \\\n",
       "25    Checks if a task is either queued or running i...   \n",
       "28    returns aws_access_key_id, aws_secret_access_k...   \n",
       "30    Executes SQL using psycopg2 copy_expert method...   \n",
       "31     Dumps a database table into a tab-delimited file   \n",
       "32             Uploads the file to Google cloud storage   \n",
       "...                                                 ...   \n",
       "2350                     Convert flow map to RGB image.   \n",
       "2351                               Build a color wheel.   \n",
       "2352  Computes the precision@k for the specified val...   \n",
       "2353  Scatter inputs to target gpus. The only differ...   \n",
       "2354         Scatter with support for kwargs dictionary   \n",
       "\n",
       "                                            method_code  \n",
       "25    def has_task(self, task_instance):\\n        if...  \n",
       "28    def _get_aws_credentials(self):\\n        if se...  \n",
       "30    def copy_expert(self, sql, filename, open=open...  \n",
       "31    def bulk_dump(self, table, tmp_file):\\n       ...  \n",
       "32    def execute(self, context):\\n        hook = Go...  \n",
       "...                                                 ...  \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...  \n",
       "2351  def make_color_wheel(bins=None):\\n    if bins ...  \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...  \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...  \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...  \n",
       "\n",
       "[846 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove docstrings from code\n",
    "def remove_docstrings(code):\n",
    "    docstrings = re.findall(r\"'''(.*?)'''\", code, re.DOTALL) + re.findall(r'\"\"\"(.*?)\"\"\"', code, re.DOTALL)\n",
    "    for docstring in docstrings:\n",
    "        code = code.replace(docstring, '')\n",
    "    code = re.sub(r'\"\"\"\"\"\"\\s*', '', code, re.DOTALL)\n",
    "    code = re.sub(r\"''''''\\s*\", '', code, re.DOTALL)\n",
    "    return code\n",
    "\n",
    "selected_df['method_code'] = selected_df['code'].apply(remove_docstrings)\n",
    "selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>repo</th>\n",
       "      <th>path</th>\n",
       "      <th>func_name</th>\n",
       "      <th>original_string</th>\n",
       "      <th>language</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>sha</th>\n",
       "      <th>url</th>\n",
       "      <th>method_summary</th>\n",
       "      <th>method_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/executors/base_executor.py</td>\n",
       "      <td>BaseExecutor.has_task</td>\n",
       "      <td>def has_task(self, task_instance):\\n        \"\"...</td>\n",
       "      <td>python</td>\n",
       "      <td>def has_task(self, task_instance):\\n        \"\"...</td>\n",
       "      <td>[def, has_task, (, self, ,, task_instance, ), ...</td>\n",
       "      <td>Checks if a task is either queued or running i...</td>\n",
       "      <td>[Checks, if, a, task, is, either, queued, or, ...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Checks if a task is either queued or running i...</td>\n",
       "      <td>def has_task(self, task_instance):\\n        if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/contrib/hooks/snowflake_hook.py</td>\n",
       "      <td>SnowflakeHook._get_aws_credentials</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        \"\"\"\\n...</td>\n",
       "      <td>python</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        \"\"\"\\n...</td>\n",
       "      <td>[def, _get_aws_credentials, (, self, ), :, if,...</td>\n",
       "      <td>returns aws_access_key_id, aws_secret_access_k...</td>\n",
       "      <td>[returns, aws_access_key_id, aws_secret_access...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>returns aws_access_key_id, aws_secret_access_k...</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        if se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/hooks/postgres_hook.py</td>\n",
       "      <td>PostgresHook.copy_expert</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "      <td>python</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "      <td>[def, copy_expert, (, self, ,, sql, ,, filenam...</td>\n",
       "      <td>Executes SQL using psycopg2 copy_expert method...</td>\n",
       "      <td>[Executes, SQL, using, psycopg2, copy_expert, ...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Executes SQL using psycopg2 copy_expert method...</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/hooks/postgres_hook.py</td>\n",
       "      <td>PostgresHook.bulk_dump</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "      <td>[def, bulk_dump, (, self, ,, table, ,, tmp_fil...</td>\n",
       "      <td>Dumps a database table into a tab-delimited file</td>\n",
       "      <td>[Dumps, a, database, table, into, a, tab, -, d...</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Dumps a database table into a tab-delimited file</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>airflow/contrib/operators/file_to_gcs.py</td>\n",
       "      <td>FileToGoogleCloudStorageOperator.execute</td>\n",
       "      <td>def execute(self, context):\\n        \"\"\"\\n    ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def execute(self, context):\\n        \"\"\"\\n    ...</td>\n",
       "      <td>[def, execute, (, self, ,, context, ), :, hook...</td>\n",
       "      <td>Uploads the file to Google cloud storage</td>\n",
       "      <td>[Uploads, the, file, to, Google, cloud, storage]</td>\n",
       "      <td>b69c686ad8a0c89b9136bb4b31767257eb7b2597</td>\n",
       "      <td>https://github.com/apache/airflow/blob/b69c686...</td>\n",
       "      <td>Uploads the file to Google cloud storage</td>\n",
       "      <td>def execute(self, context):\\n        hook = Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>2350</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "      <td>flow2rgb</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>python</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>[def, flow2rgb, (, flow, ,, color_wheel, =, No...</td>\n",
       "      <td>Convert flow map to RGB image.\\n\\n    Args:\\n ...</td>\n",
       "      <td>[Convert, flow, map, to, RGB, image, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Convert flow map to RGB image.</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>2351</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "      <td>make_color_wheel</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    \"\"\"Build...</td>\n",
       "      <td>python</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    \"\"\"Build...</td>\n",
       "      <td>[def, make_color_wheel, (, bins, =, None, ), :...</td>\n",
       "      <td>Build a color wheel.\\n\\n    Args:\\n        bin...</td>\n",
       "      <td>[Build, a, color, wheel, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Build a color wheel.</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    if bins ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>2352</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>examples/train_cifar10.py</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>[def, accuracy, (, output, ,, target, ,, topk,...</td>\n",
       "      <td>Computes the precision@k for the specified val...</td>\n",
       "      <td>[Computes, the, precision]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Computes the precision@k for the specified val...</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>2353</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "      <td>scatter</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>[def, scatter, (, inputs, ,, target_gpus, ,, d...</td>\n",
       "      <td>Scatter inputs to target gpus.\\n\\n    The only...</td>\n",
       "      <td>[Scatter, inputs, to, target, gpus, .]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Scatter inputs to target gpus. The only differ...</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>2354</td>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "      <td>scatter_kwargs</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>python</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>[def, scatter_kwargs, (, inputs, ,, kwargs, ,,...</td>\n",
       "      <td>Scatter with support for kwargs dictionary</td>\n",
       "      <td>[Scatter, with, support, for, kwargs, dictionary]</td>\n",
       "      <td>0d77f61450aab4dde8b8585a577cc496acb95d7f</td>\n",
       "      <td>https://github.com/open-mmlab/mmcv/blob/0d77f6...</td>\n",
       "      <td>Scatter with support for kwargs dictionary</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>846 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             repo                                      path  \\\n",
       "25      25   apache/airflow        airflow/executors/base_executor.py   \n",
       "28      28   apache/airflow   airflow/contrib/hooks/snowflake_hook.py   \n",
       "30      30   apache/airflow            airflow/hooks/postgres_hook.py   \n",
       "31      31   apache/airflow            airflow/hooks/postgres_hook.py   \n",
       "32      32   apache/airflow  airflow/contrib/operators/file_to_gcs.py   \n",
       "...    ...              ...                                       ...   \n",
       "2350  2350  open-mmlab/mmcv             mmcv/visualization/optflow.py   \n",
       "2351  2351  open-mmlab/mmcv             mmcv/visualization/optflow.py   \n",
       "2352  2352  open-mmlab/mmcv                 examples/train_cifar10.py   \n",
       "2353  2353  open-mmlab/mmcv           mmcv/parallel/scatter_gather.py   \n",
       "2354  2354  open-mmlab/mmcv           mmcv/parallel/scatter_gather.py   \n",
       "\n",
       "                                     func_name  \\\n",
       "25                       BaseExecutor.has_task   \n",
       "28          SnowflakeHook._get_aws_credentials   \n",
       "30                    PostgresHook.copy_expert   \n",
       "31                      PostgresHook.bulk_dump   \n",
       "32    FileToGoogleCloudStorageOperator.execute   \n",
       "...                                        ...   \n",
       "2350                                  flow2rgb   \n",
       "2351                          make_color_wheel   \n",
       "2352                                  accuracy   \n",
       "2353                                   scatter   \n",
       "2354                            scatter_kwargs   \n",
       "\n",
       "                                        original_string language  \\\n",
       "25    def has_task(self, task_instance):\\n        \"\"...   python   \n",
       "28    def _get_aws_credentials(self):\\n        \"\"\"\\n...   python   \n",
       "30    def copy_expert(self, sql, filename, open=open...   python   \n",
       "31    def bulk_dump(self, table, tmp_file):\\n       ...   python   \n",
       "32    def execute(self, context):\\n        \"\"\"\\n    ...   python   \n",
       "...                                                 ...      ...   \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...   python   \n",
       "2351  def make_color_wheel(bins=None):\\n    \"\"\"Build...   python   \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...   python   \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...   python   \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...   python   \n",
       "\n",
       "                                                   code  \\\n",
       "25    def has_task(self, task_instance):\\n        \"\"...   \n",
       "28    def _get_aws_credentials(self):\\n        \"\"\"\\n...   \n",
       "30    def copy_expert(self, sql, filename, open=open...   \n",
       "31    def bulk_dump(self, table, tmp_file):\\n       ...   \n",
       "32    def execute(self, context):\\n        \"\"\"\\n    ...   \n",
       "...                                                 ...   \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...   \n",
       "2351  def make_color_wheel(bins=None):\\n    \"\"\"Build...   \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...   \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...   \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...   \n",
       "\n",
       "                                            code_tokens  \\\n",
       "25    [def, has_task, (, self, ,, task_instance, ), ...   \n",
       "28    [def, _get_aws_credentials, (, self, ), :, if,...   \n",
       "30    [def, copy_expert, (, self, ,, sql, ,, filenam...   \n",
       "31    [def, bulk_dump, (, self, ,, table, ,, tmp_fil...   \n",
       "32    [def, execute, (, self, ,, context, ), :, hook...   \n",
       "...                                                 ...   \n",
       "2350  [def, flow2rgb, (, flow, ,, color_wheel, =, No...   \n",
       "2351  [def, make_color_wheel, (, bins, =, None, ), :...   \n",
       "2352  [def, accuracy, (, output, ,, target, ,, topk,...   \n",
       "2353  [def, scatter, (, inputs, ,, target_gpus, ,, d...   \n",
       "2354  [def, scatter_kwargs, (, inputs, ,, kwargs, ,,...   \n",
       "\n",
       "                                              docstring  \\\n",
       "25    Checks if a task is either queued or running i...   \n",
       "28    returns aws_access_key_id, aws_secret_access_k...   \n",
       "30    Executes SQL using psycopg2 copy_expert method...   \n",
       "31     Dumps a database table into a tab-delimited file   \n",
       "32             Uploads the file to Google cloud storage   \n",
       "...                                                 ...   \n",
       "2350  Convert flow map to RGB image.\\n\\n    Args:\\n ...   \n",
       "2351  Build a color wheel.\\n\\n    Args:\\n        bin...   \n",
       "2352  Computes the precision@k for the specified val...   \n",
       "2353  Scatter inputs to target gpus.\\n\\n    The only...   \n",
       "2354         Scatter with support for kwargs dictionary   \n",
       "\n",
       "                                       docstring_tokens  \\\n",
       "25    [Checks, if, a, task, is, either, queued, or, ...   \n",
       "28    [returns, aws_access_key_id, aws_secret_access...   \n",
       "30    [Executes, SQL, using, psycopg2, copy_expert, ...   \n",
       "31    [Dumps, a, database, table, into, a, tab, -, d...   \n",
       "32     [Uploads, the, file, to, Google, cloud, storage]   \n",
       "...                                                 ...   \n",
       "2350            [Convert, flow, map, to, RGB, image, .]   \n",
       "2351                        [Build, a, color, wheel, .]   \n",
       "2352                         [Computes, the, precision]   \n",
       "2353             [Scatter, inputs, to, target, gpus, .]   \n",
       "2354  [Scatter, with, support, for, kwargs, dictionary]   \n",
       "\n",
       "                                           sha  \\\n",
       "25    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "28    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "30    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "31    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "32    b69c686ad8a0c89b9136bb4b31767257eb7b2597   \n",
       "...                                        ...   \n",
       "2350  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2351  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2352  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2353  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "2354  0d77f61450aab4dde8b8585a577cc496acb95d7f   \n",
       "\n",
       "                                                    url  \\\n",
       "25    https://github.com/apache/airflow/blob/b69c686...   \n",
       "28    https://github.com/apache/airflow/blob/b69c686...   \n",
       "30    https://github.com/apache/airflow/blob/b69c686...   \n",
       "31    https://github.com/apache/airflow/blob/b69c686...   \n",
       "32    https://github.com/apache/airflow/blob/b69c686...   \n",
       "...                                                 ...   \n",
       "2350  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2351  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2352  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2353  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "2354  https://github.com/open-mmlab/mmcv/blob/0d77f6...   \n",
       "\n",
       "                                         method_summary  \\\n",
       "25    Checks if a task is either queued or running i...   \n",
       "28    returns aws_access_key_id, aws_secret_access_k...   \n",
       "30    Executes SQL using psycopg2 copy_expert method...   \n",
       "31     Dumps a database table into a tab-delimited file   \n",
       "32             Uploads the file to Google cloud storage   \n",
       "...                                                 ...   \n",
       "2350                     Convert flow map to RGB image.   \n",
       "2351                               Build a color wheel.   \n",
       "2352  Computes the precision@k for the specified val...   \n",
       "2353  Scatter inputs to target gpus. The only differ...   \n",
       "2354         Scatter with support for kwargs dictionary   \n",
       "\n",
       "                                            method_code  \n",
       "25    def has_task(self, task_instance):\\n        if...  \n",
       "28    def _get_aws_credentials(self):\\n        if se...  \n",
       "30    def copy_expert(self, sql, filename, open=open...  \n",
       "31    def bulk_dump(self, table, tmp_file):\\n       ...  \n",
       "32    def execute(self, context):\\n        hook = Go...  \n",
       "...                                                 ...  \n",
       "2350  def flow2rgb(flow, color_wheel=None, unknown_t...  \n",
       "2351  def make_color_wheel(bins=None):\\n    if bins ...  \n",
       "2352  def accuracy(output, target, topk=(1, )):\\n   ...  \n",
       "2353  def scatter(inputs, target_gpus, dim=0):\\n    ...  \n",
       "2354  def scatter_kwargs(inputs, kwargs, target_gpus...  \n",
       "\n",
       "[846 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove comments from code\n",
    "def remove_comments(code):\n",
    "    pattern = r\"(\\\".*?\\\"|\\'.*?\\')|(#[^\\r\\n]*$)\"\n",
    "    # first group captures quoted strings (double or single)\n",
    "    # second group captures comments (starting with '#')\n",
    "    regex = re.compile(pattern, re.MULTILINE|re.DOTALL)\n",
    "    def _replacer(match):\n",
    "        # if the 2nd group (capturing comments) is not None,\n",
    "        # it means we have captured a non-quoted (real) comment string.\n",
    "        if match.group(2) is not None:\n",
    "            return \"\" # so we will return empty to remove the comment\n",
    "        else: # otherwise, we will return the 1st group\n",
    "            return match.group(1) # captured quoted-string\n",
    "    return regex.sub(_replacer, code)\n",
    "\n",
    "selected_df['method_code'] = selected_df['method_code'].apply(remove_comments)\n",
    "selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>method_name</th>\n",
       "      <th>method_code</th>\n",
       "      <th>method_summary</th>\n",
       "      <th>original_method_code</th>\n",
       "      <th>method_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>BaseExecutor.has_task</td>\n",
       "      <td>def has_task(self, task_instance):\\n        if...</td>\n",
       "      <td>Checks if a task is either queued or running i...</td>\n",
       "      <td>def has_task(self, task_instance):\\n        \"\"...</td>\n",
       "      <td>airflow/executors/base_executor.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>SnowflakeHook._get_aws_credentials</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        if se...</td>\n",
       "      <td>returns aws_access_key_id, aws_secret_access_k...</td>\n",
       "      <td>def _get_aws_credentials(self):\\n        \"\"\"\\n...</td>\n",
       "      <td>airflow/contrib/hooks/snowflake_hook.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>PostgresHook.copy_expert</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "      <td>Executes SQL using psycopg2 copy_expert method...</td>\n",
       "      <td>def copy_expert(self, sql, filename, open=open...</td>\n",
       "      <td>airflow/hooks/postgres_hook.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>PostgresHook.bulk_dump</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "      <td>Dumps a database table into a tab-delimited file</td>\n",
       "      <td>def bulk_dump(self, table, tmp_file):\\n       ...</td>\n",
       "      <td>airflow/hooks/postgres_hook.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apache/airflow</td>\n",
       "      <td>FileToGoogleCloudStorageOperator.execute</td>\n",
       "      <td>def execute(self, context):\\n        hook = Go...</td>\n",
       "      <td>Uploads the file to Google cloud storage</td>\n",
       "      <td>def execute(self, context):\\n        \"\"\"\\n    ...</td>\n",
       "      <td>airflow/contrib/operators/file_to_gcs.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>flow2rgb</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>Convert flow map to RGB image.</td>\n",
       "      <td>def flow2rgb(flow, color_wheel=None, unknown_t...</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>make_color_wheel</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    if bins ...</td>\n",
       "      <td>Build a color wheel.</td>\n",
       "      <td>def make_color_wheel(bins=None):\\n    \"\"\"Build...</td>\n",
       "      <td>mmcv/visualization/optflow.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>Computes the precision@k for the specified val...</td>\n",
       "      <td>def accuracy(output, target, topk=(1, )):\\n   ...</td>\n",
       "      <td>examples/train_cifar10.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>scatter</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>Scatter inputs to target gpus. The only differ...</td>\n",
       "      <td>def scatter(inputs, target_gpus, dim=0):\\n    ...</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>open-mmlab/mmcv</td>\n",
       "      <td>scatter_kwargs</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>Scatter with support for kwargs dictionary</td>\n",
       "      <td>def scatter_kwargs(inputs, kwargs, target_gpus...</td>\n",
       "      <td>mmcv/parallel/scatter_gather.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>846 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           repo_name                               method_name  \\\n",
       "0     apache/airflow                     BaseExecutor.has_task   \n",
       "1     apache/airflow        SnowflakeHook._get_aws_credentials   \n",
       "2     apache/airflow                  PostgresHook.copy_expert   \n",
       "3     apache/airflow                    PostgresHook.bulk_dump   \n",
       "4     apache/airflow  FileToGoogleCloudStorageOperator.execute   \n",
       "..               ...                                       ...   \n",
       "841  open-mmlab/mmcv                                  flow2rgb   \n",
       "842  open-mmlab/mmcv                          make_color_wheel   \n",
       "843  open-mmlab/mmcv                                  accuracy   \n",
       "844  open-mmlab/mmcv                                   scatter   \n",
       "845  open-mmlab/mmcv                            scatter_kwargs   \n",
       "\n",
       "                                           method_code  \\\n",
       "0    def has_task(self, task_instance):\\n        if...   \n",
       "1    def _get_aws_credentials(self):\\n        if se...   \n",
       "2    def copy_expert(self, sql, filename, open=open...   \n",
       "3    def bulk_dump(self, table, tmp_file):\\n       ...   \n",
       "4    def execute(self, context):\\n        hook = Go...   \n",
       "..                                                 ...   \n",
       "841  def flow2rgb(flow, color_wheel=None, unknown_t...   \n",
       "842  def make_color_wheel(bins=None):\\n    if bins ...   \n",
       "843  def accuracy(output, target, topk=(1, )):\\n   ...   \n",
       "844  def scatter(inputs, target_gpus, dim=0):\\n    ...   \n",
       "845  def scatter_kwargs(inputs, kwargs, target_gpus...   \n",
       "\n",
       "                                        method_summary  \\\n",
       "0    Checks if a task is either queued or running i...   \n",
       "1    returns aws_access_key_id, aws_secret_access_k...   \n",
       "2    Executes SQL using psycopg2 copy_expert method...   \n",
       "3     Dumps a database table into a tab-delimited file   \n",
       "4             Uploads the file to Google cloud storage   \n",
       "..                                                 ...   \n",
       "841                     Convert flow map to RGB image.   \n",
       "842                               Build a color wheel.   \n",
       "843  Computes the precision@k for the specified val...   \n",
       "844  Scatter inputs to target gpus. The only differ...   \n",
       "845         Scatter with support for kwargs dictionary   \n",
       "\n",
       "                                  original_method_code  \\\n",
       "0    def has_task(self, task_instance):\\n        \"\"...   \n",
       "1    def _get_aws_credentials(self):\\n        \"\"\"\\n...   \n",
       "2    def copy_expert(self, sql, filename, open=open...   \n",
       "3    def bulk_dump(self, table, tmp_file):\\n       ...   \n",
       "4    def execute(self, context):\\n        \"\"\"\\n    ...   \n",
       "..                                                 ...   \n",
       "841  def flow2rgb(flow, color_wheel=None, unknown_t...   \n",
       "842  def make_color_wheel(bins=None):\\n    \"\"\"Build...   \n",
       "843  def accuracy(output, target, topk=(1, )):\\n   ...   \n",
       "844  def scatter(inputs, target_gpus, dim=0):\\n    ...   \n",
       "845  def scatter_kwargs(inputs, kwargs, target_gpus...   \n",
       "\n",
       "                                  method_path  \n",
       "0          airflow/executors/base_executor.py  \n",
       "1     airflow/contrib/hooks/snowflake_hook.py  \n",
       "2              airflow/hooks/postgres_hook.py  \n",
       "3              airflow/hooks/postgres_hook.py  \n",
       "4    airflow/contrib/operators/file_to_gcs.py  \n",
       "..                                        ...  \n",
       "841             mmcv/visualization/optflow.py  \n",
       "842             mmcv/visualization/optflow.py  \n",
       "843                 examples/train_cifar10.py  \n",
       "844           mmcv/parallel/scatter_gather.py  \n",
       "845           mmcv/parallel/scatter_gather.py  \n",
       "\n",
       "[846 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns and rename\n",
    "final_df = selected_df[['repo', 'func_name', 'method_code', 'method_summary', 'original_string', 'path']]\n",
    "final_df = final_df.rename(columns={'repo': 'repo_name', 'func_name': 'method_name', 'original_string': 'original_method_code', 'path': 'method_path'}).reset_index(drop=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 806)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 10 for each repository and remove the from final_df\n",
    "fewshot_df = final_df.groupby('repo_name').head(10).reset_index(drop=True)\n",
    "\n",
    "eval_df = final_df[~final_df.index.isin(fewshot_df.index)]\n",
    "len(fewshot_df), len(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40 entries, 0 to 39\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   repo_name             40 non-null     object\n",
      " 1   method_name           40 non-null     object\n",
      " 2   method_code           40 non-null     object\n",
      " 3   method_summary        40 non-null     object\n",
      " 4   original_method_code  40 non-null     object\n",
      " 5   method_path           40 non-null     object\n",
      "dtypes: object(6)\n",
      "memory usage: 2.0+ KB\n"
     ]
    }
   ],
   "source": [
    "fewshot_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 806 entries, 40 to 845\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   repo_name             806 non-null    object\n",
      " 1   method_name           806 non-null    object\n",
      " 2   method_code           806 non-null    object\n",
      " 3   method_summary        806 non-null    object\n",
      " 4   original_method_code  806 non-null    object\n",
      " 5   method_path           806 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 44.1+ KB\n"
     ]
    }
   ],
   "source": [
    "eval_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to jsonl\n",
    "DATA_DIR = Path('../../data/preprocessed')\n",
    "\n",
    "# final_df.to_json(DATA_DIR / 'method-level-mcsn-full.jsonl', orient='records', lines=True)\n",
    "eval_df.to_json(DATA_DIR / 'method-level-mcsn.jsonl', orient='records', lines=True)\n",
    "fewshot_df.to_json(DATA_DIR / 'method-level-mcsn-few-shot.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdf = final_df.copy()\n",
    "tdf['method_summary_length'] = tdf['method_summary'].str.len()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=tdf, x='method_summary_length', bins=100, kde=True, color='green')\n",
    "plt.xlabel('Length')\n",
    "plt.xticks(np.arange(0, 210, 10))\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vladimir-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
