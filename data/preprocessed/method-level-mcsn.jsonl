{"repo_name":"apache\/airflow","method_name":"HttpHook.run","method_code":"def run(self, endpoint, data=None, headers=None, extra_options=None):\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('\/') and \\\n           endpoint and not endpoint.startswith('\/'):\n            url = self.base_url + '\/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            \n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            \n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            \n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)","method_summary":"Performs the request","original_method_code":"def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource\/v1\/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('\/') and \\\n           endpoint and not endpoint.startswith('\/'):\n            url = self.base_url + '\/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)","method_path":"airflow\/hooks\/http_hook.py"}
{"repo_name":"apache\/airflow","method_name":"HttpHook.check_response","method_code":"def check_response(self, response):\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in ['GET', 'HEAD']:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)","method_summary":"Checks the status code and raise an AirflowException exception on non 2XX or 3XX status codes","original_method_code":"def check_response(self, response):\n        \"\"\"\n        Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in ['GET', 'HEAD']:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)","method_path":"airflow\/hooks\/http_hook.py"}
{"repo_name":"apache\/airflow","method_name":"HttpHook.run_and_check","method_code":"def run_and_check(self, session, prepped_request, extra_options):\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\n            raise ex","method_summary":"Grabs extra options like timeout and actually runs the request, checking for the result","original_method_code":"def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\n            raise ex","method_path":"airflow\/hooks\/http_hook.py"}
{"repo_name":"apache\/airflow","method_name":"create_session","method_code":"def create_session():\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()","method_summary":"Contextmanager that will create and teardown a session.","original_method_code":"def create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()","method_path":"airflow\/utils\/db.py"}
{"repo_name":"apache\/airflow","method_name":"resetdb","method_code":"def resetdb():\n    from airflow import models\n\n    \n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()","method_summary":"Clear out the database","original_method_code":"def resetdb():\n    \"\"\"\n    Clear out the database\n    \"\"\"\n    from airflow import models\n\n    # alembic adds significant import time, so we import it lazily\n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()","method_path":"airflow\/utils\/db.py"}
{"repo_name":"apache\/airflow","method_name":"PrestoHook._get_pretty_exception_message","method_code":"def _get_pretty_exception_message(e):\n        if (hasattr(e, 'message') and\n            'errorName' in e.message and\n                'message' in e.message):\n            return ('{name}: {message}'.format(\n                    name=e.message['errorName'],\n                    message=e.message['message']))\n        else:\n            return str(e)","method_summary":"Parses some DatabaseError to provide a better error message","original_method_code":"def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (hasattr(e, 'message') and\n            'errorName' in e.message and\n                'message' in e.message):\n            return ('{name}: {message}'.format(\n                    name=e.message['errorName'],\n                    message=e.message['message']))\n        else:\n            return str(e)","method_path":"airflow\/hooks\/presto_hook.py"}
{"repo_name":"apache\/airflow","method_name":"PrestoHook.get_records","method_code":"def get_records(self, hql, parameters=None):\n        try:\n            return super().get_records(\n                self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))","method_summary":"Get a set of records from Presto","original_method_code":"def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(\n                self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))","method_path":"airflow\/hooks\/presto_hook.py"}
{"repo_name":"apache\/airflow","method_name":"PrestoHook.get_pandas_df","method_code":"def get_pandas_df(self, hql, parameters=None):\n        import pandas\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df","method_summary":"Get a pandas dataframe from a sql query.","original_method_code":"def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.\n        \"\"\"\n        import pandas\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df","method_path":"airflow\/hooks\/presto_hook.py"}
{"repo_name":"apache\/airflow","method_name":"PrestoHook.run","method_code":"def run(self, hql, parameters=None):\n        return super().run(self._strip_sql(hql), parameters)","method_summary":"Execute the statement against Presto. Can be used to create views.","original_method_code":"def run(self, hql, parameters=None):\n        \"\"\"\n        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)","method_path":"airflow\/hooks\/presto_hook.py"}
{"repo_name":"apache\/airflow","method_name":"PrestoHook.insert_rows","method_code":"def insert_rows(self, table, rows, target_fields=None):\n        super().insert_rows(table, rows, target_fields, 0)","method_summary":"A generic way to insert a set of tuples into a table.","original_method_code":"def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)","method_path":"airflow\/hooks\/presto_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureCosmosDBHook.get_conn","method_code":"def get_conn(self):\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        \n        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})\n\n        return self.cosmos_client","method_summary":"Return a cosmos db client.","original_method_code":"def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})\n\n        return self.cosmos_client","method_path":"airflow\/contrib\/hooks\/azure_cosmos_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureCosmosDBHook.does_collection_exist","method_code":"def does_collection_exist(self, collection_name, database_name=None):\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n        if len(existing_container) == 0:\n            return False\n\n        return True","method_summary":"Checks if a collection exists in CosmosDB.","original_method_code":"def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n        if len(existing_container) == 0:\n            return False\n\n        return True","method_path":"airflow\/contrib\/hooks\/azure_cosmos_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureCosmosDBHook.create_collection","method_code":"def create_collection(self, collection_name, database_name=None):\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        \n        \n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n\n        \n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name})","method_summary":"Creates a new collection in the CosmosDB database.","original_method_code":"def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name})","method_path":"airflow\/contrib\/hooks\/azure_cosmos_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureCosmosDBHook.does_database_exist","method_code":"def does_database_exist(self, database_name):\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n        if len(existing_database) == 0:\n            return False\n\n        return True","method_summary":"Checks if a database exists in CosmosDB.","original_method_code":"def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n        if len(existing_database) == 0:\n            return False\n\n        return True","method_path":"airflow\/contrib\/hooks\/azure_cosmos_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureCosmosDBHook.create_database","method_code":"def create_database(self, database_name):\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        \n        \n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n\n        \n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})","method_summary":"Creates a new database in CosmosDB.","original_method_code":"def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})","method_path":"airflow\/contrib\/hooks\/azure_cosmos_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureCosmosDBHook.delete_database","method_code":"def delete_database(self, database_name):\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))","method_summary":"Deletes an existing database in CosmosDB.","original_method_code":"def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))","method_path":"airflow\/contrib\/hooks\/azure_cosmos_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureCosmosDBHook.delete_collection","method_code":"def delete_collection(self, collection_name, database_name=None):\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(self.__get_database_name(database_name), collection_name))","method_summary":"Deletes an existing collection in the CosmosDB database.","original_method_code":"def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(self.__get_database_name(database_name), collection_name))","method_path":"airflow\/contrib\/hooks\/azure_cosmos_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureCosmosDBHook.insert_documents","method_code":"def insert_documents(self, documents, database_name=None, collection_name=None):\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name)),\n                    single_document))\n\n        return created_documents","method_summary":"Insert a list of new documents into an existing collection in the CosmosDB database.","original_method_code":"def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name)),\n                    single_document))\n\n        return created_documents","method_path":"airflow\/contrib\/hooks\/azure_cosmos_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureCosmosDBHook.delete_document","method_code":"def delete_document(self, document_id, database_name=None, collection_name=None):\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id))","method_summary":"Delete an existing document out of a collection in the CosmosDB database.","original_method_code":"def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id))","method_path":"airflow\/contrib\/hooks\/azure_cosmos_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureCosmosDBHook.get_document","method_code":"def get_document(self, document_id, database_name=None, collection_name=None):\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id))\n        except HTTPFailure:\n            return None","method_summary":"Get a document from an existing collection in the CosmosDB database.","original_method_code":"def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id))\n        except HTTPFailure:\n            return None","method_path":"airflow\/contrib\/hooks\/azure_cosmos_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureCosmosDBHook.get_documents","method_code":"def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        \n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None","method_summary":"Get a list of documents from an existing collection in the CosmosDB database via SQL query.","original_method_code":"def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None","method_path":"airflow\/contrib\/hooks\/azure_cosmos_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GcfHook.create_new_function","method_code":"def create_new_function(self, location, body, project_id=None):\n        response = self.get_conn().projects().locations().functions().create(\n            location=self._full_location(project_id, location),\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)","method_summary":"Creates a new function in Cloud Function in the location specified in the body.","original_method_code":"def create_new_function(self, location, body, project_id=None):\n        \"\"\"\n        Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().create(\n            location=self._full_location(project_id, location),\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)","method_path":"airflow\/contrib\/hooks\/gcp_function_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GcfHook.update_function","method_code":"def update_function(self, name, body, update_mask):\n        response = self.get_conn().projects().locations().functions().patch(\n            updateMask=\",\".join(update_mask),\n            name=name,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)","method_summary":"Updates Cloud Functions according to the specified update mask.","original_method_code":"def update_function(self, name, body, update_mask):\n        \"\"\"\n        Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().patch(\n            updateMask=\",\".join(update_mask),\n            name=name,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)","method_path":"airflow\/contrib\/hooks\/gcp_function_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GcfHook.upload_function_zip","method_code":"def upload_function_zip(self, location, zip_path, project_id=None):\n        response = self.get_conn().projects().locations().functions().generateUploadUrl(\n            parent=self._full_location(project_id, location)\n        ).execute(num_retries=self.num_retries)\n        upload_url = response.get('uploadUrl')\n        with open(zip_path, 'rb') as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                \n                \n                \n                headers={\n                    'Content-type': 'application\/zip',\n                    'x-goog-content-length-range': '0,104857600',\n                }\n            )\n        return upload_url","method_summary":"Uploads zip file with sources.","original_method_code":"def upload_function_zip(self, location, zip_path, project_id=None):\n        \"\"\"\n        Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().generateUploadUrl(\n            parent=self._full_location(project_id, location)\n        ).execute(num_retries=self.num_retries)\n        upload_url = response.get('uploadUrl')\n        with open(zip_path, 'rb') as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                # Those two headers needs to be specified according to:\n                # https:\/\/cloud.google.com\/functions\/docs\/reference\/rest\/v1\/projects.locations.functions\/generateUploadUrl\n                # nopep8\n                headers={\n                    'Content-type': 'application\/zip',\n                    'x-goog-content-length-range': '0,104857600',\n                }\n            )\n        return upload_url","method_path":"airflow\/contrib\/hooks\/gcp_function_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GcfHook.delete_function","method_code":"def delete_function(self, name):\n        response = self.get_conn().projects().locations().functions().delete(\n            name=name).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)","method_summary":"Deletes the specified Cloud Function.","original_method_code":"def delete_function(self, name):\n        \"\"\"\n        Deletes the specified Cloud Function.\n\n        :param name: The name of the function.\n        :type name: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().delete(\n            name=name).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)","method_path":"airflow\/contrib\/hooks\/gcp_function_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BaseTIDep.get_dep_statuses","method_code":"def get_dep_statuses(self, ti, session, dep_context=None):\n        \n        from airflow.ti_deps.dep_context import DepContext\n\n        if dep_context is None:\n            dep_context = DepContext()\n\n        if self.IGNOREABLE and dep_context.ignore_all_deps:\n            yield self._passing_status(\n                reason=\"Context specified all dependencies should be ignored.\")\n            return\n\n        if self.IS_TASK_DEP and dep_context.ignore_task_deps:\n            yield self._passing_status(\n                reason=\"Context specified all task dependencies should be ignored.\")\n            return\n\n        for dep_status in self._get_dep_statuses(ti, session, dep_context):\n            yield dep_status","method_summary":"Wrapper around the private _get_dep_statuses method that contains some global checks for all dependencies.","original_method_code":"def get_dep_statuses(self, ti, session, dep_context=None):\n        \"\"\"\n        Wrapper around the private _get_dep_statuses method that contains some global\n        checks for all dependencies.\n\n        :param ti: the task instance to get the dependency status for\n        :type ti: airflow.models.TaskInstance\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param dep_context: the context for which this dependency should be evaluated for\n        :type dep_context: DepContext\n        \"\"\"\n        # this avoids a circular dependency\n        from airflow.ti_deps.dep_context import DepContext\n\n        if dep_context is None:\n            dep_context = DepContext()\n\n        if self.IGNOREABLE and dep_context.ignore_all_deps:\n            yield self._passing_status(\n                reason=\"Context specified all dependencies should be ignored.\")\n            return\n\n        if self.IS_TASK_DEP and dep_context.ignore_task_deps:\n            yield self._passing_status(\n                reason=\"Context specified all task dependencies should be ignored.\")\n            return\n\n        for dep_status in self._get_dep_statuses(ti, session, dep_context):\n            yield dep_status","method_path":"airflow\/ti_deps\/deps\/base_ti_dep.py"}
{"repo_name":"apache\/airflow","method_name":"_parse_s3_config","method_code":"def _parse_s3_config(config_file_name, config_format='boto', profile=None):\n    config = configparser.ConfigParser()\n    if config.read(config_file_name):  \n        sections = config.sections()\n    else:\n        raise AirflowException(\"Couldn't read {0}\".format(config_file_name))\n    \n    if config_format is None:\n        config_format = 'boto'\n    conf_format = config_format.lower()\n    if conf_format == 'boto':  \n        if profile is not None and 'profile ' + profile in sections:\n            cred_section = 'profile ' + profile\n        else:\n            cred_section = 'Credentials'\n    elif conf_format == 'aws' and profile is not None:\n        cred_section = profile\n    else:\n        cred_section = 'default'\n    \n    if conf_format in ('boto', 'aws'):  \n        key_id_option = 'aws_access_key_id'\n        secret_key_option = 'aws_secret_access_key'\n        \n    else:\n        key_id_option = 'access_key'\n        secret_key_option = 'secret_key'\n    \n    if cred_section not in sections:\n        raise AirflowException(\"This config file format is not recognized\")\n    else:\n        try:\n            access_key = config.get(cred_section, key_id_option)\n            secret_key = config.get(cred_section, secret_key_option)\n        except Exception:\n            logging.warning(\"Option Error in parsing s3 config file\")\n            raise\n        return access_key, secret_key","method_summary":"Parses a config file for s3 credentials. Can currently parse boto, s3cmd.conf and AWS SDK config formats","original_method_code":"def _parse_s3_config(config_file_name, config_format='boto', profile=None):\n    \"\"\"\n    Parses a config file for s3 credentials. Can currently\n    parse boto, s3cmd.conf and AWS SDK config formats\n\n    :param config_file_name: path to the config file\n    :type config_file_name: str\n    :param config_format: config type. One of \"boto\", \"s3cmd\" or \"aws\".\n        Defaults to \"boto\"\n    :type config_format: str\n    :param profile: profile name in AWS type config file\n    :type profile: str\n    \"\"\"\n    config = configparser.ConfigParser()\n    if config.read(config_file_name):  # pragma: no cover\n        sections = config.sections()\n    else:\n        raise AirflowException(\"Couldn't read {0}\".format(config_file_name))\n    # Setting option names depending on file format\n    if config_format is None:\n        config_format = 'boto'\n    conf_format = config_format.lower()\n    if conf_format == 'boto':  # pragma: no cover\n        if profile is not None and 'profile ' + profile in sections:\n            cred_section = 'profile ' + profile\n        else:\n            cred_section = 'Credentials'\n    elif conf_format == 'aws' and profile is not None:\n        cred_section = profile\n    else:\n        cred_section = 'default'\n    # Option names\n    if conf_format in ('boto', 'aws'):  # pragma: no cover\n        key_id_option = 'aws_access_key_id'\n        secret_key_option = 'aws_secret_access_key'\n        # security_token_option = 'aws_security_token'\n    else:\n        key_id_option = 'access_key'\n        secret_key_option = 'secret_key'\n    # Actual Parsing\n    if cred_section not in sections:\n        raise AirflowException(\"This config file format is not recognized\")\n    else:\n        try:\n            access_key = config.get(cred_section, key_id_option)\n            secret_key = config.get(cred_section, secret_key_option)\n        except Exception:\n            logging.warning(\"Option Error in parsing s3 config file\")\n            raise\n        return access_key, secret_key","method_path":"airflow\/contrib\/hooks\/aws_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AwsHook.get_credentials","method_code":"def get_credentials(self, region_name=None):\n        session, _ = self._get_credentials(region_name)\n        \n        \n        \n        return session.get_credentials().get_frozen_credentials()","method_summary":"Get the underlying `botocore.Credentials` object. This contains the following authentication","original_method_code":"def get_credentials(self, region_name=None):\n        \"\"\"Get the underlying `botocore.Credentials` object.\n\n        This contains the following authentication attributes: access_key, secret_key and token.\n        \"\"\"\n        session, _ = self._get_credentials(region_name)\n        # Credentials are refreshable, so accessing your access key and\n        # secret key separately can lead to a race condition.\n        # See https:\/\/stackoverflow.com\/a\/36291428\/8283373\n        return session.get_credentials().get_frozen_credentials()","method_path":"airflow\/contrib\/hooks\/aws_hook.py"}
{"repo_name":"apache\/airflow","method_name":"StreamLogWriter.flush","method_code":"def flush(self):\n        if len(self._buffer) > 0:\n            self.logger.log(self.level, self._buffer)\n            self._buffer = str()","method_summary":"Ensure all logging output has been flushed","original_method_code":"def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed\n        \"\"\"\n        if len(self._buffer) > 0:\n            self.logger.log(self.level, self._buffer)\n            self._buffer = str()","method_path":"airflow\/utils\/log\/logging_mixin.py"}
{"repo_name":"apache\/airflow","method_name":"correct_maybe_zipped","method_code":"def correct_maybe_zipped(fileloc):\n    _, archive, filename = re.search(\n        r'((.*\\.zip){})?(.*)'.format(re.escape(os.sep)), fileloc).groups()\n    if archive and zipfile.is_zipfile(archive):\n        return archive\n    else:\n        return fileloc","method_summary":"If the path contains a folder with a .zip suffix, then the folder is treated as a zip archive and path to zip is returned.","original_method_code":"def correct_maybe_zipped(fileloc):\n    \"\"\"\n    If the path contains a folder with a .zip suffix, then\n    the folder is treated as a zip archive and path to zip is returned.\n    \"\"\"\n\n    _, archive, filename = re.search(\n        r'((.*\\.zip){})?(.*)'.format(re.escape(os.sep)), fileloc).groups()\n    if archive and zipfile.is_zipfile(archive):\n        return archive\n    else:\n        return fileloc","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"list_py_file_paths","method_code":"def list_py_file_paths(directory, safe_mode=True,\n                       include_examples=None):\n    if include_examples is None:\n        include_examples = conf.getboolean('core', 'LOAD_EXAMPLES')\n    file_paths = []\n    if directory is None:\n        return []\n    elif os.path.isfile(directory):\n        return [directory]\n    elif os.path.isdir(directory):\n        patterns_by_dir = {}\n        for root, dirs, files in os.walk(directory, followlinks=True):\n            patterns = patterns_by_dir.get(root, [])\n            ignore_file = os.path.join(root, '.airflowignore')\n            if os.path.isfile(ignore_file):\n                with open(ignore_file, 'r') as f:\n                    \n                    \n                    patterns += [re.compile(p) for p in f.read().split('\\n') if p]\n\n            \n            \n            \n            dirs[:] = [\n                d\n                for d in dirs\n                if not any(p.search(os.path.join(root, d)) for p in patterns)\n            ]\n\n            \n            \n            for d in dirs:\n                patterns_by_dir[os.path.join(root, d)] = patterns\n\n            for f in files:\n                try:\n                    file_path = os.path.join(root, f)\n                    if not os.path.isfile(file_path):\n                        continue\n                    mod_name, file_ext = os.path.splitext(\n                        os.path.split(file_path)[-1])\n                    if file_ext != '.py' and not zipfile.is_zipfile(file_path):\n                        continue\n                    if any([re.findall(p, file_path) for p in patterns]):\n                        continue\n\n                    \n                    \n                    might_contain_dag = True\n                    if safe_mode and not zipfile.is_zipfile(file_path):\n                        with open(file_path, 'rb') as fp:\n                            content = fp.read()\n                            might_contain_dag = all(\n                                [s in content for s in (b'DAG', b'airflow')])\n\n                    if not might_contain_dag:\n                        continue\n\n                    file_paths.append(file_path)\n                except Exception:\n                    log = LoggingMixin().log\n                    log.exception(\"Error while examining %s\", f)\n    if include_examples:\n        import airflow.example_dags\n        example_dag_folder = airflow.example_dags.__path__[0]\n        file_paths.extend(list_py_file_paths(example_dag_folder, safe_mode, False))\n    return file_paths","method_summary":"Traverse a directory and look for Python files.","original_method_code":"def list_py_file_paths(directory, safe_mode=True,\n                       include_examples=None):\n    \"\"\"\n    Traverse a directory and look for Python files.\n\n    :param directory: the directory to traverse\n    :type directory: unicode\n    :param safe_mode: whether to use a heuristic to determine whether a file\n        contains Airflow DAG definitions\n    :return: a list of paths to Python files in the specified directory\n    :rtype: list[unicode]\n    \"\"\"\n    if include_examples is None:\n        include_examples = conf.getboolean('core', 'LOAD_EXAMPLES')\n    file_paths = []\n    if directory is None:\n        return []\n    elif os.path.isfile(directory):\n        return [directory]\n    elif os.path.isdir(directory):\n        patterns_by_dir = {}\n        for root, dirs, files in os.walk(directory, followlinks=True):\n            patterns = patterns_by_dir.get(root, [])\n            ignore_file = os.path.join(root, '.airflowignore')\n            if os.path.isfile(ignore_file):\n                with open(ignore_file, 'r') as f:\n                    # If we have new patterns create a copy so we don't change\n                    # the previous list (which would affect other subdirs)\n                    patterns += [re.compile(p) for p in f.read().split('\\n') if p]\n\n            # If we can ignore any subdirs entirely we should - fewer paths\n            # to walk is better. We have to modify the ``dirs`` array in\n            # place for this to affect os.walk\n            dirs[:] = [\n                d\n                for d in dirs\n                if not any(p.search(os.path.join(root, d)) for p in patterns)\n            ]\n\n            # We want patterns defined in a parent folder's .airflowignore to\n            # apply to subdirs too\n            for d in dirs:\n                patterns_by_dir[os.path.join(root, d)] = patterns\n\n            for f in files:\n                try:\n                    file_path = os.path.join(root, f)\n                    if not os.path.isfile(file_path):\n                        continue\n                    mod_name, file_ext = os.path.splitext(\n                        os.path.split(file_path)[-1])\n                    if file_ext != '.py' and not zipfile.is_zipfile(file_path):\n                        continue\n                    if any([re.findall(p, file_path) for p in patterns]):\n                        continue\n\n                    # Heuristic that guesses whether a Python file contains an\n                    # Airflow DAG definition.\n                    might_contain_dag = True\n                    if safe_mode and not zipfile.is_zipfile(file_path):\n                        with open(file_path, 'rb') as fp:\n                            content = fp.read()\n                            might_contain_dag = all(\n                                [s in content for s in (b'DAG', b'airflow')])\n\n                    if not might_contain_dag:\n                        continue\n\n                    file_paths.append(file_path)\n                except Exception:\n                    log = LoggingMixin().log\n                    log.exception(\"Error while examining %s\", f)\n    if include_examples:\n        import airflow.example_dags\n        example_dag_folder = airflow.example_dags.__path__[0]\n        file_paths.extend(list_py_file_paths(example_dag_folder, safe_mode, False))\n    return file_paths","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"SimpleTaskInstance.construct_task_instance","method_code":"def construct_task_instance(self, session=None, lock_for_update=False):\n        TI = airflow.models.TaskInstance\n\n        qry = session.query(TI).filter(\n            TI.dag_id == self._dag_id,\n            TI.task_id == self._task_id,\n            TI.execution_date == self._execution_date)\n\n        if lock_for_update:\n            ti = qry.with_for_update().first()\n        else:\n            ti = qry.first()\n        return ti","method_summary":"Construct a TaskInstance from the database based on the primary key","original_method_code":"def construct_task_instance(self, session=None, lock_for_update=False):\n        \"\"\"\n        Construct a TaskInstance from the database based on the primary key\n\n        :param session: DB session.\n        :param lock_for_update: if True, indicates that the database should\n            lock the TaskInstance (issuing a FOR UPDATE clause) until the\n            session is committed.\n        \"\"\"\n        TI = airflow.models.TaskInstance\n\n        qry = session.query(TI).filter(\n            TI.dag_id == self._dag_id,\n            TI.task_id == self._task_id,\n            TI.execution_date == self._execution_date)\n\n        if lock_for_update:\n            ti = qry.with_for_update().first()\n        else:\n            ti = qry.first()\n        return ti","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorAgent.start","method_code":"def start(self):\n        self._process = self._launch_process(self._dag_directory,\n                                             self._file_paths,\n                                             self._max_runs,\n                                             self._processor_factory,\n                                             self._child_signal_conn,\n                                             self._stat_queue,\n                                             self._result_queue,\n                                             self._async_mode)\n        self.log.info(\"Launched DagFileProcessorManager with pid: %s\", self._process.pid)","method_summary":"Launch DagFileProcessorManager processor and start DAG parsing loop in manager.","original_method_code":"def start(self):\n        \"\"\"\n        Launch DagFileProcessorManager processor and start DAG parsing loop in manager.\n        \"\"\"\n        self._process = self._launch_process(self._dag_directory,\n                                             self._file_paths,\n                                             self._max_runs,\n                                             self._processor_factory,\n                                             self._child_signal_conn,\n                                             self._stat_queue,\n                                             self._result_queue,\n                                             self._async_mode)\n        self.log.info(\"Launched DagFileProcessorManager with pid: %s\", self._process.pid)","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorAgent.terminate","method_code":"def terminate(self):\n        self.log.info(\"Sending termination message to manager.\")\n        self._child_signal_conn.send(DagParsingSignal.TERMINATE_MANAGER)","method_summary":"Send termination signal to DAG parsing processor manager and expect it to terminate all DAG file processors.","original_method_code":"def terminate(self):\n        \"\"\"\n        Send termination signal to DAG parsing processor manager\n        and expect it to terminate all DAG file processors.\n        \"\"\"\n        self.log.info(\"Sending termination message to manager.\")\n        self._child_signal_conn.send(DagParsingSignal.TERMINATE_MANAGER)","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager._exit_gracefully","method_code":"def _exit_gracefully(self, signum, frame):\n        self.log.info(\"Exiting gracefully upon receiving signal %s\", signum)\n        self.terminate()\n        self.end()\n        self.log.debug(\"Finished terminating DAG processors.\")\n        sys.exit(os.EX_OK)","method_summary":"Helper method to clean up DAG file processors to avoid leaving orphan processes.","original_method_code":"def _exit_gracefully(self, signum, frame):\n        \"\"\"\n        Helper method to clean up DAG file processors to avoid leaving orphan processes.\n        \"\"\"\n        self.log.info(\"Exiting gracefully upon receiving signal %s\", signum)\n        self.terminate()\n        self.end()\n        self.log.debug(\"Finished terminating DAG processors.\")\n        sys.exit(os.EX_OK)","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager.start","method_code":"def start(self):\n        self.log.info(\"Processing files using up to %s processes at a time \", self._parallelism)\n        self.log.info(\"Process each file at most once every %s seconds\", self._file_process_interval)\n        self.log.info(\n            \"Checking for new files in %s every %s seconds\", self._dag_directory, self.dag_dir_list_interval\n        )\n\n        if self._async_mode:\n            self.log.debug(\"Starting DagFileProcessorManager in async mode\")\n            self.start_in_async()\n        else:\n            self.log.debug(\"Starting DagFileProcessorManager in sync mode\")\n            self.start_in_sync()","method_summary":"Use multiple processes to parse and generate tasks for the DAGs in parallel. By processing them in separate processes, we can get parallelism and isolation from potentially harmful user code.","original_method_code":"def start(self):\n        \"\"\"\n        Use multiple processes to parse and generate tasks for the\n        DAGs in parallel. By processing them in separate processes,\n        we can get parallelism and isolation from potentially harmful\n        user code.\n        \"\"\"\n\n        self.log.info(\"Processing files using up to %s processes at a time \", self._parallelism)\n        self.log.info(\"Process each file at most once every %s seconds\", self._file_process_interval)\n        self.log.info(\n            \"Checking for new files in %s every %s seconds\", self._dag_directory, self.dag_dir_list_interval\n        )\n\n        if self._async_mode:\n            self.log.debug(\"Starting DagFileProcessorManager in async mode\")\n            self.start_in_async()\n        else:\n            self.log.debug(\"Starting DagFileProcessorManager in sync mode\")\n            self.start_in_sync()","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager.start_in_async","method_code":"def start_in_async(self):\n        while True:\n            loop_start_time = time.time()\n\n            if self._signal_conn.poll():\n                agent_signal = self._signal_conn.recv()\n                if agent_signal == DagParsingSignal.TERMINATE_MANAGER:\n                    self.terminate()\n                    break\n                elif agent_signal == DagParsingSignal.END_MANAGER:\n                    self.end()\n                    sys.exit(os.EX_OK)\n\n            self._refresh_dag_dir()\n\n            simple_dags = self.heartbeat()\n            for simple_dag in simple_dags:\n                self._result_queue.put(simple_dag)\n\n            self._print_stat()\n\n            all_files_processed = all(self.get_last_finish_time(x) is not None\n                                      for x in self.file_paths)\n            max_runs_reached = self.max_runs_reached()\n\n            dag_parsing_stat = DagParsingStat(self._file_paths,\n                                              self.get_all_pids(),\n                                              max_runs_reached,\n                                              all_files_processed,\n                                              len(simple_dags))\n            self._stat_queue.put(dag_parsing_stat)\n\n            if max_runs_reached:\n                self.log.info(\"Exiting dag parsing loop as all files \"\n                              \"have been processed %s times\", self._max_runs)\n                break\n\n            loop_duration = time.time() - loop_start_time\n            if loop_duration < 1:\n                sleep_length = 1 - loop_duration\n                self.log.debug(\"Sleeping for %.2f seconds to prevent excessive logging\", sleep_length)\n                time.sleep(sleep_length)","method_summary":"Parse DAG files repeatedly in a standalone loop.","original_method_code":"def start_in_async(self):\n        \"\"\"\n        Parse DAG files repeatedly in a standalone loop.\n        \"\"\"\n        while True:\n            loop_start_time = time.time()\n\n            if self._signal_conn.poll():\n                agent_signal = self._signal_conn.recv()\n                if agent_signal == DagParsingSignal.TERMINATE_MANAGER:\n                    self.terminate()\n                    break\n                elif agent_signal == DagParsingSignal.END_MANAGER:\n                    self.end()\n                    sys.exit(os.EX_OK)\n\n            self._refresh_dag_dir()\n\n            simple_dags = self.heartbeat()\n            for simple_dag in simple_dags:\n                self._result_queue.put(simple_dag)\n\n            self._print_stat()\n\n            all_files_processed = all(self.get_last_finish_time(x) is not None\n                                      for x in self.file_paths)\n            max_runs_reached = self.max_runs_reached()\n\n            dag_parsing_stat = DagParsingStat(self._file_paths,\n                                              self.get_all_pids(),\n                                              max_runs_reached,\n                                              all_files_processed,\n                                              len(simple_dags))\n            self._stat_queue.put(dag_parsing_stat)\n\n            if max_runs_reached:\n                self.log.info(\"Exiting dag parsing loop as all files \"\n                              \"have been processed %s times\", self._max_runs)\n                break\n\n            loop_duration = time.time() - loop_start_time\n            if loop_duration < 1:\n                sleep_length = 1 - loop_duration\n                self.log.debug(\"Sleeping for %.2f seconds to prevent excessive logging\", sleep_length)\n                time.sleep(sleep_length)","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager.start_in_sync","method_code":"def start_in_sync(self):\n        while True:\n            agent_signal = self._signal_conn.recv()\n            if agent_signal == DagParsingSignal.TERMINATE_MANAGER:\n                self.terminate()\n                break\n            elif agent_signal == DagParsingSignal.END_MANAGER:\n                self.end()\n                sys.exit(os.EX_OK)\n            elif agent_signal == DagParsingSignal.AGENT_HEARTBEAT:\n\n                self._refresh_dag_dir()\n\n                simple_dags = self.heartbeat()\n                for simple_dag in simple_dags:\n                    self._result_queue.put(simple_dag)\n\n                self._print_stat()\n\n                all_files_processed = all(self.get_last_finish_time(x) is not None\n                                          for x in self.file_paths)\n                max_runs_reached = self.max_runs_reached()\n\n                dag_parsing_stat = DagParsingStat(self._file_paths,\n                                                  self.get_all_pids(),\n                                                  self.max_runs_reached(),\n                                                  all_files_processed,\n                                                  len(simple_dags))\n                self._stat_queue.put(dag_parsing_stat)\n\n                self.wait_until_finished()\n                self._signal_conn.send(DagParsingSignal.MANAGER_DONE)\n\n                if max_runs_reached:\n                    self.log.info(\"Exiting dag parsing loop as all files \"\n                                  \"have been processed %s times\", self._max_runs)\n                    self._signal_conn.send(DagParsingSignal.MANAGER_DONE)\n                    break","method_summary":"Parse DAG files in a loop controlled by DagParsingSignal. Actual DAG parsing loop will run once upon receiving one agent heartbeat message and will report done when finished the loop.","original_method_code":"def start_in_sync(self):\n        \"\"\"\n        Parse DAG files in a loop controlled by DagParsingSignal.\n        Actual DAG parsing loop will run once upon receiving one\n        agent heartbeat message and will report done when finished the loop.\n        \"\"\"\n        while True:\n            agent_signal = self._signal_conn.recv()\n            if agent_signal == DagParsingSignal.TERMINATE_MANAGER:\n                self.terminate()\n                break\n            elif agent_signal == DagParsingSignal.END_MANAGER:\n                self.end()\n                sys.exit(os.EX_OK)\n            elif agent_signal == DagParsingSignal.AGENT_HEARTBEAT:\n\n                self._refresh_dag_dir()\n\n                simple_dags = self.heartbeat()\n                for simple_dag in simple_dags:\n                    self._result_queue.put(simple_dag)\n\n                self._print_stat()\n\n                all_files_processed = all(self.get_last_finish_time(x) is not None\n                                          for x in self.file_paths)\n                max_runs_reached = self.max_runs_reached()\n\n                dag_parsing_stat = DagParsingStat(self._file_paths,\n                                                  self.get_all_pids(),\n                                                  self.max_runs_reached(),\n                                                  all_files_processed,\n                                                  len(simple_dags))\n                self._stat_queue.put(dag_parsing_stat)\n\n                self.wait_until_finished()\n                self._signal_conn.send(DagParsingSignal.MANAGER_DONE)\n\n                if max_runs_reached:\n                    self.log.info(\"Exiting dag parsing loop as all files \"\n                                  \"have been processed %s times\", self._max_runs)\n                    self._signal_conn.send(DagParsingSignal.MANAGER_DONE)\n                    break","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager._refresh_dag_dir","method_code":"def _refresh_dag_dir(self):\n        elapsed_time_since_refresh = (timezone.utcnow() -\n                                      self.last_dag_dir_refresh_time).total_seconds()\n        if elapsed_time_since_refresh > self.dag_dir_list_interval:\n            \n            self.log.info(\"Searching for files in %s\", self._dag_directory)\n            self._file_paths = list_py_file_paths(self._dag_directory)\n            self.last_dag_dir_refresh_time = timezone.utcnow()\n            self.log.info(\"There are %s files in %s\", len(self._file_paths), self._dag_directory)\n            self.set_file_paths(self._file_paths)\n\n            try:\n                self.log.debug(\"Removing old import errors\")\n                self.clear_nonexistent_import_errors()\n            except Exception:\n                self.log.exception(\"Error removing old import errors\")","method_summary":"Refresh file paths from dag dir if we haven't done it for too long.","original_method_code":"def _refresh_dag_dir(self):\n        \"\"\"\n        Refresh file paths from dag dir if we haven't done it for too long.\n        \"\"\"\n        elapsed_time_since_refresh = (timezone.utcnow() -\n                                      self.last_dag_dir_refresh_time).total_seconds()\n        if elapsed_time_since_refresh > self.dag_dir_list_interval:\n            # Build up a list of Python files that could contain DAGs\n            self.log.info(\"Searching for files in %s\", self._dag_directory)\n            self._file_paths = list_py_file_paths(self._dag_directory)\n            self.last_dag_dir_refresh_time = timezone.utcnow()\n            self.log.info(\"There are %s files in %s\", len(self._file_paths), self._dag_directory)\n            self.set_file_paths(self._file_paths)\n\n            try:\n                self.log.debug(\"Removing old import errors\")\n                self.clear_nonexistent_import_errors()\n            except Exception:\n                self.log.exception(\"Error removing old import errors\")","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager._print_stat","method_code":"def _print_stat(self):\n        if ((timezone.utcnow() - self.last_stat_print_time).total_seconds() >\n                self.print_stats_interval):\n            if len(self._file_paths) > 0:\n                self._log_file_processing_stats(self._file_paths)\n            self.last_stat_print_time = timezone.utcnow()","method_summary":"Occasionally print out stats about how fast the files are getting processed","original_method_code":"def _print_stat(self):\n        \"\"\"\n        Occasionally print out stats about how fast the files are getting processed\n        \"\"\"\n        if ((timezone.utcnow() - self.last_stat_print_time).total_seconds() >\n                self.print_stats_interval):\n            if len(self._file_paths) > 0:\n                self._log_file_processing_stats(self._file_paths)\n            self.last_stat_print_time = timezone.utcnow()","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager.clear_nonexistent_import_errors","method_code":"def clear_nonexistent_import_errors(self, session):\n        query = session.query(errors.ImportError)\n        if self._file_paths:\n            query = query.filter(\n                ~errors.ImportError.filename.in_(self._file_paths)\n            )\n        query.delete(synchronize_session='fetch')\n        session.commit()","method_summary":"Clears import errors for files that no longer exist.","original_method_code":"def clear_nonexistent_import_errors(self, session):\n        \"\"\"\n        Clears import errors for files that no longer exist.\n\n        :param session: session for ORM operations\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n        query = session.query(errors.ImportError)\n        if self._file_paths:\n            query = query.filter(\n                ~errors.ImportError.filename.in_(self._file_paths)\n            )\n        query.delete(synchronize_session='fetch')\n        session.commit()","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager._log_file_processing_stats","method_code":"def _log_file_processing_stats(self, known_file_paths):\n        \n        \n        \n        \n        \n        \n        \n        \n        headers = [\"File Path\",\n                   \"PID\",\n                   \"Runtime\",\n                   \"Last Runtime\",\n                   \"Last Run\"]\n\n        rows = []\n        for file_path in known_file_paths:\n            last_runtime = self.get_last_runtime(file_path)\n            file_name = os.path.basename(file_path)\n            file_name = os.path.splitext(file_name)[0].replace(os.sep, '.')\n            if last_runtime:\n                Stats.gauge(\n                    'dag_processing.last_runtime.{}'.format(file_name),\n                    last_runtime\n                )\n\n            processor_pid = self.get_pid(file_path)\n            processor_start_time = self.get_start_time(file_path)\n            runtime = ((timezone.utcnow() - processor_start_time).total_seconds()\n                       if processor_start_time else None)\n            last_run = self.get_last_finish_time(file_path)\n            if last_run:\n                seconds_ago = (timezone.utcnow() - last_run).total_seconds()\n                Stats.gauge(\n                    'dag_processing.last_run.seconds_ago.{}'.format(file_name),\n                    seconds_ago\n                )\n\n            rows.append((file_path,\n                         processor_pid,\n                         runtime,\n                         last_runtime,\n                         last_run))\n\n        \n        rows = sorted(rows, key=lambda x: x[3] or 0.0)\n\n        formatted_rows = []\n        for file_path, pid, runtime, last_runtime, last_run in rows:\n            formatted_rows.append((file_path,\n                                   pid,\n                                   \"{:.2f}s\".format(runtime)\n                                   if runtime else None,\n                                   \"{:.2f}s\".format(last_runtime)\n                                   if last_runtime else None,\n                                   last_run.strftime(\"%Y-%m-%dT%H:%M:%S\")\n                                   if last_run else None))\n        log_str = (\"\\n\" +\n                   \"=\" * 80 +\n                   \"\\n\" +\n                   \"DAG File Processing Stats\\n\\n\" +\n                   tabulate(formatted_rows, headers=headers) +\n                   \"\\n\" +\n                   \"=\" * 80)\n\n        self.log.info(log_str)","method_summary":"Print out stats about how files are getting processed.","original_method_code":"def _log_file_processing_stats(self, known_file_paths):\n        \"\"\"\n        Print out stats about how files are getting processed.\n\n        :param known_file_paths: a list of file paths that may contain Airflow\n            DAG definitions\n        :type known_file_paths: list[unicode]\n        :return: None\n        \"\"\"\n\n        # File Path: Path to the file containing the DAG definition\n        # PID: PID associated with the process that's processing the file. May\n        # be empty.\n        # Runtime: If the process is currently running, how long it's been\n        # running for in seconds.\n        # Last Runtime: If the process ran before, how long did it take to\n        # finish in seconds\n        # Last Run: When the file finished processing in the previous run.\n        headers = [\"File Path\",\n                   \"PID\",\n                   \"Runtime\",\n                   \"Last Runtime\",\n                   \"Last Run\"]\n\n        rows = []\n        for file_path in known_file_paths:\n            last_runtime = self.get_last_runtime(file_path)\n            file_name = os.path.basename(file_path)\n            file_name = os.path.splitext(file_name)[0].replace(os.sep, '.')\n            if last_runtime:\n                Stats.gauge(\n                    'dag_processing.last_runtime.{}'.format(file_name),\n                    last_runtime\n                )\n\n            processor_pid = self.get_pid(file_path)\n            processor_start_time = self.get_start_time(file_path)\n            runtime = ((timezone.utcnow() - processor_start_time).total_seconds()\n                       if processor_start_time else None)\n            last_run = self.get_last_finish_time(file_path)\n            if last_run:\n                seconds_ago = (timezone.utcnow() - last_run).total_seconds()\n                Stats.gauge(\n                    'dag_processing.last_run.seconds_ago.{}'.format(file_name),\n                    seconds_ago\n                )\n\n            rows.append((file_path,\n                         processor_pid,\n                         runtime,\n                         last_runtime,\n                         last_run))\n\n        # Sort by longest last runtime. (Can't sort None values in python3)\n        rows = sorted(rows, key=lambda x: x[3] or 0.0)\n\n        formatted_rows = []\n        for file_path, pid, runtime, last_runtime, last_run in rows:\n            formatted_rows.append((file_path,\n                                   pid,\n                                   \"{:.2f}s\".format(runtime)\n                                   if runtime else None,\n                                   \"{:.2f}s\".format(last_runtime)\n                                   if last_runtime else None,\n                                   last_run.strftime(\"%Y-%m-%dT%H:%M:%S\")\n                                   if last_run else None))\n        log_str = (\"\\n\" +\n                   \"=\" * 80 +\n                   \"\\n\" +\n                   \"DAG File Processing Stats\\n\\n\" +\n                   tabulate(formatted_rows, headers=headers) +\n                   \"\\n\" +\n                   \"=\" * 80)\n\n        self.log.info(log_str)","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager.set_file_paths","method_code":"def set_file_paths(self, new_file_paths):\n        self._file_paths = new_file_paths\n        self._file_path_queue = [x for x in self._file_path_queue\n                                 if x in new_file_paths]\n        \n        filtered_processors = {}\n        for file_path, processor in self._processors.items():\n            if file_path in new_file_paths:\n                filtered_processors[file_path] = processor\n            else:\n                self.log.warning(\"Stopping processor for %s\", file_path)\n                processor.terminate()\n        self._processors = filtered_processors","method_summary":"Update this with a new set of paths to DAG definition files.","original_method_code":"def set_file_paths(self, new_file_paths):\n        \"\"\"\n        Update this with a new set of paths to DAG definition files.\n\n        :param new_file_paths: list of paths to DAG definition files\n        :type new_file_paths: list[unicode]\n        :return: None\n        \"\"\"\n        self._file_paths = new_file_paths\n        self._file_path_queue = [x for x in self._file_path_queue\n                                 if x in new_file_paths]\n        # Stop processors that are working on deleted files\n        filtered_processors = {}\n        for file_path, processor in self._processors.items():\n            if file_path in new_file_paths:\n                filtered_processors[file_path] = processor\n            else:\n                self.log.warning(\"Stopping processor for %s\", file_path)\n                processor.terminate()\n        self._processors = filtered_processors","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager.wait_until_finished","method_code":"def wait_until_finished(self):\n        for file_path, processor in self._processors.items():\n            while not processor.done:\n                time.sleep(0.1)","method_summary":"Sleeps until all the processors are done.","original_method_code":"def wait_until_finished(self):\n        \"\"\"\n        Sleeps until all the processors are done.\n        \"\"\"\n        for file_path, processor in self._processors.items():\n            while not processor.done:\n                time.sleep(0.1)","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager.heartbeat","method_code":"def heartbeat(self):\n        finished_processors = {}\n        running_processors = {}\n        for file_path, processor in self._processors.items():\n            if processor.done:\n                self.log.debug(\"Processor for %s finished\", file_path)\n                now = timezone.utcnow()\n                finished_processors[file_path] = processor\n                self._last_runtime[file_path] = (now -\n                                                 processor.start_time).total_seconds()\n                self._last_finish_time[file_path] = now\n                self._run_count[file_path] += 1\n            else:\n                running_processors[file_path] = processor\n        self._processors = running_processors\n\n        self.log.debug(\"%s\/%s DAG parsing processes running\",\n                       len(self._processors), self._parallelism)\n\n        self.log.debug(\"%s file paths queued for processing\",\n                       len(self._file_path_queue))\n\n        \n        simple_dags = []\n        for file_path, processor in finished_processors.items():\n            if processor.result is None:\n                self.log.warning(\n                    \"Processor for %s exited with return code %s.\",\n                    processor.file_path, processor.exit_code\n                )\n            else:\n                for simple_dag in processor.result:\n                    simple_dags.append(simple_dag)\n\n        \n        \n        if len(self._file_path_queue) == 0:\n            \n            \n            file_paths_in_progress = self._processors.keys()\n            now = timezone.utcnow()\n            file_paths_recently_processed = []\n            for file_path in self._file_paths:\n                last_finish_time = self.get_last_finish_time(file_path)\n                if (last_finish_time is not None and\n                    (now - last_finish_time).total_seconds() <\n                        self._file_process_interval):\n                    file_paths_recently_processed.append(file_path)\n\n            files_paths_at_run_limit = [file_path\n                                        for file_path, num_runs in self._run_count.items()\n                                        if num_runs == self._max_runs]\n\n            files_paths_to_queue = list(set(self._file_paths) -\n                                        set(file_paths_in_progress) -\n                                        set(file_paths_recently_processed) -\n                                        set(files_paths_at_run_limit))\n\n            for file_path, processor in self._processors.items():\n                self.log.debug(\n                    \"File path %s is still being processed (started: %s)\",\n                    processor.file_path, processor.start_time.isoformat()\n                )\n\n            self.log.debug(\n                \"Queuing the following files for processing:\\n\\t%s\",\n                \"\\n\\t\".join(files_paths_to_queue)\n            )\n\n            self._file_path_queue.extend(files_paths_to_queue)\n\n        zombies = self._find_zombies()\n\n        \n        while (self._parallelism - len(self._processors) > 0 and\n               len(self._file_path_queue) > 0):\n            file_path = self._file_path_queue.pop(0)\n            processor = self._processor_factory(file_path, zombies)\n\n            processor.start()\n            self.log.debug(\n                \"Started a process (PID: %s) to generate tasks for %s\",\n                processor.pid, file_path\n            )\n            self._processors[file_path] = processor\n\n        \n        self._run_count[self._heart_beat_key] += 1\n\n        return simple_dags","method_summary":"This should be periodically called by the manager loop. This method will kick off new processes to process DAG definition files and read the results from the finished processors.","original_method_code":"def heartbeat(self):\n        \"\"\"\n        This should be periodically called by the manager loop. This method will\n        kick off new processes to process DAG definition files and read the\n        results from the finished processors.\n\n        :return: a list of SimpleDags that were produced by processors that\n            have finished since the last time this was called\n        :rtype: list[airflow.utils.dag_processing.SimpleDag]\n        \"\"\"\n        finished_processors = {}\n        \"\"\":type : dict[unicode, AbstractDagFileProcessor]\"\"\"\n        running_processors = {}\n        \"\"\":type : dict[unicode, AbstractDagFileProcessor]\"\"\"\n\n        for file_path, processor in self._processors.items():\n            if processor.done:\n                self.log.debug(\"Processor for %s finished\", file_path)\n                now = timezone.utcnow()\n                finished_processors[file_path] = processor\n                self._last_runtime[file_path] = (now -\n                                                 processor.start_time).total_seconds()\n                self._last_finish_time[file_path] = now\n                self._run_count[file_path] += 1\n            else:\n                running_processors[file_path] = processor\n        self._processors = running_processors\n\n        self.log.debug(\"%s\/%s DAG parsing processes running\",\n                       len(self._processors), self._parallelism)\n\n        self.log.debug(\"%s file paths queued for processing\",\n                       len(self._file_path_queue))\n\n        # Collect all the DAGs that were found in the processed files\n        simple_dags = []\n        for file_path, processor in finished_processors.items():\n            if processor.result is None:\n                self.log.warning(\n                    \"Processor for %s exited with return code %s.\",\n                    processor.file_path, processor.exit_code\n                )\n            else:\n                for simple_dag in processor.result:\n                    simple_dags.append(simple_dag)\n\n        # Generate more file paths to process if we processed all the files\n        # already.\n        if len(self._file_path_queue) == 0:\n            # If the file path is already being processed, or if a file was\n            # processed recently, wait until the next batch\n            file_paths_in_progress = self._processors.keys()\n            now = timezone.utcnow()\n            file_paths_recently_processed = []\n            for file_path in self._file_paths:\n                last_finish_time = self.get_last_finish_time(file_path)\n                if (last_finish_time is not None and\n                    (now - last_finish_time).total_seconds() <\n                        self._file_process_interval):\n                    file_paths_recently_processed.append(file_path)\n\n            files_paths_at_run_limit = [file_path\n                                        for file_path, num_runs in self._run_count.items()\n                                        if num_runs == self._max_runs]\n\n            files_paths_to_queue = list(set(self._file_paths) -\n                                        set(file_paths_in_progress) -\n                                        set(file_paths_recently_processed) -\n                                        set(files_paths_at_run_limit))\n\n            for file_path, processor in self._processors.items():\n                self.log.debug(\n                    \"File path %s is still being processed (started: %s)\",\n                    processor.file_path, processor.start_time.isoformat()\n                )\n\n            self.log.debug(\n                \"Queuing the following files for processing:\\n\\t%s\",\n                \"\\n\\t\".join(files_paths_to_queue)\n            )\n\n            self._file_path_queue.extend(files_paths_to_queue)\n\n        zombies = self._find_zombies()\n\n        # Start more processors if we have enough slots and files to process\n        while (self._parallelism - len(self._processors) > 0 and\n               len(self._file_path_queue) > 0):\n            file_path = self._file_path_queue.pop(0)\n            processor = self._processor_factory(file_path, zombies)\n\n            processor.start()\n            self.log.debug(\n                \"Started a process (PID: %s) to generate tasks for %s\",\n                processor.pid, file_path\n            )\n            self._processors[file_path] = processor\n\n        # Update heartbeat count.\n        self._run_count[self._heart_beat_key] += 1\n\n        return simple_dags","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessorManager.end","method_code":"def end(self):\n        pids_to_kill = self.get_all_pids()\n        if len(pids_to_kill) > 0:\n            \n            this_process = psutil.Process(os.getpid())\n            \n            \n            \n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            for child in child_processes:\n                self.log.info(\"Terminating child PID: %s\", child.pid)\n                child.terminate()\n            \n            timeout = 5\n            self.log.info(\"Waiting up to %s seconds for processes to exit...\", timeout)\n            try:\n                psutil.wait_procs(\n                    child_processes, timeout=timeout,\n                    callback=lambda x: self.log.info('Terminated PID %s', x.pid))\n            except psutil.TimeoutExpired:\n                self.log.debug(\"Ran out of time while waiting for processes to exit\")\n\n            \n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            if len(child_processes) > 0:\n                self.log.info(\"SIGKILL processes that did not terminate gracefully\")\n                for child in child_processes:\n                    self.log.info(\"Killing child PID: %s\", child.pid)\n                    child.kill()\n                    child.wait()","method_summary":"Kill all child processes on exit since we don't want to leave them as orphaned.","original_method_code":"def end(self):\n        \"\"\"\n        Kill all child processes on exit since we don't want to leave\n        them as orphaned.\n        \"\"\"\n        pids_to_kill = self.get_all_pids()\n        if len(pids_to_kill) > 0:\n            # First try SIGTERM\n            this_process = psutil.Process(os.getpid())\n            # Only check child processes to ensure that we don't have a case\n            # where we kill the wrong process because a child process died\n            # but the PID got reused.\n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            for child in child_processes:\n                self.log.info(\"Terminating child PID: %s\", child.pid)\n                child.terminate()\n            # TODO: Remove magic number\n            timeout = 5\n            self.log.info(\"Waiting up to %s seconds for processes to exit...\", timeout)\n            try:\n                psutil.wait_procs(\n                    child_processes, timeout=timeout,\n                    callback=lambda x: self.log.info('Terminated PID %s', x.pid))\n            except psutil.TimeoutExpired:\n                self.log.debug(\"Ran out of time while waiting for processes to exit\")\n\n            # Then SIGKILL\n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            if len(child_processes) > 0:\n                self.log.info(\"SIGKILL processes that did not terminate gracefully\")\n                for child in child_processes:\n                    self.log.info(\"Killing child PID: %s\", child.pid)\n                    child.kill()\n                    child.wait()","method_path":"airflow\/utils\/dag_processing.py"}
{"repo_name":"apache\/airflow","method_name":"SSHHook.get_conn","method_code":"def get_conn(self):\n        self.log.debug('Creating SSH client for conn_id: %s', self.ssh_conn_id)\n        client = paramiko.SSHClient()\n        if not self.allow_host_key_change:\n            self.log.warning('Remote Identification Change is not verified. '\n                             'This wont protect against Man-In-The-Middle attacks')\n            client.load_system_host_keys()\n        if self.no_host_key_check:\n            self.log.warning('No Host Key Verification. This wont protect '\n                             'against Man-In-The-Middle attacks')\n            \n            client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n        if self.password and self.password.strip():\n            client.connect(hostname=self.remote_host,\n                           username=self.username,\n                           password=self.password,\n                           key_filename=self.key_file,\n                           timeout=self.timeout,\n                           compress=self.compress,\n                           port=self.port,\n                           sock=self.host_proxy)\n        else:\n            client.connect(hostname=self.remote_host,\n                           username=self.username,\n                           key_filename=self.key_file,\n                           timeout=self.timeout,\n                           compress=self.compress,\n                           port=self.port,\n                           sock=self.host_proxy)\n\n        if self.keepalive_interval:\n            client.get_transport().set_keepalive(self.keepalive_interval)\n\n        self.client = client\n        return client","method_summary":"Opens a ssh connection to the remote host.","original_method_code":"def get_conn(self):\n        \"\"\"\n        Opens a ssh connection to the remote host.\n\n        :rtype: paramiko.client.SSHClient\n        \"\"\"\n\n        self.log.debug('Creating SSH client for conn_id: %s', self.ssh_conn_id)\n        client = paramiko.SSHClient()\n        if not self.allow_host_key_change:\n            self.log.warning('Remote Identification Change is not verified. '\n                             'This wont protect against Man-In-The-Middle attacks')\n            client.load_system_host_keys()\n        if self.no_host_key_check:\n            self.log.warning('No Host Key Verification. This wont protect '\n                             'against Man-In-The-Middle attacks')\n            # Default is RejectPolicy\n            client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n        if self.password and self.password.strip():\n            client.connect(hostname=self.remote_host,\n                           username=self.username,\n                           password=self.password,\n                           key_filename=self.key_file,\n                           timeout=self.timeout,\n                           compress=self.compress,\n                           port=self.port,\n                           sock=self.host_proxy)\n        else:\n            client.connect(hostname=self.remote_host,\n                           username=self.username,\n                           key_filename=self.key_file,\n                           timeout=self.timeout,\n                           compress=self.compress,\n                           port=self.port,\n                           sock=self.host_proxy)\n\n        if self.keepalive_interval:\n            client.get_transport().set_keepalive(self.keepalive_interval)\n\n        self.client = client\n        return client","method_path":"airflow\/contrib\/hooks\/ssh_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GCPTransferServiceHook.create_transfer_job","method_code":"def create_transfer_job(self, body):\n        body = self._inject_project_id(body, BODY, PROJECT_ID)\n        return self.get_conn().transferJobs().create(body=body).execute(num_retries=self.num_retries)","method_summary":"Creates a transfer job that runs periodically.","original_method_code":"def create_transfer_job(self, body):\n        \"\"\"\n        Creates a transfer job that runs periodically.\n\n        :param body: (Required) A request body, as described in\n            https:\/\/cloud.google.com\/storage-transfer\/docs\/reference\/rest\/v1\/transferJobs\/patch#request-body\n        :type body: dict\n        :return: transfer job.\n            See:\n            https:\/\/cloud.google.com\/storage-transfer\/docs\/reference\/rest\/v1\/transferJobs#TransferJob\n        :rtype: dict\n        \"\"\"\n        body = self._inject_project_id(body, BODY, PROJECT_ID)\n        return self.get_conn().transferJobs().create(body=body).execute(num_retries=self.num_retries)","method_path":"airflow\/contrib\/hooks\/gcp_transfer_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GCPTransferServiceHook.get_transfer_job","method_code":"def get_transfer_job(self, job_name, project_id=None):\n        return (\n            self.get_conn()\n            .transferJobs()\n            .get(jobName=job_name, projectId=project_id)\n            .execute(num_retries=self.num_retries)\n        )","method_summary":"Gets the latest state of a long-running operation in Google Storage Transfer Service.","original_method_code":"def get_transfer_job(self, job_name, project_id=None):\n        \"\"\"\n        Gets the latest state of a long-running operation in Google Storage\n        Transfer Service.\n\n        :param job_name: (Required) Name of the job to be fetched\n        :type job_name: str\n        :param project_id: (Optional) the ID of the project that owns the Transfer\n            Job. If set to None or missing, the default project_id from the GCP\n            connection is used.\n        :type project_id: str\n        :return: Transfer Job\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .transferJobs()\n            .get(jobName=job_name, projectId=project_id)\n            .execute(num_retries=self.num_retries)\n        )","method_path":"airflow\/contrib\/hooks\/gcp_transfer_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GCPTransferServiceHook.list_transfer_job","method_code":"def list_transfer_job(self, filter):\n        conn = self.get_conn()\n        filter = self._inject_project_id(filter, FILTER, FILTER_PROJECT_ID)\n        request = conn.transferJobs().list(filter=json.dumps(filter))\n        jobs = []\n\n        while request is not None:\n            response = request.execute(num_retries=self.num_retries)\n            jobs.extend(response[TRANSFER_JOBS])\n\n            request = conn.transferJobs().list_next(previous_request=request, previous_response=response)\n\n        return jobs","method_summary":"Lists long-running operations in Google Storage Transfer Service that match the specified filter.","original_method_code":"def list_transfer_job(self, filter):\n        \"\"\"\n        Lists long-running operations in Google Storage Transfer\n        Service that match the specified filter.\n\n        :param filter: (Required) A request filter, as described in\n            https:\/\/cloud.google.com\/storage-transfer\/docs\/reference\/rest\/v1\/transferJobs\/list#body.QUERY_PARAMETERS.filter\n        :type filter: dict\n        :return: List of Transfer Jobs\n        :rtype: list[dict]\n        \"\"\"\n        conn = self.get_conn()\n        filter = self._inject_project_id(filter, FILTER, FILTER_PROJECT_ID)\n        request = conn.transferJobs().list(filter=json.dumps(filter))\n        jobs = []\n\n        while request is not None:\n            response = request.execute(num_retries=self.num_retries)\n            jobs.extend(response[TRANSFER_JOBS])\n\n            request = conn.transferJobs().list_next(previous_request=request, previous_response=response)\n\n        return jobs","method_path":"airflow\/contrib\/hooks\/gcp_transfer_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GCPTransferServiceHook.update_transfer_job","method_code":"def update_transfer_job(self, job_name, body):\n        body = self._inject_project_id(body, BODY, PROJECT_ID)\n        return (\n            self.get_conn()\n            .transferJobs()\n            .patch(jobName=job_name, body=body)\n            .execute(num_retries=self.num_retries)\n        )","method_summary":"Updates a transfer job that runs periodically.","original_method_code":"def update_transfer_job(self, job_name, body):\n        \"\"\"\n        Updates a transfer job that runs periodically.\n\n        :param job_name: (Required) Name of the job to be updated\n        :type job_name: str\n        :param body: A request body, as described in\n            https:\/\/cloud.google.com\/storage-transfer\/docs\/reference\/rest\/v1\/transferJobs\/patch#request-body\n        :type body: dict\n        :return: If successful, TransferJob.\n        :rtype: dict\n        \"\"\"\n        body = self._inject_project_id(body, BODY, PROJECT_ID)\n        return (\n            self.get_conn()\n            .transferJobs()\n            .patch(jobName=job_name, body=body)\n            .execute(num_retries=self.num_retries)\n        )","method_path":"airflow\/contrib\/hooks\/gcp_transfer_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GCPTransferServiceHook.cancel_transfer_operation","method_code":"def cancel_transfer_operation(self, operation_name):\n        self.get_conn().transferOperations().cancel(name=operation_name).execute(num_retries=self.num_retries)","method_summary":"Cancels an transfer operation in Google Storage Transfer Service.","original_method_code":"def cancel_transfer_operation(self, operation_name):\n        \"\"\"\n        Cancels an transfer operation in Google Storage Transfer Service.\n\n        :param operation_name: Name of the transfer operation.\n        :type operation_name: str\n        :rtype: None\n        \"\"\"\n        self.get_conn().transferOperations().cancel(name=operation_name).execute(num_retries=self.num_retries)","method_path":"airflow\/contrib\/hooks\/gcp_transfer_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GCPTransferServiceHook.pause_transfer_operation","method_code":"def pause_transfer_operation(self, operation_name):\n        self.get_conn().transferOperations().pause(name=operation_name).execute(num_retries=self.num_retries)","method_summary":"Pauses an transfer operation in Google Storage Transfer Service.","original_method_code":"def pause_transfer_operation(self, operation_name):\n        \"\"\"\n        Pauses an transfer operation in Google Storage Transfer Service.\n\n        :param operation_name: (Required) Name of the transfer operation.\n        :type operation_name: str\n        :rtype: None\n        \"\"\"\n        self.get_conn().transferOperations().pause(name=operation_name).execute(num_retries=self.num_retries)","method_path":"airflow\/contrib\/hooks\/gcp_transfer_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GCPTransferServiceHook.resume_transfer_operation","method_code":"def resume_transfer_operation(self, operation_name):\n        self.get_conn().transferOperations().resume(name=operation_name).execute(num_retries=self.num_retries)","method_summary":"Resumes an transfer operation in Google Storage Transfer Service.","original_method_code":"def resume_transfer_operation(self, operation_name):\n        \"\"\"\n        Resumes an transfer operation in Google Storage Transfer Service.\n\n        :param operation_name: (Required) Name of the transfer operation.\n        :type operation_name: str\n        :rtype: None\n        \"\"\"\n        self.get_conn().transferOperations().resume(name=operation_name).execute(num_retries=self.num_retries)","method_path":"airflow\/contrib\/hooks\/gcp_transfer_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GCPTransferServiceHook.wait_for_transfer_job","method_code":"def wait_for_transfer_job(self, job, expected_statuses=(GcpTransferOperationStatus.SUCCESS,), timeout=60):\n        while timeout > 0:\n            operations = self.list_transfer_operations(\n                filter={FILTER_PROJECT_ID: job[PROJECT_ID], FILTER_JOB_NAMES: [job[NAME]]}\n            )\n\n            if GCPTransferServiceHook.operations_contain_expected_statuses(operations, expected_statuses):\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n            timeout -= TIME_TO_SLEEP_IN_SECONDS\n        raise AirflowException(\"Timeout. The operation could not be completed within the allotted time.\")","method_summary":"Waits until the job reaches the expected state.","original_method_code":"def wait_for_transfer_job(self, job, expected_statuses=(GcpTransferOperationStatus.SUCCESS,), timeout=60):\n        \"\"\"\n        Waits until the job reaches the expected state.\n\n        :param job: Transfer job\n            See:\n            https:\/\/cloud.google.com\/storage-transfer\/docs\/reference\/rest\/v1\/transferJobs#TransferJob\n        :type job: dict\n        :param expected_statuses: State that is expected\n            See:\n            https:\/\/cloud.google.com\/storage-transfer\/docs\/reference\/rest\/v1\/transferOperations#Status\n        :type expected_statuses: set[str]\n        :param timeout:\n        :type timeout: time in which the operation must end in seconds\n        :rtype: None\n        \"\"\"\n        while timeout > 0:\n            operations = self.list_transfer_operations(\n                filter={FILTER_PROJECT_ID: job[PROJECT_ID], FILTER_JOB_NAMES: [job[NAME]]}\n            )\n\n            if GCPTransferServiceHook.operations_contain_expected_statuses(operations, expected_statuses):\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n            timeout -= TIME_TO_SLEEP_IN_SECONDS\n        raise AirflowException(\"Timeout. The operation could not be completed within the allotted time.\")","method_path":"airflow\/contrib\/hooks\/gcp_transfer_hook.py"}
{"repo_name":"apache\/airflow","method_name":"run_command","method_code":"def run_command(command):\n    process = subprocess.Popen(\n        shlex.split(command),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        close_fds=True)\n    output, stderr = [stream.decode(sys.getdefaultencoding(), 'ignore')\n                      for stream in process.communicate()]\n\n    if process.returncode != 0:\n        raise AirflowConfigException(\n            \"Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}\"\n            .format(command, process.returncode, output, stderr)\n        )\n\n    return output","method_summary":"Runs command and returns stdout","original_method_code":"def run_command(command):\n    \"\"\"\n    Runs command and returns stdout\n    \"\"\"\n    process = subprocess.Popen(\n        shlex.split(command),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        close_fds=True)\n    output, stderr = [stream.decode(sys.getdefaultencoding(), 'ignore')\n                      for stream in process.communicate()]\n\n    if process.returncode != 0:\n        raise AirflowConfigException(\n            \"Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}\"\n            .format(command, process.returncode, output, stderr)\n        )\n\n    return output","method_path":"airflow\/configuration.py"}
{"repo_name":"apache\/airflow","method_name":"AirflowConfigParser.remove_option","method_code":"def remove_option(self, section, option, remove_default=True):\n        if super().has_option(section, option):\n            super().remove_option(section, option)\n\n        if self.airflow_defaults.has_option(section, option) and remove_default:\n            self.airflow_defaults.remove_option(section, option)","method_summary":"Remove an option if it exists in config from a file or default config. If both of config have the same option, this removes the option in both configs unless remove_default=False.","original_method_code":"def remove_option(self, section, option, remove_default=True):\n        \"\"\"\n        Remove an option if it exists in config from a file or\n        default config. If both of config have the same option, this removes\n        the option in both configs unless remove_default=False.\n        \"\"\"\n        if super().has_option(section, option):\n            super().remove_option(section, option)\n\n        if self.airflow_defaults.has_option(section, option) and remove_default:\n            self.airflow_defaults.remove_option(section, option)","method_path":"airflow\/configuration.py"}
{"repo_name":"apache\/airflow","method_name":"DatastoreHook.allocate_ids","method_code":"def allocate_ids(self, partial_keys):\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})\n                .execute(num_retries=self.num_retries))\n\n        return resp['keys']","method_summary":"Allocate IDs for incomplete keys.","original_method_code":"def allocate_ids(self, partial_keys):\n        \"\"\"\n        Allocate IDs for incomplete keys.\n\n        .. seealso::\n            https:\/\/cloud.google.com\/datastore\/docs\/reference\/rest\/v1\/projects\/allocateIds\n\n        :param partial_keys: a list of partial keys.\n        :type partial_keys: list\n        :return: a list of full keys.\n        :rtype: list\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})\n                .execute(num_retries=self.num_retries))\n\n        return resp['keys']","method_path":"airflow\/contrib\/hooks\/datastore_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatastoreHook.begin_transaction","method_code":"def begin_transaction(self):\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .beginTransaction(projectId=self.project_id, body={})\n                .execute(num_retries=self.num_retries))\n\n        return resp['transaction']","method_summary":"Begins a new transaction.","original_method_code":"def begin_transaction(self):\n        \"\"\"\n        Begins a new transaction.\n\n        .. seealso::\n            https:\/\/cloud.google.com\/datastore\/docs\/reference\/rest\/v1\/projects\/beginTransaction\n\n        :return: a transaction handle.\n        :rtype: str\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .beginTransaction(projectId=self.project_id, body={})\n                .execute(num_retries=self.num_retries))\n\n        return resp['transaction']","method_path":"airflow\/contrib\/hooks\/datastore_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatastoreHook.commit","method_code":"def commit(self, body):\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .commit(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_summary":"Commit a transaction, optionally creating, deleting or modifying some entities.","original_method_code":"def commit(self, body):\n        \"\"\"\n        Commit a transaction, optionally creating, deleting or modifying some entities.\n\n        .. seealso::\n            https:\/\/cloud.google.com\/datastore\/docs\/reference\/rest\/v1\/projects\/commit\n\n        :param body: the body of the commit request.\n        :type body: dict\n        :return: the response body of the commit request.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .commit(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_path":"airflow\/contrib\/hooks\/datastore_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatastoreHook.lookup","method_code":"def lookup(self, keys, read_consistency=None, transaction=None):\n        conn = self.get_conn()\n\n        body = {'keys': keys}\n        if read_consistency:\n            body['readConsistency'] = read_consistency\n        if transaction:\n            body['transaction'] = transaction\n        resp = (conn\n                .projects()\n                .lookup(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_summary":"Lookup some entities by key.","original_method_code":"def lookup(self, keys, read_consistency=None, transaction=None):\n        \"\"\"\n        Lookup some entities by key.\n\n        .. seealso::\n            https:\/\/cloud.google.com\/datastore\/docs\/reference\/rest\/v1\/projects\/lookup\n\n        :param keys: the keys to lookup.\n        :type keys: list\n        :param read_consistency: the read consistency to use. default, strong or eventual.\n                                 Cannot be used with a transaction.\n        :type read_consistency: str\n        :param transaction: the transaction to use, if any.\n        :type transaction: str\n        :return: the response body of the lookup request.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        body = {'keys': keys}\n        if read_consistency:\n            body['readConsistency'] = read_consistency\n        if transaction:\n            body['transaction'] = transaction\n        resp = (conn\n                .projects()\n                .lookup(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_path":"airflow\/contrib\/hooks\/datastore_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatastoreHook.rollback","method_code":"def rollback(self, transaction):\n        conn = self.get_conn()\n\n        conn.projects().rollback(\n            projectId=self.project_id, body={'transaction': transaction}\n        ).execute(num_retries=self.num_retries)","method_summary":"Roll back a transaction.","original_method_code":"def rollback(self, transaction):\n        \"\"\"\n        Roll back a transaction.\n\n        .. seealso::\n            https:\/\/cloud.google.com\/datastore\/docs\/reference\/rest\/v1\/projects\/rollback\n\n        :param transaction: the transaction to roll back.\n        :type transaction: str\n        \"\"\"\n        conn = self.get_conn()\n\n        conn.projects().rollback(\n            projectId=self.project_id, body={'transaction': transaction}\n        ).execute(num_retries=self.num_retries)","method_path":"airflow\/contrib\/hooks\/datastore_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatastoreHook.run_query","method_code":"def run_query(self, body):\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .runQuery(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp['batch']","method_summary":"Run a query for entities.","original_method_code":"def run_query(self, body):\n        \"\"\"\n        Run a query for entities.\n\n        .. seealso::\n            https:\/\/cloud.google.com\/datastore\/docs\/reference\/rest\/v1\/projects\/runQuery\n\n        :param body: the body of the query request.\n        :type body: dict\n        :return: the batch of query results.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .runQuery(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp['batch']","method_path":"airflow\/contrib\/hooks\/datastore_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatastoreHook.get_operation","method_code":"def get_operation(self, name):\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .operations()\n                .get(name=name)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_summary":"Gets the latest state of a long-running operation.","original_method_code":"def get_operation(self, name):\n        \"\"\"\n        Gets the latest state of a long-running operation.\n\n        .. seealso::\n            https:\/\/cloud.google.com\/datastore\/docs\/reference\/data\/rest\/v1\/projects.operations\/get\n\n        :param name: the name of the operation resource.\n        :type name: str\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .operations()\n                .get(name=name)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_path":"airflow\/contrib\/hooks\/datastore_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatastoreHook.delete_operation","method_code":"def delete_operation(self, name):\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .operations()\n                .delete(name=name)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_summary":"Deletes the long-running operation.","original_method_code":"def delete_operation(self, name):\n        \"\"\"\n        Deletes the long-running operation.\n\n        .. seealso::\n            https:\/\/cloud.google.com\/datastore\/docs\/reference\/data\/rest\/v1\/projects.operations\/delete\n\n        :param name: the name of the operation resource.\n        :type name: str\n        :return: none if successful.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .operations()\n                .delete(name=name)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_path":"airflow\/contrib\/hooks\/datastore_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatastoreHook.poll_operation_until_done","method_code":"def poll_operation_until_done(self, name, polling_interval_in_seconds):\n        while True:\n            result = self.get_operation(name)\n\n            state = result['metadata']['common']['state']\n            if state == 'PROCESSING':\n                self.log.info('Operation is processing. Re-polling state in {} seconds'\n                              .format(polling_interval_in_seconds))\n                time.sleep(polling_interval_in_seconds)\n            else:\n                return result","method_summary":"Poll backup operation state until it's completed.","original_method_code":"def poll_operation_until_done(self, name, polling_interval_in_seconds):\n        \"\"\"\n        Poll backup operation state until it's completed.\n\n        :param name: the name of the operation resource\n        :type name: str\n        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.\n        :type polling_interval_in_seconds: int\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"\n        while True:\n            result = self.get_operation(name)\n\n            state = result['metadata']['common']['state']\n            if state == 'PROCESSING':\n                self.log.info('Operation is processing. Re-polling state in {} seconds'\n                              .format(polling_interval_in_seconds))\n                time.sleep(polling_interval_in_seconds)\n            else:\n                return result","method_path":"airflow\/contrib\/hooks\/datastore_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatastoreHook.export_to_storage_bucket","method_code":"def export_to_storage_bucket(self, bucket, namespace=None, entity_filter=None, labels=None):\n        admin_conn = self.get_conn()\n\n        output_uri_prefix = 'gs:\/\/' + '\/'.join(filter(None, [bucket, namespace]))\n        if not entity_filter:\n            entity_filter = {}\n        if not labels:\n            labels = {}\n        body = {\n            'outputUrlPrefix': output_uri_prefix,\n            'entityFilter': entity_filter,\n            'labels': labels,\n        }\n        resp = (admin_conn\n                .projects()\n                .export(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_summary":"Export entities from Cloud Datastore to Cloud Storage for backup.","original_method_code":"def export_to_storage_bucket(self, bucket, namespace=None, entity_filter=None, labels=None):\n        \"\"\"\n        Export entities from Cloud Datastore to Cloud Storage for backup.\n\n        .. note::\n            Keep in mind that this requests the Admin API not the Data API.\n\n        .. seealso::\n            https:\/\/cloud.google.com\/datastore\/docs\/reference\/admin\/rest\/v1\/projects\/export\n\n        :param bucket: The name of the Cloud Storage bucket.\n        :type bucket: str\n        :param namespace: The Cloud Storage namespace path.\n        :type namespace: str\n        :param entity_filter: Description of what data from the project is included in the export.\n        :type entity_filter: dict\n        :param labels: Client-assigned labels.\n        :type labels: dict of str\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"\n        admin_conn = self.get_conn()\n\n        output_uri_prefix = 'gs:\/\/' + '\/'.join(filter(None, [bucket, namespace]))\n        if not entity_filter:\n            entity_filter = {}\n        if not labels:\n            labels = {}\n        body = {\n            'outputUrlPrefix': output_uri_prefix,\n            'entityFilter': entity_filter,\n            'labels': labels,\n        }\n        resp = (admin_conn\n                .projects()\n                .export(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_path":"airflow\/contrib\/hooks\/datastore_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatastoreHook.import_from_storage_bucket","method_code":"def import_from_storage_bucket(self, bucket, file, namespace=None, entity_filter=None, labels=None):\n        admin_conn = self.get_conn()\n\n        input_url = 'gs:\/\/' + '\/'.join(filter(None, [bucket, namespace, file]))\n        if not entity_filter:\n            entity_filter = {}\n        if not labels:\n            labels = {}\n        body = {\n            'inputUrl': input_url,\n            'entityFilter': entity_filter,\n            'labels': labels,\n        }\n        resp = (admin_conn\n                .projects()\n                .import_(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_summary":"Import a backup from Cloud Storage to Cloud Datastore.","original_method_code":"def import_from_storage_bucket(self, bucket, file, namespace=None, entity_filter=None, labels=None):\n        \"\"\"\n        Import a backup from Cloud Storage to Cloud Datastore.\n\n        .. note::\n            Keep in mind that this requests the Admin API not the Data API.\n\n        .. seealso::\n            https:\/\/cloud.google.com\/datastore\/docs\/reference\/admin\/rest\/v1\/projects\/import\n\n        :param bucket: The name of the Cloud Storage bucket.\n        :type bucket: str\n        :param file: the metadata file written by the projects.export operation.\n        :type file: str\n        :param namespace: The Cloud Storage namespace path.\n        :type namespace: str\n        :param entity_filter: specify which kinds\/namespaces are to be imported.\n        :type entity_filter: dict\n        :param labels: Client-assigned labels.\n        :type labels: dict of str\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"\n        admin_conn = self.get_conn()\n\n        input_url = 'gs:\/\/' + '\/'.join(filter(None, [bucket, namespace, file]))\n        if not entity_filter:\n            entity_filter = {}\n        if not labels:\n            labels = {}\n        body = {\n            'inputUrl': input_url,\n            'entityFilter': entity_filter,\n            'labels': labels,\n        }\n        resp = (admin_conn\n                .projects()\n                .import_(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp","method_path":"airflow\/contrib\/hooks\/datastore_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AwsSnsHook.publish_to_target","method_code":"def publish_to_target(self, target_arn, message):\n        conn = self.get_conn()\n\n        messages = {\n            'default': message\n        }\n\n        return conn.publish(\n            TargetArn=target_arn,\n            Message=json.dumps(messages),\n            MessageStructure='json'\n        )","method_summary":"Publish a message to a topic or an endpoint.","original_method_code":"def publish_to_target(self, target_arn, message):\n        \"\"\"\n        Publish a message to a topic or an endpoint.\n\n        :param target_arn: either a TopicArn or an EndpointArn\n        :type target_arn: str\n        :param message: the default message you want to send\n        :param message: str\n        \"\"\"\n\n        conn = self.get_conn()\n\n        messages = {\n            'default': message\n        }\n\n        return conn.publish(\n            TargetArn=target_arn,\n            Message=json.dumps(messages),\n            MessageStructure='json'\n        )","method_path":"airflow\/contrib\/hooks\/aws_sns_hook.py"}
{"repo_name":"apache\/airflow","method_name":"get_hostname","method_code":"def get_hostname():\n    \n    try:\n        callable_path = conf.get('core', 'hostname_callable')\n    except AirflowConfigException:\n        callable_path = None\n\n    \n    \n    if not callable_path:\n        return socket.getfqdn()\n\n    \n    module_path, attr_name = callable_path.split(':')\n    module = importlib.import_module(module_path)\n    callable = getattr(module, attr_name)\n    return callable()","method_summary":"Fetch the hostname using the callable from the config or using `socket.getfqdn` as a fallback.","original_method_code":"def get_hostname():\n    \"\"\"\n    Fetch the hostname using the callable from the config or using\n    `socket.getfqdn` as a fallback.\n    \"\"\"\n    # First we attempt to fetch the callable path from the config.\n    try:\n        callable_path = conf.get('core', 'hostname_callable')\n    except AirflowConfigException:\n        callable_path = None\n\n    # Then we handle the case when the config is missing or empty. This is the\n    # default behavior.\n    if not callable_path:\n        return socket.getfqdn()\n\n    # Since we have a callable path, we try to import and run it next.\n    module_path, attr_name = callable_path.split(':')\n    module = importlib.import_module(module_path)\n    callable = getattr(module, attr_name)\n    return callable()","method_path":"airflow\/utils\/net.py"}
{"repo_name":"apache\/airflow","method_name":"CloudNaturalLanguageHook.get_conn","method_code":"def get_conn(self):\n        if not self._conn:\n            self._conn = LanguageServiceClient(credentials=self._get_credentials())\n        return self._conn","method_summary":"Retrieves connection to Cloud Natural Language service.","original_method_code":"def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Natural Language service.\n\n        :return: Cloud Natural Language service object\n        :rtype: google.cloud.language_v1.LanguageServiceClient\n        \"\"\"\n        if not self._conn:\n            self._conn = LanguageServiceClient(credentials=self._get_credentials())\n        return self._conn","method_path":"airflow\/contrib\/hooks\/gcp_natural_language_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudNaturalLanguageHook.analyze_entities","method_code":"def analyze_entities(self, document, encoding_type=None, retry=None, timeout=None, metadata=None):\n        client = self.get_conn()\n\n        return client.analyze_entities(\n            document=document, encoding_type=encoding_type, retry=retry, timeout=timeout, metadata=metadata\n        )","method_summary":"Finds named entities in the text along with entity types, salience, mentions for each entity, and other properties.","original_method_code":"def analyze_entities(self, document, encoding_type=None, retry=None, timeout=None, metadata=None):\n        \"\"\"\n        Finds named entities in the text along with entity types,\n        salience, mentions for each entity, and other properties.\n\n        :param document: Input document.\n            If a dict is provided, it must be of the same form as the protobuf message Document\n        :type document: dict or class google.cloud.language_v1.types.Document\n        :param encoding_type: The encoding type used by the API to calculate offsets.\n        :type encoding_type: google.cloud.language_v1.types.EncodingType\n        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n            retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n            retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Additional metadata that is provided to the method.\n        :type metadata: sequence[tuple[str, str]]]\n        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n        \"\"\"\n        client = self.get_conn()\n\n        return client.analyze_entities(\n            document=document, encoding_type=encoding_type, retry=retry, timeout=timeout, metadata=metadata\n        )","method_path":"airflow\/contrib\/hooks\/gcp_natural_language_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudNaturalLanguageHook.annotate_text","method_code":"def annotate_text(self, document, features, encoding_type=None, retry=None, timeout=None, metadata=None):\n        client = self.get_conn()\n\n        return client.annotate_text(\n            document=document,\n            features=features,\n            encoding_type=encoding_type,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )","method_summary":"A convenience method that provides all the features that analyzeSentiment, analyzeEntities, and analyzeSyntax provide in one call.","original_method_code":"def annotate_text(self, document, features, encoding_type=None, retry=None, timeout=None, metadata=None):\n        \"\"\"\n        A convenience method that provides all the features that analyzeSentiment,\n        analyzeEntities, and analyzeSyntax provide in one call.\n\n        :param document: Input document.\n            If a dict is provided, it must be of the same form as the protobuf message Document\n        :type document: dict or google.cloud.language_v1.types.Document\n        :param features: The enabled features.\n            If a dict is provided, it must be of the same form as the protobuf message Features\n        :type features: dict or google.cloud.language_v1.enums.Features\n        :param encoding_type: The encoding type used by the API to calculate offsets.\n        :type encoding_type: google.cloud.language_v1.types.EncodingType\n        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n            retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n            retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Additional metadata that is provided to the method.\n        :type metadata: sequence[tuple[str, str]]]\n        :rtype: google.cloud.language_v1.types.AnnotateTextResponse\n        \"\"\"\n        client = self.get_conn()\n\n        return client.annotate_text(\n            document=document,\n            features=features,\n            encoding_type=encoding_type,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )","method_path":"airflow\/contrib\/hooks\/gcp_natural_language_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudNaturalLanguageHook.classify_text","method_code":"def classify_text(self, document, retry=None, timeout=None, metadata=None):\n        client = self.get_conn()\n\n        return client.classify_text(document=document, retry=retry, timeout=timeout, metadata=metadata)","method_summary":"Classifies a document into categories.","original_method_code":"def classify_text(self, document, retry=None, timeout=None, metadata=None):\n        \"\"\"\n        Classifies a document into categories.\n\n        :param document: Input document.\n            If a dict is provided, it must be of the same form as the protobuf message Document\n        :type document: dict or class google.cloud.language_v1.types.Document\n        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n            retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n            retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Additional metadata that is provided to the method.\n        :type metadata: sequence[tuple[str, str]]]\n        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n        \"\"\"\n        client = self.get_conn()\n\n        return client.classify_text(document=document, retry=retry, timeout=timeout, metadata=metadata)","method_path":"airflow\/contrib\/hooks\/gcp_natural_language_hook.py"}
{"repo_name":"apache\/airflow","method_name":"get_template_field","method_code":"def get_template_field(env, fullname):\n    modname, classname = fullname.rsplit(\".\", 1)\n\n    try:\n        with mock(env.config.autodoc_mock_imports):\n            mod = import_module(modname)\n    except ImportError:\n        raise RoleException(\"Error loading %s module.\" % (modname, ))\n\n    clazz = getattr(mod, classname)\n    if not clazz:\n        raise RoleException(\"Error finding %s class in %s module.\" % (classname, modname))\n\n    template_fields = getattr(clazz, \"template_fields\")\n\n    if not template_fields:\n        raise RoleException(\n            \"Could not find the template fields for %s class in %s module.\" % (classname, modname)\n        )\n\n    return list(template_fields)","method_summary":"Gets template fields for specific operator class.","original_method_code":"def get_template_field(env, fullname):\n    \"\"\"\n    Gets template fields for specific operator class.\n\n    :param fullname: Full path to operator class.\n        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``\n    :return: List of template field\n    :rtype: list[str]\n    \"\"\"\n    modname, classname = fullname.rsplit(\".\", 1)\n\n    try:\n        with mock(env.config.autodoc_mock_imports):\n            mod = import_module(modname)\n    except ImportError:\n        raise RoleException(\"Error loading %s module.\" % (modname, ))\n\n    clazz = getattr(mod, classname)\n    if not clazz:\n        raise RoleException(\"Error finding %s class in %s module.\" % (classname, modname))\n\n    template_fields = getattr(clazz, \"template_fields\")\n\n    if not template_fields:\n        raise RoleException(\n            \"Could not find the template fields for %s class in %s module.\" % (classname, modname)\n        )\n\n    return list(template_fields)","method_path":"docs\/exts\/docroles.py"}
{"repo_name":"apache\/airflow","method_name":"dispose_orm","method_code":"def dispose_orm():\n    log.debug(\"Disposing DB connection pool (PID %s)\", os.getpid())\n    global engine\n    global Session\n\n    if Session:\n        Session.remove()\n        Session = None\n    if engine:\n        engine.dispose()\n        engine = None","method_summary":"Properly close pooled database connections","original_method_code":"def dispose_orm():\n    \"\"\" Properly close pooled database connections \"\"\"\n    log.debug(\"Disposing DB connection pool (PID %s)\", os.getpid())\n    global engine\n    global Session\n\n    if Session:\n        Session.remove()\n        Session = None\n    if engine:\n        engine.dispose()\n        engine = None","method_path":"airflow\/settings.py"}
{"repo_name":"apache\/airflow","method_name":"prepare_classpath","method_code":"def prepare_classpath():\n    if DAGS_FOLDER not in sys.path:\n        sys.path.append(DAGS_FOLDER)\n\n    \n    \n    config_path = os.path.join(AIRFLOW_HOME, 'config')\n    if config_path not in sys.path:\n        sys.path.append(config_path)\n\n    if PLUGINS_FOLDER not in sys.path:\n        sys.path.append(PLUGINS_FOLDER)","method_summary":"Ensures that certain subfolders of AIRFLOW_HOME are on the classpath","original_method_code":"def prepare_classpath():\n    \"\"\"\n    Ensures that certain subfolders of AIRFLOW_HOME are on the classpath\n    \"\"\"\n\n    if DAGS_FOLDER not in sys.path:\n        sys.path.append(DAGS_FOLDER)\n\n    # Add .\/config\/ for loading custom log parsers etc, or\n    # airflow_local_settings etc.\n    config_path = os.path.join(AIRFLOW_HOME, 'config')\n    if config_path not in sys.path:\n        sys.path.append(config_path)\n\n    if PLUGINS_FOLDER not in sys.path:\n        sys.path.append(PLUGINS_FOLDER)","method_path":"airflow\/settings.py"}
{"repo_name":"apache\/airflow","method_name":"CeleryQueueSensor._check_task_id","method_code":"def _check_task_id(self, context):\n        ti = context['ti']\n        celery_result = ti.xcom_pull(task_ids=self.target_task_id)\n        return celery_result.ready()","method_summary":"Gets the returned Celery result from the Airflow task ID provided to the sensor, and returns True if the celery result has been finished execution.","original_method_code":"def _check_task_id(self, context):\n        \"\"\"\n        Gets the returned Celery result from the Airflow task\n        ID provided to the sensor, and returns True if the\n        celery result has been finished execution.\n\n        :param context: Airflow's execution context\n        :type context: dict\n        :return: True if task has been executed, otherwise False\n        :rtype: bool\n        \"\"\"\n        ti = context['ti']\n        celery_result = ti.xcom_pull(task_ids=self.target_task_id)\n        return celery_result.ready()","method_path":"airflow\/contrib\/sensors\/celery_queue_sensor.py"}
{"repo_name":"apache\/airflow","method_name":"alchemy_to_dict","method_code":"def alchemy_to_dict(obj):\n    if not obj:\n        return None\n    d = {}\n    for c in obj.__table__.columns:\n        value = getattr(obj, c.name)\n        if type(value) == datetime:\n            value = value.isoformat()\n        d[c.name] = value\n    return d","method_summary":"Transforms a SQLAlchemy model instance into a dictionary","original_method_code":"def alchemy_to_dict(obj):\n    \"\"\"\n    Transforms a SQLAlchemy model instance into a dictionary\n    \"\"\"\n    if not obj:\n        return None\n    d = {}\n    for c in obj.__table__.columns:\n        value = getattr(obj, c.name)\n        if type(value) == datetime:\n            value = value.isoformat()\n        d[c.name] = value\n    return d","method_path":"airflow\/utils\/helpers.py"}
{"repo_name":"apache\/airflow","method_name":"chunks","method_code":"def chunks(items, chunk_size):\n    if chunk_size <= 0:\n        raise ValueError('Chunk size must be a positive integer')\n    for i in range(0, len(items), chunk_size):\n        yield items[i:i + chunk_size]","method_summary":"Yield successive chunks of a given size from a list of items","original_method_code":"def chunks(items, chunk_size):\n    \"\"\"\n    Yield successive chunks of a given size from a list of items\n    \"\"\"\n    if chunk_size <= 0:\n        raise ValueError('Chunk size must be a positive integer')\n    for i in range(0, len(items), chunk_size):\n        yield items[i:i + chunk_size]","method_path":"airflow\/utils\/helpers.py"}
{"repo_name":"apache\/airflow","method_name":"reduce_in_chunks","method_code":"def reduce_in_chunks(fn, iterable, initializer, chunk_size=0):\n    if len(iterable) == 0:\n        return initializer\n    if chunk_size == 0:\n        chunk_size = len(iterable)\n    return reduce(fn, chunks(iterable, chunk_size), initializer)","method_summary":"Reduce the given list of items by splitting it into chunks of the given size and passing each chunk through the reducer","original_method_code":"def reduce_in_chunks(fn, iterable, initializer, chunk_size=0):\n    \"\"\"\n    Reduce the given list of items by splitting it into chunks\n    of the given size and passing each chunk through the reducer\n    \"\"\"\n    if len(iterable) == 0:\n        return initializer\n    if chunk_size == 0:\n        chunk_size = len(iterable)\n    return reduce(fn, chunks(iterable, chunk_size), initializer)","method_path":"airflow\/utils\/helpers.py"}
{"repo_name":"apache\/airflow","method_name":"chain","method_code":"def chain(*tasks):\n    for up_task, down_task in zip(tasks[:-1], tasks[1:]):\n        up_task.set_downstream(down_task)","method_summary":"Given a number of tasks, builds a dependency chain. chain(task_1, task_2, task_3, task_4) is equivalent to task_1.set_downstream(task_2) task_2.set_downstream(task_3) task_3.set_downstream(task_4)","original_method_code":"def chain(*tasks):\n    \"\"\"\n    Given a number of tasks, builds a dependency chain.\n\n    chain(task_1, task_2, task_3, task_4)\n\n    is equivalent to\n\n    task_1.set_downstream(task_2)\n    task_2.set_downstream(task_3)\n    task_3.set_downstream(task_4)\n    \"\"\"\n    for up_task, down_task in zip(tasks[:-1], tasks[1:]):\n        up_task.set_downstream(down_task)","method_path":"airflow\/utils\/helpers.py"}
{"repo_name":"apache\/airflow","method_name":"render_log_filename","method_code":"def render_log_filename(ti, try_number, filename_template):\n    filename_template, filename_jinja_template = parse_template_string(filename_template)\n    if filename_jinja_template:\n        jinja_context = ti.get_template_context()\n        jinja_context['try_number'] = try_number\n        return filename_jinja_template.render(**jinja_context)\n\n    return filename_template.format(dag_id=ti.dag_id,\n                                    task_id=ti.task_id,\n                                    execution_date=ti.execution_date.isoformat(),\n                                    try_number=try_number)","method_summary":"Given task instance, try_number, filename_template, return the rendered log filename","original_method_code":"def render_log_filename(ti, try_number, filename_template):\n    \"\"\"\n    Given task instance, try_number, filename_template, return the rendered log\n    filename\n\n    :param ti: task instance\n    :param try_number: try_number of the task\n    :param filename_template: filename template, which can be jinja template or\n        python string template\n    \"\"\"\n    filename_template, filename_jinja_template = parse_template_string(filename_template)\n    if filename_jinja_template:\n        jinja_context = ti.get_template_context()\n        jinja_context['try_number'] = try_number\n        return filename_jinja_template.render(**jinja_context)\n\n    return filename_template.format(dag_id=ti.dag_id,\n                                    task_id=ti.task_id,\n                                    execution_date=ti.execution_date.isoformat(),\n                                    try_number=try_number)","method_path":"airflow\/utils\/helpers.py"}
{"repo_name":"apache\/airflow","method_name":"DataProcHook.wait","method_code":"def wait(self, operation):\n        submitted = _DataProcOperation(self.get_conn(), operation,\n                                       self.num_retries)\n        submitted.wait_for_done()","method_summary":"Awaits for Google Cloud Dataproc Operation to complete.","original_method_code":"def wait(self, operation):\n        \"\"\"Awaits for Google Cloud Dataproc Operation to complete.\"\"\"\n        submitted = _DataProcOperation(self.get_conn(), operation,\n                                       self.num_retries)\n        submitted.wait_for_done()","method_path":"airflow\/contrib\/hooks\/gcp_dataproc_hook.py"}
{"repo_name":"apache\/airflow","method_name":"_handle_databricks_operator_execution","method_code":"def _handle_databricks_operator_execution(operator, hook, log, context):\n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_ID_KEY, value=operator.run_id)\n    log.info('Run submitted with run_id: %s', operator.run_id)\n    run_page_url = hook.get_run_page_url(operator.run_id)\n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_PAGE_URL_KEY, value=run_page_url)\n\n    log.info('View run status, Spark UI, and logs at %s', run_page_url)\n    while True:\n        run_state = hook.get_run_state(operator.run_id)\n        if run_state.is_terminal:\n            if run_state.is_successful:\n                log.info('%s completed successfully.', operator.task_id)\n                log.info('View run status, Spark UI, and logs at %s', run_page_url)\n                return\n            else:\n                error_message = '{t} failed with terminal state: {s}'.format(\n                    t=operator.task_id,\n                    s=run_state)\n                raise AirflowException(error_message)\n        else:\n            log.info('%s in run state: %s', operator.task_id, run_state)\n            log.info('View run status, Spark UI, and logs at %s', run_page_url)\n            log.info('Sleeping for %s seconds.', operator.polling_period_seconds)\n            time.sleep(operator.polling_period_seconds)","method_summary":"Handles the Airflow + Databricks lifecycle logic for a Databricks operator","original_method_code":"def _handle_databricks_operator_execution(operator, hook, log, context):\n    \"\"\"\n    Handles the Airflow + Databricks lifecycle logic for a Databricks operator\n\n    :param operator: Databricks operator being handled\n    :param context: Airflow context\n    \"\"\"\n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_ID_KEY, value=operator.run_id)\n    log.info('Run submitted with run_id: %s', operator.run_id)\n    run_page_url = hook.get_run_page_url(operator.run_id)\n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_PAGE_URL_KEY, value=run_page_url)\n\n    log.info('View run status, Spark UI, and logs at %s', run_page_url)\n    while True:\n        run_state = hook.get_run_state(operator.run_id)\n        if run_state.is_terminal:\n            if run_state.is_successful:\n                log.info('%s completed successfully.', operator.task_id)\n                log.info('View run status, Spark UI, and logs at %s', run_page_url)\n                return\n            else:\n                error_message = '{t} failed with terminal state: {s}'.format(\n                    t=operator.task_id,\n                    s=run_state)\n                raise AirflowException(error_message)\n        else:\n            log.info('%s in run state: %s', operator.task_id, run_state)\n            log.info('View run status, Spark UI, and logs at %s', run_page_url)\n            log.info('Sleeping for %s seconds.', operator.polling_period_seconds)\n            time.sleep(operator.polling_period_seconds)","method_path":"airflow\/contrib\/operators\/databricks_operator.py"}
{"repo_name":"apache\/airflow","method_name":"PigCliHook.run_cli","method_code":"def run_cli(self, pig, verbose=True):\n        with TemporaryDirectory(prefix='airflow_pigop_') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir) as f:\n                f.write(pig.encode('utf-8'))\n                f.flush()\n                fname = f.name\n                pig_bin = 'pig'\n                cmd_extra = []\n\n                pig_cmd = [pig_bin, '-f', fname] + cmd_extra\n\n                if self.pig_properties:\n                    pig_properties_list = self.pig_properties.split()\n                    pig_cmd.extend(pig_properties_list)\n                if verbose:\n                    self.log.info(\"%s\", \" \".join(pig_cmd))\n                sp = subprocess.Popen(\n                    pig_cmd,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.STDOUT,\n                    cwd=tmp_dir,\n                    close_fds=True)\n                self.sp = sp\n                stdout = ''\n                for line in iter(sp.stdout.readline, b''):\n                    stdout += line.decode('utf-8')\n                    if verbose:\n                        self.log.info(line.strip())\n                sp.wait()\n\n                if sp.returncode:\n                    raise AirflowException(stdout)\n\n                return stdout","method_summary":"Run an pig script using the pig cli","original_method_code":"def run_cli(self, pig, verbose=True):\n        \"\"\"\n        Run an pig script using the pig cli\n\n        >>> ph = PigCliHook()\n        >>> result = ph.run_cli(\"ls \/;\")\n        >>> (\"hdfs:\/\/\" in result)\n        True\n        \"\"\"\n\n        with TemporaryDirectory(prefix='airflow_pigop_') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir) as f:\n                f.write(pig.encode('utf-8'))\n                f.flush()\n                fname = f.name\n                pig_bin = 'pig'\n                cmd_extra = []\n\n                pig_cmd = [pig_bin, '-f', fname] + cmd_extra\n\n                if self.pig_properties:\n                    pig_properties_list = self.pig_properties.split()\n                    pig_cmd.extend(pig_properties_list)\n                if verbose:\n                    self.log.info(\"%s\", \" \".join(pig_cmd))\n                sp = subprocess.Popen(\n                    pig_cmd,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.STDOUT,\n                    cwd=tmp_dir,\n                    close_fds=True)\n                self.sp = sp\n                stdout = ''\n                for line in iter(sp.stdout.readline, b''):\n                    stdout += line.decode('utf-8')\n                    if verbose:\n                        self.log.info(line.strip())\n                sp.wait()\n\n                if sp.returncode:\n                    raise AirflowException(stdout)\n\n                return stdout","method_path":"airflow\/hooks\/pig_hook.py"}
{"repo_name":"apache\/airflow","method_name":"fetch_celery_task_state","method_code":"def fetch_celery_task_state(celery_task):\n    try:\n        with timeout(seconds=2):\n            \n            \n            res = (celery_task[0], celery_task[1].state)\n    except Exception as e:\n        exception_traceback = \"Celery Task ID: {}\\n{}\".format(celery_task[0],\n                                                              traceback.format_exc())\n        res = ExceptionWithTraceback(e, exception_traceback)\n    return res","method_summary":"Fetch and return the state of the given celery task. The scope of this function is global so that it can be called by subprocesses in the pool.","original_method_code":"def fetch_celery_task_state(celery_task):\n    \"\"\"\n    Fetch and return the state of the given celery task. The scope of this function is\n    global so that it can be called by subprocesses in the pool.\n\n    :param celery_task: a tuple of the Celery task key and the async Celery object used\n        to fetch the task's state\n    :type celery_task: tuple(str, celery.result.AsyncResult)\n    :return: a tuple of the Celery task key and the Celery state of the task\n    :rtype: tuple[str, str]\n    \"\"\"\n\n    try:\n        with timeout(seconds=2):\n            # Accessing state property of celery task will make actual network request\n            # to get the current state of the task.\n            res = (celery_task[0], celery_task[1].state)\n    except Exception as e:\n        exception_traceback = \"Celery Task ID: {}\\n{}\".format(celery_task[0],\n                                                              traceback.format_exc())\n        res = ExceptionWithTraceback(e, exception_traceback)\n    return res","method_path":"airflow\/executors\/celery_executor.py"}
{"repo_name":"apache\/airflow","method_name":"CeleryExecutor._num_tasks_per_send_process","method_code":"def _num_tasks_per_send_process(self, to_send_count):\n        return max(1,\n                   int(math.ceil(1.0 * to_send_count \/ self._sync_parallelism)))","method_summary":"How many Celery tasks should each worker process send.","original_method_code":"def _num_tasks_per_send_process(self, to_send_count):\n        \"\"\"\n        How many Celery tasks should each worker process send.\n\n        :return: Number of tasks that should be sent per process\n        :rtype: int\n        \"\"\"\n        return max(1,\n                   int(math.ceil(1.0 * to_send_count \/ self._sync_parallelism)))","method_path":"airflow\/executors\/celery_executor.py"}
{"repo_name":"apache\/airflow","method_name":"CeleryExecutor._num_tasks_per_fetch_process","method_code":"def _num_tasks_per_fetch_process(self):\n        return max(1,\n                   int(math.ceil(1.0 * len(self.tasks) \/ self._sync_parallelism)))","method_summary":"How many Celery tasks should be sent to each worker process.","original_method_code":"def _num_tasks_per_fetch_process(self):\n        \"\"\"\n        How many Celery tasks should be sent to each worker process.\n\n        :return: Number of tasks that should be used per process\n        :rtype: int\n        \"\"\"\n        return max(1,\n                   int(math.ceil(1.0 * len(self.tasks) \/ self._sync_parallelism)))","method_path":"airflow\/executors\/celery_executor.py"}
{"repo_name":"apache\/airflow","method_name":"Variable.setdefault","method_code":"def setdefault(cls, key, default, deserialize_json=False):\n        obj = Variable.get(key, default_var=None,\n                           deserialize_json=deserialize_json)\n        if obj is None:\n            if default is not None:\n                Variable.set(key, default, serialize_json=deserialize_json)\n                return default\n            else:\n                raise ValueError('Default Value must be set')\n        else:\n            return obj","method_summary":"Like a Python builtin dict object, setdefault returns the current value for a key, and if it isn't there, stores the default value and returns it.","original_method_code":"def setdefault(cls, key, default, deserialize_json=False):\n        \"\"\"\n        Like a Python builtin dict object, setdefault returns the current value\n        for a key, and if it isn't there, stores the default value and returns it.\n\n        :param key: Dict key for this Variable\n        :type key: str\n        :param default: Default value to set and return if the variable\n            isn't already in the DB\n        :type default: Mixed\n        :param deserialize_json: Store this as a JSON encoded value in the DB\n            and un-encode it when retrieving a value\n        :return: Mixed\n        \"\"\"\n        obj = Variable.get(key, default_var=None,\n                           deserialize_json=deserialize_json)\n        if obj is None:\n            if default is not None:\n                Variable.set(key, default, serialize_json=deserialize_json)\n                return default\n            else:\n                raise ValueError('Default Value must be set')\n        else:\n            return obj","method_path":"airflow\/models\/variable.py"}
{"repo_name":"apache\/airflow","method_name":"MLEngineHook.create_job","method_code":"def create_job(self, project_id, job, use_existing_job_fn=None):\n        request = self._mlengine.projects().jobs().create(\n            parent='projects\/{}'.format(project_id),\n            body=job)\n        job_id = job['jobId']\n\n        try:\n            request.execute()\n        except HttpError as e:\n            \n            if e.resp.status == 409:\n                if use_existing_job_fn is not None:\n                    existing_job = self._get_job(project_id, job_id)\n                    if not use_existing_job_fn(existing_job):\n                        self.log.error(\n                            'Job with job_id %s already exist, but it does '\n                            'not match our expectation: %s',\n                            job_id, existing_job\n                        )\n                        raise\n                self.log.info(\n                    'Job with job_id %s already exist. Will waiting for it to finish',\n                    job_id\n                )\n            else:\n                self.log.error('Failed to create MLEngine job: {}'.format(e))\n                raise\n\n        return self._wait_for_job_done(project_id, job_id)","method_summary":"Launches a MLEngine job and wait for it to reach a terminal state.","original_method_code":"def create_job(self, project_id, job, use_existing_job_fn=None):\n        \"\"\"\n        Launches a MLEngine job and wait for it to reach a terminal state.\n\n        :param project_id: The Google Cloud project id within which MLEngine\n            job will be launched.\n        :type project_id: str\n\n        :param job: MLEngine Job object that should be provided to the MLEngine\n            API, such as: ::\n\n                {\n                  'jobId': 'my_job_id',\n                  'trainingInput': {\n                    'scaleTier': 'STANDARD_1',\n                    ...\n                  }\n                }\n\n        :type job: dict\n\n        :param use_existing_job_fn: In case that a MLEngine job with the same\n            job_id already exist, this method (if provided) will decide whether\n            we should use this existing job, continue waiting for it to finish\n            and returning the job object. It should accepts a MLEngine job\n            object, and returns a boolean value indicating whether it is OK to\n            reuse the existing job. If 'use_existing_job_fn' is not provided,\n            we by default reuse the existing MLEngine job.\n        :type use_existing_job_fn: function\n\n        :return: The MLEngine job object if the job successfully reach a\n            terminal state (which might be FAILED or CANCELLED state).\n        :rtype: dict\n        \"\"\"\n        request = self._mlengine.projects().jobs().create(\n            parent='projects\/{}'.format(project_id),\n            body=job)\n        job_id = job['jobId']\n\n        try:\n            request.execute()\n        except HttpError as e:\n            # 409 means there is an existing job with the same job ID.\n            if e.resp.status == 409:\n                if use_existing_job_fn is not None:\n                    existing_job = self._get_job(project_id, job_id)\n                    if not use_existing_job_fn(existing_job):\n                        self.log.error(\n                            'Job with job_id %s already exist, but it does '\n                            'not match our expectation: %s',\n                            job_id, existing_job\n                        )\n                        raise\n                self.log.info(\n                    'Job with job_id %s already exist. Will waiting for it to finish',\n                    job_id\n                )\n            else:\n                self.log.error('Failed to create MLEngine job: {}'.format(e))\n                raise\n\n        return self._wait_for_job_done(project_id, job_id)","method_path":"airflow\/contrib\/hooks\/gcp_mlengine_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MLEngineHook._get_job","method_code":"def _get_job(self, project_id, job_id):\n        job_name = 'projects\/{}\/jobs\/{}'.format(project_id, job_id)\n        request = self._mlengine.projects().jobs().get(name=job_name)\n        while True:\n            try:\n                return request.execute()\n            except HttpError as e:\n                if e.resp.status == 429:\n                    \n                    time.sleep(30)\n                else:\n                    self.log.error('Failed to get MLEngine job: {}'.format(e))\n                    raise","method_summary":"Gets a MLEngine job based on the job name.","original_method_code":"def _get_job(self, project_id, job_id):\n        \"\"\"\n        Gets a MLEngine job based on the job name.\n\n        :return: MLEngine job object if succeed.\n        :rtype: dict\n\n        Raises:\n            googleapiclient.errors.HttpError: if HTTP error is returned from server\n        \"\"\"\n        job_name = 'projects\/{}\/jobs\/{}'.format(project_id, job_id)\n        request = self._mlengine.projects().jobs().get(name=job_name)\n        while True:\n            try:\n                return request.execute()\n            except HttpError as e:\n                if e.resp.status == 429:\n                    # polling after 30 seconds when quota failure occurs\n                    time.sleep(30)\n                else:\n                    self.log.error('Failed to get MLEngine job: {}'.format(e))\n                    raise","method_path":"airflow\/contrib\/hooks\/gcp_mlengine_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MLEngineHook._wait_for_job_done","method_code":"def _wait_for_job_done(self, project_id, job_id, interval=30):\n        if interval <= 0:\n            raise ValueError(\"Interval must be > 0\")\n        while True:\n            job = self._get_job(project_id, job_id)\n            if job['state'] in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n                return job\n            time.sleep(interval)","method_summary":"Waits for the Job to reach a terminal state. This method will periodically check the job state until the job reach a terminal state.","original_method_code":"def _wait_for_job_done(self, project_id, job_id, interval=30):\n        \"\"\"\n        Waits for the Job to reach a terminal state.\n\n        This method will periodically check the job state until the job reach\n        a terminal state.\n\n        Raises:\n            googleapiclient.errors.HttpError: if HTTP error is returned when getting\n            the job\n        \"\"\"\n        if interval <= 0:\n            raise ValueError(\"Interval must be > 0\")\n        while True:\n            job = self._get_job(project_id, job_id)\n            if job['state'] in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n                return job\n            time.sleep(interval)","method_path":"airflow\/contrib\/hooks\/gcp_mlengine_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MLEngineHook.create_version","method_code":"def create_version(self, project_id, model_name, version_spec):\n        parent_name = 'projects\/{}\/models\/{}'.format(project_id, model_name)\n        create_request = self._mlengine.projects().models().versions().create(\n            parent=parent_name, body=version_spec)\n        response = create_request.execute()\n        get_request = self._mlengine.projects().operations().get(\n            name=response['name'])\n\n        return _poll_with_exponential_delay(\n            request=get_request,\n            max_n=9,\n            is_done_func=lambda resp: resp.get('done', False),\n            is_error_func=lambda resp: resp.get('error', None) is not None)","method_summary":"Creates the Version on Google Cloud ML Engine.","original_method_code":"def create_version(self, project_id, model_name, version_spec):\n        \"\"\"\n        Creates the Version on Google Cloud ML Engine.\n\n        Returns the operation if the version was created successfully and\n        raises an error otherwise.\n        \"\"\"\n        parent_name = 'projects\/{}\/models\/{}'.format(project_id, model_name)\n        create_request = self._mlengine.projects().models().versions().create(\n            parent=parent_name, body=version_spec)\n        response = create_request.execute()\n        get_request = self._mlengine.projects().operations().get(\n            name=response['name'])\n\n        return _poll_with_exponential_delay(\n            request=get_request,\n            max_n=9,\n            is_done_func=lambda resp: resp.get('done', False),\n            is_error_func=lambda resp: resp.get('error', None) is not None)","method_path":"airflow\/contrib\/hooks\/gcp_mlengine_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MLEngineHook.set_default_version","method_code":"def set_default_version(self, project_id, model_name, version_name):\n        full_version_name = 'projects\/{}\/models\/{}\/versions\/{}'.format(\n            project_id, model_name, version_name)\n        request = self._mlengine.projects().models().versions().setDefault(\n            name=full_version_name, body={})\n\n        try:\n            response = request.execute()\n            self.log.info('Successfully set version: %s to default', response)\n            return response\n        except HttpError as e:\n            self.log.error('Something went wrong: %s', e)\n            raise","method_summary":"Sets a version to be the default. Blocks until finished.","original_method_code":"def set_default_version(self, project_id, model_name, version_name):\n        \"\"\"\n        Sets a version to be the default. Blocks until finished.\n        \"\"\"\n        full_version_name = 'projects\/{}\/models\/{}\/versions\/{}'.format(\n            project_id, model_name, version_name)\n        request = self._mlengine.projects().models().versions().setDefault(\n            name=full_version_name, body={})\n\n        try:\n            response = request.execute()\n            self.log.info('Successfully set version: %s to default', response)\n            return response\n        except HttpError as e:\n            self.log.error('Something went wrong: %s', e)\n            raise","method_path":"airflow\/contrib\/hooks\/gcp_mlengine_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MLEngineHook.list_versions","method_code":"def list_versions(self, project_id, model_name):\n        result = []\n        full_parent_name = 'projects\/{}\/models\/{}'.format(\n            project_id, model_name)\n        request = self._mlengine.projects().models().versions().list(\n            parent=full_parent_name, pageSize=100)\n\n        response = request.execute()\n        next_page_token = response.get('nextPageToken', None)\n        result.extend(response.get('versions', []))\n        while next_page_token is not None:\n            next_request = self._mlengine.projects().models().versions().list(\n                parent=full_parent_name,\n                pageToken=next_page_token,\n                pageSize=100)\n            response = next_request.execute()\n            next_page_token = response.get('nextPageToken', None)\n            result.extend(response.get('versions', []))\n            time.sleep(5)\n        return result","method_summary":"Lists all available versions of a model. Blocks until finished.","original_method_code":"def list_versions(self, project_id, model_name):\n        \"\"\"\n        Lists all available versions of a model. Blocks until finished.\n        \"\"\"\n        result = []\n        full_parent_name = 'projects\/{}\/models\/{}'.format(\n            project_id, model_name)\n        request = self._mlengine.projects().models().versions().list(\n            parent=full_parent_name, pageSize=100)\n\n        response = request.execute()\n        next_page_token = response.get('nextPageToken', None)\n        result.extend(response.get('versions', []))\n        while next_page_token is not None:\n            next_request = self._mlengine.projects().models().versions().list(\n                parent=full_parent_name,\n                pageToken=next_page_token,\n                pageSize=100)\n            response = next_request.execute()\n            next_page_token = response.get('nextPageToken', None)\n            result.extend(response.get('versions', []))\n            time.sleep(5)\n        return result","method_path":"airflow\/contrib\/hooks\/gcp_mlengine_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MLEngineHook.delete_version","method_code":"def delete_version(self, project_id, model_name, version_name):\n        full_name = 'projects\/{}\/models\/{}\/versions\/{}'.format(\n            project_id, model_name, version_name)\n        delete_request = self._mlengine.projects().models().versions().delete(\n            name=full_name)\n        response = delete_request.execute()\n        get_request = self._mlengine.projects().operations().get(\n            name=response['name'])\n\n        return _poll_with_exponential_delay(\n            request=get_request,\n            max_n=9,\n            is_done_func=lambda resp: resp.get('done', False),\n            is_error_func=lambda resp: resp.get('error', None) is not None)","method_summary":"Deletes the given version of a model. Blocks until finished.","original_method_code":"def delete_version(self, project_id, model_name, version_name):\n        \"\"\"\n        Deletes the given version of a model. Blocks until finished.\n        \"\"\"\n        full_name = 'projects\/{}\/models\/{}\/versions\/{}'.format(\n            project_id, model_name, version_name)\n        delete_request = self._mlengine.projects().models().versions().delete(\n            name=full_name)\n        response = delete_request.execute()\n        get_request = self._mlengine.projects().operations().get(\n            name=response['name'])\n\n        return _poll_with_exponential_delay(\n            request=get_request,\n            max_n=9,\n            is_done_func=lambda resp: resp.get('done', False),\n            is_error_func=lambda resp: resp.get('error', None) is not None)","method_path":"airflow\/contrib\/hooks\/gcp_mlengine_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MLEngineHook.create_model","method_code":"def create_model(self, project_id, model):\n        if not model['name']:\n            raise ValueError(\"Model name must be provided and \"\n                             \"could not be an empty string\")\n        project = 'projects\/{}'.format(project_id)\n\n        request = self._mlengine.projects().models().create(\n            parent=project, body=model)\n        return request.execute()","method_summary":"Create a Model. Blocks until finished.","original_method_code":"def create_model(self, project_id, model):\n        \"\"\"\n        Create a Model. Blocks until finished.\n        \"\"\"\n        if not model['name']:\n            raise ValueError(\"Model name must be provided and \"\n                             \"could not be an empty string\")\n        project = 'projects\/{}'.format(project_id)\n\n        request = self._mlengine.projects().models().create(\n            parent=project, body=model)\n        return request.execute()","method_path":"airflow\/contrib\/hooks\/gcp_mlengine_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MLEngineHook.get_model","method_code":"def get_model(self, project_id, model_name):\n        if not model_name:\n            raise ValueError(\"Model name must be provided and \"\n                             \"it could not be an empty string\")\n        full_model_name = 'projects\/{}\/models\/{}'.format(\n            project_id, model_name)\n        request = self._mlengine.projects().models().get(name=full_model_name)\n        try:\n            return request.execute()\n        except HttpError as e:\n            if e.resp.status == 404:\n                self.log.error('Model was not found: %s', e)\n                return None\n            raise","method_summary":"Gets a Model. Blocks until finished.","original_method_code":"def get_model(self, project_id, model_name):\n        \"\"\"\n        Gets a Model. Blocks until finished.\n        \"\"\"\n        if not model_name:\n            raise ValueError(\"Model name must be provided and \"\n                             \"it could not be an empty string\")\n        full_model_name = 'projects\/{}\/models\/{}'.format(\n            project_id, model_name)\n        request = self._mlengine.projects().models().get(name=full_model_name)\n        try:\n            return request.execute()\n        except HttpError as e:\n            if e.resp.status == 404:\n                self.log.error('Model was not found: %s', e)\n                return None\n            raise","method_path":"airflow\/contrib\/hooks\/gcp_mlengine_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AwsDynamoDBHook.write_batch_data","method_code":"def write_batch_data(self, items):\n        dynamodb_conn = self.get_conn()\n\n        try:\n            table = dynamodb_conn.Table(self.table_name)\n\n            with table.batch_writer(overwrite_by_pkeys=self.table_keys) as batch:\n                for item in items:\n                    batch.put_item(Item=item)\n            return True\n        except Exception as general_error:\n            raise AirflowException(\n                'Failed to insert items in dynamodb, error: {error}'.format(\n                    error=str(general_error)\n                )\n            )","method_summary":"Write batch items to dynamodb table with provisioned throughout capacity.","original_method_code":"def write_batch_data(self, items):\n        \"\"\"\n        Write batch items to dynamodb table with provisioned throughout capacity.\n        \"\"\"\n\n        dynamodb_conn = self.get_conn()\n\n        try:\n            table = dynamodb_conn.Table(self.table_name)\n\n            with table.batch_writer(overwrite_by_pkeys=self.table_keys) as batch:\n                for item in items:\n                    batch.put_item(Item=item)\n            return True\n        except Exception as general_error:\n            raise AirflowException(\n                'Failed to insert items in dynamodb, error: {error}'.format(\n                    error=str(general_error)\n                )\n            )","method_path":"airflow\/contrib\/hooks\/aws_dynamodb_hook.py"}
{"repo_name":"apache\/airflow","method_name":"_integrate_plugins","method_code":"def _integrate_plugins():\n    from airflow.plugins_manager import executors_modules\n    for executors_module in executors_modules:\n        sys.modules[executors_module.__name__] = executors_module\n        globals()[executors_module._name] = executors_module","method_summary":"Integrate plugins to the context.","original_method_code":"def _integrate_plugins():\n    \"\"\"Integrate plugins to the context.\"\"\"\n    from airflow.plugins_manager import executors_modules\n    for executors_module in executors_modules:\n        sys.modules[executors_module.__name__] = executors_module\n        globals()[executors_module._name] = executors_module","method_path":"airflow\/executors\/__init__.py"}
{"repo_name":"apache\/airflow","method_name":"get_default_executor","method_code":"def get_default_executor():\n    global DEFAULT_EXECUTOR\n\n    if DEFAULT_EXECUTOR is not None:\n        return DEFAULT_EXECUTOR\n\n    executor_name = configuration.conf.get('core', 'EXECUTOR')\n\n    DEFAULT_EXECUTOR = _get_executor(executor_name)\n\n    log = LoggingMixin().log\n    log.info(\"Using executor %s\", executor_name)\n\n    return DEFAULT_EXECUTOR","method_summary":"Creates a new instance of the configured executor if none exists and returns it","original_method_code":"def get_default_executor():\n    \"\"\"Creates a new instance of the configured executor if none exists and returns it\"\"\"\n    global DEFAULT_EXECUTOR\n\n    if DEFAULT_EXECUTOR is not None:\n        return DEFAULT_EXECUTOR\n\n    executor_name = configuration.conf.get('core', 'EXECUTOR')\n\n    DEFAULT_EXECUTOR = _get_executor(executor_name)\n\n    log = LoggingMixin().log\n    log.info(\"Using executor %s\", executor_name)\n\n    return DEFAULT_EXECUTOR","method_path":"airflow\/executors\/__init__.py"}
{"repo_name":"apache\/airflow","method_name":"_get_executor","method_code":"def _get_executor(executor_name):\n    if executor_name == Executors.LocalExecutor:\n        return LocalExecutor()\n    elif executor_name == Executors.SequentialExecutor:\n        return SequentialExecutor()\n    elif executor_name == Executors.CeleryExecutor:\n        from airflow.executors.celery_executor import CeleryExecutor\n        return CeleryExecutor()\n    elif executor_name == Executors.DaskExecutor:\n        from airflow.executors.dask_executor import DaskExecutor\n        return DaskExecutor()\n    elif executor_name == Executors.KubernetesExecutor:\n        from airflow.contrib.executors.kubernetes_executor import KubernetesExecutor\n        return KubernetesExecutor()\n    else:\n        \n        _integrate_plugins()\n        executor_path = executor_name.split('.')\n        if len(executor_path) != 2:\n            raise AirflowException(\n                \"Executor {0} not supported: \"\n                \"please specify in format plugin_module.executor\".format(executor_name))\n\n        if executor_path[0] in globals():\n            return globals()[executor_path[0]].__dict__[executor_path[1]]()\n        else:\n            raise AirflowException(\"Executor {0} not supported.\".format(executor_name))","method_summary":"Creates a new instance of the named executor. In case the executor name is not know in airflow, look for it in the plugins","original_method_code":"def _get_executor(executor_name):\n    \"\"\"\n    Creates a new instance of the named executor.\n    In case the executor name is not know in airflow,\n    look for it in the plugins\n    \"\"\"\n    if executor_name == Executors.LocalExecutor:\n        return LocalExecutor()\n    elif executor_name == Executors.SequentialExecutor:\n        return SequentialExecutor()\n    elif executor_name == Executors.CeleryExecutor:\n        from airflow.executors.celery_executor import CeleryExecutor\n        return CeleryExecutor()\n    elif executor_name == Executors.DaskExecutor:\n        from airflow.executors.dask_executor import DaskExecutor\n        return DaskExecutor()\n    elif executor_name == Executors.KubernetesExecutor:\n        from airflow.contrib.executors.kubernetes_executor import KubernetesExecutor\n        return KubernetesExecutor()\n    else:\n        # Loading plugins\n        _integrate_plugins()\n        executor_path = executor_name.split('.')\n        if len(executor_path) != 2:\n            raise AirflowException(\n                \"Executor {0} not supported: \"\n                \"please specify in format plugin_module.executor\".format(executor_name))\n\n        if executor_path[0] in globals():\n            return globals()[executor_path[0]].__dict__[executor_path[1]]()\n        else:\n            raise AirflowException(\"Executor {0} not supported.\".format(executor_name))","method_path":"airflow\/executors\/__init__.py"}
{"repo_name":"apache\/airflow","method_name":"SegmentHook.on_error","method_code":"def on_error(self, error, items):\n        self.log.error('Encountered Segment error: {segment_error} with '\n                       'items: {with_items}'.format(segment_error=error,\n                                                    with_items=items))\n        raise AirflowException('Segment error: {}'.format(error))","method_summary":"Handles error callbacks when using Segment with segment_debug_mode set to True","original_method_code":"def on_error(self, error, items):\n        \"\"\"\n        Handles error callbacks when using Segment with segment_debug_mode set to True\n        \"\"\"\n        self.log.error('Encountered Segment error: {segment_error} with '\n                       'items: {with_items}'.format(segment_error=error,\n                                                    with_items=items))\n        raise AirflowException('Segment error: {}'.format(error))","method_path":"airflow\/contrib\/hooks\/segment_hook.py"}
{"repo_name":"apache\/airflow","method_name":"trigger_dag","method_code":"def trigger_dag(dag_id):\n    data = request.get_json(force=True)\n\n    run_id = None\n    if 'run_id' in data:\n        run_id = data['run_id']\n\n    conf = None\n    if 'conf' in data:\n        conf = data['conf']\n\n    execution_date = None\n    if 'execution_date' in data and data['execution_date'] is not None:\n        execution_date = data['execution_date']\n\n        \n        try:\n            execution_date = timezone.parse(execution_date)\n        except ValueError:\n            error_message = (\n                'Given execution date, {}, could not be identified '\n                'as a date. Example date format: 2015-11-16T14:34:15+00:00'\n                .format(execution_date))\n            _log.info(error_message)\n            response = jsonify({'error': error_message})\n            response.status_code = 400\n\n            return response\n\n    try:\n        dr = trigger.trigger_dag(dag_id, run_id, conf, execution_date)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n\n    if getattr(g, 'user', None):\n        _log.info(\"User %s created %s\", g.user, dr)\n\n    response = jsonify(message=\"Created {}\".format(dr))\n    return response","method_summary":"Trigger a new dag run for a Dag with an execution date of now unless specified in the data.","original_method_code":"def trigger_dag(dag_id):\n    \"\"\"\n    Trigger a new dag run for a Dag with an execution date of now unless\n    specified in the data.\n    \"\"\"\n    data = request.get_json(force=True)\n\n    run_id = None\n    if 'run_id' in data:\n        run_id = data['run_id']\n\n    conf = None\n    if 'conf' in data:\n        conf = data['conf']\n\n    execution_date = None\n    if 'execution_date' in data and data['execution_date'] is not None:\n        execution_date = data['execution_date']\n\n        # Convert string datetime into actual datetime\n        try:\n            execution_date = timezone.parse(execution_date)\n        except ValueError:\n            error_message = (\n                'Given execution date, {}, could not be identified '\n                'as a date. Example date format: 2015-11-16T14:34:15+00:00'\n                .format(execution_date))\n            _log.info(error_message)\n            response = jsonify({'error': error_message})\n            response.status_code = 400\n\n            return response\n\n    try:\n        dr = trigger.trigger_dag(dag_id, run_id, conf, execution_date)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n\n    if getattr(g, 'user', None):\n        _log.info(\"User %s created %s\", g.user, dr)\n\n    response = jsonify(message=\"Created {}\".format(dr))\n    return response","method_path":"airflow\/www\/api\/experimental\/endpoints.py"}
{"repo_name":"apache\/airflow","method_name":"delete_dag","method_code":"def delete_dag(dag_id):\n    try:\n        count = delete.delete_dag(dag_id)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    return jsonify(message=\"Removed {} record(s)\".format(count), count=count)","method_summary":"Delete all DB records related to the specified Dag.","original_method_code":"def delete_dag(dag_id):\n    \"\"\"\n    Delete all DB records related to the specified Dag.\n    \"\"\"\n    try:\n        count = delete.delete_dag(dag_id)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    return jsonify(message=\"Removed {} record(s)\".format(count), count=count)","method_path":"airflow\/www\/api\/experimental\/endpoints.py"}
{"repo_name":"apache\/airflow","method_name":"get_pools","method_code":"def get_pools():\n    try:\n        pools = pool_api.get_pools()\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify([p.to_json() for p in pools])","method_summary":"Get all pools.","original_method_code":"def get_pools():\n    \"\"\"Get all pools.\"\"\"\n    try:\n        pools = pool_api.get_pools()\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify([p.to_json() for p in pools])","method_path":"airflow\/www\/api\/experimental\/endpoints.py"}
{"repo_name":"apache\/airflow","method_name":"create_pool","method_code":"def create_pool():\n    params = request.get_json(force=True)\n    try:\n        pool = pool_api.create_pool(**params)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify(pool.to_json())","method_summary":"Create a pool.","original_method_code":"def create_pool():\n    \"\"\"Create a pool.\"\"\"\n    params = request.get_json(force=True)\n    try:\n        pool = pool_api.create_pool(**params)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify(pool.to_json())","method_path":"airflow\/www\/api\/experimental\/endpoints.py"}
{"repo_name":"apache\/airflow","method_name":"delete_pool","method_code":"def delete_pool(name):\n    try:\n        pool = pool_api.delete_pool(name=name)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify(pool.to_json())","method_summary":"Delete pool.","original_method_code":"def delete_pool(name):\n    \"\"\"Delete pool.\"\"\"\n    try:\n        pool = pool_api.delete_pool(name=name)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify(pool.to_json())","method_path":"airflow\/www\/api\/experimental\/endpoints.py"}
{"repo_name":"apache\/airflow","method_name":"AzureContainerInstanceHook.create_or_update","method_code":"def create_or_update(self, resource_group, name, container_group):\n        self.connection.container_groups.create_or_update(resource_group,\n                                                          name,\n                                                          container_group)","method_summary":"Create a new container group","original_method_code":"def create_or_update(self, resource_group, name, container_group):\n        \"\"\"\n        Create a new container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        :param container_group: the properties of the container group\n        :type container_group: azure.mgmt.containerinstance.models.ContainerGroup\n        \"\"\"\n        self.connection.container_groups.create_or_update(resource_group,\n                                                          name,\n                                                          container_group)","method_path":"airflow\/contrib\/hooks\/azure_container_instance_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureContainerInstanceHook.get_state_exitcode_details","method_code":"def get_state_exitcode_details(self, resource_group, name):\n        current_state = self._get_instance_view(resource_group, name).current_state\n        return (current_state.state,\n                current_state.exit_code,\n                current_state.detail_status)","method_summary":"Get the state and exitcode of a container group","original_method_code":"def get_state_exitcode_details(self, resource_group, name):\n        \"\"\"\n        Get the state and exitcode of a container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        :return: A tuple with the state, exitcode, and details.\n            If the exitcode is unknown 0 is returned.\n        :rtype: tuple(state,exitcode,details)\n        \"\"\"\n        current_state = self._get_instance_view(resource_group, name).current_state\n        return (current_state.state,\n                current_state.exit_code,\n                current_state.detail_status)","method_path":"airflow\/contrib\/hooks\/azure_container_instance_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureContainerInstanceHook.get_messages","method_code":"def get_messages(self, resource_group, name):\n        instance_view = self._get_instance_view(resource_group, name)\n\n        return [event.message for event in instance_view.events]","method_summary":"Get the messages of a container group","original_method_code":"def get_messages(self, resource_group, name):\n        \"\"\"\n        Get the messages of a container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        :return: A list of the event messages\n        :rtype: list[str]\n        \"\"\"\n        instance_view = self._get_instance_view(resource_group, name)\n\n        return [event.message for event in instance_view.events]","method_path":"airflow\/contrib\/hooks\/azure_container_instance_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureContainerInstanceHook.get_logs","method_code":"def get_logs(self, resource_group, name, tail=1000):\n        logs = self.connection.container.list_logs(resource_group, name, name, tail=tail)\n        return logs.content.splitlines(True)","method_summary":"Get the tail from logs of a container group","original_method_code":"def get_logs(self, resource_group, name, tail=1000):\n        \"\"\"\n        Get the tail from logs of a container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        :param tail: the size of the tail\n        :type tail: int\n        :return: A list of log messages\n        :rtype: list[str]\n        \"\"\"\n        logs = self.connection.container.list_logs(resource_group, name, name, tail=tail)\n        return logs.content.splitlines(True)","method_path":"airflow\/contrib\/hooks\/azure_container_instance_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureContainerInstanceHook.delete","method_code":"def delete(self, resource_group, name):\n        self.connection.container_groups.delete(resource_group, name)","method_summary":"Delete a container group","original_method_code":"def delete(self, resource_group, name):\n        \"\"\"\n        Delete a container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        \"\"\"\n        self.connection.container_groups.delete(resource_group, name)","method_path":"airflow\/contrib\/hooks\/azure_container_instance_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureContainerInstanceHook.exists","method_code":"def exists(self, resource_group, name):\n        for container in self.connection.container_groups.list_by_resource_group(resource_group):\n            if container.name == name:\n                return True\n        return False","method_summary":"Test if a container group exists","original_method_code":"def exists(self, resource_group, name):\n        \"\"\"\n        Test if a container group exists\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        \"\"\"\n        for container in self.connection.container_groups.list_by_resource_group(resource_group):\n            if container.name == name:\n                return True\n        return False","method_path":"airflow\/contrib\/hooks\/azure_container_instance_hook.py"}
{"repo_name":"apache\/airflow","method_name":"apply_defaults","method_code":"def apply_defaults(func):\n    \n    \n    \n    \n    sig_cache = signature(func)\n    non_optional_args = {\n        name for (name, param) in sig_cache.parameters.items()\n        if param.default == param.empty and\n        param.name != 'self' and\n        param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)}\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if len(args) > 1:\n            raise AirflowException(\n                \"Use keyword arguments when initializing operators\")\n        dag_args = {}\n        dag_params = {}\n\n        dag = kwargs.get('dag', None) or settings.CONTEXT_MANAGER_DAG\n        if dag:\n            dag_args = copy(dag.default_args) or {}\n            dag_params = copy(dag.params) or {}\n\n        params = {}\n        if 'params' in kwargs:\n            params = kwargs['params']\n        dag_params.update(params)\n\n        default_args = {}\n        if 'default_args' in kwargs:\n            default_args = kwargs['default_args']\n            if 'params' in default_args:\n                dag_params.update(default_args['params'])\n                del default_args['params']\n\n        dag_args.update(default_args)\n        default_args = dag_args\n\n        for arg in sig_cache.parameters:\n            if arg not in kwargs and arg in default_args:\n                kwargs[arg] = default_args[arg]\n        missing_args = list(non_optional_args - set(kwargs))\n        if missing_args:\n            msg = \"Argument {0} is required\".format(missing_args)\n            raise AirflowException(msg)\n\n        kwargs['params'] = dag_params\n\n        result = func(*args, **kwargs)\n        return result\n    return wrapper","method_summary":"Function decorator that Looks for an argument named \"default_args\", and fills the unspecified arguments from it. Since python2.","original_method_code":"def apply_defaults(func):\n    \"\"\"\n    Function decorator that Looks for an argument named \"default_args\", and\n    fills the unspecified arguments from it.\n\n    Since python2.* isn't clear about which arguments are missing when\n    calling a function, and that this can be quite confusing with multi-level\n    inheritance and argument defaults, this decorator also alerts with\n    specific information about the missing arguments.\n    \"\"\"\n\n    # Cache inspect.signature for the wrapper closure to avoid calling it\n    # at every decorated invocation. This is separate sig_cache created\n    # per decoration, i.e. each function decorated using apply_defaults will\n    # have a different sig_cache.\n    sig_cache = signature(func)\n    non_optional_args = {\n        name for (name, param) in sig_cache.parameters.items()\n        if param.default == param.empty and\n        param.name != 'self' and\n        param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)}\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if len(args) > 1:\n            raise AirflowException(\n                \"Use keyword arguments when initializing operators\")\n        dag_args = {}\n        dag_params = {}\n\n        dag = kwargs.get('dag', None) or settings.CONTEXT_MANAGER_DAG\n        if dag:\n            dag_args = copy(dag.default_args) or {}\n            dag_params = copy(dag.params) or {}\n\n        params = {}\n        if 'params' in kwargs:\n            params = kwargs['params']\n        dag_params.update(params)\n\n        default_args = {}\n        if 'default_args' in kwargs:\n            default_args = kwargs['default_args']\n            if 'params' in default_args:\n                dag_params.update(default_args['params'])\n                del default_args['params']\n\n        dag_args.update(default_args)\n        default_args = dag_args\n\n        for arg in sig_cache.parameters:\n            if arg not in kwargs and arg in default_args:\n                kwargs[arg] = default_args[arg]\n        missing_args = list(non_optional_args - set(kwargs))\n        if missing_args:\n            msg = \"Argument {0} is required\".format(missing_args)\n            raise AirflowException(msg)\n\n        kwargs['params'] = dag_params\n\n        result = func(*args, **kwargs)\n        return result\n    return wrapper","method_path":"airflow\/utils\/decorators.py"}
{"repo_name":"apache\/airflow","method_name":"HiveToDruidTransfer.construct_ingest_query","method_code":"def construct_ingest_query(self, static_path, columns):\n        \n        \n        \n        num_shards = self.num_shards\n        target_partition_size = self.target_partition_size\n        if self.target_partition_size == -1:\n            if self.num_shards == -1:\n                target_partition_size = DEFAULT_TARGET_PARTITION_SIZE\n        else:\n            num_shards = -1\n\n        metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']\n\n        \n        \n        dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]\n\n        ingest_query_dict = {\n            \"type\": \"index_hadoop\",\n            \"spec\": {\n                \"dataSchema\": {\n                    \"metricsSpec\": self.metric_spec,\n                    \"granularitySpec\": {\n                        \"queryGranularity\": self.query_granularity,\n                        \"intervals\": self.intervals,\n                        \"type\": \"uniform\",\n                        \"segmentGranularity\": self.segment_granularity,\n                    },\n                    \"parser\": {\n                        \"type\": \"string\",\n                        \"parseSpec\": {\n                            \"columns\": columns,\n                            \"dimensionsSpec\": {\n                                \"dimensionExclusions\": [],\n                                \"dimensions\": dimensions,  \n                                \"spatialDimensions\": []\n                            },\n                            \"timestampSpec\": {\n                                \"column\": self.ts_dim,\n                                \"format\": \"auto\"\n                            },\n                            \"format\": \"tsv\"\n                        }\n                    },\n                    \"dataSource\": self.druid_datasource\n                },\n                \"tuningConfig\": {\n                    \"type\": \"hadoop\",\n                    \"jobProperties\": {\n                        \"mapreduce.job.user.classpath.first\": \"false\",\n                        \"mapreduce.map.output.compress\": \"false\",\n                        \"mapreduce.output.fileoutputformat.compress\": \"false\",\n                    },\n                    \"partitionsSpec\": {\n                        \"type\": \"hashed\",\n                        \"targetPartitionSize\": target_partition_size,\n                        \"numShards\": num_shards,\n                    },\n                },\n                \"ioConfig\": {\n                    \"inputSpec\": {\n                        \"paths\": static_path,\n                        \"type\": \"static\"\n                    },\n                    \"type\": \"hadoop\"\n                }\n            }\n        }\n\n        if self.job_properties:\n            ingest_query_dict['spec']['tuningConfig']['jobProperties'] \\\n                .update(self.job_properties)\n\n        if self.hadoop_dependency_coordinates:\n            ingest_query_dict['hadoopDependencyCoordinates'] \\\n                = self.hadoop_dependency_coordinates\n\n        return ingest_query_dict","method_summary":"Builds an ingest query for an HDFS TSV load.","original_method_code":"def construct_ingest_query(self, static_path, columns):\n        \"\"\"\n        Builds an ingest query for an HDFS TSV load.\n\n        :param static_path: The path on hdfs where the data is\n        :type static_path: str\n        :param columns: List of all the columns that are available\n        :type columns: list\n        \"\"\"\n\n        # backward compatibility for num_shards,\n        # but target_partition_size is the default setting\n        # and overwrites the num_shards\n        num_shards = self.num_shards\n        target_partition_size = self.target_partition_size\n        if self.target_partition_size == -1:\n            if self.num_shards == -1:\n                target_partition_size = DEFAULT_TARGET_PARTITION_SIZE\n        else:\n            num_shards = -1\n\n        metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']\n\n        # Take all the columns, which are not the time dimension\n        # or a metric, as the dimension columns\n        dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]\n\n        ingest_query_dict = {\n            \"type\": \"index_hadoop\",\n            \"spec\": {\n                \"dataSchema\": {\n                    \"metricsSpec\": self.metric_spec,\n                    \"granularitySpec\": {\n                        \"queryGranularity\": self.query_granularity,\n                        \"intervals\": self.intervals,\n                        \"type\": \"uniform\",\n                        \"segmentGranularity\": self.segment_granularity,\n                    },\n                    \"parser\": {\n                        \"type\": \"string\",\n                        \"parseSpec\": {\n                            \"columns\": columns,\n                            \"dimensionsSpec\": {\n                                \"dimensionExclusions\": [],\n                                \"dimensions\": dimensions,  # list of names\n                                \"spatialDimensions\": []\n                            },\n                            \"timestampSpec\": {\n                                \"column\": self.ts_dim,\n                                \"format\": \"auto\"\n                            },\n                            \"format\": \"tsv\"\n                        }\n                    },\n                    \"dataSource\": self.druid_datasource\n                },\n                \"tuningConfig\": {\n                    \"type\": \"hadoop\",\n                    \"jobProperties\": {\n                        \"mapreduce.job.user.classpath.first\": \"false\",\n                        \"mapreduce.map.output.compress\": \"false\",\n                        \"mapreduce.output.fileoutputformat.compress\": \"false\",\n                    },\n                    \"partitionsSpec\": {\n                        \"type\": \"hashed\",\n                        \"targetPartitionSize\": target_partition_size,\n                        \"numShards\": num_shards,\n                    },\n                },\n                \"ioConfig\": {\n                    \"inputSpec\": {\n                        \"paths\": static_path,\n                        \"type\": \"static\"\n                    },\n                    \"type\": \"hadoop\"\n                }\n            }\n        }\n\n        if self.job_properties:\n            ingest_query_dict['spec']['tuningConfig']['jobProperties'] \\\n                .update(self.job_properties)\n\n        if self.hadoop_dependency_coordinates:\n            ingest_query_dict['hadoopDependencyCoordinates'] \\\n                = self.hadoop_dependency_coordinates\n\n        return ingest_query_dict","method_path":"airflow\/operators\/hive_to_druid.py"}
{"repo_name":"apache\/airflow","method_name":"RedisPubSubSensor.poke","method_code":"def poke(self, context):\n        self.log.info('RedisPubSubSensor checking for message on channels: %s', self.channels)\n\n        message = self.pubsub.get_message()\n        self.log.info('Message %s from channel %s', message, self.channels)\n\n        \n        if message and message['type'] == 'message':\n\n            context['ti'].xcom_push(key='message', value=message)\n            self.pubsub.unsubscribe(self.channels)\n\n            return True\n\n        return False","method_summary":"Check for message on subscribed channels and write to xcom the message with key ``message`` An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``","original_method_code":"def poke(self, context):\n        \"\"\"\n        Check for message on subscribed channels and write to xcom the message with key ``message``\n\n        An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``\n\n        :param context: the context object\n        :type context: dict\n        :return: ``True`` if message (with type 'message') is available or ``False`` if not\n        \"\"\"\n        self.log.info('RedisPubSubSensor checking for message on channels: %s', self.channels)\n\n        message = self.pubsub.get_message()\n        self.log.info('Message %s from channel %s', message, self.channels)\n\n        # Process only message types\n        if message and message['type'] == 'message':\n\n            context['ti'].xcom_push(key='message', value=message)\n            self.pubsub.unsubscribe(self.channels)\n\n            return True\n\n        return False","method_path":"airflow\/contrib\/sensors\/redis_pub_sub_sensor.py"}
{"repo_name":"apache\/airflow","method_name":"DagRun.get_previous_dagrun","method_code":"def get_previous_dagrun(self, session=None):\n        return session.query(DagRun).filter(\n            DagRun.dag_id == self.dag_id,\n            DagRun.execution_date < self.execution_date\n        ).order_by(\n            DagRun.execution_date.desc()\n        ).first()","method_summary":"The previous DagRun, if there is one","original_method_code":"def get_previous_dagrun(self, session=None):\n        \"\"\"The previous DagRun, if there is one\"\"\"\n\n        return session.query(DagRun).filter(\n            DagRun.dag_id == self.dag_id,\n            DagRun.execution_date < self.execution_date\n        ).order_by(\n            DagRun.execution_date.desc()\n        ).first()","method_path":"airflow\/models\/dagrun.py"}
{"repo_name":"apache\/airflow","method_name":"DagRun.get_previous_scheduled_dagrun","method_code":"def get_previous_scheduled_dagrun(self, session=None):\n        dag = self.get_dag()\n\n        return session.query(DagRun).filter(\n            DagRun.dag_id == self.dag_id,\n            DagRun.execution_date == dag.previous_schedule(self.execution_date)\n        ).first()","method_summary":"The previous, SCHEDULED DagRun, if there is one","original_method_code":"def get_previous_scheduled_dagrun(self, session=None):\n        \"\"\"The previous, SCHEDULED DagRun, if there is one\"\"\"\n        dag = self.get_dag()\n\n        return session.query(DagRun).filter(\n            DagRun.dag_id == self.dag_id,\n            DagRun.execution_date == dag.previous_schedule(self.execution_date)\n        ).first()","method_path":"airflow\/models\/dagrun.py"}
{"repo_name":"apache\/airflow","method_name":"DagRun.update_state","method_code":"def update_state(self, session=None):\n        dag = self.get_dag()\n\n        tis = self.get_task_instances(session=session)\n        self.log.debug(\"Updating state for %s considering %s task(s)\", self, len(tis))\n\n        for ti in list(tis):\n            \n            if ti.state == State.REMOVED:\n                tis.remove(ti)\n            else:\n                ti.task = dag.get_task(ti.task_id)\n\n        \n        \n        start_dttm = timezone.utcnow()\n        unfinished_tasks = self.get_task_instances(\n            state=State.unfinished(),\n            session=session\n        )\n        none_depends_on_past = all(not t.task.depends_on_past for t in unfinished_tasks)\n        none_task_concurrency = all(t.task.task_concurrency is None\n                                    for t in unfinished_tasks)\n        \n        if unfinished_tasks and none_depends_on_past and none_task_concurrency:\n            \n            no_dependencies_met = True\n            for ut in unfinished_tasks:\n                \n                \n                old_state = ut.state\n                deps_met = ut.are_dependencies_met(\n                    dep_context=DepContext(\n                        flag_upstream_failed=True,\n                        ignore_in_retry_period=True,\n                        ignore_in_reschedule_period=True),\n                    session=session)\n                if deps_met or old_state != ut.current_state(session=session):\n                    no_dependencies_met = False\n                    break\n\n        duration = (timezone.utcnow() - start_dttm).total_seconds() * 1000\n        Stats.timing(\"dagrun.dependency-check.{}\".format(self.dag_id), duration)\n\n        root_ids = [t.task_id for t in dag.roots]\n        roots = [t for t in tis if t.task_id in root_ids]\n\n        \n        if (not unfinished_tasks and\n                any(r.state in (State.FAILED, State.UPSTREAM_FAILED) for r in roots)):\n            self.log.info('Marking run %s failed', self)\n            self.set_state(State.FAILED)\n            dag.handle_callback(self, success=False, reason='task_failure',\n                                session=session)\n\n        \n        elif not unfinished_tasks and all(r.state in (State.SUCCESS, State.SKIPPED)\n                                          for r in roots):\n            self.log.info('Marking run %s successful', self)\n            self.set_state(State.SUCCESS)\n            dag.handle_callback(self, success=True, reason='success', session=session)\n\n        \n        elif (unfinished_tasks and none_depends_on_past and\n              none_task_concurrency and no_dependencies_met):\n            self.log.info('Deadlock; marking run %s failed', self)\n            self.set_state(State.FAILED)\n            dag.handle_callback(self, success=False, reason='all_tasks_deadlocked',\n                                session=session)\n\n        \n        else:\n            self.set_state(State.RUNNING)\n\n        self._emit_duration_stats_for_finished_state()\n\n        \n        session.merge(self)\n        session.commit()\n\n        return self.state","method_summary":"Determines the overall state of the DagRun based on the state of its TaskInstances.","original_method_code":"def update_state(self, session=None):\n        \"\"\"\n        Determines the overall state of the DagRun based on the state\n        of its TaskInstances.\n\n        :return: State\n        \"\"\"\n\n        dag = self.get_dag()\n\n        tis = self.get_task_instances(session=session)\n        self.log.debug(\"Updating state for %s considering %s task(s)\", self, len(tis))\n\n        for ti in list(tis):\n            # skip in db?\n            if ti.state == State.REMOVED:\n                tis.remove(ti)\n            else:\n                ti.task = dag.get_task(ti.task_id)\n\n        # pre-calculate\n        # db is faster\n        start_dttm = timezone.utcnow()\n        unfinished_tasks = self.get_task_instances(\n            state=State.unfinished(),\n            session=session\n        )\n        none_depends_on_past = all(not t.task.depends_on_past for t in unfinished_tasks)\n        none_task_concurrency = all(t.task.task_concurrency is None\n                                    for t in unfinished_tasks)\n        # small speed up\n        if unfinished_tasks and none_depends_on_past and none_task_concurrency:\n            # todo: this can actually get pretty slow: one task costs between 0.01-015s\n            no_dependencies_met = True\n            for ut in unfinished_tasks:\n                # We need to flag upstream and check for changes because upstream\n                # failures\/re-schedules can result in deadlock false positives\n                old_state = ut.state\n                deps_met = ut.are_dependencies_met(\n                    dep_context=DepContext(\n                        flag_upstream_failed=True,\n                        ignore_in_retry_period=True,\n                        ignore_in_reschedule_period=True),\n                    session=session)\n                if deps_met or old_state != ut.current_state(session=session):\n                    no_dependencies_met = False\n                    break\n\n        duration = (timezone.utcnow() - start_dttm).total_seconds() * 1000\n        Stats.timing(\"dagrun.dependency-check.{}\".format(self.dag_id), duration)\n\n        root_ids = [t.task_id for t in dag.roots]\n        roots = [t for t in tis if t.task_id in root_ids]\n\n        # if all roots finished and at least one failed, the run failed\n        if (not unfinished_tasks and\n                any(r.state in (State.FAILED, State.UPSTREAM_FAILED) for r in roots)):\n            self.log.info('Marking run %s failed', self)\n            self.set_state(State.FAILED)\n            dag.handle_callback(self, success=False, reason='task_failure',\n                                session=session)\n\n        # if all roots succeeded and no unfinished tasks, the run succeeded\n        elif not unfinished_tasks and all(r.state in (State.SUCCESS, State.SKIPPED)\n                                          for r in roots):\n            self.log.info('Marking run %s successful', self)\n            self.set_state(State.SUCCESS)\n            dag.handle_callback(self, success=True, reason='success', session=session)\n\n        # if *all tasks* are deadlocked, the run failed\n        elif (unfinished_tasks and none_depends_on_past and\n              none_task_concurrency and no_dependencies_met):\n            self.log.info('Deadlock; marking run %s failed', self)\n            self.set_state(State.FAILED)\n            dag.handle_callback(self, success=False, reason='all_tasks_deadlocked',\n                                session=session)\n\n        # finally, if the roots aren't done, the dag is still running\n        else:\n            self.set_state(State.RUNNING)\n\n        self._emit_duration_stats_for_finished_state()\n\n        # todo: determine we want to use with_for_update to make sure to lock the run\n        session.merge(self)\n        session.commit()\n\n        return self.state","method_path":"airflow\/models\/dagrun.py"}
{"repo_name":"apache\/airflow","method_name":"DagRun.verify_integrity","method_code":"def verify_integrity(self, session=None):\n        from airflow.models.taskinstance import TaskInstance  \n\n        dag = self.get_dag()\n        tis = self.get_task_instances(session=session)\n\n        \n        task_ids = []\n        for ti in tis:\n            task_ids.append(ti.task_id)\n            task = None\n            try:\n                task = dag.get_task(ti.task_id)\n            except AirflowException:\n                if ti.state == State.REMOVED:\n                    pass  \n                elif self.state is not State.RUNNING and not dag.partial:\n                    self.log.warning(\"Failed to get task '{}' for dag '{}'. \"\n                                     \"Marking it as removed.\".format(ti, dag))\n                    Stats.incr(\n                        \"task_removed_from_dag.{}\".format(dag.dag_id), 1, 1)\n                    ti.state = State.REMOVED\n\n            is_task_in_dag = task is not None\n            should_restore_task = is_task_in_dag and ti.state == State.REMOVED\n            if should_restore_task:\n                self.log.info(\"Restoring task '{}' which was previously \"\n                              \"removed from DAG '{}'\".format(ti, dag))\n                Stats.incr(\"task_restored_to_dag.{}\".format(dag.dag_id), 1, 1)\n                ti.state = State.NONE\n\n        \n        for task in six.itervalues(dag.task_dict):\n            if task.start_date > self.execution_date and not self.is_backfill:\n                continue\n\n            if task.task_id not in task_ids:\n                Stats.incr(\n                    \"task_instance_created-{}\".format(task.__class__.__name__),\n                    1, 1)\n                ti = TaskInstance(task, self.execution_date)\n                session.add(ti)\n\n        session.commit()","method_summary":"Verifies the DagRun by checking for removed tasks or tasks that are not in the database yet. It will set state to removed or add the task if required.","original_method_code":"def verify_integrity(self, session=None):\n        \"\"\"\n        Verifies the DagRun by checking for removed tasks or tasks that are not in the\n        database yet. It will set state to removed or add the task if required.\n        \"\"\"\n        from airflow.models.taskinstance import TaskInstance  # Avoid circular import\n\n        dag = self.get_dag()\n        tis = self.get_task_instances(session=session)\n\n        # check for removed or restored tasks\n        task_ids = []\n        for ti in tis:\n            task_ids.append(ti.task_id)\n            task = None\n            try:\n                task = dag.get_task(ti.task_id)\n            except AirflowException:\n                if ti.state == State.REMOVED:\n                    pass  # ti has already been removed, just ignore it\n                elif self.state is not State.RUNNING and not dag.partial:\n                    self.log.warning(\"Failed to get task '{}' for dag '{}'. \"\n                                     \"Marking it as removed.\".format(ti, dag))\n                    Stats.incr(\n                        \"task_removed_from_dag.{}\".format(dag.dag_id), 1, 1)\n                    ti.state = State.REMOVED\n\n            is_task_in_dag = task is not None\n            should_restore_task = is_task_in_dag and ti.state == State.REMOVED\n            if should_restore_task:\n                self.log.info(\"Restoring task '{}' which was previously \"\n                              \"removed from DAG '{}'\".format(ti, dag))\n                Stats.incr(\"task_restored_to_dag.{}\".format(dag.dag_id), 1, 1)\n                ti.state = State.NONE\n\n        # check for missing tasks\n        for task in six.itervalues(dag.task_dict):\n            if task.start_date > self.execution_date and not self.is_backfill:\n                continue\n\n            if task.task_id not in task_ids:\n                Stats.incr(\n                    \"task_instance_created-{}\".format(task.__class__.__name__),\n                    1, 1)\n                ti = TaskInstance(task, self.execution_date)\n                session.add(ti)\n\n        session.commit()","method_path":"airflow\/models\/dagrun.py"}
{"repo_name":"apache\/airflow","method_name":"jenkins_request_with_headers","method_code":"def jenkins_request_with_headers(jenkins_server, req):\n    try:\n        response = jenkins_server.jenkins_request(req)\n        response_body = response.content\n        response_headers = response.headers\n        if response_body is None:\n            raise jenkins.EmptyResponseException(\n                \"Error communicating with server[%s]: \"\n                \"empty response\" % jenkins_server.server)\n        return {'body': response_body.decode('utf-8'), 'headers': response_headers}\n    except HTTPError as e:\n        \n        \n        if e.code in [401, 403, 500]:\n            \n            \n            \n            \n            raise JenkinsException(\n                'Error in request. ' +\n                'Possibly authentication failed [%s]: %s' % (\n                    e.code, e.msg)\n            )\n        elif e.code == 404:\n            raise jenkins.NotFoundException('Requested item could not be found')\n        else:\n            raise\n    except socket.timeout as e:\n        raise jenkins.TimeoutException('Error in request: %s' % e)\n    except URLError as e:\n        \n        \n        if str(e.reason) == \"timed out\":\n            raise jenkins.TimeoutException('Error in request: %s' % e.reason)\n        raise JenkinsException('Error in request: %s' % e.reason)","method_summary":"We need to get the headers in addition to the body answer to get the location from them This function uses jenkins_request method from python-jenkins library with just the return call changed","original_method_code":"def jenkins_request_with_headers(jenkins_server, req):\n    \"\"\"\n    We need to get the headers in addition to the body answer\n    to get the location from them\n    This function uses jenkins_request method from python-jenkins library\n    with just the return call changed\n\n    :param jenkins_server: The server to query\n    :param req: The request to execute\n    :return: Dict containing the response body (key body)\n        and the headers coming along (headers)\n    \"\"\"\n    try:\n        response = jenkins_server.jenkins_request(req)\n        response_body = response.content\n        response_headers = response.headers\n        if response_body is None:\n            raise jenkins.EmptyResponseException(\n                \"Error communicating with server[%s]: \"\n                \"empty response\" % jenkins_server.server)\n        return {'body': response_body.decode('utf-8'), 'headers': response_headers}\n    except HTTPError as e:\n        # Jenkins's funky authentication means its nigh impossible to\n        # distinguish errors.\n        if e.code in [401, 403, 500]:\n            # six.moves.urllib.error.HTTPError provides a 'reason'\n            # attribute for all python version except for ver 2.6\n            # Falling back to HTTPError.msg since it contains the\n            # same info as reason\n            raise JenkinsException(\n                'Error in request. ' +\n                'Possibly authentication failed [%s]: %s' % (\n                    e.code, e.msg)\n            )\n        elif e.code == 404:\n            raise jenkins.NotFoundException('Requested item could not be found')\n        else:\n            raise\n    except socket.timeout as e:\n        raise jenkins.TimeoutException('Error in request: %s' % e)\n    except URLError as e:\n        # python 2.6 compatibility to ensure same exception raised\n        # since URLError wraps a socket timeout on python 2.6.\n        if str(e.reason) == \"timed out\":\n            raise jenkins.TimeoutException('Error in request: %s' % e.reason)\n        raise JenkinsException('Error in request: %s' % e.reason)","method_path":"airflow\/contrib\/operators\/jenkins_job_trigger_operator.py"}
{"repo_name":"apache\/airflow","method_name":"conditionally_trigger","method_code":"def conditionally_trigger(context, dag_run_obj):\n    c_p = context['params']['condition_param']\n    print(\"Controller DAG : conditionally_trigger = {}\".format(c_p))\n    if context['params']['condition_param']:\n        dag_run_obj.payload = {'message': context['params']['message']}\n        pp.pprint(dag_run_obj.payload)\n        return dag_run_obj","method_summary":"This function decides whether or not to Trigger the remote DAG","original_method_code":"def conditionally_trigger(context, dag_run_obj):\n    \"\"\"This function decides whether or not to Trigger the remote DAG\"\"\"\n    c_p = context['params']['condition_param']\n    print(\"Controller DAG : conditionally_trigger = {}\".format(c_p))\n    if context['params']['condition_param']:\n        dag_run_obj.payload = {'message': context['params']['message']}\n        pp.pprint(dag_run_obj.payload)\n        return dag_run_obj","method_path":"airflow\/example_dags\/example_trigger_controller_dag.py"}
{"repo_name":"apache\/airflow","method_name":"DatadogHook.send_metric","method_code":"def send_metric(self, metric_name, datapoint, tags=None, type_=None, interval=None):\n        response = api.Metric.send(\n            metric=metric_name,\n            points=datapoint,\n            host=self.host,\n            tags=tags,\n            type=type_,\n            interval=interval)\n\n        self.validate_response(response)\n        return response","method_summary":"Sends a single datapoint metric to DataDog","original_method_code":"def send_metric(self, metric_name, datapoint, tags=None, type_=None, interval=None):\n        \"\"\"\n        Sends a single datapoint metric to DataDog\n\n        :param metric_name: The name of the metric\n        :type metric_name: str\n        :param datapoint: A single integer or float related to the metric\n        :type datapoint: int or float\n        :param tags: A list of tags associated with the metric\n        :type tags: list\n        :param type_: Type of your metric: gauge, rate, or count\n        :type type_: str\n        :param interval: If the type of the metric is rate or count, define the corresponding interval\n        :type interval: int\n        \"\"\"\n        response = api.Metric.send(\n            metric=metric_name,\n            points=datapoint,\n            host=self.host,\n            tags=tags,\n            type=type_,\n            interval=interval)\n\n        self.validate_response(response)\n        return response","method_path":"airflow\/contrib\/hooks\/datadog_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatadogHook.query_metric","method_code":"def query_metric(self,\n                     query,\n                     from_seconds_ago,\n                     to_seconds_ago):\n        now = int(time.time())\n\n        response = api.Metric.query(\n            start=now - from_seconds_ago,\n            end=now - to_seconds_ago,\n            query=query)\n\n        self.validate_response(response)\n        return response","method_summary":"Queries datadog for a specific metric, potentially with some function applied to it and returns the results.","original_method_code":"def query_metric(self,\n                     query,\n                     from_seconds_ago,\n                     to_seconds_ago):\n        \"\"\"\n        Queries datadog for a specific metric, potentially with some\n        function applied to it and returns the results.\n\n        :param query: The datadog query to execute (see datadog docs)\n        :type query: str\n        :param from_seconds_ago: How many seconds ago to start querying for.\n        :type from_seconds_ago: int\n        :param to_seconds_ago: Up to how many seconds ago to query for.\n        :type to_seconds_ago: int\n        \"\"\"\n        now = int(time.time())\n\n        response = api.Metric.query(\n            start=now - from_seconds_ago,\n            end=now - to_seconds_ago,\n            query=query)\n\n        self.validate_response(response)\n        return response","method_path":"airflow\/contrib\/hooks\/datadog_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DagBag.get_dag","method_code":"def get_dag(self, dag_id):\n        from airflow.models.dag import DagModel  \n\n        \n        root_dag_id = dag_id\n        if dag_id in self.dags:\n            dag = self.dags[dag_id]\n            if dag.is_subdag:\n                root_dag_id = dag.parent_dag.dag_id\n\n        \n        orm_dag = DagModel.get_current(root_dag_id)\n        if orm_dag and (\n                root_dag_id not in self.dags or\n                (\n                    orm_dag.last_expired and\n                    dag.last_loaded < orm_dag.last_expired\n                )\n        ):\n            \n            found_dags = self.process_file(\n                filepath=orm_dag.fileloc, only_if_updated=False)\n\n            \n            if found_dags and dag_id in [found_dag.dag_id for found_dag in found_dags]:\n                return self.dags[dag_id]\n            elif dag_id in self.dags:\n                del self.dags[dag_id]\n        return self.dags.get(dag_id)","method_summary":"Gets the DAG out of the dictionary, and refreshes it if expired","original_method_code":"def get_dag(self, dag_id):\n        \"\"\"\n        Gets the DAG out of the dictionary, and refreshes it if expired\n        \"\"\"\n        from airflow.models.dag import DagModel  # Avoid circular import\n\n        # If asking for a known subdag, we want to refresh the parent\n        root_dag_id = dag_id\n        if dag_id in self.dags:\n            dag = self.dags[dag_id]\n            if dag.is_subdag:\n                root_dag_id = dag.parent_dag.dag_id\n\n        # If the dag corresponding to root_dag_id is absent or expired\n        orm_dag = DagModel.get_current(root_dag_id)\n        if orm_dag and (\n                root_dag_id not in self.dags or\n                (\n                    orm_dag.last_expired and\n                    dag.last_loaded < orm_dag.last_expired\n                )\n        ):\n            # Reprocess source file\n            found_dags = self.process_file(\n                filepath=orm_dag.fileloc, only_if_updated=False)\n\n            # If the source file no longer exports `dag_id`, delete it from self.dags\n            if found_dags and dag_id in [found_dag.dag_id for found_dag in found_dags]:\n                return self.dags[dag_id]\n            elif dag_id in self.dags:\n                del self.dags[dag_id]\n        return self.dags.get(dag_id)","method_path":"airflow\/models\/dagbag.py"}
{"repo_name":"apache\/airflow","method_name":"DagBag.kill_zombies","method_code":"def kill_zombies(self, zombies, session=None):\n        from airflow.models.taskinstance import TaskInstance  \n\n        for zombie in zombies:\n            if zombie.dag_id in self.dags:\n                dag = self.dags[zombie.dag_id]\n                if zombie.task_id in dag.task_ids:\n                    task = dag.get_task(zombie.task_id)\n                    ti = TaskInstance(task, zombie.execution_date)\n                    \n                    ti.start_date = zombie.start_date\n                    ti.end_date = zombie.end_date\n                    ti.try_number = zombie.try_number\n                    ti.state = zombie.state\n                    ti.test_mode = configuration.getboolean('core', 'unit_test_mode')\n                    ti.handle_failure(\"{} detected as zombie\".format(ti),\n                                      ti.test_mode, ti.get_template_context())\n                    self.log.info(\n                        'Marked zombie job %s as %s', ti, ti.state)\n                    Stats.incr('zombies_killed')\n        session.commit()","method_summary":"Fail given zombie tasks, which are tasks that haven't had a heartbeat for too long, in the current DagBag.","original_method_code":"def kill_zombies(self, zombies, session=None):\n        \"\"\"\n        Fail given zombie tasks, which are tasks that haven't\n        had a heartbeat for too long, in the current DagBag.\n\n        :param zombies: zombie task instances to kill.\n        :type zombies: airflow.utils.dag_processing.SimpleTaskInstance\n        :param session: DB session.\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n        from airflow.models.taskinstance import TaskInstance  # Avoid circular import\n\n        for zombie in zombies:\n            if zombie.dag_id in self.dags:\n                dag = self.dags[zombie.dag_id]\n                if zombie.task_id in dag.task_ids:\n                    task = dag.get_task(zombie.task_id)\n                    ti = TaskInstance(task, zombie.execution_date)\n                    # Get properties needed for failure handling from SimpleTaskInstance.\n                    ti.start_date = zombie.start_date\n                    ti.end_date = zombie.end_date\n                    ti.try_number = zombie.try_number\n                    ti.state = zombie.state\n                    ti.test_mode = configuration.getboolean('core', 'unit_test_mode')\n                    ti.handle_failure(\"{} detected as zombie\".format(ti),\n                                      ti.test_mode, ti.get_template_context())\n                    self.log.info(\n                        'Marked zombie job %s as %s', ti, ti.state)\n                    Stats.incr('zombies_killed')\n        session.commit()","method_path":"airflow\/models\/dagbag.py"}
{"repo_name":"apache\/airflow","method_name":"DagBag.bag_dag","method_code":"def bag_dag(self, dag, parent_dag, root_dag):\n        dag.test_cycle()  \n\n        dag.resolve_template_files()\n        dag.last_loaded = timezone.utcnow()\n\n        for task in dag.tasks:\n            settings.policy(task)\n\n        subdags = dag.subdags\n\n        try:\n            for subdag in subdags:\n                subdag.full_filepath = dag.full_filepath\n                subdag.parent_dag = dag\n                subdag.is_subdag = True\n                self.bag_dag(subdag, parent_dag=dag, root_dag=root_dag)\n\n            self.dags[dag.dag_id] = dag\n            self.log.debug('Loaded DAG %s', dag)\n        except AirflowDagCycleException as cycle_exception:\n            \n            self.log.exception('Exception bagging dag: %s', dag.dag_id)\n            \n            \n            if dag == root_dag:\n                for subdag in subdags:\n                    if subdag.dag_id in self.dags:\n                        del self.dags[subdag.dag_id]\n            raise cycle_exception","method_summary":"Adds the DAG into the bag, recurses into sub dags. Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags","original_method_code":"def bag_dag(self, dag, parent_dag, root_dag):\n        \"\"\"\n        Adds the DAG into the bag, recurses into sub dags.\n        Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags\n        \"\"\"\n\n        dag.test_cycle()  # throws if a task cycle is found\n\n        dag.resolve_template_files()\n        dag.last_loaded = timezone.utcnow()\n\n        for task in dag.tasks:\n            settings.policy(task)\n\n        subdags = dag.subdags\n\n        try:\n            for subdag in subdags:\n                subdag.full_filepath = dag.full_filepath\n                subdag.parent_dag = dag\n                subdag.is_subdag = True\n                self.bag_dag(subdag, parent_dag=dag, root_dag=root_dag)\n\n            self.dags[dag.dag_id] = dag\n            self.log.debug('Loaded DAG %s', dag)\n        except AirflowDagCycleException as cycle_exception:\n            # There was an error in bagging the dag. Remove it from the list of dags\n            self.log.exception('Exception bagging dag: %s', dag.dag_id)\n            # Only necessary at the root level since DAG.subdags automatically\n            # performs DFS to search through all subdags\n            if dag == root_dag:\n                for subdag in subdags:\n                    if subdag.dag_id in self.dags:\n                        del self.dags[subdag.dag_id]\n            raise cycle_exception","method_path":"airflow\/models\/dagbag.py"}
{"repo_name":"apache\/airflow","method_name":"DagBag.dagbag_report","method_code":"def dagbag_report(self):\n        report = textwrap.dedent()\n        stats = self.dagbag_stats\n        return report.format(\n            dag_folder=self.dag_folder,\n            duration=sum([o.duration for o in stats]),\n            dag_num=sum([o.dag_num for o in stats]),\n            task_num=sum([o.task_num for o in stats]),\n            table=pprinttable(stats),\n        )","method_summary":"Prints a report around DagBag loading stats","original_method_code":"def dagbag_report(self):\n        \"\"\"Prints a report around DagBag loading stats\"\"\"\n        report = textwrap.dedent(\"\"\"\\n\n        -------------------------------------------------------------------\n        DagBag loading stats for {dag_folder}\n        -------------------------------------------------------------------\n        Number of DAGs: {dag_num}\n        Total task number: {task_num}\n        DagBag parsing time: {duration}\n        {table}\n        \"\"\")\n        stats = self.dagbag_stats\n        return report.format(\n            dag_folder=self.dag_folder,\n            duration=sum([o.duration for o in stats]),\n            dag_num=sum([o.dag_num for o in stats]),\n            task_num=sum([o.task_num for o in stats]),\n            table=pprinttable(stats),\n        )","method_path":"airflow\/models\/dagbag.py"}
{"repo_name":"apache\/airflow","method_name":"ds_add","method_code":"def ds_add(ds, days):\n    ds = datetime.strptime(ds, '%Y-%m-%d')\n    if days:\n        ds = ds + timedelta(days)\n    return ds.isoformat()[:10]","method_summary":"Add or subtract days from a YYYY-MM-DD","original_method_code":"def ds_add(ds, days):\n    \"\"\"\n    Add or subtract days from a YYYY-MM-DD\n\n    :param ds: anchor date in ``YYYY-MM-DD`` format to add to\n    :type ds: str\n    :param days: number of days to add to the ds, you can use negative values\n    :type days: int\n\n    >>> ds_add('2015-01-01', 5)\n    '2015-01-06'\n    >>> ds_add('2015-01-06', -5)\n    '2015-01-01'\n    \"\"\"\n\n    ds = datetime.strptime(ds, '%Y-%m-%d')\n    if days:\n        ds = ds + timedelta(days)\n    return ds.isoformat()[:10]","method_path":"airflow\/macros\/__init__.py"}
{"repo_name":"apache\/airflow","method_name":"ds_format","method_code":"def ds_format(ds, input_format, output_format):\n    return datetime.strptime(ds, input_format).strftime(output_format)","method_summary":"Takes an input string and outputs another string as specified in the output format","original_method_code":"def ds_format(ds, input_format, output_format):\n    \"\"\"\n    Takes an input string and outputs another string\n    as specified in the output format\n\n    :param ds: input string which contains a date\n    :type ds: str\n    :param input_format: input string format. E.g. %Y-%m-%d\n    :type input_format: str\n    :param output_format: output string format  E.g. %Y-%m-%d\n    :type output_format: str\n\n    >>> ds_format('2015-01-01', \"%Y-%m-%d\", \"%m-%d-%y\")\n    '01-01-15'\n    >>> ds_format('1\/5\/2015', \"%m\/%d\/%Y\",  \"%Y-%m-%d\")\n    '2015-01-05'\n    \"\"\"\n    return datetime.strptime(ds, input_format).strftime(output_format)","method_path":"airflow\/macros\/__init__.py"}
{"repo_name":"apache\/airflow","method_name":"HdfsSensorRegex.poke","method_code":"def poke(self, context):\n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        self.log.info(\n            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern\n        )\n        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if\n                  f['file_type'] == 'f' and\n                  self.regex.match(f['path'].replace('%s\/' % self.filepath, ''))]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        return bool(result)","method_summary":"poke matching files in a directory with self.regex","original_method_code":"def poke(self, context):\n        \"\"\"\n        poke matching files in a directory with self.regex\n\n        :return: Bool depending on the search criteria\n        \"\"\"\n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        self.log.info(\n            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern\n        )\n        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if\n                  f['file_type'] == 'f' and\n                  self.regex.match(f['path'].replace('%s\/' % self.filepath, ''))]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        return bool(result)","method_path":"airflow\/contrib\/sensors\/hdfs_sensor.py"}
{"repo_name":"apache\/airflow","method_name":"HdfsSensorFolder.poke","method_code":"def poke(self, context):\n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        result = [f for f in sb.ls([self.filepath], include_toplevel=True)]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        if self.be_empty:\n            self.log.info('Poking for filepath %s to a empty directory', self.filepath)\n            return len(result) == 1 and result[0]['path'] == self.filepath\n        else:\n            self.log.info('Poking for filepath %s to a non empty directory', self.filepath)\n            result.pop(0)\n            return bool(result) and result[0]['file_type'] == 'f'","method_summary":"poke for a non empty directory","original_method_code":"def poke(self, context):\n        \"\"\"\n        poke for a non empty directory\n\n        :return: Bool depending on the search criteria\n        \"\"\"\n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        result = [f for f in sb.ls([self.filepath], include_toplevel=True)]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        if self.be_empty:\n            self.log.info('Poking for filepath %s to a empty directory', self.filepath)\n            return len(result) == 1 and result[0]['path'] == self.filepath\n        else:\n            self.log.info('Poking for filepath %s to a non empty directory', self.filepath)\n            result.pop(0)\n            return bool(result) and result[0]['file_type'] == 'f'","method_path":"airflow\/contrib\/sensors\/hdfs_sensor.py"}
{"repo_name":"apache\/airflow","method_name":"clear_task_instances","method_code":"def clear_task_instances(tis,\n                         session,\n                         activate_dag_runs=True,\n                         dag=None,\n                         ):\n    job_ids = []\n    for ti in tis:\n        if ti.state == State.RUNNING:\n            if ti.job_id:\n                ti.state = State.SHUTDOWN\n                job_ids.append(ti.job_id)\n        else:\n            task_id = ti.task_id\n            if dag and dag.has_task(task_id):\n                task = dag.get_task(task_id)\n                task_retries = task.retries\n                ti.max_tries = ti.try_number + task_retries - 1\n            else:\n                \n                \n                \n                \n                ti.max_tries = max(ti.max_tries, ti.try_number - 1)\n            ti.state = State.NONE\n            session.merge(ti)\n\n    if job_ids:\n        from airflow.jobs import BaseJob as BJ\n        for job in session.query(BJ).filter(BJ.id.in_(job_ids)).all():\n            job.state = State.SHUTDOWN\n\n    if activate_dag_runs and tis:\n        from airflow.models.dagrun import DagRun  \n        drs = session.query(DagRun).filter(\n            DagRun.dag_id.in_({ti.dag_id for ti in tis}),\n            DagRun.execution_date.in_({ti.execution_date for ti in tis}),\n        ).all()\n        for dr in drs:\n            dr.state = State.RUNNING\n            dr.start_date = timezone.utcnow()","method_summary":"Clears a set of task instances, but makes sure the running ones get killed.","original_method_code":"def clear_task_instances(tis,\n                         session,\n                         activate_dag_runs=True,\n                         dag=None,\n                         ):\n    \"\"\"\n    Clears a set of task instances, but makes sure the running ones\n    get killed.\n\n    :param tis: a list of task instances\n    :param session: current session\n    :param activate_dag_runs: flag to check for active dag run\n    :param dag: DAG object\n    \"\"\"\n    job_ids = []\n    for ti in tis:\n        if ti.state == State.RUNNING:\n            if ti.job_id:\n                ti.state = State.SHUTDOWN\n                job_ids.append(ti.job_id)\n        else:\n            task_id = ti.task_id\n            if dag and dag.has_task(task_id):\n                task = dag.get_task(task_id)\n                task_retries = task.retries\n                ti.max_tries = ti.try_number + task_retries - 1\n            else:\n                # Ignore errors when updating max_tries if dag is None or\n                # task not found in dag since database records could be\n                # outdated. We make max_tries the maximum value of its\n                # original max_tries or the current task try number.\n                ti.max_tries = max(ti.max_tries, ti.try_number - 1)\n            ti.state = State.NONE\n            session.merge(ti)\n\n    if job_ids:\n        from airflow.jobs import BaseJob as BJ\n        for job in session.query(BJ).filter(BJ.id.in_(job_ids)).all():\n            job.state = State.SHUTDOWN\n\n    if activate_dag_runs and tis:\n        from airflow.models.dagrun import DagRun  # Avoid circular import\n        drs = session.query(DagRun).filter(\n            DagRun.dag_id.in_({ti.dag_id for ti in tis}),\n            DagRun.execution_date.in_({ti.execution_date for ti in tis}),\n        ).all()\n        for dr in drs:\n            dr.state = State.RUNNING\n            dr.start_date = timezone.utcnow()","method_path":"airflow\/models\/taskinstance.py"}
{"repo_name":"apache\/airflow","method_name":"TaskInstance.try_number","method_code":"def try_number(self):\n        \n        if self.state == State.RUNNING:\n            return self._try_number\n        return self._try_number + 1","method_summary":"Return the try number that this task number will be when it is actually run. If the TI is currently running, this will match the column in the databse, in all othercases this will be incremenetd","original_method_code":"def try_number(self):\n        \"\"\"\n        Return the try number that this task number will be when it is actually\n        run.\n\n        If the TI is currently running, this will match the column in the\n        databse, in all othercases this will be incremenetd\n        \"\"\"\n        # This is designed so that task logs end up in the right file.\n        if self.state == State.RUNNING:\n            return self._try_number\n        return self._try_number + 1","method_path":"airflow\/models\/taskinstance.py"}
{"repo_name":"apache\/airflow","method_name":"TaskInstance.generate_command","method_code":"def generate_command(dag_id,\n                         task_id,\n                         execution_date,\n                         mark_success=False,\n                         ignore_all_deps=False,\n                         ignore_depends_on_past=False,\n                         ignore_task_deps=False,\n                         ignore_ti_state=False,\n                         local=False,\n                         pickle_id=None,\n                         file_path=None,\n                         raw=False,\n                         job_id=None,\n                         pool=None,\n                         cfg_path=None\n                         ):\n        iso = execution_date.isoformat()\n        cmd = [\"airflow\", \"run\", str(dag_id), str(task_id), str(iso)]\n        cmd.extend([\"--mark_success\"]) if mark_success else None\n        cmd.extend([\"--pickle\", str(pickle_id)]) if pickle_id else None\n        cmd.extend([\"--job_id\", str(job_id)]) if job_id else None\n        cmd.extend([\"-A\"]) if ignore_all_deps else None\n        cmd.extend([\"-i\"]) if ignore_task_deps else None\n        cmd.extend([\"-I\"]) if ignore_depends_on_past else None\n        cmd.extend([\"--force\"]) if ignore_ti_state else None\n        cmd.extend([\"--local\"]) if local else None\n        cmd.extend([\"--pool\", pool]) if pool else None\n        cmd.extend([\"--raw\"]) if raw else None\n        cmd.extend([\"-sd\", file_path]) if file_path else None\n        cmd.extend([\"--cfg_path\", cfg_path]) if cfg_path else None\n        return cmd","method_summary":"Generates the shell command required to execute this task instance.","original_method_code":"def generate_command(dag_id,\n                         task_id,\n                         execution_date,\n                         mark_success=False,\n                         ignore_all_deps=False,\n                         ignore_depends_on_past=False,\n                         ignore_task_deps=False,\n                         ignore_ti_state=False,\n                         local=False,\n                         pickle_id=None,\n                         file_path=None,\n                         raw=False,\n                         job_id=None,\n                         pool=None,\n                         cfg_path=None\n                         ):\n        \"\"\"\n        Generates the shell command required to execute this task instance.\n\n        :param dag_id: DAG ID\n        :type dag_id: unicode\n        :param task_id: Task ID\n        :type task_id: unicode\n        :param execution_date: Execution date for the task\n        :type execution_date: datetime\n        :param mark_success: Whether to mark the task as successful\n        :type mark_success: bool\n        :param ignore_all_deps: Ignore all ignorable dependencies.\n            Overrides the other ignore_* parameters.\n        :type ignore_all_deps: bool\n        :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs\n            (e.g. for Backfills)\n        :type ignore_depends_on_past: bool\n        :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past\n            and trigger rule\n        :type ignore_task_deps: bool\n        :param ignore_ti_state: Ignore the task instance's previous failure\/success\n        :type ignore_ti_state: bool\n        :param local: Whether to run the task locally\n        :type local: bool\n        :param pickle_id: If the DAG was serialized to the DB, the ID\n            associated with the pickled DAG\n        :type pickle_id: unicode\n        :param file_path: path to the file containing the DAG definition\n        :param raw: raw mode (needs more details)\n        :param job_id: job ID (needs more details)\n        :param pool: the Airflow pool that the task should run in\n        :type pool: unicode\n        :param cfg_path: the Path to the configuration file\n        :type cfg_path: basestring\n        :return: shell command that can be used to run the task instance\n        \"\"\"\n        iso = execution_date.isoformat()\n        cmd = [\"airflow\", \"run\", str(dag_id), str(task_id), str(iso)]\n        cmd.extend([\"--mark_success\"]) if mark_success else None\n        cmd.extend([\"--pickle\", str(pickle_id)]) if pickle_id else None\n        cmd.extend([\"--job_id\", str(job_id)]) if job_id else None\n        cmd.extend([\"-A\"]) if ignore_all_deps else None\n        cmd.extend([\"-i\"]) if ignore_task_deps else None\n        cmd.extend([\"-I\"]) if ignore_depends_on_past else None\n        cmd.extend([\"--force\"]) if ignore_ti_state else None\n        cmd.extend([\"--local\"]) if local else None\n        cmd.extend([\"--pool\", pool]) if pool else None\n        cmd.extend([\"--raw\"]) if raw else None\n        cmd.extend([\"-sd\", file_path]) if file_path else None\n        cmd.extend([\"--cfg_path\", cfg_path]) if cfg_path else None\n        return cmd","method_path":"airflow\/models\/taskinstance.py"}
{"repo_name":"apache\/airflow","method_name":"TaskInstance.current_state","method_code":"def current_state(self, session=None):\n        TI = TaskInstance\n        ti = session.query(TI).filter(\n            TI.dag_id == self.dag_id,\n            TI.task_id == self.task_id,\n            TI.execution_date == self.execution_date,\n        ).all()\n        if ti:\n            state = ti[0].state\n        else:\n            state = None\n        return state","method_summary":"Get the very latest state from the database, if a session is passed, we use and looking up the state becomes part of the session, otherwise a new session is used.","original_method_code":"def current_state(self, session=None):\n        \"\"\"\n        Get the very latest state from the database, if a session is passed,\n        we use and looking up the state becomes part of the session, otherwise\n        a new session is used.\n        \"\"\"\n        TI = TaskInstance\n        ti = session.query(TI).filter(\n            TI.dag_id == self.dag_id,\n            TI.task_id == self.task_id,\n            TI.execution_date == self.execution_date,\n        ).all()\n        if ti:\n            state = ti[0].state\n        else:\n            state = None\n        return state","method_path":"airflow\/models\/taskinstance.py"}
{"repo_name":"apache\/airflow","method_name":"TaskInstance.error","method_code":"def error(self, session=None):\n        self.log.error(\"Recording the task instance as FAILED\")\n        self.state = State.FAILED\n        session.merge(self)\n        session.commit()","method_summary":"Forces the task instance's state to FAILED in the database.","original_method_code":"def error(self, session=None):\n        \"\"\"\n        Forces the task instance's state to FAILED in the database.\n        \"\"\"\n        self.log.error(\"Recording the task instance as FAILED\")\n        self.state = State.FAILED\n        session.merge(self)\n        session.commit()","method_path":"airflow\/models\/taskinstance.py"}
{"repo_name":"apache\/airflow","method_name":"TaskInstance.refresh_from_db","method_code":"def refresh_from_db(self, session=None, lock_for_update=False):\n        TI = TaskInstance\n\n        qry = session.query(TI).filter(\n            TI.dag_id == self.dag_id,\n            TI.task_id == self.task_id,\n            TI.execution_date == self.execution_date)\n\n        if lock_for_update:\n            ti = qry.with_for_update().first()\n        else:\n            ti = qry.first()\n        if ti:\n            self.state = ti.state\n            self.start_date = ti.start_date\n            self.end_date = ti.end_date\n            \n            \n            self.try_number = ti._try_number\n            self.max_tries = ti.max_tries\n            self.hostname = ti.hostname\n            self.pid = ti.pid\n            self.executor_config = ti.executor_config\n        else:\n            self.state = None","method_summary":"Refreshes the task instance from the database based on the primary key","original_method_code":"def refresh_from_db(self, session=None, lock_for_update=False):\n        \"\"\"\n        Refreshes the task instance from the database based on the primary key\n\n        :param lock_for_update: if True, indicates that the database should\n            lock the TaskInstance (issuing a FOR UPDATE clause) until the\n            session is committed.\n        \"\"\"\n        TI = TaskInstance\n\n        qry = session.query(TI).filter(\n            TI.dag_id == self.dag_id,\n            TI.task_id == self.task_id,\n            TI.execution_date == self.execution_date)\n\n        if lock_for_update:\n            ti = qry.with_for_update().first()\n        else:\n            ti = qry.first()\n        if ti:\n            self.state = ti.state\n            self.start_date = ti.start_date\n            self.end_date = ti.end_date\n            # Get the raw value of try_number column, don't read through the\n            # accessor here otherwise it will be incremeneted by one already.\n            self.try_number = ti._try_number\n            self.max_tries = ti.max_tries\n            self.hostname = ti.hostname\n            self.pid = ti.pid\n            self.executor_config = ti.executor_config\n        else:\n            self.state = None","method_path":"airflow\/models\/taskinstance.py"}
{"repo_name":"apache\/airflow","method_name":"TaskInstance.clear_xcom_data","method_code":"def clear_xcom_data(self, session=None):\n        session.query(XCom).filter(\n            XCom.dag_id == self.dag_id,\n            XCom.task_id == self.task_id,\n            XCom.execution_date == self.execution_date\n        ).delete()\n        session.commit()","method_summary":"Clears all XCom data from the database for the task instance","original_method_code":"def clear_xcom_data(self, session=None):\n        \"\"\"\n        Clears all XCom data from the database for the task instance\n        \"\"\"\n        session.query(XCom).filter(\n            XCom.dag_id == self.dag_id,\n            XCom.task_id == self.task_id,\n            XCom.execution_date == self.execution_date\n        ).delete()\n        session.commit()","method_path":"airflow\/models\/taskinstance.py"}
{"repo_name":"apache\/airflow","method_name":"TaskInstance.next_retry_datetime","method_code":"def next_retry_datetime(self):\n        delay = self.task.retry_delay\n        if self.task.retry_exponential_backoff:\n            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))\n            \n            hash = int(hashlib.sha1(\"{}#{}#{}#{}\".format(self.dag_id,\n                                                         self.task_id,\n                                                         self.execution_date,\n                                                         self.try_number)\n                                    .encode('utf-8')).hexdigest(), 16)\n            \n            modded_hash = min_backoff + hash % min_backoff\n            \n            \n            \n            \n            \n            delay_backoff_in_seconds = min(\n                modded_hash,\n                timedelta.max.total_seconds() - 1\n            )\n            delay = timedelta(seconds=delay_backoff_in_seconds)\n            if self.task.max_retry_delay:\n                delay = min(self.task.max_retry_delay, delay)\n        return self.end_date + delay","method_summary":"Get datetime of the next retry if the task instance fails. For exponential backoff, retry_delay is used as base and will be converted to seconds.","original_method_code":"def next_retry_datetime(self):\n        \"\"\"\n        Get datetime of the next retry if the task instance fails. For exponential\n        backoff, retry_delay is used as base and will be converted to seconds.\n        \"\"\"\n        delay = self.task.retry_delay\n        if self.task.retry_exponential_backoff:\n            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))\n            # deterministic per task instance\n            hash = int(hashlib.sha1(\"{}#{}#{}#{}\".format(self.dag_id,\n                                                         self.task_id,\n                                                         self.execution_date,\n                                                         self.try_number)\n                                    .encode('utf-8')).hexdigest(), 16)\n            # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)\n            modded_hash = min_backoff + hash % min_backoff\n            # timedelta has a maximum representable value. The exponentiation\n            # here means this value can be exceeded after a certain number\n            # of tries (around 50 if the initial delay is 1s, even fewer if\n            # the delay is larger). Cap the value here before creating a\n            # timedelta object so the operation doesn't fail.\n            delay_backoff_in_seconds = min(\n                modded_hash,\n                timedelta.max.total_seconds() - 1\n            )\n            delay = timedelta(seconds=delay_backoff_in_seconds)\n            if self.task.max_retry_delay:\n                delay = min(self.task.max_retry_delay, delay)\n        return self.end_date + delay","method_path":"airflow\/models\/taskinstance.py"}
{"repo_name":"apache\/airflow","method_name":"TaskInstance.ready_for_retry","method_code":"def ready_for_retry(self):\n        return (self.state == State.UP_FOR_RETRY and\n                self.next_retry_datetime() < timezone.utcnow())","method_summary":"Checks on whether the task instance is in the right state and timeframe to be retried.","original_method_code":"def ready_for_retry(self):\n        \"\"\"\n        Checks on whether the task instance is in the right state and timeframe\n        to be retried.\n        \"\"\"\n        return (self.state == State.UP_FOR_RETRY and\n                self.next_retry_datetime() < timezone.utcnow())","method_path":"airflow\/models\/taskinstance.py"}
{"repo_name":"apache\/airflow","method_name":"TaskInstance.xcom_push","method_code":"def xcom_push(\n            self,\n            key,\n            value,\n            execution_date=None):\n        if execution_date and execution_date < self.execution_date:\n            raise ValueError(\n                'execution_date can not be in the past (current '\n                'execution_date is {}; received {})'.format(\n                    self.execution_date, execution_date))\n\n        XCom.set(\n            key=key,\n            value=value,\n            task_id=self.task_id,\n            dag_id=self.dag_id,\n            execution_date=execution_date or self.execution_date)","method_summary":"Make an XCom available for tasks to pull.","original_method_code":"def xcom_push(\n            self,\n            key,\n            value,\n            execution_date=None):\n        \"\"\"\n        Make an XCom available for tasks to pull.\n\n        :param key: A key for the XCom\n        :type key: str\n        :param value: A value for the XCom. The value is pickled and stored\n            in the database.\n        :type value: any pickleable object\n        :param execution_date: if provided, the XCom will not be visible until\n            this date. This can be used, for example, to send a message to a\n            task on a future date without it being immediately visible.\n        :type execution_date: datetime\n        \"\"\"\n\n        if execution_date and execution_date < self.execution_date:\n            raise ValueError(\n                'execution_date can not be in the past (current '\n                'execution_date is {}; received {})'.format(\n                    self.execution_date, execution_date))\n\n        XCom.set(\n            key=key,\n            value=value,\n            task_id=self.task_id,\n            dag_id=self.dag_id,\n            execution_date=execution_date or self.execution_date)","method_path":"airflow\/models\/taskinstance.py"}
{"repo_name":"apache\/airflow","method_name":"TaskInstance.init_run_context","method_code":"def init_run_context(self, raw=False):\n        self.raw = raw\n        self._set_context(self)","method_summary":"Sets the log context.","original_method_code":"def init_run_context(self, raw=False):\n        \"\"\"\n        Sets the log context.\n        \"\"\"\n        self.raw = raw\n        self._set_context(self)","method_path":"airflow\/models\/taskinstance.py"}
{"repo_name":"apache\/airflow","method_name":"WasbTaskHandler.close","method_code":"def close(self):\n        \n        \n        \n        \n        if self.closed:\n            return\n\n        super().close()\n\n        if not self.upload_on_close:\n            return\n\n        local_loc = os.path.join(self.local_base, self.log_relative_path)\n        remote_loc = os.path.join(self.remote_base, self.log_relative_path)\n        if os.path.exists(local_loc):\n            \n            with open(local_loc, 'r') as logfile:\n                log = logfile.read()\n            self.wasb_write(log, remote_loc, append=True)\n\n            if self.delete_local_copy:\n                shutil.rmtree(os.path.dirname(local_loc))\n        \n        self.closed = True","method_summary":"Close and upload local log file to remote storage Wasb.","original_method_code":"def close(self):\n        \"\"\"\n        Close and upload local log file to remote storage Wasb.\n        \"\"\"\n        # When application exit, system shuts down all handlers by\n        # calling close method. Here we check if logger is already\n        # closed to prevent uploading the log to remote storage multiple\n        # times when `logging.shutdown` is called.\n        if self.closed:\n            return\n\n        super().close()\n\n        if not self.upload_on_close:\n            return\n\n        local_loc = os.path.join(self.local_base, self.log_relative_path)\n        remote_loc = os.path.join(self.remote_base, self.log_relative_path)\n        if os.path.exists(local_loc):\n            # read log and remove old logs to get just the latest additions\n            with open(local_loc, 'r') as logfile:\n                log = logfile.read()\n            self.wasb_write(log, remote_loc, append=True)\n\n            if self.delete_local_copy:\n                shutil.rmtree(os.path.dirname(local_loc))\n        # Mark closed so we don't double write if close is called twice\n        self.closed = True","method_path":"airflow\/utils\/log\/wasb_task_handler.py"}
{"repo_name":"apache\/airflow","method_name":"GceHook.get_conn","method_code":"def get_conn(self):\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build('compute', self.api_version,\n                               http=http_authorized, cache_discovery=False)\n        return self._conn","method_summary":"Retrieves connection to Google Compute Engine.","original_method_code":"def get_conn(self):\n        \"\"\"\n        Retrieves connection to Google Compute Engine.\n\n        :return: Google Compute Engine services object\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build('compute', self.api_version,\n                               http=http_authorized, cache_discovery=False)\n        return self._conn","method_path":"airflow\/contrib\/hooks\/gcp_compute_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GceHook.start_instance","method_code":"def start_instance(self, zone, resource_id, project_id=None):\n        response = self.get_conn().instances().start(\n            project=project_id,\n            zone=zone,\n            instance=resource_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)","method_summary":"Starts an existing instance defined by project_id, zone and resource_id. Must be called with keyword arguments rather than positional.","original_method_code":"def start_instance(self, zone, resource_id, project_id=None):\n        \"\"\"\n        Starts an existing instance defined by project_id, zone and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the instance exists\n        :type zone: str\n        :param resource_id: Name of the Compute Engine instance resource\n        :type resource_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().start(\n            project=project_id,\n            zone=zone,\n            instance=resource_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)","method_path":"airflow\/contrib\/hooks\/gcp_compute_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GceHook.set_machine_type","method_code":"def set_machine_type(self, zone, resource_id, body, project_id=None):\n        response = self._execute_set_machine_type(zone, resource_id, body, project_id)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)","method_summary":"Sets machine type of an instance defined by project_id, zone and resource_id. Must be called with keyword arguments rather than positional.","original_method_code":"def set_machine_type(self, zone, resource_id, body, project_id=None):\n        \"\"\"\n        Sets machine type of an instance defined by project_id, zone and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the instance exists.\n        :type zone: str\n        :param resource_id: Name of the Compute Engine instance resource\n        :type resource_id: str\n        :param body: Body required by the Compute Engine setMachineType API,\n            as described in\n            https:\/\/cloud.google.com\/compute\/docs\/reference\/rest\/v1\/instances\/setMachineType\n        :type body: dict\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self._execute_set_machine_type(zone, resource_id, body, project_id)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)","method_path":"airflow\/contrib\/hooks\/gcp_compute_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GceHook.get_instance_template","method_code":"def get_instance_template(self, resource_id, project_id=None):\n        response = self.get_conn().instanceTemplates().get(\n            project=project_id,\n            instanceTemplate=resource_id\n        ).execute(num_retries=self.num_retries)\n        return response","method_summary":"Retrieves instance template by project_id and resource_id. Must be called with keyword arguments rather than positional.","original_method_code":"def get_instance_template(self, resource_id, project_id=None):\n        \"\"\"\n        Retrieves instance template by project_id and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param resource_id: Name of the instance template\n        :type resource_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: Instance template representation as object according to\n            https:\/\/cloud.google.com\/compute\/docs\/reference\/rest\/v1\/instanceTemplates\n        :rtype: dict\n        \"\"\"\n        response = self.get_conn().instanceTemplates().get(\n            project=project_id,\n            instanceTemplate=resource_id\n        ).execute(num_retries=self.num_retries)\n        return response","method_path":"airflow\/contrib\/hooks\/gcp_compute_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GceHook.insert_instance_template","method_code":"def insert_instance_template(self, body, request_id=None, project_id=None):\n        response = self.get_conn().instanceTemplates().insert(\n            project=project_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)","method_summary":"Inserts instance template using body specified Must be called with keyword arguments rather than positional.","original_method_code":"def insert_instance_template(self, body, request_id=None, project_id=None):\n        \"\"\"\n        Inserts instance template using body specified\n        Must be called with keyword arguments rather than positional.\n\n        :param body: Instance template representation as object according to\n            https:\/\/cloud.google.com\/compute\/docs\/reference\/rest\/v1\/instanceTemplates\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again)\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instanceTemplates().insert(\n            project=project_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)","method_path":"airflow\/contrib\/hooks\/gcp_compute_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GceHook.get_instance_group_manager","method_code":"def get_instance_group_manager(self, zone, resource_id, project_id=None):\n        response = self.get_conn().instanceGroupManagers().get(\n            project=project_id,\n            zone=zone,\n            instanceGroupManager=resource_id\n        ).execute(num_retries=self.num_retries)\n        return response","method_summary":"Retrieves Instance Group Manager by project_id, zone and resource_id. Must be called with keyword arguments rather than positional.","original_method_code":"def get_instance_group_manager(self, zone, resource_id, project_id=None):\n        \"\"\"\n        Retrieves Instance Group Manager by project_id, zone and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the Instance Group Manager exists\n        :type zone: str\n        :param resource_id: Name of the Instance Group Manager\n        :type resource_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: Instance group manager representation as object according to\n            https:\/\/cloud.google.com\/compute\/docs\/reference\/rest\/beta\/instanceGroupManagers\n        :rtype: dict\n        \"\"\"\n        response = self.get_conn().instanceGroupManagers().get(\n            project=project_id,\n            zone=zone,\n            instanceGroupManager=resource_id\n        ).execute(num_retries=self.num_retries)\n        return response","method_path":"airflow\/contrib\/hooks\/gcp_compute_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GceHook.patch_instance_group_manager","method_code":"def patch_instance_group_manager(self, zone, resource_id,\n                                     body, request_id=None, project_id=None):\n        response = self.get_conn().instanceGroupManagers().patch(\n            project=project_id,\n            zone=zone,\n            instanceGroupManager=resource_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)","method_summary":"Patches Instance Group Manager with the specified body. Must be called with keyword arguments rather than positional.","original_method_code":"def patch_instance_group_manager(self, zone, resource_id,\n                                     body, request_id=None, project_id=None):\n        \"\"\"\n        Patches Instance Group Manager with the specified body.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the Instance Group Manager exists\n        :type zone: str\n        :param resource_id: Name of the Instance Group Manager\n        :type resource_id: str\n        :param body: Instance Group Manager representation as json-merge-patch object\n            according to\n            https:\/\/cloud.google.com\/compute\/docs\/reference\/rest\/beta\/instanceTemplates\/patch\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again).\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instanceGroupManagers().patch(\n            project=project_id,\n            zone=zone,\n            instanceGroupManager=resource_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)","method_path":"airflow\/contrib\/hooks\/gcp_compute_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GceHook._wait_for_operation_to_complete","method_code":"def _wait_for_operation_to_complete(self, project_id, operation_name, zone=None):\n        service = self.get_conn()\n        while True:\n            if zone is None:\n                \n                operation_response = self._check_global_operation_status(\n                    service, operation_name, project_id)\n            else:\n                \n                operation_response = self._check_zone_operation_status(\n                    service, operation_name, project_id, zone, self.num_retries)\n            if operation_response.get(\"status\") == GceOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    code = operation_response.get(\"httpErrorStatusCode\")\n                    msg = operation_response.get(\"httpErrorMessage\")\n                    \n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(\"{} {}: \".format(code, msg) + error_msg)\n                \n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)","method_summary":"Waits for the named operation to complete - checks status of the async call.","original_method_code":"def _wait_for_operation_to_complete(self, project_id, operation_name, zone=None):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the async call.\n\n        :param operation_name: name of the operation\n        :type operation_name: str\n        :param zone: optional region of the request (might be None for global operations)\n        :type zone: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            if zone is None:\n                # noinspection PyTypeChecker\n                operation_response = self._check_global_operation_status(\n                    service, operation_name, project_id)\n            else:\n                # noinspection PyTypeChecker\n                operation_response = self._check_zone_operation_status(\n                    service, operation_name, project_id, zone, self.num_retries)\n            if operation_response.get(\"status\") == GceOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    code = operation_response.get(\"httpErrorStatusCode\")\n                    msg = operation_response.get(\"httpErrorMessage\")\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(\"{} {}: \".format(code, msg) + error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)","method_path":"airflow\/contrib\/hooks\/gcp_compute_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.check_for_bucket","method_code":"def check_for_bucket(self, bucket_name):\n        try:\n            self.get_conn().head_bucket(Bucket=bucket_name)\n            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False","method_summary":"Check if bucket_name exists.","original_method_code":"def check_for_bucket(self, bucket_name):\n        \"\"\"\n        Check if bucket_name exists.\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"\n        try:\n            self.get_conn().head_bucket(Bucket=bucket_name)\n            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.create_bucket","method_code":"def create_bucket(self, bucket_name, region_name=None):\n        s3_conn = self.get_conn()\n        if not region_name:\n            region_name = s3_conn.meta.region_name\n        if region_name == 'us-east-1':\n            self.get_conn().create_bucket(Bucket=bucket_name)\n        else:\n            self.get_conn().create_bucket(Bucket=bucket_name,\n                                          CreateBucketConfiguration={\n                                              'LocationConstraint': region_name\n                                          })","method_summary":"Creates an Amazon S3 bucket.","original_method_code":"def create_bucket(self, bucket_name, region_name=None):\n        \"\"\"\n        Creates an Amazon S3 bucket.\n\n        :param bucket_name: The name of the bucket\n        :type bucket_name: str\n        :param region_name: The name of the aws region in which to create the bucket.\n        :type region_name: str\n        \"\"\"\n        s3_conn = self.get_conn()\n        if not region_name:\n            region_name = s3_conn.meta.region_name\n        if region_name == 'us-east-1':\n            self.get_conn().create_bucket(Bucket=bucket_name)\n        else:\n            self.get_conn().create_bucket(Bucket=bucket_name,\n                                          CreateBucketConfiguration={\n                                              'LocationConstraint': region_name\n                                          })","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.check_for_prefix","method_code":"def check_for_prefix(self, bucket_name, prefix, delimiter):\n        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix\n        prefix_split = re.split(r'(\\w+[{d}])$'.format(d=delimiter), prefix, 1)\n        previous_level = prefix_split[0]\n        plist = self.list_prefixes(bucket_name, previous_level, delimiter)\n        return False if plist is None else prefix in plist","method_summary":"Checks that a prefix exists in a bucket","original_method_code":"def check_for_prefix(self, bucket_name, prefix, delimiter):\n        \"\"\"\n        Checks that a prefix exists in a bucket\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        \"\"\"\n        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix\n        prefix_split = re.split(r'(\\w+[{d}])$'.format(d=delimiter), prefix, 1)\n        previous_level = prefix_split[0]\n        plist = self.list_prefixes(bucket_name, previous_level, delimiter)\n        return False if plist is None else prefix in plist","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.list_prefixes","method_code":"def list_prefixes(self, bucket_name, prefix='', delimiter='',\n                      page_size=None, max_items=None):\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False\n        prefixes = []\n        for page in response:\n            if 'CommonPrefixes' in page:\n                has_results = True\n                for p in page['CommonPrefixes']:\n                    prefixes.append(p['Prefix'])\n\n        if has_results:\n            return prefixes","method_summary":"Lists prefixes in a bucket under prefix","original_method_code":"def list_prefixes(self, bucket_name, prefix='', delimiter='',\n                      page_size=None, max_items=None):\n        \"\"\"\n        Lists prefixes in a bucket under prefix\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        \"\"\"\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False\n        prefixes = []\n        for page in response:\n            if 'CommonPrefixes' in page:\n                has_results = True\n                for p in page['CommonPrefixes']:\n                    prefixes.append(p['Prefix'])\n\n        if has_results:\n            return prefixes","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.list_keys","method_code":"def list_keys(self, bucket_name, prefix='', delimiter='',\n                  page_size=None, max_items=None):\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False\n        keys = []\n        for page in response:\n            if 'Contents' in page:\n                has_results = True\n                for k in page['Contents']:\n                    keys.append(k['Key'])\n\n        if has_results:\n            return keys","method_summary":"Lists keys in a bucket under prefix and not containing delimiter","original_method_code":"def list_keys(self, bucket_name, prefix='', delimiter='',\n                  page_size=None, max_items=None):\n        \"\"\"\n        Lists keys in a bucket under prefix and not containing delimiter\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        \"\"\"\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False\n        keys = []\n        for page in response:\n            if 'Contents' in page:\n                has_results = True\n                for k in page['Contents']:\n                    keys.append(k['Key'])\n\n        if has_results:\n            return keys","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.check_for_key","method_code":"def check_for_key(self, key, bucket_name=None):\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        try:\n            self.get_conn().head_object(Bucket=bucket_name, Key=key)\n            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False","method_summary":"Checks if a key exists in a bucket","original_method_code":"def check_for_key(self, key, bucket_name=None):\n        \"\"\"\n        Checks if a key exists in a bucket\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str\n        \"\"\"\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        try:\n            self.get_conn().head_object(Bucket=bucket_name, Key=key)\n            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.read_key","method_code":"def read_key(self, key, bucket_name=None):\n        obj = self.get_key(key, bucket_name)\n        return obj.get()['Body'].read().decode('utf-8')","method_summary":"Reads a key from S3","original_method_code":"def read_key(self, key, bucket_name=None):\n        \"\"\"\n        Reads a key from S3\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str\n        \"\"\"\n\n        obj = self.get_key(key, bucket_name)\n        return obj.get()['Body'].read().decode('utf-8')","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.select_key","method_code":"def select_key(self, key, bucket_name=None,\n                   expression='SELECT * FROM S3Object',\n                   expression_type='SQL',\n                   input_serialization=None,\n                   output_serialization=None):\n        if input_serialization is None:\n            input_serialization = {'CSV': {}}\n        if output_serialization is None:\n            output_serialization = {'CSV': {}}\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        response = self.get_conn().select_object_content(\n            Bucket=bucket_name,\n            Key=key,\n            Expression=expression,\n            ExpressionType=expression_type,\n            InputSerialization=input_serialization,\n            OutputSerialization=output_serialization)\n\n        return ''.join(event['Records']['Payload'].decode('utf-8')\n                       for event in response['Payload']\n                       if 'Records' in event)","method_summary":"Reads a key with S3 Select.","original_method_code":"def select_key(self, key, bucket_name=None,\n                   expression='SELECT * FROM S3Object',\n                   expression_type='SQL',\n                   input_serialization=None,\n                   output_serialization=None):\n        \"\"\"\n        Reads a key with S3 Select.\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str\n        :param expression: S3 Select expression\n        :type expression: str\n        :param expression_type: S3 Select expression type\n        :type expression_type: str\n        :param input_serialization: S3 Select input data serialization format\n        :type input_serialization: dict\n        :param output_serialization: S3 Select output data serialization format\n        :type output_serialization: dict\n        :return: retrieved subset of original data by S3 Select\n        :rtype: str\n\n        .. seealso::\n            For more details about S3 Select parameters:\n            http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Client.select_object_content\n        \"\"\"\n        if input_serialization is None:\n            input_serialization = {'CSV': {}}\n        if output_serialization is None:\n            output_serialization = {'CSV': {}}\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        response = self.get_conn().select_object_content(\n            Bucket=bucket_name,\n            Key=key,\n            Expression=expression,\n            ExpressionType=expression_type,\n            InputSerialization=input_serialization,\n            OutputSerialization=output_serialization)\n\n        return ''.join(event['Records']['Payload'].decode('utf-8')\n                       for event in response['Payload']\n                       if 'Records' in event)","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.check_for_wildcard_key","method_code":"def check_for_wildcard_key(self,\n                               wildcard_key, bucket_name=None, delimiter=''):\n        return self.get_wildcard_key(wildcard_key=wildcard_key,\n                                     bucket_name=bucket_name,\n                                     delimiter=delimiter) is not None","method_summary":"Checks that a key matching a wildcard expression exists in a bucket","original_method_code":"def check_for_wildcard_key(self,\n                               wildcard_key, bucket_name=None, delimiter=''):\n        \"\"\"\n        Checks that a key matching a wildcard expression exists in a bucket\n\n        :param wildcard_key: the path to the key\n        :type wildcard_key: str\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param delimiter: the delimiter marks key hierarchy\n        :type delimiter: str\n        \"\"\"\n        return self.get_wildcard_key(wildcard_key=wildcard_key,\n                                     bucket_name=bucket_name,\n                                     delimiter=delimiter) is not None","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.load_file","method_code":"def load_file(self,\n                  filename,\n                  key,\n                  bucket_name=None,\n                  replace=False,\n                  encrypt=False):\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        client = self.get_conn()\n        client.upload_file(filename, bucket_name, key, ExtraArgs=extra_args)","method_summary":"Loads a local file to S3","original_method_code":"def load_file(self,\n                  filename,\n                  key,\n                  bucket_name=None,\n                  replace=False,\n                  encrypt=False):\n        \"\"\"\n        Loads a local file to S3\n\n        :param filename: name of the file to load.\n        :type filename: str\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag to decide whether or not to overwrite the key\n            if it already exists. If replace is False and the key exists, an\n            error will be raised.\n        :type replace: bool\n        :param encrypt: If True, the file will be encrypted on the server-side\n            by S3 and will be stored in an encrypted form while at rest in S3.\n        :type encrypt: bool\n        \"\"\"\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        client = self.get_conn()\n        client.upload_file(filename, bucket_name, key, ExtraArgs=extra_args)","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.load_string","method_code":"def load_string(self,\n                    string_data,\n                    key,\n                    bucket_name=None,\n                    replace=False,\n                    encrypt=False,\n                    encoding='utf-8'):\n        self.load_bytes(string_data.encode(encoding),\n                        key=key,\n                        bucket_name=bucket_name,\n                        replace=replace,\n                        encrypt=encrypt)","method_summary":"Loads a string to S3 This is provided as a convenience to drop a string in S3. It uses the boto infrastructure to ship a file to s3.","original_method_code":"def load_string(self,\n                    string_data,\n                    key,\n                    bucket_name=None,\n                    replace=False,\n                    encrypt=False,\n                    encoding='utf-8'):\n        \"\"\"\n        Loads a string to S3\n\n        This is provided as a convenience to drop a string in S3. It uses the\n        boto infrastructure to ship a file to s3.\n\n        :param string_data: str to set as content for the key.\n        :type string_data: str\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag to decide whether or not to overwrite the key\n            if it already exists\n        :type replace: bool\n        :param encrypt: If True, the file will be encrypted on the server-side\n            by S3 and will be stored in an encrypted form while at rest in S3.\n        :type encrypt: bool\n        \"\"\"\n        self.load_bytes(string_data.encode(encoding),\n                        key=key,\n                        bucket_name=bucket_name,\n                        replace=replace,\n                        encrypt=encrypt)","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.load_bytes","method_code":"def load_bytes(self,\n                   bytes_data,\n                   key,\n                   bucket_name=None,\n                   replace=False,\n                   encrypt=False):\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        filelike_buffer = BytesIO(bytes_data)\n\n        client = self.get_conn()\n        client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)","method_summary":"Loads bytes to S3 This is provided as a convenience to drop a string in S3. It uses the boto infrastructure to ship a file to s3.","original_method_code":"def load_bytes(self,\n                   bytes_data,\n                   key,\n                   bucket_name=None,\n                   replace=False,\n                   encrypt=False):\n        \"\"\"\n        Loads bytes to S3\n\n        This is provided as a convenience to drop a string in S3. It uses the\n        boto infrastructure to ship a file to s3.\n\n        :param bytes_data: bytes to set as content for the key.\n        :type bytes_data: bytes\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag to decide whether or not to overwrite the key\n            if it already exists\n        :type replace: bool\n        :param encrypt: If True, the file will be encrypted on the server-side\n            by S3 and will be stored in an encrypted form while at rest in S3.\n        :type encrypt: bool\n        \"\"\"\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        filelike_buffer = BytesIO(bytes_data)\n\n        client = self.get_conn()\n        client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.load_file_obj","method_code":"def load_file_obj(self,\n                      file_obj,\n                      key,\n                      bucket_name=None,\n                      replace=False,\n                      encrypt=False):\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        client = self.get_conn()\n        client.upload_fileobj(file_obj, bucket_name, key, ExtraArgs=extra_args)","method_summary":"Loads a file object to S3","original_method_code":"def load_file_obj(self,\n                      file_obj,\n                      key,\n                      bucket_name=None,\n                      replace=False,\n                      encrypt=False):\n        \"\"\"\n        Loads a file object to S3\n\n        :param file_obj: The file-like object to set as the content for the S3 key.\n        :type file_obj: file-like object\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag that indicates whether to overwrite the key\n            if it already exists.\n        :type replace: bool\n        :param encrypt: If True, S3 encrypts the file on the server,\n            and the file is stored in encrypted form at rest in S3.\n        :type encrypt: bool\n        \"\"\"\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        client = self.get_conn()\n        client.upload_fileobj(file_obj, bucket_name, key, ExtraArgs=extra_args)","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3Hook.copy_object","method_code":"def copy_object(self,\n                    source_bucket_key,\n                    dest_bucket_key,\n                    source_bucket_name=None,\n                    dest_bucket_name=None,\n                    source_version_id=None):\n        if dest_bucket_name is None:\n            dest_bucket_name, dest_bucket_key = self.parse_s3_url(dest_bucket_key)\n        else:\n            parsed_url = urlparse(dest_bucket_key)\n            if parsed_url.scheme != '' or parsed_url.netloc != '':\n                raise AirflowException('If dest_bucket_name is provided, ' +\n                                       'dest_bucket_key should be relative path ' +\n                                       'from root level, rather than a full s3:\/\/ url')\n\n        if source_bucket_name is None:\n            source_bucket_name, source_bucket_key = self.parse_s3_url(source_bucket_key)\n        else:\n            parsed_url = urlparse(source_bucket_key)\n            if parsed_url.scheme != '' or parsed_url.netloc != '':\n                raise AirflowException('If source_bucket_name is provided, ' +\n                                       'source_bucket_key should be relative path ' +\n                                       'from root level, rather than a full s3:\/\/ url')\n\n        CopySource = {'Bucket': source_bucket_name,\n                      'Key': source_bucket_key,\n                      'VersionId': source_version_id}\n        response = self.get_conn().copy_object(Bucket=dest_bucket_name,\n                                               Key=dest_bucket_key,\n                                               CopySource=CopySource)\n        return response","method_summary":"Creates a copy of an object that is already stored in S3.","original_method_code":"def copy_object(self,\n                    source_bucket_key,\n                    dest_bucket_key,\n                    source_bucket_name=None,\n                    dest_bucket_name=None,\n                    source_version_id=None):\n        \"\"\"\n        Creates a copy of an object that is already stored in S3.\n\n        Note: the S3 connection used here needs to have access to both\n        source and destination bucket\/key.\n\n        :param source_bucket_key: The key of the source object.\n\n            It can be either full s3:\/\/ style url or relative path from root level.\n\n            When it's specified as a full s3:\/\/ url, please omit source_bucket_name.\n        :type source_bucket_key: str\n        :param dest_bucket_key: The key of the object to copy to.\n\n            The convention to specify `dest_bucket_key` is the same\n            as `source_bucket_key`.\n        :type dest_bucket_key: str\n        :param source_bucket_name: Name of the S3 bucket where the source object is in.\n\n            It should be omitted when `source_bucket_key` is provided as a full s3:\/\/ url.\n        :type source_bucket_name: str\n        :param dest_bucket_name: Name of the S3 bucket to where the object is copied.\n\n            It should be omitted when `dest_bucket_key` is provided as a full s3:\/\/ url.\n        :type dest_bucket_name: str\n        :param source_version_id: Version ID of the source object (OPTIONAL)\n        :type source_version_id: str\n        \"\"\"\n\n        if dest_bucket_name is None:\n            dest_bucket_name, dest_bucket_key = self.parse_s3_url(dest_bucket_key)\n        else:\n            parsed_url = urlparse(dest_bucket_key)\n            if parsed_url.scheme != '' or parsed_url.netloc != '':\n                raise AirflowException('If dest_bucket_name is provided, ' +\n                                       'dest_bucket_key should be relative path ' +\n                                       'from root level, rather than a full s3:\/\/ url')\n\n        if source_bucket_name is None:\n            source_bucket_name, source_bucket_key = self.parse_s3_url(source_bucket_key)\n        else:\n            parsed_url = urlparse(source_bucket_key)\n            if parsed_url.scheme != '' or parsed_url.netloc != '':\n                raise AirflowException('If source_bucket_name is provided, ' +\n                                       'source_bucket_key should be relative path ' +\n                                       'from root level, rather than a full s3:\/\/ url')\n\n        CopySource = {'Bucket': source_bucket_name,\n                      'Key': source_bucket_key,\n                      'VersionId': source_version_id}\n        response = self.get_conn().copy_object(Bucket=dest_bucket_name,\n                                               Key=dest_bucket_key,\n                                               CopySource=CopySource)\n        return response","method_path":"airflow\/hooks\/S3_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CassandraToGoogleCloudStorageOperator._query_cassandra","method_code":"def _query_cassandra(self):\n        self.hook = CassandraHook(cassandra_conn_id=self.cassandra_conn_id)\n        session = self.hook.get_conn()\n        cursor = session.execute(self.cql)\n        return cursor","method_summary":"Queries cassandra and returns a cursor to the results.","original_method_code":"def _query_cassandra(self):\n        \"\"\"\n        Queries cassandra and returns a cursor to the results.\n        \"\"\"\n        self.hook = CassandraHook(cassandra_conn_id=self.cassandra_conn_id)\n        session = self.hook.get_conn()\n        cursor = session.execute(self.cql)\n        return cursor","method_path":"airflow\/contrib\/operators\/cassandra_to_gcs.py"}
{"repo_name":"apache\/airflow","method_name":"CassandraToGoogleCloudStorageOperator.convert_user_type","method_code":"def convert_user_type(cls, name, value):\n        names = value._fields\n        values = [cls.convert_value(name, getattr(value, name)) for name in names]\n        return cls.generate_data_dict(names, values)","method_summary":"Converts a user type to RECORD that contains n fields, where n is the number of attributes. Each element in the user type class will be converted to its corresponding data type in BQ.","original_method_code":"def convert_user_type(cls, name, value):\n        \"\"\"\n        Converts a user type to RECORD that contains n fields, where n is the\n        number of attributes. Each element in the user type class will be converted to its\n        corresponding data type in BQ.\n        \"\"\"\n        names = value._fields\n        values = [cls.convert_value(name, getattr(value, name)) for name in names]\n        return cls.generate_data_dict(names, values)","method_path":"airflow\/contrib\/operators\/cassandra_to_gcs.py"}
{"repo_name":"apache\/airflow","method_name":"send_email","method_code":"def send_email(to, subject, html_content, files=None, dryrun=False, cc=None,\n               bcc=None, mime_subtype='mixed', sandbox_mode=False, **kwargs):\n    if files is None:\n        files = []\n\n    mail = Mail()\n    from_email = kwargs.get('from_email') or os.environ.get('SENDGRID_MAIL_FROM')\n    from_name = kwargs.get('from_name') or os.environ.get('SENDGRID_MAIL_SENDER')\n    mail.from_email = Email(from_email, from_name)\n    mail.subject = subject\n    mail.mail_settings = MailSettings()\n\n    if sandbox_mode:\n        mail.mail_settings.sandbox_mode = SandBoxMode(enable=True)\n\n    \n    personalization = Personalization()\n    to = get_email_address_list(to)\n    for to_address in to:\n        personalization.add_to(Email(to_address))\n    if cc:\n        cc = get_email_address_list(cc)\n        for cc_address in cc:\n            personalization.add_cc(Email(cc_address))\n    if bcc:\n        bcc = get_email_address_list(bcc)\n        for bcc_address in bcc:\n            personalization.add_bcc(Email(bcc_address))\n\n    \n    pers_custom_args = kwargs.get('personalization_custom_args', None)\n    if isinstance(pers_custom_args, dict):\n        for key in pers_custom_args.keys():\n            personalization.add_custom_arg(CustomArg(key, pers_custom_args[key]))\n\n    mail.add_personalization(personalization)\n    mail.add_content(Content('text\/html', html_content))\n\n    categories = kwargs.get('categories', [])\n    for cat in categories:\n        mail.add_category(Category(cat))\n\n    \n    for fname in files:\n        basename = os.path.basename(fname)\n\n        attachment = Attachment()\n        attachment.type = mimetypes.guess_type(basename)[0]\n        attachment.filename = basename\n        attachment.disposition = \"attachment\"\n        attachment.content_id = '<{0}>'.format(basename)\n\n        with open(fname, \"rb\") as f:\n            attachment.content = base64.b64encode(f.read()).decode('utf-8')\n\n        mail.add_attachment(attachment)\n    _post_sendgrid_mail(mail.get())","method_summary":"Send an email with html content using sendgrid. To use this","original_method_code":"def send_email(to, subject, html_content, files=None, dryrun=False, cc=None,\n               bcc=None, mime_subtype='mixed', sandbox_mode=False, **kwargs):\n    \"\"\"\n    Send an email with html content using sendgrid.\n\n    To use this plugin:\n    0. include sendgrid subpackage as part of your Airflow installation, e.g.,\n    pip install 'apache-airflow[sendgrid]'\n    1. update [email] backend in airflow.cfg, i.e.,\n    [email]\n    email_backend = airflow.contrib.utils.sendgrid.send_email\n    2. configure Sendgrid specific environment variables at all Airflow instances:\n    SENDGRID_MAIL_FROM={your-mail-from}\n    SENDGRID_API_KEY={your-sendgrid-api-key}.\n    \"\"\"\n    if files is None:\n        files = []\n\n    mail = Mail()\n    from_email = kwargs.get('from_email') or os.environ.get('SENDGRID_MAIL_FROM')\n    from_name = kwargs.get('from_name') or os.environ.get('SENDGRID_MAIL_SENDER')\n    mail.from_email = Email(from_email, from_name)\n    mail.subject = subject\n    mail.mail_settings = MailSettings()\n\n    if sandbox_mode:\n        mail.mail_settings.sandbox_mode = SandBoxMode(enable=True)\n\n    # Add the recipient list of to emails.\n    personalization = Personalization()\n    to = get_email_address_list(to)\n    for to_address in to:\n        personalization.add_to(Email(to_address))\n    if cc:\n        cc = get_email_address_list(cc)\n        for cc_address in cc:\n            personalization.add_cc(Email(cc_address))\n    if bcc:\n        bcc = get_email_address_list(bcc)\n        for bcc_address in bcc:\n            personalization.add_bcc(Email(bcc_address))\n\n    # Add custom_args to personalization if present\n    pers_custom_args = kwargs.get('personalization_custom_args', None)\n    if isinstance(pers_custom_args, dict):\n        for key in pers_custom_args.keys():\n            personalization.add_custom_arg(CustomArg(key, pers_custom_args[key]))\n\n    mail.add_personalization(personalization)\n    mail.add_content(Content('text\/html', html_content))\n\n    categories = kwargs.get('categories', [])\n    for cat in categories:\n        mail.add_category(Category(cat))\n\n    # Add email attachment.\n    for fname in files:\n        basename = os.path.basename(fname)\n\n        attachment = Attachment()\n        attachment.type = mimetypes.guess_type(basename)[0]\n        attachment.filename = basename\n        attachment.disposition = \"attachment\"\n        attachment.content_id = '<{0}>'.format(basename)\n\n        with open(fname, \"rb\") as f:\n            attachment.content = base64.b64encode(f.read()).decode('utf-8')\n\n        mail.add_attachment(attachment)\n    _post_sendgrid_mail(mail.get())","method_path":"airflow\/contrib\/utils\/sendgrid.py"}
{"repo_name":"apache\/airflow","method_name":"GCPSpeechToTextHook.get_conn","method_code":"def get_conn(self):\n        if not self._client:\n            self._client = SpeechClient(credentials=self._get_credentials())\n        return self._client","method_summary":"Retrieves connection to Cloud Speech.","original_method_code":"def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Speech.\n\n        :return: Google Cloud Speech client object.\n        :rtype: google.cloud.speech_v1.SpeechClient\n        \"\"\"\n        if not self._client:\n            self._client = SpeechClient(credentials=self._get_credentials())\n        return self._client","method_path":"airflow\/contrib\/hooks\/gcp_speech_to_text_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GCPSpeechToTextHook.recognize_speech","method_code":"def recognize_speech(self, config, audio, retry=None, timeout=None):\n        client = self.get_conn()\n        response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout)\n        self.log.info(\"Recognised speech: %s\" % response)\n        return response","method_summary":"Recognizes audio input","original_method_code":"def recognize_speech(self, config, audio, retry=None, timeout=None):\n        \"\"\"\n        Recognizes audio input\n\n        :param config: information to the recognizer that specifies how to process the request.\n            https:\/\/googleapis.github.io\/google-cloud-python\/latest\/speech\/gapic\/v1\/types.html#google.cloud.speech_v1.types.RecognitionConfig\n        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig\n        :param audio: audio data to be recognized\n            https:\/\/googleapis.github.io\/google-cloud-python\/latest\/speech\/gapic\/v1\/types.html#google.cloud.speech_v1.types.RecognitionAudio\n        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio\n        :param retry: (Optional) A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        \"\"\"\n        client = self.get_conn()\n        response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout)\n        self.log.info(\"Recognised speech: %s\" % response)\n        return response","method_path":"airflow\/contrib\/hooks\/gcp_speech_to_text_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SparkSqlOperator.execute","method_code":"def execute(self, context):\n        self._hook = SparkSqlHook(sql=self._sql,\n                                  conf=self._conf,\n                                  conn_id=self._conn_id,\n                                  total_executor_cores=self._total_executor_cores,\n                                  executor_cores=self._executor_cores,\n                                  executor_memory=self._executor_memory,\n                                  keytab=self._keytab,\n                                  principal=self._principal,\n                                  name=self._name,\n                                  num_executors=self._num_executors,\n                                  master=self._master,\n                                  yarn_queue=self._yarn_queue\n                                  )\n        self._hook.run_query()","method_summary":"Call the SparkSqlHook to run the provided sql query","original_method_code":"def execute(self, context):\n        \"\"\"\n        Call the SparkSqlHook to run the provided sql query\n        \"\"\"\n        self._hook = SparkSqlHook(sql=self._sql,\n                                  conf=self._conf,\n                                  conn_id=self._conn_id,\n                                  total_executor_cores=self._total_executor_cores,\n                                  executor_cores=self._executor_cores,\n                                  executor_memory=self._executor_memory,\n                                  keytab=self._keytab,\n                                  principal=self._principal,\n                                  name=self._name,\n                                  num_executors=self._num_executors,\n                                  master=self._master,\n                                  yarn_queue=self._yarn_queue\n                                  )\n        self._hook.run_query()","method_path":"airflow\/contrib\/operators\/spark_sql_operator.py"}
{"repo_name":"apache\/airflow","method_name":"load_entrypoint_plugins","method_code":"def load_entrypoint_plugins(entry_points, airflow_plugins):\n    for entry_point in entry_points:\n        log.debug('Importing entry_point plugin %s', entry_point.name)\n        plugin_obj = entry_point.load()\n        if is_valid_plugin(plugin_obj, airflow_plugins):\n            if callable(getattr(plugin_obj, 'on_load', None)):\n                plugin_obj.on_load()\n                airflow_plugins.append(plugin_obj)\n    return airflow_plugins","method_summary":"Load AirflowPlugin subclasses from the entrypoints provided. The entry_point group should be 'airflow.plugins'.","original_method_code":"def load_entrypoint_plugins(entry_points, airflow_plugins):\n    \"\"\"\n    Load AirflowPlugin subclasses from the entrypoints\n    provided. The entry_point group should be 'airflow.plugins'.\n\n    :param entry_points: A collection of entrypoints to search for plugins\n    :type entry_points: Generator[setuptools.EntryPoint, None, None]\n    :param airflow_plugins: A collection of existing airflow plugins to\n        ensure we don't load duplicates\n    :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]\n    :rtype: list[airflow.plugins_manager.AirflowPlugin]\n    \"\"\"\n    for entry_point in entry_points:\n        log.debug('Importing entry_point plugin %s', entry_point.name)\n        plugin_obj = entry_point.load()\n        if is_valid_plugin(plugin_obj, airflow_plugins):\n            if callable(getattr(plugin_obj, 'on_load', None)):\n                plugin_obj.on_load()\n                airflow_plugins.append(plugin_obj)\n    return airflow_plugins","method_path":"airflow\/plugins_manager.py"}
{"repo_name":"apache\/airflow","method_name":"is_valid_plugin","method_code":"def is_valid_plugin(plugin_obj, existing_plugins):\n    if (\n        inspect.isclass(plugin_obj) and\n        issubclass(plugin_obj, AirflowPlugin) and\n        (plugin_obj is not AirflowPlugin)\n    ):\n        plugin_obj.validate()\n        return plugin_obj not in existing_plugins\n    return False","method_summary":"Check whether a potential object is a subclass of the AirflowPlugin class.","original_method_code":"def is_valid_plugin(plugin_obj, existing_plugins):\n    \"\"\"\n    Check whether a potential object is a subclass of\n    the AirflowPlugin class.\n\n    :param plugin_obj: potential subclass of AirflowPlugin\n    :param existing_plugins: Existing list of AirflowPlugin subclasses\n    :return: Whether or not the obj is a valid subclass of\n        AirflowPlugin\n    \"\"\"\n    if (\n        inspect.isclass(plugin_obj) and\n        issubclass(plugin_obj, AirflowPlugin) and\n        (plugin_obj is not AirflowPlugin)\n    ):\n        plugin_obj.validate()\n        return plugin_obj not in existing_plugins\n    return False","method_path":"airflow\/plugins_manager.py"}
{"repo_name":"apache\/airflow","method_name":"SkipMixin.skip","method_code":"def skip(self, dag_run, execution_date, tasks, session=None):\n        if not tasks:\n            return\n\n        task_ids = [d.task_id for d in tasks]\n        now = timezone.utcnow()\n\n        if dag_run:\n            session.query(TaskInstance).filter(\n                TaskInstance.dag_id == dag_run.dag_id,\n                TaskInstance.execution_date == dag_run.execution_date,\n                TaskInstance.task_id.in_(task_ids)\n            ).update({TaskInstance.state: State.SKIPPED,\n                      TaskInstance.start_date: now,\n                      TaskInstance.end_date: now},\n                     synchronize_session=False)\n            session.commit()\n        else:\n            assert execution_date is not None, \"Execution date is None and no dag run\"\n\n            self.log.warning(\"No DAG RUN present this should not happen\")\n            \n            for task in tasks:\n                ti = TaskInstance(task, execution_date=execution_date)\n                ti.state = State.SKIPPED\n                ti.start_date = now\n                ti.end_date = now\n                session.merge(ti)\n\n            session.commit()","method_summary":"Sets tasks instances to skipped from the same dag run.","original_method_code":"def skip(self, dag_run, execution_date, tasks, session=None):\n        \"\"\"\n        Sets tasks instances to skipped from the same dag run.\n\n        :param dag_run: the DagRun for which to set the tasks to skipped\n        :param execution_date: execution_date\n        :param tasks: tasks to skip (not task_ids)\n        :param session: db session to use\n        \"\"\"\n        if not tasks:\n            return\n\n        task_ids = [d.task_id for d in tasks]\n        now = timezone.utcnow()\n\n        if dag_run:\n            session.query(TaskInstance).filter(\n                TaskInstance.dag_id == dag_run.dag_id,\n                TaskInstance.execution_date == dag_run.execution_date,\n                TaskInstance.task_id.in_(task_ids)\n            ).update({TaskInstance.state: State.SKIPPED,\n                      TaskInstance.start_date: now,\n                      TaskInstance.end_date: now},\n                     synchronize_session=False)\n            session.commit()\n        else:\n            assert execution_date is not None, \"Execution date is None and no dag run\"\n\n            self.log.warning(\"No DAG RUN present this should not happen\")\n            # this is defensive against dag runs that are not complete\n            for task in tasks:\n                ti = TaskInstance(task, execution_date=execution_date)\n                ti.state = State.SKIPPED\n                ti.start_date = now\n                ti.end_date = now\n                session.merge(ti)\n\n            session.commit()","method_path":"airflow\/models\/skipmixin.py"}
{"repo_name":"apache\/airflow","method_name":"AzureDataLakeHook.get_conn","method_code":"def get_conn(self):\n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        self.account_name = service_options.get('account_name')\n\n        adlCreds = lib.auth(tenant_id=service_options.get('tenant'),\n                            client_secret=conn.password,\n                            client_id=conn.login)\n        adlsFileSystemClient = core.AzureDLFileSystem(adlCreds,\n                                                      store_name=self.account_name)\n        adlsFileSystemClient.connect()\n        return adlsFileSystemClient","method_summary":"Return a AzureDLFileSystem object.","original_method_code":"def get_conn(self):\n        \"\"\"Return a AzureDLFileSystem object.\"\"\"\n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        self.account_name = service_options.get('account_name')\n\n        adlCreds = lib.auth(tenant_id=service_options.get('tenant'),\n                            client_secret=conn.password,\n                            client_id=conn.login)\n        adlsFileSystemClient = core.AzureDLFileSystem(adlCreds,\n                                                      store_name=self.account_name)\n        adlsFileSystemClient.connect()\n        return adlsFileSystemClient","method_path":"airflow\/contrib\/hooks\/azure_data_lake_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureDataLakeHook.check_for_file","method_code":"def check_for_file(self, file_path):\n        try:\n            files = self.connection.glob(file_path, details=False, invalidate_cache=True)\n            return len(files) == 1\n        except FileNotFoundError:\n            return False","method_summary":"Check if a file exists on Azure Data Lake.","original_method_code":"def check_for_file(self, file_path):\n        \"\"\"\n        Check if a file exists on Azure Data Lake.\n\n        :param file_path: Path and name of the file.\n        :type file_path: str\n        :return: True if the file exists, False otherwise.\n        :rtype: bool\n        \"\"\"\n        try:\n            files = self.connection.glob(file_path, details=False, invalidate_cache=True)\n            return len(files) == 1\n        except FileNotFoundError:\n            return False","method_path":"airflow\/contrib\/hooks\/azure_data_lake_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureDataLakeHook.upload_file","method_code":"def upload_file(self, local_path, remote_path, nthreads=64, overwrite=True,\n                    buffersize=4194304, blocksize=4194304):\n        multithread.ADLUploader(self.connection,\n                                lpath=local_path,\n                                rpath=remote_path,\n                                nthreads=nthreads,\n                                overwrite=overwrite,\n                                buffersize=buffersize,\n                                blocksize=blocksize)","method_summary":"Upload a file to Azure Data Lake.","original_method_code":"def upload_file(self, local_path, remote_path, nthreads=64, overwrite=True,\n                    buffersize=4194304, blocksize=4194304):\n        \"\"\"\n        Upload a file to Azure Data Lake.\n\n        :param local_path: local path. Can be single file, directory (in which case,\n            upload recursively) or glob pattern. Recursive glob patterns using `**`\n            are not supported.\n        :type local_path: str\n        :param remote_path: Remote path to upload to; if multiple files, this is the\n            directory root to write within.\n        :type remote_path: str\n        :param nthreads: Number of threads to use. If None, uses the number of cores.\n        :type nthreads: int\n        :param overwrite: Whether to forcibly overwrite existing files\/directories.\n            If False and remote path is a directory, will quit regardless if any files\n            would be overwritten or not. If True, only matching filenames are actually\n            overwritten.\n        :type overwrite: bool\n        :param buffersize: int [2**22]\n            Number of bytes for internal buffer. This block cannot be bigger than\n            a chunk and cannot be smaller than a block.\n        :type buffersize: int\n        :param blocksize: int [2**22]\n            Number of bytes for a block. Within each chunk, we write a smaller\n            block for each API call. This block cannot be bigger than a chunk.\n        :type blocksize: int\n        \"\"\"\n        multithread.ADLUploader(self.connection,\n                                lpath=local_path,\n                                rpath=remote_path,\n                                nthreads=nthreads,\n                                overwrite=overwrite,\n                                buffersize=buffersize,\n                                blocksize=blocksize)","method_path":"airflow\/contrib\/hooks\/azure_data_lake_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureDataLakeHook.list","method_code":"def list(self, path):\n        if \"*\" in path:\n            return self.connection.glob(path)\n        else:\n            return self.connection.walk(path)","method_summary":"List files in Azure Data Lake Storage","original_method_code":"def list(self, path):\n        \"\"\"\n        List files in Azure Data Lake Storage\n\n        :param path: full path\/globstring to use to list files in ADLS\n        :type path: str\n        \"\"\"\n        if \"*\" in path:\n            return self.connection.glob(path)\n        else:\n            return self.connection.walk(path)","method_path":"airflow\/contrib\/hooks\/azure_data_lake_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AWSAthenaOperator.execute","method_code":"def execute(self, context):\n        self.hook = self.get_hook()\n        self.hook.get_conn()\n\n        self.query_execution_context['Database'] = self.database\n        self.result_configuration['OutputLocation'] = self.output_location\n        self.query_execution_id = self.hook.run_query(self.query, self.query_execution_context,\n                                                      self.result_configuration, self.client_request_token)\n        query_status = self.hook.poll_query_status(self.query_execution_id, self.max_tries)\n\n        if query_status in AWSAthenaHook.FAILURE_STATES:\n            raise Exception(\n                'Final state of Athena job is {}, query_execution_id is {}.'\n                .format(query_status, self.query_execution_id))\n        elif not query_status or query_status in AWSAthenaHook.INTERMEDIATE_STATES:\n            raise Exception(\n                'Final state of Athena job is {}. '\n                'Max tries of poll status exceeded, query_execution_id is {}.'\n                .format(query_status, self.query_execution_id))","method_summary":"Run Presto Query on Athena","original_method_code":"def execute(self, context):\n        \"\"\"\n        Run Presto Query on Athena\n        \"\"\"\n        self.hook = self.get_hook()\n        self.hook.get_conn()\n\n        self.query_execution_context['Database'] = self.database\n        self.result_configuration['OutputLocation'] = self.output_location\n        self.query_execution_id = self.hook.run_query(self.query, self.query_execution_context,\n                                                      self.result_configuration, self.client_request_token)\n        query_status = self.hook.poll_query_status(self.query_execution_id, self.max_tries)\n\n        if query_status in AWSAthenaHook.FAILURE_STATES:\n            raise Exception(\n                'Final state of Athena job is {}, query_execution_id is {}.'\n                .format(query_status, self.query_execution_id))\n        elif not query_status or query_status in AWSAthenaHook.INTERMEDIATE_STATES:\n            raise Exception(\n                'Final state of Athena job is {}. '\n                'Max tries of poll status exceeded, query_execution_id is {}.'\n                .format(query_status, self.query_execution_id))","method_path":"airflow\/contrib\/operators\/aws_athena_operator.py"}
{"repo_name":"apache\/airflow","method_name":"uncompress_file","method_code":"def uncompress_file(input_file_name, file_extension, dest_dir):\n    if file_extension.lower() not in ('.gz', '.bz2'):\n        raise NotImplementedError(\"Received {} format. Only gz and bz2 \"\n                                  \"files can currently be uncompressed.\"\n                                  .format(file_extension))\n    if file_extension.lower() == '.gz':\n        fmodule = gzip.GzipFile\n    elif file_extension.lower() == '.bz2':\n        fmodule = bz2.BZ2File\n    with fmodule(input_file_name, mode='rb') as f_compressed,\\\n        NamedTemporaryFile(dir=dest_dir,\n                           mode='wb',\n                           delete=False) as f_uncompressed:\n        shutil.copyfileobj(f_compressed, f_uncompressed)\n    return f_uncompressed.name","method_summary":"Uncompress gz and bz2 files","original_method_code":"def uncompress_file(input_file_name, file_extension, dest_dir):\n    \"\"\"\n    Uncompress gz and bz2 files\n    \"\"\"\n    if file_extension.lower() not in ('.gz', '.bz2'):\n        raise NotImplementedError(\"Received {} format. Only gz and bz2 \"\n                                  \"files can currently be uncompressed.\"\n                                  .format(file_extension))\n    if file_extension.lower() == '.gz':\n        fmodule = gzip.GzipFile\n    elif file_extension.lower() == '.bz2':\n        fmodule = bz2.BZ2File\n    with fmodule(input_file_name, mode='rb') as f_compressed,\\\n        NamedTemporaryFile(dir=dest_dir,\n                           mode='wb',\n                           delete=False) as f_uncompressed:\n        shutil.copyfileobj(f_compressed, f_uncompressed)\n    return f_uncompressed.name","method_path":"airflow\/utils\/compression.py"}
{"repo_name":"apache\/airflow","method_name":"MsSqlToGoogleCloudStorageOperator._query_mssql","method_code":"def _query_mssql(self):\n        mssql = MsSqlHook(mssql_conn_id=self.mssql_conn_id)\n        conn = mssql.get_conn()\n        cursor = conn.cursor()\n        cursor.execute(self.sql)\n        return cursor","method_summary":"Queries MSSQL and returns a cursor of results.","original_method_code":"def _query_mssql(self):\n        \"\"\"\n        Queries MSSQL and returns a cursor of results.\n\n        :return: mssql cursor\n        \"\"\"\n        mssql = MsSqlHook(mssql_conn_id=self.mssql_conn_id)\n        conn = mssql.get_conn()\n        cursor = conn.cursor()\n        cursor.execute(self.sql)\n        return cursor","method_path":"airflow\/contrib\/operators\/mssql_to_gcs.py"}
{"repo_name":"apache\/airflow","method_name":"CgroupTaskRunner._create_cgroup","method_code":"def _create_cgroup(self, path):\n        node = trees.Tree().root\n        path_split = path.split(os.sep)\n        for path_element in path_split:\n            name_to_node = {x.name: x for x in node.children}\n            if path_element not in name_to_node:\n                self.log.debug(\"Creating cgroup %s in %s\", path_element, node.path)\n                node = node.create_cgroup(path_element)\n            else:\n                self.log.debug(\n                    \"Not creating cgroup %s in %s since it already exists\",\n                    path_element, node.path\n                )\n                node = name_to_node[path_element]\n        return node","method_summary":"Create the specified cgroup.","original_method_code":"def _create_cgroup(self, path):\n        \"\"\"\n        Create the specified cgroup.\n\n        :param path: The path of the cgroup to create.\n        E.g. cpu\/mygroup\/mysubgroup\n        :return: the Node associated with the created cgroup.\n        :rtype: cgroupspy.nodes.Node\n        \"\"\"\n        node = trees.Tree().root\n        path_split = path.split(os.sep)\n        for path_element in path_split:\n            name_to_node = {x.name: x for x in node.children}\n            if path_element not in name_to_node:\n                self.log.debug(\"Creating cgroup %s in %s\", path_element, node.path)\n                node = node.create_cgroup(path_element)\n            else:\n                self.log.debug(\n                    \"Not creating cgroup %s in %s since it already exists\",\n                    path_element, node.path\n                )\n                node = name_to_node[path_element]\n        return node","method_path":"airflow\/contrib\/task_runner\/cgroup_task_runner.py"}
{"repo_name":"apache\/airflow","method_name":"CgroupTaskRunner._delete_cgroup","method_code":"def _delete_cgroup(self, path):\n        node = trees.Tree().root\n        path_split = path.split(\"\/\")\n        for path_element in path_split:\n            name_to_node = {x.name: x for x in node.children}\n            if path_element not in name_to_node:\n                self.log.warning(\"Cgroup does not exist: %s\", path)\n                return\n            else:\n                node = name_to_node[path_element]\n        \n        parent = node.parent\n        self.log.debug(\"Deleting cgroup %s\/%s\", parent, node.name)\n        parent.delete_cgroup(node.name)","method_summary":"Delete the specified cgroup.","original_method_code":"def _delete_cgroup(self, path):\n        \"\"\"\n        Delete the specified cgroup.\n\n        :param path: The path of the cgroup to delete.\n        E.g. cpu\/mygroup\/mysubgroup\n        \"\"\"\n        node = trees.Tree().root\n        path_split = path.split(\"\/\")\n        for path_element in path_split:\n            name_to_node = {x.name: x for x in node.children}\n            if path_element not in name_to_node:\n                self.log.warning(\"Cgroup does not exist: %s\", path)\n                return\n            else:\n                node = name_to_node[path_element]\n        # node is now the leaf node\n        parent = node.parent\n        self.log.debug(\"Deleting cgroup %s\/%s\", parent, node.name)\n        parent.delete_cgroup(node.name)","method_path":"airflow\/contrib\/task_runner\/cgroup_task_runner.py"}
{"repo_name":"apache\/airflow","method_name":"DatabricksHook._parse_host","method_code":"def _parse_host(host):\n        urlparse_host = urlparse.urlparse(host).hostname\n        if urlparse_host:\n            \n            return urlparse_host\n        else:\n            \n            return host","method_summary":"The purpose of this function is to be robust to improper connections settings provided by users, specifically in the host field. For example -- when users supply ``","original_method_code":"def _parse_host(host):\n        \"\"\"\n        The purpose of this function is to be robust to improper connections\n        settings provided by users, specifically in the host field.\n\n        For example -- when users supply ``https:\/\/xx.cloud.databricks.com`` as the\n        host, we must strip out the protocol to get the host.::\n\n            h = DatabricksHook()\n            assert h._parse_host('https:\/\/xx.cloud.databricks.com') == \\\n                'xx.cloud.databricks.com'\n\n        In the case where users supply the correct ``xx.cloud.databricks.com`` as the\n        host, this function is a no-op.::\n\n            assert h._parse_host('xx.cloud.databricks.com') == 'xx.cloud.databricks.com'\n\n        \"\"\"\n        urlparse_host = urlparse.urlparse(host).hostname\n        if urlparse_host:\n            # In this case, host = https:\/\/xx.cloud.databricks.com\n            return urlparse_host\n        else:\n            # In this case, host = xx.cloud.databricks.com\n            return host","method_path":"airflow\/contrib\/hooks\/databricks_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DatabricksHook._do_api_call","method_code":"def _do_api_call(self, endpoint_info, json):\n        method, endpoint = endpoint_info\n        url = 'https:\/\/{host}\/{endpoint}'.format(\n            host=self._parse_host(self.databricks_conn.host),\n            endpoint=endpoint)\n        if 'token' in self.databricks_conn.extra_dejson:\n            self.log.info('Using token auth.')\n            auth = _TokenAuth(self.databricks_conn.extra_dejson['token'])\n        else:\n            self.log.info('Using basic auth.')\n            auth = (self.databricks_conn.login, self.databricks_conn.password)\n        if method == 'GET':\n            request_func = requests.get\n        elif method == 'POST':\n            request_func = requests.post\n        else:\n            raise AirflowException('Unexpected HTTP Method: ' + method)\n\n        attempt_num = 1\n        while True:\n            try:\n                response = request_func(\n                    url,\n                    json=json,\n                    auth=auth,\n                    headers=USER_AGENT_HEADER,\n                    timeout=self.timeout_seconds)\n                response.raise_for_status()\n                return response.json()\n            except requests_exceptions.RequestException as e:\n                if not _retryable_error(e):\n                    \n                    \n                    raise AirflowException('Response: {0}, Status Code: {1}'.format(\n                        e.response.content, e.response.status_code))\n\n                self._log_request_error(attempt_num, e)\n\n            if attempt_num == self.retry_limit:\n                raise AirflowException(('API requests to Databricks failed {} times. ' +\n                                        'Giving up.').format(self.retry_limit))\n\n            attempt_num += 1\n            sleep(self.retry_delay)","method_summary":"Utility function to perform an API call with retries","original_method_code":"def _do_api_call(self, endpoint_info, json):\n        \"\"\"\n        Utility function to perform an API call with retries\n\n        :param endpoint_info: Tuple of method and endpoint\n        :type endpoint_info: tuple[string, string]\n        :param json: Parameters for this API call.\n        :type json: dict\n        :return: If the api call returns a OK status code,\n            this function returns the response in JSON. Otherwise,\n            we throw an AirflowException.\n        :rtype: dict\n        \"\"\"\n        method, endpoint = endpoint_info\n        url = 'https:\/\/{host}\/{endpoint}'.format(\n            host=self._parse_host(self.databricks_conn.host),\n            endpoint=endpoint)\n        if 'token' in self.databricks_conn.extra_dejson:\n            self.log.info('Using token auth.')\n            auth = _TokenAuth(self.databricks_conn.extra_dejson['token'])\n        else:\n            self.log.info('Using basic auth.')\n            auth = (self.databricks_conn.login, self.databricks_conn.password)\n        if method == 'GET':\n            request_func = requests.get\n        elif method == 'POST':\n            request_func = requests.post\n        else:\n            raise AirflowException('Unexpected HTTP Method: ' + method)\n\n        attempt_num = 1\n        while True:\n            try:\n                response = request_func(\n                    url,\n                    json=json,\n                    auth=auth,\n                    headers=USER_AGENT_HEADER,\n                    timeout=self.timeout_seconds)\n                response.raise_for_status()\n                return response.json()\n            except requests_exceptions.RequestException as e:\n                if not _retryable_error(e):\n                    # In this case, the user probably made a mistake.\n                    # Don't retry.\n                    raise AirflowException('Response: {0}, Status Code: {1}'.format(\n                        e.response.content, e.response.status_code))\n\n                self._log_request_error(attempt_num, e)\n\n            if attempt_num == self.retry_limit:\n                raise AirflowException(('API requests to Databricks failed {} times. ' +\n                                        'Giving up.').format(self.retry_limit))\n\n            attempt_num += 1\n            sleep(self.retry_delay)","method_path":"airflow\/contrib\/hooks\/databricks_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SalesforceHook.get_conn","method_code":"def get_conn(self):\n        if not self.conn:\n            connection = self.get_connection(self.conn_id)\n            extras = connection.extra_dejson\n            self.conn = Salesforce(\n                username=connection.login,\n                password=connection.password,\n                security_token=extras['security_token'],\n                instance_url=connection.host,\n                sandbox=extras.get('sandbox', False)\n            )\n        return self.conn","method_summary":"Sign into Salesforce, only if we are not already signed in.","original_method_code":"def get_conn(self):\n        \"\"\"\n        Sign into Salesforce, only if we are not already signed in.\n        \"\"\"\n        if not self.conn:\n            connection = self.get_connection(self.conn_id)\n            extras = connection.extra_dejson\n            self.conn = Salesforce(\n                username=connection.login,\n                password=connection.password,\n                security_token=extras['security_token'],\n                instance_url=connection.host,\n                sandbox=extras.get('sandbox', False)\n            )\n        return self.conn","method_path":"airflow\/contrib\/hooks\/salesforce_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SalesforceHook.make_query","method_code":"def make_query(self, query):\n        conn = self.get_conn()\n\n        self.log.info(\"Querying for all objects\")\n        query_results = conn.query_all(query)\n\n        self.log.info(\"Received results: Total size: %s; Done: %s\",\n                      query_results['totalSize'], query_results['done'])\n\n        return query_results","method_summary":"Make a query to Salesforce.","original_method_code":"def make_query(self, query):\n        \"\"\"\n        Make a query to Salesforce.\n\n        :param query: The query to make to Salesforce.\n        :type query: str\n        :return: The query result.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        self.log.info(\"Querying for all objects\")\n        query_results = conn.query_all(query)\n\n        self.log.info(\"Received results: Total size: %s; Done: %s\",\n                      query_results['totalSize'], query_results['done'])\n\n        return query_results","method_path":"airflow\/contrib\/hooks\/salesforce_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SalesforceHook.describe_object","method_code":"def describe_object(self, obj):\n        conn = self.get_conn()\n\n        return conn.__getattr__(obj).describe()","method_summary":"Get the description of an object from Salesforce. This description is the object's schema and some extra metadata that Salesforce stores for each object.","original_method_code":"def describe_object(self, obj):\n        \"\"\"\n        Get the description of an object from Salesforce.\n        This description is the object's schema and\n        some extra metadata that Salesforce stores for each object.\n\n        :param obj: The name of the Salesforce object that we are getting a description of.\n        :type obj: str\n        :return: the description of the Salesforce object.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        return conn.__getattr__(obj).describe()","method_path":"airflow\/contrib\/hooks\/salesforce_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SalesforceHook.get_available_fields","method_code":"def get_available_fields(self, obj):\n        self.get_conn()\n\n        obj_description = self.describe_object(obj)\n\n        return [field['name'] for field in obj_description['fields']]","method_summary":"Get a list of all available fields for an object.","original_method_code":"def get_available_fields(self, obj):\n        \"\"\"\n        Get a list of all available fields for an object.\n\n        :param obj: The name of the Salesforce object that we are getting a description of.\n        :type obj: str\n        :return: the names of the fields.\n        :rtype: list of str\n        \"\"\"\n        self.get_conn()\n\n        obj_description = self.describe_object(obj)\n\n        return [field['name'] for field in obj_description['fields']]","method_path":"airflow\/contrib\/hooks\/salesforce_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SalesforceHook.get_object_from_salesforce","method_code":"def get_object_from_salesforce(self, obj, fields):\n        query = \"SELECT {} FROM {}\".format(\",\".join(fields), obj)\n\n        self.log.info(\"Making query to Salesforce: %s\",\n                      query if len(query) < 30 else \" ... \".join([query[:15], query[-15:]]))\n\n        return self.make_query(query)","method_summary":"Get all instances of the `object` from Salesforce. For each model, only get the fields specified in fields. All we really do underneath the hood is","original_method_code":"def get_object_from_salesforce(self, obj, fields):\n        \"\"\"\n        Get all instances of the `object` from Salesforce.\n        For each model, only get the fields specified in fields.\n\n        All we really do underneath the hood is run:\n            SELECT <fields> FROM <obj>;\n\n        :param obj: The object name to get from Salesforce.\n        :type obj: str\n        :param fields: The fields to get from the object.\n        :type fields: iterable\n        :return: all instances of the object from Salesforce.\n        :rtype: dict\n        \"\"\"\n        query = \"SELECT {} FROM {}\".format(\",\".join(fields), obj)\n\n        self.log.info(\"Making query to Salesforce: %s\",\n                      query if len(query) < 30 else \" ... \".join([query[:15], query[-15:]]))\n\n        return self.make_query(query)","method_path":"airflow\/contrib\/hooks\/salesforce_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SalesforceHook._to_timestamp","method_code":"def _to_timestamp(cls, column):\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        try:\n            column = pd.to_datetime(column)\n        except ValueError:\n            log = LoggingMixin().log\n            log.warning(\"Could not convert field to timestamps: %s\", column.name)\n            return column\n\n        \n        \n        \n        \n        converted = []\n        for value in column:\n            try:\n                converted.append(value.timestamp())\n            except (ValueError, AttributeError):\n                converted.append(pd.np.NaN)\n\n        return pd.Series(converted, index=column.index)","method_summary":"Convert a column of a dataframe to UNIX timestamps if applicable","original_method_code":"def _to_timestamp(cls, column):\n        \"\"\"\n        Convert a column of a dataframe to UNIX timestamps if applicable\n\n        :param column: A Series object representing a column of a dataframe.\n        :type column: pd.Series\n        :return: a new series that maintains the same index as the original\n        :rtype: pd.Series\n        \"\"\"\n        # try and convert the column to datetimes\n        # the column MUST have a four digit year somewhere in the string\n        # there should be a better way to do this,\n        # but just letting pandas try and convert every column without a format\n        # caused it to convert floats as well\n        # For example, a column of integers\n        # between 0 and 10 are turned into timestamps\n        # if the column cannot be converted,\n        # just return the original column untouched\n        try:\n            column = pd.to_datetime(column)\n        except ValueError:\n            log = LoggingMixin().log\n            log.warning(\"Could not convert field to timestamps: %s\", column.name)\n            return column\n\n        # now convert the newly created datetimes into timestamps\n        # we have to be careful here\n        # because NaT cannot be converted to a timestamp\n        # so we have to return NaN\n        converted = []\n        for value in column:\n            try:\n                converted.append(value.timestamp())\n            except (ValueError, AttributeError):\n                converted.append(pd.np.NaN)\n\n        return pd.Series(converted, index=column.index)","method_path":"airflow\/contrib\/hooks\/salesforce_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SalesforceHook.write_object_to_file","method_code":"def write_object_to_file(self,\n                             query_results,\n                             filename,\n                             fmt=\"csv\",\n                             coerce_to_timestamp=False,\n                             record_time_added=False):\n        fmt = fmt.lower()\n        if fmt not in ['csv', 'json', 'ndjson']:\n            raise ValueError(\"Format value is not recognized: {}\".format(fmt))\n\n        \n        \n        \n        \n        df = pd.DataFrame.from_records(query_results, exclude=[\"attributes\"])\n\n        df.columns = [column.lower() for column in df.columns]\n\n        \n        \n        \n        \n        if coerce_to_timestamp and df.shape[0] > 0:\n            \n            \n            \n            object_name = query_results[0]['attributes']['type']\n\n            self.log.info(\"Coercing timestamps for: %s\", object_name)\n\n            schema = self.describe_object(object_name)\n\n            \n            \n            \n            possible_timestamp_cols = [\n                field['name'].lower()\n                for field in schema['fields']\n                if field['type'] in [\"date\", \"datetime\"] and field['name'].lower() in df.columns\n            ]\n            df[possible_timestamp_cols] = df[possible_timestamp_cols].apply(self._to_timestamp)\n\n        if record_time_added:\n            fetched_time = time.time()\n            df[\"time_fetched_from_salesforce\"] = fetched_time\n\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        if fmt == \"csv\":\n            \n            \n            self.log.info(\"Cleaning data and writing to CSV\")\n            possible_strings = df.columns[df.dtypes == \"object\"]\n            df[possible_strings] = df[possible_strings].apply(\n                lambda x: x.str.replace(\"\\r\\n\", \"\").str.replace(\"\\n\", \"\")\n            )\n            \n            df.to_csv(filename, index=False)\n        elif fmt == \"json\":\n            df.to_json(filename, \"records\", date_unit=\"s\")\n        elif fmt == \"ndjson\":\n            df.to_json(filename, \"records\", lines=True, date_unit=\"s\")\n\n        return df","method_summary":"Write query results to file. Acceptable formats","original_method_code":"def write_object_to_file(self,\n                             query_results,\n                             filename,\n                             fmt=\"csv\",\n                             coerce_to_timestamp=False,\n                             record_time_added=False):\n        \"\"\"\n        Write query results to file.\n\n        Acceptable formats are:\n            - csv:\n                comma-separated-values file. This is the default format.\n            - json:\n                JSON array. Each element in the array is a different row.\n            - ndjson:\n                JSON array but each element is new-line delimited instead of comma delimited like in `json`\n\n        This requires a significant amount of cleanup.\n        Pandas doesn't handle output to CSV and json in a uniform way.\n        This is especially painful for datetime types.\n        Pandas wants to write them as strings in CSV, but as millisecond Unix timestamps.\n\n        By default, this function will try and leave all values as they are represented in Salesforce.\n        You use the `coerce_to_timestamp` flag to force all datetimes to become Unix timestamps (UTC).\n        This is can be greatly beneficial as it will make all of your datetime fields look the same,\n        and makes it easier to work with in other database environments\n\n        :param query_results: the results from a SQL query\n        :type query_results: list of dict\n        :param filename: the name of the file where the data should be dumped to\n        :type filename: str\n        :param fmt: the format you want the output in. Default:  'csv'\n        :type fmt: str\n        :param coerce_to_timestamp: True if you want all datetime fields to be converted into Unix timestamps.\n            False if you want them to be left in the same format as they were in Salesforce.\n            Leaving the value as False will result in datetimes being strings. Default: False\n        :type coerce_to_timestamp: bool\n        :param record_time_added: True if you want to add a Unix timestamp field\n            to the resulting data that marks when the data was fetched from Salesforce. Default: False\n        :type record_time_added: bool\n        :return: the dataframe that gets written to the file.\n        :rtype: pd.Dataframe\n        \"\"\"\n        fmt = fmt.lower()\n        if fmt not in ['csv', 'json', 'ndjson']:\n            raise ValueError(\"Format value is not recognized: {}\".format(fmt))\n\n        # this line right here will convert all integers to floats\n        # if there are any None\/np.nan values in the column\n        # that's because None\/np.nan cannot exist in an integer column\n        # we should write all of our timestamps as FLOATS in our final schema\n        df = pd.DataFrame.from_records(query_results, exclude=[\"attributes\"])\n\n        df.columns = [column.lower() for column in df.columns]\n\n        # convert columns with datetime strings to datetimes\n        # not all strings will be datetimes, so we ignore any errors that occur\n        # we get the object's definition at this point and only consider\n        # features that are DATE or DATETIME\n        if coerce_to_timestamp and df.shape[0] > 0:\n            # get the object name out of the query results\n            # it's stored in the \"attributes\" dictionary\n            # for each returned record\n            object_name = query_results[0]['attributes']['type']\n\n            self.log.info(\"Coercing timestamps for: %s\", object_name)\n\n            schema = self.describe_object(object_name)\n\n            # possible columns that can be converted to timestamps\n            # are the ones that are either date or datetime types\n            # strings are too general and we risk unintentional conversion\n            possible_timestamp_cols = [\n                field['name'].lower()\n                for field in schema['fields']\n                if field['type'] in [\"date\", \"datetime\"] and field['name'].lower() in df.columns\n            ]\n            df[possible_timestamp_cols] = df[possible_timestamp_cols].apply(self._to_timestamp)\n\n        if record_time_added:\n            fetched_time = time.time()\n            df[\"time_fetched_from_salesforce\"] = fetched_time\n\n        # write the CSV or JSON file depending on the option\n        # NOTE:\n        #   datetimes here are an issue.\n        #   There is no good way to manage the difference\n        #   for to_json, the options are an epoch or a ISO string\n        #   but for to_csv, it will be a string output by datetime\n        #   For JSON we decided to output the epoch timestamp in seconds\n        #   (as is fairly standard for JavaScript)\n        #   And for csv, we do a string\n        if fmt == \"csv\":\n            # there are also a ton of newline objects that mess up our ability to write to csv\n            # we remove these newlines so that the output is a valid CSV format\n            self.log.info(\"Cleaning data and writing to CSV\")\n            possible_strings = df.columns[df.dtypes == \"object\"]\n            df[possible_strings] = df[possible_strings].apply(\n                lambda x: x.str.replace(\"\\r\\n\", \"\").str.replace(\"\\n\", \"\")\n            )\n            # write the dataframe\n            df.to_csv(filename, index=False)\n        elif fmt == \"json\":\n            df.to_json(filename, \"records\", date_unit=\"s\")\n        elif fmt == \"ndjson\":\n            df.to_json(filename, \"records\", lines=True, date_unit=\"s\")\n\n        return df","method_path":"airflow\/contrib\/hooks\/salesforce_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MongoHook.get_conn","method_code":"def get_conn(self):\n        if self.client is not None:\n            return self.client\n\n        \n        options = self.extras\n\n        \n        if options.get('ssl', False):\n            options.update({'ssl_cert_reqs': CERT_NONE})\n\n        self.client = MongoClient(self.uri, **options)\n\n        return self.client","method_summary":"Fetches PyMongo Client","original_method_code":"def get_conn(self):\n        \"\"\"\n        Fetches PyMongo Client\n        \"\"\"\n        if self.client is not None:\n            return self.client\n\n        # Mongo Connection Options dict that is unpacked when passed to MongoClient\n        options = self.extras\n\n        # If we are using SSL disable requiring certs from specific hostname\n        if options.get('ssl', False):\n            options.update({'ssl_cert_reqs': CERT_NONE})\n\n        self.client = MongoClient(self.uri, **options)\n\n        return self.client","method_path":"airflow\/contrib\/hooks\/mongo_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MongoHook.get_collection","method_code":"def get_collection(self, mongo_collection, mongo_db=None):\n        mongo_db = mongo_db if mongo_db is not None else self.connection.schema\n        mongo_conn = self.get_conn()\n\n        return mongo_conn.get_database(mongo_db).get_collection(mongo_collection)","method_summary":"Fetches a mongo collection object for querying. Uses connection schema as DB unless specified.","original_method_code":"def get_collection(self, mongo_collection, mongo_db=None):\n        \"\"\"\n        Fetches a mongo collection object for querying.\n\n        Uses connection schema as DB unless specified.\n        \"\"\"\n        mongo_db = mongo_db if mongo_db is not None else self.connection.schema\n        mongo_conn = self.get_conn()\n\n        return mongo_conn.get_database(mongo_db).get_collection(mongo_collection)","method_path":"airflow\/contrib\/hooks\/mongo_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MongoHook.replace_many","method_code":"def replace_many(self, mongo_collection, docs,\n                     filter_docs=None, mongo_db=None, upsert=False, collation=None,\n                     **kwargs):\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        if not filter_docs:\n            filter_docs = [{'_id': doc['_id']} for doc in docs]\n\n        requests = [\n            ReplaceOne(\n                filter_docs[i],\n                docs[i],\n                upsert=upsert,\n                collation=collation)\n            for i in range(len(docs))\n        ]\n\n        return collection.bulk_write(requests, **kwargs)","method_summary":"Replaces many documents in a mongo collection. Uses bulk_write with multiple ReplaceOne operations","original_method_code":"def replace_many(self, mongo_collection, docs,\n                     filter_docs=None, mongo_db=None, upsert=False, collation=None,\n                     **kwargs):\n        \"\"\"\n        Replaces many documents in a mongo collection.\n\n        Uses bulk_write with multiple ReplaceOne operations\n        https:\/\/api.mongodb.com\/python\/current\/api\/pymongo\/collection.html#pymongo.collection.Collection.bulk_write\n\n        .. note::\n            If no ``filter_docs``are given, it is assumed that all\n            replacement documents contain the ``_id`` field which are then\n            used as filters.\n\n        :param mongo_collection: The name of the collection to update.\n        :type mongo_collection: str\n        :param docs: The new documents.\n        :type docs: list[dict]\n        :param filter_docs: A list of queries that match the documents to replace.\n            Can be omitted; then the _id fields from docs will be used.\n        :type filter_docs: list[dict]\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n        :param upsert: If ``True``, perform an insert if no documents\n            match the filters for the replace operation.\n        :type upsert: bool\n        :param collation: An instance of\n            :class:`~pymongo.collation.Collation`. This option is only\n            supported on MongoDB 3.4 and above.\n        :type collation: pymongo.collation.Collation\n\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        if not filter_docs:\n            filter_docs = [{'_id': doc['_id']} for doc in docs]\n\n        requests = [\n            ReplaceOne(\n                filter_docs[i],\n                docs[i],\n                upsert=upsert,\n                collation=collation)\n            for i in range(len(docs))\n        ]\n\n        return collection.bulk_write(requests, **kwargs)","method_path":"airflow\/contrib\/hooks\/mongo_hook.py"}
{"repo_name":"apache\/airflow","method_name":"ImapHook.has_mail_attachment","method_code":"def has_mail_attachment(self, name, mail_folder='INBOX', check_regex=False):\n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only=True)\n        return len(mail_attachments) > 0","method_summary":"Checks the mail folder for mails containing attachments with the given name.","original_method_code":"def has_mail_attachment(self, name, mail_folder='INBOX', check_regex=False):\n        \"\"\"\n        Checks the mail folder for mails containing attachments with the given name.\n\n        :param name: The name of the attachment that will be searched for.\n        :type name: str\n        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :returns: True if there is an attachment with the given name and False if not.\n        :rtype: bool\n        \"\"\"\n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only=True)\n        return len(mail_attachments) > 0","method_path":"airflow\/contrib\/hooks\/imap_hook.py"}
{"repo_name":"apache\/airflow","method_name":"ImapHook.retrieve_mail_attachments","method_code":"def retrieve_mail_attachments(self,\n                                  name,\n                                  mail_folder='INBOX',\n                                  check_regex=False,\n                                  latest_only=False,\n                                  not_found_mode='raise'):\n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only)\n        if not mail_attachments:\n            self._handle_not_found_mode(not_found_mode)\n\n        return mail_attachments","method_summary":"Retrieves mail's attachments in the mail folder by its name.","original_method_code":"def retrieve_mail_attachments(self,\n                                  name,\n                                  mail_folder='INBOX',\n                                  check_regex=False,\n                                  latest_only=False,\n                                  not_found_mode='raise'):\n        \"\"\"\n        Retrieves mail's attachments in the mail folder by its name.\n\n        :param name: The name of the attachment that will be downloaded.\n        :type name: str\n        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :param latest_only: If set to True it will only retrieve\n                            the first matched attachment.\n        :type latest_only: bool\n        :param not_found_mode: Specify what should happen if no attachment has been found.\n                               Supported values are 'raise', 'warn' and 'ignore'.\n                               If it is set to 'raise' it will raise an exception,\n                               if set to 'warn' it will only print a warning and\n                               if set to 'ignore' it won't notify you at all.\n        :type not_found_mode: str\n        :returns: a list of tuple each containing the attachment filename and its payload.\n        :rtype: a list of tuple\n        \"\"\"\n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only)\n        if not mail_attachments:\n            self._handle_not_found_mode(not_found_mode)\n\n        return mail_attachments","method_path":"airflow\/contrib\/hooks\/imap_hook.py"}
{"repo_name":"apache\/airflow","method_name":"ImapHook.download_mail_attachments","method_code":"def download_mail_attachments(self,\n                                  name,\n                                  local_output_directory,\n                                  mail_folder='INBOX',\n                                  check_regex=False,\n                                  latest_only=False,\n                                  not_found_mode='raise'):\n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only)\n\n        if not mail_attachments:\n            self._handle_not_found_mode(not_found_mode)\n\n        self._create_files(mail_attachments, local_output_directory)","method_summary":"Downloads mail's attachments in the mail folder by its name to the local directory.","original_method_code":"def download_mail_attachments(self,\n                                  name,\n                                  local_output_directory,\n                                  mail_folder='INBOX',\n                                  check_regex=False,\n                                  latest_only=False,\n                                  not_found_mode='raise'):\n        \"\"\"\n        Downloads mail's attachments in the mail folder by its name to the local directory.\n\n        :param name: The name of the attachment that will be downloaded.\n        :type name: str\n        :param local_output_directory: The output directory on the local machine\n                                       where the files will be downloaded to.\n        :type local_output_directory: str\n        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :param latest_only: If set to True it will only download\n                            the first matched attachment.\n        :type latest_only: bool\n        :param not_found_mode: Specify what should happen if no attachment has been found.\n                               Supported values are 'raise', 'warn' and 'ignore'.\n                               If it is set to 'raise' it will raise an exception,\n                               if set to 'warn' it will only print a warning and\n                               if set to 'ignore' it won't notify you at all.\n        :type not_found_mode: str\n        \"\"\"\n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only)\n\n        if not mail_attachments:\n            self._handle_not_found_mode(not_found_mode)\n\n        self._create_files(mail_attachments, local_output_directory)","method_path":"airflow\/contrib\/hooks\/imap_hook.py"}
{"repo_name":"apache\/airflow","method_name":"Mail.get_attachments_by_name","method_code":"def get_attachments_by_name(self, name, check_regex, find_first=False):\n        attachments = []\n\n        for part in self.mail.walk():\n            mail_part = MailPart(part)\n            if mail_part.is_attachment():\n                found_attachment = mail_part.has_matching_name(name) if check_regex \\\n                    else mail_part.has_equal_name(name)\n                if found_attachment:\n                    file_name, file_payload = mail_part.get_file()\n                    self.log.info('Found attachment: {}'.format(file_name))\n                    attachments.append((file_name, file_payload))\n                    if find_first:\n                        break\n\n        return attachments","method_summary":"Gets all attachments by name for the mail.","original_method_code":"def get_attachments_by_name(self, name, check_regex, find_first=False):\n        \"\"\"\n        Gets all attachments by name for the mail.\n\n        :param name: The name of the attachment to look for.\n        :type name: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :param find_first: If set to True it will only find the first match and then quit.\n        :type find_first: bool\n        :returns: a list of tuples each containing name and payload\n                  where the attachments name matches the given name.\n        :rtype: list of tuple\n        \"\"\"\n        attachments = []\n\n        for part in self.mail.walk():\n            mail_part = MailPart(part)\n            if mail_part.is_attachment():\n                found_attachment = mail_part.has_matching_name(name) if check_regex \\\n                    else mail_part.has_equal_name(name)\n                if found_attachment:\n                    file_name, file_payload = mail_part.get_file()\n                    self.log.info('Found attachment: {}'.format(file_name))\n                    attachments.append((file_name, file_payload))\n                    if find_first:\n                        break\n\n        return attachments","method_path":"airflow\/contrib\/hooks\/imap_hook.py"}
{"repo_name":"apache\/airflow","method_name":"MailPart.get_file","method_code":"def get_file(self):\n        return self.part.get_filename(), self.part.get_payload(decode=True)","method_summary":"Gets the file including name and payload.","original_method_code":"def get_file(self):\n        \"\"\"\n        Gets the file including name and payload.\n\n        :returns: the part's name and payload.\n        :rtype: tuple\n        \"\"\"\n        return self.part.get_filename(), self.part.get_payload(decode=True)","method_path":"airflow\/contrib\/hooks\/imap_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AwsFirehoseHook.put_records","method_code":"def put_records(self, records):\n        firehose_conn = self.get_conn()\n\n        response = firehose_conn.put_record_batch(\n            DeliveryStreamName=self.delivery_stream,\n            Records=records\n        )\n\n        return response","method_summary":"Write batch records to Kinesis Firehose","original_method_code":"def put_records(self, records):\n        \"\"\"\n        Write batch records to Kinesis Firehose\n        \"\"\"\n\n        firehose_conn = self.get_conn()\n\n        response = firehose_conn.put_record_batch(\n            DeliveryStreamName=self.delivery_stream,\n            Records=records\n        )\n\n        return response","method_path":"airflow\/contrib\/hooks\/aws_firehose_hook.py"}
{"repo_name":"apache\/airflow","method_name":"send_email","method_code":"def send_email(to, subject, html_content,\n               files=None, dryrun=False, cc=None, bcc=None,\n               mime_subtype='mixed', mime_charset='utf-8', **kwargs):\n    path, attr = configuration.conf.get('email', 'EMAIL_BACKEND').rsplit('.', 1)\n    module = importlib.import_module(path)\n    backend = getattr(module, attr)\n    to = get_email_address_list(to)\n    to = \", \".join(to)\n\n    return backend(to, subject, html_content, files=files,\n                   dryrun=dryrun, cc=cc, bcc=bcc,\n                   mime_subtype=mime_subtype, mime_charset=mime_charset, **kwargs)","method_summary":"Send email using backend specified in EMAIL_BACKEND.","original_method_code":"def send_email(to, subject, html_content,\n               files=None, dryrun=False, cc=None, bcc=None,\n               mime_subtype='mixed', mime_charset='utf-8', **kwargs):\n    \"\"\"\n    Send email using backend specified in EMAIL_BACKEND.\n    \"\"\"\n    path, attr = configuration.conf.get('email', 'EMAIL_BACKEND').rsplit('.', 1)\n    module = importlib.import_module(path)\n    backend = getattr(module, attr)\n    to = get_email_address_list(to)\n    to = \", \".join(to)\n\n    return backend(to, subject, html_content, files=files,\n                   dryrun=dryrun, cc=cc, bcc=bcc,\n                   mime_subtype=mime_subtype, mime_charset=mime_charset, **kwargs)","method_path":"airflow\/utils\/email.py"}
{"repo_name":"apache\/airflow","method_name":"send_email_smtp","method_code":"def send_email_smtp(to, subject, html_content, files=None,\n                    dryrun=False, cc=None, bcc=None,\n                    mime_subtype='mixed', mime_charset='utf-8',\n                    **kwargs):\n    smtp_mail_from = configuration.conf.get('smtp', 'SMTP_MAIL_FROM')\n\n    to = get_email_address_list(to)\n\n    msg = MIMEMultipart(mime_subtype)\n    msg['Subject'] = subject\n    msg['From'] = smtp_mail_from\n    msg['To'] = \", \".join(to)\n    recipients = to\n    if cc:\n        cc = get_email_address_list(cc)\n        msg['CC'] = \", \".join(cc)\n        recipients = recipients + cc\n\n    if bcc:\n        \n        bcc = get_email_address_list(bcc)\n        recipients = recipients + bcc\n\n    msg['Date'] = formatdate(localtime=True)\n    mime_text = MIMEText(html_content, 'html', mime_charset)\n    msg.attach(mime_text)\n\n    for fname in files or []:\n        basename = os.path.basename(fname)\n        with open(fname, \"rb\") as f:\n            part = MIMEApplication(\n                f.read(),\n                Name=basename\n            )\n            part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename\n            part['Content-ID'] = '<%s>' % basename\n            msg.attach(part)\n\n    send_MIME_email(smtp_mail_from, recipients, msg, dryrun)","method_summary":"Send an email with html content","original_method_code":"def send_email_smtp(to, subject, html_content, files=None,\n                    dryrun=False, cc=None, bcc=None,\n                    mime_subtype='mixed', mime_charset='utf-8',\n                    **kwargs):\n    \"\"\"\n    Send an email with html content\n\n    >>> send_email('test@example.com', 'foo', '<b>Foo<\/b> bar', ['\/dev\/null'], dryrun=True)\n    \"\"\"\n    smtp_mail_from = configuration.conf.get('smtp', 'SMTP_MAIL_FROM')\n\n    to = get_email_address_list(to)\n\n    msg = MIMEMultipart(mime_subtype)\n    msg['Subject'] = subject\n    msg['From'] = smtp_mail_from\n    msg['To'] = \", \".join(to)\n    recipients = to\n    if cc:\n        cc = get_email_address_list(cc)\n        msg['CC'] = \", \".join(cc)\n        recipients = recipients + cc\n\n    if bcc:\n        # don't add bcc in header\n        bcc = get_email_address_list(bcc)\n        recipients = recipients + bcc\n\n    msg['Date'] = formatdate(localtime=True)\n    mime_text = MIMEText(html_content, 'html', mime_charset)\n    msg.attach(mime_text)\n\n    for fname in files or []:\n        basename = os.path.basename(fname)\n        with open(fname, \"rb\") as f:\n            part = MIMEApplication(\n                f.read(),\n                Name=basename\n            )\n            part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename\n            part['Content-ID'] = '<%s>' % basename\n            msg.attach(part)\n\n    send_MIME_email(smtp_mail_from, recipients, msg, dryrun)","method_path":"airflow\/utils\/email.py"}
{"repo_name":"apache\/airflow","method_name":"WasbHook.check_for_blob","method_code":"def check_for_blob(self, container_name, blob_name, **kwargs):\n        return self.connection.exists(container_name, blob_name, **kwargs)","method_summary":"Check if a blob exists on Azure Blob Storage.","original_method_code":"def check_for_blob(self, container_name, blob_name, **kwargs):\n        \"\"\"\n        Check if a blob exists on Azure Blob Storage.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.exists()` takes.\n        :type kwargs: object\n        :return: True if the blob exists, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.connection.exists(container_name, blob_name, **kwargs)","method_path":"airflow\/contrib\/hooks\/wasb_hook.py"}
{"repo_name":"apache\/airflow","method_name":"WasbHook.check_for_prefix","method_code":"def check_for_prefix(self, container_name, prefix, **kwargs):\n        matches = self.connection.list_blobs(container_name, prefix,\n                                             num_results=1, **kwargs)\n        return len(list(matches)) > 0","method_summary":"Check if a prefix exists on Azure Blob storage.","original_method_code":"def check_for_prefix(self, container_name, prefix, **kwargs):\n        \"\"\"\n        Check if a prefix exists on Azure Blob storage.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param prefix: Prefix of the blob.\n        :type prefix: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.list_blobs()` takes.\n        :type kwargs: object\n        :return: True if blobs matching the prefix exist, False otherwise.\n        :rtype: bool\n        \"\"\"\n        matches = self.connection.list_blobs(container_name, prefix,\n                                             num_results=1, **kwargs)\n        return len(list(matches)) > 0","method_path":"airflow\/contrib\/hooks\/wasb_hook.py"}
{"repo_name":"apache\/airflow","method_name":"WasbHook.load_string","method_code":"def load_string(self, string_data, container_name, blob_name, **kwargs):\n        \n        self.connection.create_blob_from_text(container_name, blob_name,\n                                              string_data, **kwargs)","method_summary":"Upload a string to Azure Blob Storage.","original_method_code":"def load_string(self, string_data, container_name, blob_name, **kwargs):\n        \"\"\"\n        Upload a string to Azure Blob Storage.\n\n        :param string_data: String to load.\n        :type string_data: str\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_text()` takes.\n        :type kwargs: object\n        \"\"\"\n        # Reorder the argument order from airflow.hooks.S3_hook.load_string.\n        self.connection.create_blob_from_text(container_name, blob_name,\n                                              string_data, **kwargs)","method_path":"airflow\/contrib\/hooks\/wasb_hook.py"}
{"repo_name":"apache\/airflow","method_name":"WasbHook.read_file","method_code":"def read_file(self, container_name, blob_name, **kwargs):\n        return self.connection.get_blob_to_text(container_name,\n                                                blob_name,\n                                                **kwargs).content","method_summary":"Read a file from Azure Blob Storage and return as a string.","original_method_code":"def read_file(self, container_name, blob_name, **kwargs):\n        \"\"\"\n        Read a file from Azure Blob Storage and return as a string.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_path()` takes.\n        :type kwargs: object\n        \"\"\"\n        return self.connection.get_blob_to_text(container_name,\n                                                blob_name,\n                                                **kwargs).content","method_path":"airflow\/contrib\/hooks\/wasb_hook.py"}
{"repo_name":"apache\/airflow","method_name":"WasbHook.delete_file","method_code":"def delete_file(self, container_name, blob_name, is_prefix=False,\n                    ignore_if_missing=False, **kwargs):\n        if is_prefix:\n            blobs_to_delete = [\n                blob.name for blob in self.connection.list_blobs(\n                    container_name, prefix=blob_name, **kwargs\n                )\n            ]\n        elif self.check_for_blob(container_name, blob_name):\n            blobs_to_delete = [blob_name]\n        else:\n            blobs_to_delete = []\n\n        if not ignore_if_missing and len(blobs_to_delete) == 0:\n            raise AirflowException('Blob(s) not found: {}'.format(blob_name))\n\n        for blob_uri in blobs_to_delete:\n            self.log.info(\"Deleting blob: \" + blob_uri)\n            self.connection.delete_blob(container_name,\n                                        blob_uri,\n                                        delete_snapshots='include',\n                                        **kwargs)","method_summary":"Delete a file from Azure Blob Storage.","original_method_code":"def delete_file(self, container_name, blob_name, is_prefix=False,\n                    ignore_if_missing=False, **kwargs):\n        \"\"\"\n        Delete a file from Azure Blob Storage.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param is_prefix: If blob_name is a prefix, delete all matching files\n        :type is_prefix: bool\n        :param ignore_if_missing: if True, then return success even if the\n            blob does not exist.\n        :type ignore_if_missing: bool\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_path()` takes.\n        :type kwargs: object\n        \"\"\"\n\n        if is_prefix:\n            blobs_to_delete = [\n                blob.name for blob in self.connection.list_blobs(\n                    container_name, prefix=blob_name, **kwargs\n                )\n            ]\n        elif self.check_for_blob(container_name, blob_name):\n            blobs_to_delete = [blob_name]\n        else:\n            blobs_to_delete = []\n\n        if not ignore_if_missing and len(blobs_to_delete) == 0:\n            raise AirflowException('Blob(s) not found: {}'.format(blob_name))\n\n        for blob_uri in blobs_to_delete:\n            self.log.info(\"Deleting blob: \" + blob_uri)\n            self.connection.delete_blob(container_name,\n                                        blob_uri,\n                                        delete_snapshots='include',\n                                        **kwargs)","method_path":"airflow\/contrib\/hooks\/wasb_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DiscordWebhookOperator.execute","method_code":"def execute(self, context):\n        self.hook = DiscordWebhookHook(\n            self.http_conn_id,\n            self.webhook_endpoint,\n            self.message,\n            self.username,\n            self.avatar_url,\n            self.tts,\n            self.proxy\n        )\n        self.hook.execute()","method_summary":"Call the DiscordWebhookHook to post message","original_method_code":"def execute(self, context):\n        \"\"\"\n        Call the DiscordWebhookHook to post message\n        \"\"\"\n        self.hook = DiscordWebhookHook(\n            self.http_conn_id,\n            self.webhook_endpoint,\n            self.message,\n            self.username,\n            self.avatar_url,\n            self.tts,\n            self.proxy\n        )\n        self.hook.execute()","method_path":"airflow\/contrib\/operators\/discord_webhook_operator.py"}
{"repo_name":"apache\/airflow","method_name":"AzureFileShareHook.get_conn","method_code":"def get_conn(self):\n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        return FileService(account_name=conn.login,\n                           account_key=conn.password, **service_options)","method_summary":"Return the FileService object.","original_method_code":"def get_conn(self):\n        \"\"\"Return the FileService object.\"\"\"\n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        return FileService(account_name=conn.login,\n                           account_key=conn.password, **service_options)","method_path":"airflow\/contrib\/hooks\/azure_fileshare_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureFileShareHook.check_for_directory","method_code":"def check_for_directory(self, share_name, directory_name, **kwargs):\n        return self.connection.exists(share_name, directory_name,\n                                      **kwargs)","method_summary":"Check if a directory exists on Azure File Share.","original_method_code":"def check_for_directory(self, share_name, directory_name, **kwargs):\n        \"\"\"\n        Check if a directory exists on Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.exists()` takes.\n        :type kwargs: object\n        :return: True if the file exists, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.connection.exists(share_name, directory_name,\n                                      **kwargs)","method_path":"airflow\/contrib\/hooks\/azure_fileshare_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureFileShareHook.check_for_file","method_code":"def check_for_file(self, share_name, directory_name, file_name, **kwargs):\n        return self.connection.exists(share_name, directory_name,\n                                      file_name, **kwargs)","method_summary":"Check if a file exists on Azure File Share.","original_method_code":"def check_for_file(self, share_name, directory_name, file_name, **kwargs):\n        \"\"\"\n        Check if a file exists on Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.exists()` takes.\n        :type kwargs: object\n        :return: True if the file exists, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.connection.exists(share_name, directory_name,\n                                      file_name, **kwargs)","method_path":"airflow\/contrib\/hooks\/azure_fileshare_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureFileShareHook.list_directories_and_files","method_code":"def list_directories_and_files(self, share_name, directory_name=None, **kwargs):\n        return self.connection.list_directories_and_files(share_name,\n                                                          directory_name,\n                                                          **kwargs)","method_summary":"Return the list of directories and files stored on a Azure File Share.","original_method_code":"def list_directories_and_files(self, share_name, directory_name=None, **kwargs):\n        \"\"\"\n        Return the list of directories and files stored on a Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.list_directories_and_files()` takes.\n        :type kwargs: object\n        :return: A list of files and directories\n        :rtype: list\n        \"\"\"\n        return self.connection.list_directories_and_files(share_name,\n                                                          directory_name,\n                                                          **kwargs)","method_path":"airflow\/contrib\/hooks\/azure_fileshare_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureFileShareHook.create_directory","method_code":"def create_directory(self, share_name, directory_name, **kwargs):\n        return self.connection.create_directory(share_name, directory_name, **kwargs)","method_summary":"Create a new directory on a Azure File Share.","original_method_code":"def create_directory(self, share_name, directory_name, **kwargs):\n        \"\"\"\n        Create a new directory on a Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.create_directory()` takes.\n        :type kwargs: object\n        :return: A list of files and directories\n        :rtype: list\n        \"\"\"\n        return self.connection.create_directory(share_name, directory_name, **kwargs)","method_path":"airflow\/contrib\/hooks\/azure_fileshare_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureFileShareHook.load_file","method_code":"def load_file(self, file_path, share_name, directory_name, file_name, **kwargs):\n        self.connection.create_file_from_path(share_name, directory_name,\n                                              file_name, file_path, **kwargs)","method_summary":"Upload a file to Azure File Share.","original_method_code":"def load_file(self, file_path, share_name, directory_name, file_name, **kwargs):\n        \"\"\"\n        Upload a file to Azure File Share.\n\n        :param file_path: Path to the file to load.\n        :type file_path: str\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.create_file_from_path()` takes.\n        :type kwargs: object\n        \"\"\"\n        self.connection.create_file_from_path(share_name, directory_name,\n                                              file_name, file_path, **kwargs)","method_path":"airflow\/contrib\/hooks\/azure_fileshare_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureFileShareHook.load_string","method_code":"def load_string(self, string_data, share_name, directory_name, file_name, **kwargs):\n        self.connection.create_file_from_text(share_name, directory_name,\n                                              file_name, string_data, **kwargs)","method_summary":"Upload a string to Azure File Share.","original_method_code":"def load_string(self, string_data, share_name, directory_name, file_name, **kwargs):\n        \"\"\"\n        Upload a string to Azure File Share.\n\n        :param string_data: String to load.\n        :type string_data: str\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.create_file_from_text()` takes.\n        :type kwargs: object\n        \"\"\"\n        self.connection.create_file_from_text(share_name, directory_name,\n                                              file_name, string_data, **kwargs)","method_path":"airflow\/contrib\/hooks\/azure_fileshare_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AzureFileShareHook.load_stream","method_code":"def load_stream(self, stream, share_name, directory_name, file_name, count, **kwargs):\n        self.connection.create_file_from_stream(share_name, directory_name,\n                                                file_name, stream, count, **kwargs)","method_summary":"Upload a stream to Azure File Share.","original_method_code":"def load_stream(self, stream, share_name, directory_name, file_name, count, **kwargs):\n        \"\"\"\n        Upload a stream to Azure File Share.\n\n        :param stream: Opened file\/stream to upload as the file content.\n        :type stream: file-like\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param count: Size of the stream in bytes\n        :type count: int\n        :param kwargs: Optional keyword arguments that\n            `FileService.create_file_from_stream()` takes.\n        :type kwargs: object\n        \"\"\"\n        self.connection.create_file_from_stream(share_name, directory_name,\n                                                file_name, stream, count, **kwargs)","method_path":"airflow\/contrib\/hooks\/azure_fileshare_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.copy","method_code":"def copy(self, source_bucket, source_object, destination_bucket=None,\n             destination_object=None):\n        destination_bucket = destination_bucket or source_bucket\n        destination_object = destination_object or source_object\n        if source_bucket == destination_bucket and \\\n                source_object == destination_object:\n\n            raise ValueError(\n                'Either source\/destination bucket or source\/destination object '\n                'must be different, not both the same: bucket=%s, object=%s' %\n                (source_bucket, source_object))\n        if not source_bucket or not source_object:\n            raise ValueError('source_bucket and source_object cannot be empty.')\n\n        client = self.get_conn()\n        source_bucket = client.get_bucket(source_bucket)\n        source_object = source_bucket.blob(source_object)\n        destination_bucket = client.get_bucket(destination_bucket)\n        destination_object = source_bucket.copy_blob(\n            blob=source_object,\n            destination_bucket=destination_bucket,\n            new_name=destination_object)\n\n        self.log.info('Object %s in bucket %s copied to object %s in bucket %s',\n                      source_object.name, source_bucket.name,\n                      destination_object.name, destination_bucket.name)","method_summary":"Copies an object from a bucket to another, with renaming if requested. destination_bucket or destination_object can be omitted, in which case source bucket\/object is used, but not both.","original_method_code":"def copy(self, source_bucket, source_object, destination_bucket=None,\n             destination_object=None):\n        \"\"\"\n        Copies an object from a bucket to another, with renaming if requested.\n\n        destination_bucket or destination_object can be omitted, in which case\n        source bucket\/object is used, but not both.\n\n        :param source_bucket: The bucket of the object to copy from.\n        :type source_bucket: str\n        :param source_object: The object to copy.\n        :type source_object: str\n        :param destination_bucket: The destination of the object to copied to.\n            Can be omitted; then the same bucket is used.\n        :type destination_bucket: str\n        :param destination_object: The (renamed) path of the object if given.\n            Can be omitted; then the same name is used.\n        :type destination_object: str\n        \"\"\"\n        destination_bucket = destination_bucket or source_bucket\n        destination_object = destination_object or source_object\n        if source_bucket == destination_bucket and \\\n                source_object == destination_object:\n\n            raise ValueError(\n                'Either source\/destination bucket or source\/destination object '\n                'must be different, not both the same: bucket=%s, object=%s' %\n                (source_bucket, source_object))\n        if not source_bucket or not source_object:\n            raise ValueError('source_bucket and source_object cannot be empty.')\n\n        client = self.get_conn()\n        source_bucket = client.get_bucket(source_bucket)\n        source_object = source_bucket.blob(source_object)\n        destination_bucket = client.get_bucket(destination_bucket)\n        destination_object = source_bucket.copy_blob(\n            blob=source_object,\n            destination_bucket=destination_bucket,\n            new_name=destination_object)\n\n        self.log.info('Object %s in bucket %s copied to object %s in bucket %s',\n                      source_object.name, source_bucket.name,\n                      destination_object.name, destination_bucket.name)","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.download","method_code":"def download(self, bucket_name, object_name, filename=None):\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n\n        if filename:\n            blob.download_to_filename(filename)\n            self.log.info('File downloaded to %s', filename)\n\n        return blob.download_as_string()","method_summary":"Get a file from Google Cloud Storage.","original_method_code":"def download(self, bucket_name, object_name, filename=None):\n        \"\"\"\n        Get a file from Google Cloud Storage.\n\n        :param bucket_name: The bucket to fetch from.\n        :type bucket_name: str\n        :param object_name: The object to fetch.\n        :type object_name: str\n        :param filename: If set, a local file path where the file should be written to.\n        :type filename: str\n        \"\"\"\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n\n        if filename:\n            blob.download_to_filename(filename)\n            self.log.info('File downloaded to %s', filename)\n\n        return blob.download_as_string()","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.upload","method_code":"def upload(self, bucket_name, object_name, filename,\n               mime_type='application\/octet-stream', gzip=False):\n        if gzip:\n            filename_gz = filename + '.gz'\n\n            with open(filename, 'rb') as f_in:\n                with gz.open(filename_gz, 'wb') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n                    filename = filename_gz\n\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        blob.upload_from_filename(filename=filename,\n                                  content_type=mime_type)\n\n        if gzip:\n            os.remove(filename)\n        self.log.info('File %s uploaded to %s in %s bucket', filename, object_name, bucket_name)","method_summary":"Uploads a local file to Google Cloud Storage.","original_method_code":"def upload(self, bucket_name, object_name, filename,\n               mime_type='application\/octet-stream', gzip=False):\n        \"\"\"\n        Uploads a local file to Google Cloud Storage.\n\n        :param bucket_name: The bucket to upload to.\n        :type bucket_name: str\n        :param object_name: The object name to set when uploading the local file.\n        :type object_name: str\n        :param filename: The local file path to the file to be uploaded.\n        :type filename: str\n        :param mime_type: The MIME type to set when uploading the file.\n        :type mime_type: str\n        :param gzip: Option to compress file for upload\n        :type gzip: bool\n        \"\"\"\n\n        if gzip:\n            filename_gz = filename + '.gz'\n\n            with open(filename, 'rb') as f_in:\n                with gz.open(filename_gz, 'wb') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n                    filename = filename_gz\n\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        blob.upload_from_filename(filename=filename,\n                                  content_type=mime_type)\n\n        if gzip:\n            os.remove(filename)\n        self.log.info('File %s uploaded to %s in %s bucket', filename, object_name, bucket_name)","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.exists","method_code":"def exists(self, bucket_name, object_name):\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        return blob.exists()","method_summary":"Checks for the existence of a file in Google Cloud Storage.","original_method_code":"def exists(self, bucket_name, object_name):\n        \"\"\"\n        Checks for the existence of a file in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the object is.\n        :type bucket_name: str\n        :param object_name: The name of the blob_name to check in the Google cloud\n            storage bucket.\n        :type object_name: str\n        \"\"\"\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        return blob.exists()","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.is_updated_after","method_code":"def is_updated_after(self, bucket_name, object_name, ts):\n        client = self.get_conn()\n        bucket = storage.Bucket(client=client, name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n\n        blob_update_time = blob.updated\n\n        if blob_update_time is not None:\n            import dateutil.tz\n\n            if not ts.tzinfo:\n                ts = ts.replace(tzinfo=dateutil.tz.tzutc())\n\n            self.log.info(\"Verify object date: %s > %s\", blob_update_time, ts)\n\n            if blob_update_time > ts:\n                return True\n\n        return False","method_summary":"Checks if an blob_name is updated in Google Cloud Storage.","original_method_code":"def is_updated_after(self, bucket_name, object_name, ts):\n        \"\"\"\n        Checks if an blob_name is updated in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the object is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google cloud\n            storage bucket.\n        :type object_name: str\n        :param ts: The timestamp to check against.\n        :type ts: datetime.datetime\n        \"\"\"\n        client = self.get_conn()\n        bucket = storage.Bucket(client=client, name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n\n        blob_update_time = blob.updated\n\n        if blob_update_time is not None:\n            import dateutil.tz\n\n            if not ts.tzinfo:\n                ts = ts.replace(tzinfo=dateutil.tz.tzutc())\n\n            self.log.info(\"Verify object date: %s > %s\", blob_update_time, ts)\n\n            if blob_update_time > ts:\n                return True\n\n        return False","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.delete","method_code":"def delete(self, bucket_name, object_name):\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        blob.delete()\n\n        self.log.info('Blob %s deleted.', object_name)","method_summary":"Deletes an object from the bucket.","original_method_code":"def delete(self, bucket_name, object_name):\n        \"\"\"\n        Deletes an object from the bucket.\n\n        :param bucket_name: name of the bucket, where the object resides\n        :type bucket_name: str\n        :param object_name: name of the object to delete\n        :type object_name: str\n        \"\"\"\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        blob.delete()\n\n        self.log.info('Blob %s deleted.', object_name)","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.list","method_code":"def list(self, bucket_name, versions=None, max_results=None, prefix=None, delimiter=None):\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n\n        ids = []\n        pageToken = None\n        while True:\n            blobs = bucket.list_blobs(\n                max_results=max_results,\n                page_token=pageToken,\n                prefix=prefix,\n                delimiter=delimiter,\n                versions=versions\n            )\n\n            blob_names = []\n            for blob in blobs:\n                blob_names.append(blob.name)\n\n            prefixes = blobs.prefixes\n            if prefixes:\n                ids += list(prefixes)\n            else:\n                ids += blob_names\n\n            pageToken = blobs.next_page_token\n            if pageToken is None:\n                \n                break\n        return ids","method_summary":"List all objects from the bucket with the give string prefix in name","original_method_code":"def list(self, bucket_name, versions=None, max_results=None, prefix=None, delimiter=None):\n        \"\"\"\n        List all objects from the bucket with the give string prefix in name\n\n        :param bucket_name: bucket name\n        :type bucket_name: str\n        :param versions: if true, list all versions of the objects\n        :type versions: bool\n        :param max_results: max count of items to return in a single page of responses\n        :type max_results: int\n        :param prefix: prefix string which filters objects whose name begin with\n            this prefix\n        :type prefix: str\n        :param delimiter: filters objects based on the delimiter (for e.g '.csv')\n        :type delimiter: str\n        :return: a stream of object names matching the filtering criteria\n        \"\"\"\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n\n        ids = []\n        pageToken = None\n        while True:\n            blobs = bucket.list_blobs(\n                max_results=max_results,\n                page_token=pageToken,\n                prefix=prefix,\n                delimiter=delimiter,\n                versions=versions\n            )\n\n            blob_names = []\n            for blob in blobs:\n                blob_names.append(blob.name)\n\n            prefixes = blobs.prefixes\n            if prefixes:\n                ids += list(prefixes)\n            else:\n                ids += blob_names\n\n            pageToken = blobs.next_page_token\n            if pageToken is None:\n                # empty next page token\n                break\n        return ids","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.get_size","method_code":"def get_size(self, bucket_name, object_name):\n        self.log.info('Checking the file size of object: %s in bucket_name: %s',\n                      object_name,\n                      bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_size = blob.size\n        self.log.info('The file size of %s is %s bytes.', object_name, blob_size)\n        return blob_size","method_summary":"Gets the size of a file in Google Cloud Storage.","original_method_code":"def get_size(self, bucket_name, object_name):\n        \"\"\"\n        Gets the size of a file in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google\n            cloud storage bucket_name.\n        :type object_name: str\n\n        \"\"\"\n        self.log.info('Checking the file size of object: %s in bucket_name: %s',\n                      object_name,\n                      bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_size = blob.size\n        self.log.info('The file size of %s is %s bytes.', object_name, blob_size)\n        return blob_size","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.get_crc32c","method_code":"def get_crc32c(self, bucket_name, object_name):\n        self.log.info('Retrieving the crc32c checksum of '\n                      'object_name: %s in bucket_name: %s', object_name, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_crc32c = blob.crc32c\n        self.log.info('The crc32c checksum of %s is %s', object_name, blob_crc32c)\n        return blob_crc32c","method_summary":"Gets the CRC32c checksum of an object in Google Cloud Storage.","original_method_code":"def get_crc32c(self, bucket_name, object_name):\n        \"\"\"\n        Gets the CRC32c checksum of an object in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google cloud\n            storage bucket_name.\n        :type object_name: str\n        \"\"\"\n        self.log.info('Retrieving the crc32c checksum of '\n                      'object_name: %s in bucket_name: %s', object_name, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_crc32c = blob.crc32c\n        self.log.info('The crc32c checksum of %s is %s', object_name, blob_crc32c)\n        return blob_crc32c","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.get_md5hash","method_code":"def get_md5hash(self, bucket_name, object_name):\n        self.log.info('Retrieving the MD5 hash of '\n                      'object: %s in bucket: %s', object_name, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_md5hash = blob.md5_hash\n        self.log.info('The md5Hash of %s is %s', object_name, blob_md5hash)\n        return blob_md5hash","method_summary":"Gets the MD5 hash of an object in Google Cloud Storage.","original_method_code":"def get_md5hash(self, bucket_name, object_name):\n        \"\"\"\n        Gets the MD5 hash of an object in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google cloud\n            storage bucket_name.\n        :type object_name: str\n        \"\"\"\n        self.log.info('Retrieving the MD5 hash of '\n                      'object: %s in bucket: %s', object_name, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_md5hash = blob.md5_hash\n        self.log.info('The md5Hash of %s is %s', object_name, blob_md5hash)\n        return blob_md5hash","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.create_bucket","method_code":"def create_bucket(self,\n                      bucket_name,\n                      resource=None,\n                      storage_class='MULTI_REGIONAL',\n                      location='US',\n                      project_id=None,\n                      labels=None\n                      ):\n        self.log.info('Creating Bucket: %s; Location: %s; Storage Class: %s',\n                      bucket_name, location, storage_class)\n\n        client = self.get_conn()\n        bucket = client.bucket(bucket_name=bucket_name)\n        bucket_resource = resource or {}\n\n        for item in bucket_resource:\n            if item != \"name\":\n                bucket._patch_property(name=item, value=resource[item])\n\n        bucket.storage_class = storage_class\n        bucket.labels = labels or {}\n        bucket.create(project=project_id, location=location)\n        return bucket.id","method_summary":"Creates a new bucket. Google Cloud Storage uses a flat namespace, so you can't create a bucket with a name that is already in use.","original_method_code":"def create_bucket(self,\n                      bucket_name,\n                      resource=None,\n                      storage_class='MULTI_REGIONAL',\n                      location='US',\n                      project_id=None,\n                      labels=None\n                      ):\n        \"\"\"\n        Creates a new bucket. Google Cloud Storage uses a flat namespace, so\n        you can't create a bucket with a name that is already in use.\n\n        .. seealso::\n            For more information, see Bucket Naming Guidelines:\n            https:\/\/cloud.google.com\/storage\/docs\/bucketnaming.html#requirements\n\n        :param bucket_name: The name of the bucket.\n        :type bucket_name: str\n        :param resource: An optional dict with parameters for creating the bucket.\n            For information on available parameters, see Cloud Storage API doc:\n            https:\/\/cloud.google.com\/storage\/docs\/json_api\/v1\/buckets\/insert\n        :type resource: dict\n        :param storage_class: This defines how objects in the bucket are stored\n            and determines the SLA and the cost of storage. Values include\n\n            - ``MULTI_REGIONAL``\n            - ``REGIONAL``\n            - ``STANDARD``\n            - ``NEARLINE``\n            - ``COLDLINE``.\n\n            If this value is not specified when the bucket is\n            created, it will default to STANDARD.\n        :type storage_class: str\n        :param location: The location of the bucket.\n            Object data for objects in the bucket resides in physical storage\n            within this region. Defaults to US.\n\n            .. seealso::\n                https:\/\/developers.google.com\/storage\/docs\/bucket-locations\n\n        :type location: str\n        :param project_id: The ID of the GCP Project.\n        :type project_id: str\n        :param labels: User-provided labels, in key\/value pairs.\n        :type labels: dict\n        :return: If successful, it returns the ``id`` of the bucket.\n        \"\"\"\n\n        self.log.info('Creating Bucket: %s; Location: %s; Storage Class: %s',\n                      bucket_name, location, storage_class)\n\n        client = self.get_conn()\n        bucket = client.bucket(bucket_name=bucket_name)\n        bucket_resource = resource or {}\n\n        for item in bucket_resource:\n            if item != \"name\":\n                bucket._patch_property(name=item, value=resource[item])\n\n        bucket.storage_class = storage_class\n        bucket.labels = labels or {}\n        bucket.create(project=project_id, location=location)\n        return bucket.id","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudStorageHook.compose","method_code":"def compose(self, bucket_name, source_objects, destination_object):\n        if not source_objects or not len(source_objects):\n            raise ValueError('source_objects cannot be empty.')\n\n        if not bucket_name or not destination_object:\n            raise ValueError('bucket_name and destination_object cannot be empty.')\n\n        self.log.info(\"Composing %s to %s in the bucket %s\",\n                      source_objects, destination_object, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        destination_blob = bucket.blob(destination_object)\n        destination_blob.compose(\n            sources=[\n                bucket.blob(blob_name=source_object) for source_object in source_objects\n            ])\n\n        self.log.info(\"Completed successfully.\")","method_summary":"Composes a list of existing object into a new object in the same storage bucket_name Currently it only supports up to 32 objects that can be concatenated in a single operation","original_method_code":"def compose(self, bucket_name, source_objects, destination_object):\n        \"\"\"\n        Composes a list of existing object into a new object in the same storage bucket_name\n\n        Currently it only supports up to 32 objects that can be concatenated\n        in a single operation\n\n        https:\/\/cloud.google.com\/storage\/docs\/json_api\/v1\/objects\/compose\n\n        :param bucket_name: The name of the bucket containing the source objects.\n            This is also the same bucket to store the composed destination object.\n        :type bucket_name: str\n        :param source_objects: The list of source objects that will be composed\n            into a single object.\n        :type source_objects: list\n        :param destination_object: The path of the object if given.\n        :type destination_object: str\n        \"\"\"\n\n        if not source_objects or not len(source_objects):\n            raise ValueError('source_objects cannot be empty.')\n\n        if not bucket_name or not destination_object:\n            raise ValueError('bucket_name and destination_object cannot be empty.')\n\n        self.log.info(\"Composing %s to %s in the bucket %s\",\n                      source_objects, destination_object, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        destination_blob = bucket.blob(destination_object)\n        destination_blob.compose(\n            sources=[\n                bucket.blob(blob_name=source_object) for source_object in source_objects\n            ])\n\n        self.log.info(\"Completed successfully.\")","method_path":"airflow\/contrib\/hooks\/gcs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SageMakerHook.tar_and_s3_upload","method_code":"def tar_and_s3_upload(self, path, key, bucket):\n        with tempfile.TemporaryFile() as temp_file:\n            if os.path.isdir(path):\n                files = [os.path.join(path, name) for name in os.listdir(path)]\n            else:\n                files = [path]\n            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:\n                for f in files:\n                    tar_file.add(f, arcname=os.path.basename(f))\n            temp_file.seek(0)\n            self.s3_hook.load_file_obj(temp_file, key, bucket, replace=True)","method_summary":"Tar the local file or directory and upload to s3","original_method_code":"def tar_and_s3_upload(self, path, key, bucket):\n        \"\"\"\n        Tar the local file or directory and upload to s3\n\n        :param path: local file or directory\n        :type path: str\n        :param key: s3 key\n        :type key: str\n        :param bucket: s3 bucket\n        :type bucket: str\n        :return: None\n        \"\"\"\n        with tempfile.TemporaryFile() as temp_file:\n            if os.path.isdir(path):\n                files = [os.path.join(path, name) for name in os.listdir(path)]\n            else:\n                files = [path]\n            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:\n                for f in files:\n                    tar_file.add(f, arcname=os.path.basename(f))\n            temp_file.seek(0)\n            self.s3_hook.load_file_obj(temp_file, key, bucket, replace=True)","method_path":"airflow\/contrib\/hooks\/sagemaker_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SageMakerHook.configure_s3_resources","method_code":"def configure_s3_resources(self, config):\n        s3_operations = config.pop('S3Operations', None)\n\n        if s3_operations is not None:\n            create_bucket_ops = s3_operations.get('S3CreateBucket', [])\n            upload_ops = s3_operations.get('S3Upload', [])\n            for op in create_bucket_ops:\n                self.s3_hook.create_bucket(bucket_name=op['Bucket'])\n            for op in upload_ops:\n                if op['Tar']:\n                    self.tar_and_s3_upload(op['Path'], op['Key'],\n                                           op['Bucket'])\n                else:\n                    self.s3_hook.load_file(op['Path'], op['Key'],\n                                           op['Bucket'])","method_summary":"Extract the S3 operations from the configuration and execute them.","original_method_code":"def configure_s3_resources(self, config):\n        \"\"\"\n        Extract the S3 operations from the configuration and execute them.\n\n        :param config: config of SageMaker operation\n        :type config: dict\n        :rtype: dict\n        \"\"\"\n        s3_operations = config.pop('S3Operations', None)\n\n        if s3_operations is not None:\n            create_bucket_ops = s3_operations.get('S3CreateBucket', [])\n            upload_ops = s3_operations.get('S3Upload', [])\n            for op in create_bucket_ops:\n                self.s3_hook.create_bucket(bucket_name=op['Bucket'])\n            for op in upload_ops:\n                if op['Tar']:\n                    self.tar_and_s3_upload(op['Path'], op['Key'],\n                                           op['Bucket'])\n                else:\n                    self.s3_hook.load_file(op['Path'], op['Key'],\n                                           op['Bucket'])","method_path":"airflow\/contrib\/hooks\/sagemaker_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SageMakerHook.check_s3_url","method_code":"def check_s3_url(self, s3url):\n        bucket, key = S3Hook.parse_s3_url(s3url)\n        if not self.s3_hook.check_for_bucket(bucket_name=bucket):\n            raise AirflowException(\n                \"The input S3 Bucket {} does not exist \".format(bucket))\n        if key and not self.s3_hook.check_for_key(key=key, bucket_name=bucket)\\\n           and not self.s3_hook.check_for_prefix(\n                prefix=key, bucket_name=bucket, delimiter='\/'):\n            \n            \n            \n            raise AirflowException(\"The input S3 Key \"\n                                   \"or Prefix {} does not exist in the Bucket {}\"\n                                   .format(s3url, bucket))\n        return True","method_summary":"Check if an S3 URL exists","original_method_code":"def check_s3_url(self, s3url):\n        \"\"\"\n        Check if an S3 URL exists\n\n        :param s3url: S3 url\n        :type s3url: str\n        :rtype: bool\n        \"\"\"\n        bucket, key = S3Hook.parse_s3_url(s3url)\n        if not self.s3_hook.check_for_bucket(bucket_name=bucket):\n            raise AirflowException(\n                \"The input S3 Bucket {} does not exist \".format(bucket))\n        if key and not self.s3_hook.check_for_key(key=key, bucket_name=bucket)\\\n           and not self.s3_hook.check_for_prefix(\n                prefix=key, bucket_name=bucket, delimiter='\/'):\n            # check if s3 key exists in the case user provides a single file\n            # or if s3 prefix exists in the case user provides multiple files in\n            # a prefix\n            raise AirflowException(\"The input S3 Key \"\n                                   \"or Prefix {} does not exist in the Bucket {}\"\n                                   .format(s3url, bucket))\n        return True","method_path":"airflow\/contrib\/hooks\/sagemaker_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SageMakerHook.get_log_conn","method_code":"def get_log_conn(self):\n        config = botocore.config.Config(retries={'max_attempts': 15})\n        return self.get_client_type('logs', config=config)","method_summary":"Establish an AWS connection for retrieving logs during training","original_method_code":"def get_log_conn(self):\n        \"\"\"\n        Establish an AWS connection for retrieving logs during training\n\n        :rtype: CloudWatchLogs.Client\n        \"\"\"\n        config = botocore.config.Config(retries={'max_attempts': 15})\n        return self.get_client_type('logs', config=config)","method_path":"airflow\/contrib\/hooks\/sagemaker_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SageMakerHook.create_training_job","method_code":"def create_training_job(self, config, wait_for_completion=True, print_log=True,\n                            check_interval=30, max_ingestion_time=None):\n        self.check_training_config(config)\n\n        response = self.get_conn().create_training_job(**config)\n        if print_log:\n            self.check_training_status_with_log(config['TrainingJobName'],\n                                                self.non_terminal_states,\n                                                self.failed_states,\n                                                wait_for_completion,\n                                                check_interval, max_ingestion_time\n                                                )\n        elif wait_for_completion:\n            describe_response = self.check_status(config['TrainingJobName'],\n                                                  'TrainingJobStatus',\n                                                  self.describe_training_job,\n                                                  check_interval, max_ingestion_time\n                                                  )\n\n            billable_time = \\\n                (describe_response['TrainingEndTime'] - describe_response['TrainingStartTime']) * \\\n                describe_response['ResourceConfig']['InstanceCount']\n            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))\n\n        return response","method_summary":"Create a training job","original_method_code":"def create_training_job(self, config, wait_for_completion=True, print_log=True,\n                            check_interval=30, max_ingestion_time=None):\n        \"\"\"\n        Create a training job\n\n        :param config: the config for training\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: A response to training job creation\n        \"\"\"\n\n        self.check_training_config(config)\n\n        response = self.get_conn().create_training_job(**config)\n        if print_log:\n            self.check_training_status_with_log(config['TrainingJobName'],\n                                                self.non_terminal_states,\n                                                self.failed_states,\n                                                wait_for_completion,\n                                                check_interval, max_ingestion_time\n                                                )\n        elif wait_for_completion:\n            describe_response = self.check_status(config['TrainingJobName'],\n                                                  'TrainingJobStatus',\n                                                  self.describe_training_job,\n                                                  check_interval, max_ingestion_time\n                                                  )\n\n            billable_time = \\\n                (describe_response['TrainingEndTime'] - describe_response['TrainingStartTime']) * \\\n                describe_response['ResourceConfig']['InstanceCount']\n            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))\n\n        return response","method_path":"airflow\/contrib\/hooks\/sagemaker_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SageMakerHook.create_tuning_job","method_code":"def create_tuning_job(self, config, wait_for_completion=True,\n                          check_interval=30, max_ingestion_time=None):\n        self.check_tuning_config(config)\n\n        response = self.get_conn().create_hyper_parameter_tuning_job(**config)\n        if wait_for_completion:\n            self.check_status(config['HyperParameterTuningJobName'],\n                              'HyperParameterTuningJobStatus',\n                              self.describe_tuning_job,\n                              check_interval, max_ingestion_time\n                              )\n        return response","method_summary":"Create a tuning job","original_method_code":"def create_tuning_job(self, config, wait_for_completion=True,\n                          check_interval=30, max_ingestion_time=None):\n        \"\"\"\n        Create a tuning job\n\n        :param config: the config for tuning\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: A response to tuning job creation\n        \"\"\"\n\n        self.check_tuning_config(config)\n\n        response = self.get_conn().create_hyper_parameter_tuning_job(**config)\n        if wait_for_completion:\n            self.check_status(config['HyperParameterTuningJobName'],\n                              'HyperParameterTuningJobStatus',\n                              self.describe_tuning_job,\n                              check_interval, max_ingestion_time\n                              )\n        return response","method_path":"airflow\/contrib\/hooks\/sagemaker_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SageMakerHook.create_transform_job","method_code":"def create_transform_job(self, config, wait_for_completion=True,\n                             check_interval=30, max_ingestion_time=None):\n        self.check_s3_url(config['TransformInput']['DataSource']['S3DataSource']['S3Uri'])\n\n        response = self.get_conn().create_transform_job(**config)\n        if wait_for_completion:\n            self.check_status(config['TransformJobName'],\n                              'TransformJobStatus',\n                              self.describe_transform_job,\n                              check_interval, max_ingestion_time\n                              )\n        return response","method_summary":"Create a transform job","original_method_code":"def create_transform_job(self, config, wait_for_completion=True,\n                             check_interval=30, max_ingestion_time=None):\n        \"\"\"\n        Create a transform job\n\n        :param config: the config for transform job\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: A response to transform job creation\n        \"\"\"\n\n        self.check_s3_url(config['TransformInput']['DataSource']['S3DataSource']['S3Uri'])\n\n        response = self.get_conn().create_transform_job(**config)\n        if wait_for_completion:\n            self.check_status(config['TransformJobName'],\n                              'TransformJobStatus',\n                              self.describe_transform_job,\n                              check_interval, max_ingestion_time\n                              )\n        return response","method_path":"airflow\/contrib\/hooks\/sagemaker_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SageMakerHook.create_endpoint","method_code":"def create_endpoint(self, config, wait_for_completion=True,\n                        check_interval=30, max_ingestion_time=None):\n        response = self.get_conn().create_endpoint(**config)\n        if wait_for_completion:\n            self.check_status(config['EndpointName'],\n                              'EndpointStatus',\n                              self.describe_endpoint,\n                              check_interval, max_ingestion_time,\n                              non_terminal_states=self.endpoint_non_terminal_states\n                              )\n        return response","method_summary":"Create an endpoint","original_method_code":"def create_endpoint(self, config, wait_for_completion=True,\n                        check_interval=30, max_ingestion_time=None):\n        \"\"\"\n        Create an endpoint\n\n        :param config: the config for endpoint\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: A response to endpoint creation\n        \"\"\"\n\n        response = self.get_conn().create_endpoint(**config)\n        if wait_for_completion:\n            self.check_status(config['EndpointName'],\n                              'EndpointStatus',\n                              self.describe_endpoint,\n                              check_interval, max_ingestion_time,\n                              non_terminal_states=self.endpoint_non_terminal_states\n                              )\n        return response","method_path":"airflow\/contrib\/hooks\/sagemaker_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SageMakerHook.describe_training_job_with_log","method_code":"def describe_training_job_with_log(self, job_name, positions, stream_names,\n                                       instance_count, state, last_description,\n                                       last_describe_job_call):\n        log_group = '\/aws\/sagemaker\/TrainingJobs'\n\n        if len(stream_names) < instance_count:\n            \n            \n            logs_conn = self.get_log_conn()\n            try:\n                streams = logs_conn.describe_log_streams(\n                    logGroupName=log_group,\n                    logStreamNamePrefix=job_name + '\/',\n                    orderBy='LogStreamName',\n                    limit=instance_count\n                )\n                stream_names = [s['logStreamName'] for s in streams['logStreams']]\n                positions.update([(s, Position(timestamp=0, skip=0))\n                                  for s in stream_names if s not in positions])\n            except logs_conn.exceptions.ResourceNotFoundException:\n                \n                \n                pass\n\n        if len(stream_names) > 0:\n            for idx, event in self.multi_stream_iter(log_group, stream_names, positions):\n                self.log.info(event['message'])\n                ts, count = positions[stream_names[idx]]\n                if event['timestamp'] == ts:\n                    positions[stream_names[idx]] = Position(timestamp=ts, skip=count + 1)\n                else:\n                    positions[stream_names[idx]] = Position(timestamp=event['timestamp'], skip=1)\n\n        if state == LogState.COMPLETE:\n            return state, last_description, last_describe_job_call\n\n        if state == LogState.JOB_COMPLETE:\n            state = LogState.COMPLETE\n        elif time.time() - last_describe_job_call >= 30:\n            description = self.describe_training_job(job_name)\n            last_describe_job_call = time.time()\n\n            if secondary_training_status_changed(description, last_description):\n                self.log.info(secondary_training_status_message(description, last_description))\n                last_description = description\n\n            status = description['TrainingJobStatus']\n\n            if status not in self.non_terminal_states:\n                state = LogState.JOB_COMPLETE\n        return state, last_description, last_describe_job_call","method_summary":"Return the training job info associated with job_name and print CloudWatch logs","original_method_code":"def describe_training_job_with_log(self, job_name, positions, stream_names,\n                                       instance_count, state, last_description,\n                                       last_describe_job_call):\n        \"\"\"\n        Return the training job info associated with job_name and print CloudWatch logs\n        \"\"\"\n        log_group = '\/aws\/sagemaker\/TrainingJobs'\n\n        if len(stream_names) < instance_count:\n            # Log streams are created whenever a container starts writing to stdout\/err, so this list\n            # may be dynamic until we have a stream for every instance.\n            logs_conn = self.get_log_conn()\n            try:\n                streams = logs_conn.describe_log_streams(\n                    logGroupName=log_group,\n                    logStreamNamePrefix=job_name + '\/',\n                    orderBy='LogStreamName',\n                    limit=instance_count\n                )\n                stream_names = [s['logStreamName'] for s in streams['logStreams']]\n                positions.update([(s, Position(timestamp=0, skip=0))\n                                  for s in stream_names if s not in positions])\n            except logs_conn.exceptions.ResourceNotFoundException:\n                # On the very first training job run on an account, there's no log group until\n                # the container starts logging, so ignore any errors thrown about that\n                pass\n\n        if len(stream_names) > 0:\n            for idx, event in self.multi_stream_iter(log_group, stream_names, positions):\n                self.log.info(event['message'])\n                ts, count = positions[stream_names[idx]]\n                if event['timestamp'] == ts:\n                    positions[stream_names[idx]] = Position(timestamp=ts, skip=count + 1)\n                else:\n                    positions[stream_names[idx]] = Position(timestamp=event['timestamp'], skip=1)\n\n        if state == LogState.COMPLETE:\n            return state, last_description, last_describe_job_call\n\n        if state == LogState.JOB_COMPLETE:\n            state = LogState.COMPLETE\n        elif time.time() - last_describe_job_call >= 30:\n            description = self.describe_training_job(job_name)\n            last_describe_job_call = time.time()\n\n            if secondary_training_status_changed(description, last_description):\n                self.log.info(secondary_training_status_message(description, last_description))\n                last_description = description\n\n            status = description['TrainingJobStatus']\n\n            if status not in self.non_terminal_states:\n                state = LogState.JOB_COMPLETE\n        return state, last_description, last_describe_job_call","method_path":"airflow\/contrib\/hooks\/sagemaker_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SageMakerHook.check_status","method_code":"def check_status(self, job_name, key,\n                     describe_function, check_interval,\n                     max_ingestion_time,\n                     non_terminal_states=None):\n        if not non_terminal_states:\n            non_terminal_states = self.non_terminal_states\n\n        sec = 0\n        running = True\n\n        while running:\n            time.sleep(check_interval)\n            sec = sec + check_interval\n\n            try:\n                response = describe_function(job_name)\n                status = response[key]\n                self.log.info('Job still running for %s seconds... '\n                              'current status is %s' % (sec, status))\n            except KeyError:\n                raise AirflowException('Could not get status of the SageMaker job')\n            except ClientError:\n                raise AirflowException('AWS request failed, check logs for more info')\n\n            if status in non_terminal_states:\n                running = True\n            elif status in self.failed_states:\n                raise AirflowException('SageMaker job failed because %s' % response['FailureReason'])\n            else:\n                running = False\n\n            if max_ingestion_time and sec > max_ingestion_time:\n                \n                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)\n\n        self.log.info('SageMaker Job Compeleted')\n        response = describe_function(job_name)\n        return response","method_summary":"Check status of a SageMaker job","original_method_code":"def check_status(self, job_name, key,\n                     describe_function, check_interval,\n                     max_ingestion_time,\n                     non_terminal_states=None):\n        \"\"\"\n        Check status of a SageMaker job\n\n        :param job_name: name of the job to check status\n        :type job_name: str\n        :param key: the key of the response dict\n            that points to the state\n        :type key: str\n        :param describe_function: the function used to retrieve the status\n        :type describe_function: python callable\n        :param args: the arguments for the function\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :param non_terminal_states: the set of nonterminal states\n        :type non_terminal_states: set\n        :return: response of describe call after job is done\n        \"\"\"\n        if not non_terminal_states:\n            non_terminal_states = self.non_terminal_states\n\n        sec = 0\n        running = True\n\n        while running:\n            time.sleep(check_interval)\n            sec = sec + check_interval\n\n            try:\n                response = describe_function(job_name)\n                status = response[key]\n                self.log.info('Job still running for %s seconds... '\n                              'current status is %s' % (sec, status))\n            except KeyError:\n                raise AirflowException('Could not get status of the SageMaker job')\n            except ClientError:\n                raise AirflowException('AWS request failed, check logs for more info')\n\n            if status in non_terminal_states:\n                running = True\n            elif status in self.failed_states:\n                raise AirflowException('SageMaker job failed because %s' % response['FailureReason'])\n            else:\n                running = False\n\n            if max_ingestion_time and sec > max_ingestion_time:\n                # ensure that the job gets killed if the max ingestion time is exceeded\n                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)\n\n        self.log.info('SageMaker Job Compeleted')\n        response = describe_function(job_name)\n        return response","method_path":"airflow\/contrib\/hooks\/sagemaker_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SageMakerHook.check_training_status_with_log","method_code":"def check_training_status_with_log(self, job_name, non_terminal_states, failed_states,\n                                       wait_for_completion, check_interval, max_ingestion_time):\n        sec = 0\n        description = self.describe_training_job(job_name)\n        self.log.info(secondary_training_status_message(description, None))\n        instance_count = description['ResourceConfig']['InstanceCount']\n        status = description['TrainingJobStatus']\n\n        stream_names = []  \n        positions = {}     \n\n        job_already_completed = status not in non_terminal_states\n\n        state = LogState.TAILING if wait_for_completion and not job_already_completed else LogState.COMPLETE\n\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        last_describe_job_call = time.time()\n        last_description = description\n\n        while True:\n            time.sleep(check_interval)\n            sec = sec + check_interval\n\n            state, last_description, last_describe_job_call = \\\n                self.describe_training_job_with_log(job_name, positions, stream_names,\n                                                    instance_count, state, last_description,\n                                                    last_describe_job_call)\n            if state == LogState.COMPLETE:\n                break\n\n            if max_ingestion_time and sec > max_ingestion_time:\n                \n                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)\n\n        if wait_for_completion:\n            status = last_description['TrainingJobStatus']\n            if status in failed_states:\n                reason = last_description.get('FailureReason', '(No reason provided)')\n                raise AirflowException('Error training {}: {} Reason: {}'.format(job_name, status, reason))\n            billable_time = (last_description['TrainingEndTime'] - last_description['TrainingStartTime']) \\\n                * instance_count\n            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))","method_summary":"Display the logs for a given training job, optionally tailing them until the job is complete.","original_method_code":"def check_training_status_with_log(self, job_name, non_terminal_states, failed_states,\n                                       wait_for_completion, check_interval, max_ingestion_time):\n        \"\"\"\n        Display the logs for a given training job, optionally tailing them until the\n        job is complete.\n\n        :param job_name: name of the training job to check status and display logs for\n        :type job_name: str\n        :param non_terminal_states: the set of non_terminal states\n        :type non_terminal_states: set\n        :param failed_states: the set of failed states\n        :type failed_states: set\n        :param wait_for_completion: Whether to keep looking for new log entries\n            until the job completes\n        :type wait_for_completion: bool\n        :param check_interval: The interval in seconds between polling for new log entries and job completion\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: None\n        \"\"\"\n\n        sec = 0\n        description = self.describe_training_job(job_name)\n        self.log.info(secondary_training_status_message(description, None))\n        instance_count = description['ResourceConfig']['InstanceCount']\n        status = description['TrainingJobStatus']\n\n        stream_names = []  # The list of log streams\n        positions = {}     # The current position in each stream, map of stream name -> position\n\n        job_already_completed = status not in non_terminal_states\n\n        state = LogState.TAILING if wait_for_completion and not job_already_completed else LogState.COMPLETE\n\n        # The loop below implements a state machine that alternates between checking the job status and\n        # reading whatever is available in the logs at this point. Note, that if we were called with\n        # wait_for_completion == False, we never check the job status.\n        #\n        # If wait_for_completion == TRUE and job is not completed, the initial state is TAILING\n        # If wait_for_completion == FALSE, the initial state is COMPLETE\n        # (doesn't matter if the job really is complete).\n        #\n        # The state table:\n        #\n        # STATE               ACTIONS                        CONDITION             NEW STATE\n        # ----------------    ----------------               -----------------     ----------------\n        # TAILING             Read logs, Pause, Get status   Job complete          JOB_COMPLETE\n        #                                                    Else                  TAILING\n        # JOB_COMPLETE        Read logs, Pause               Any                   COMPLETE\n        # COMPLETE            Read logs, Exit                                      N\/A\n        #\n        # Notes:\n        # - The JOB_COMPLETE state forces us to do an extra pause and read any items that\n        # got to Cloudwatch after the job was marked complete.\n        last_describe_job_call = time.time()\n        last_description = description\n\n        while True:\n            time.sleep(check_interval)\n            sec = sec + check_interval\n\n            state, last_description, last_describe_job_call = \\\n                self.describe_training_job_with_log(job_name, positions, stream_names,\n                                                    instance_count, state, last_description,\n                                                    last_describe_job_call)\n            if state == LogState.COMPLETE:\n                break\n\n            if max_ingestion_time and sec > max_ingestion_time:\n                # ensure that the job gets killed if the max ingestion time is exceeded\n                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)\n\n        if wait_for_completion:\n            status = last_description['TrainingJobStatus']\n            if status in failed_states:\n                reason = last_description.get('FailureReason', '(No reason provided)')\n                raise AirflowException('Error training {}: {} Reason: {}'.format(job_name, status, reason))\n            billable_time = (last_description['TrainingEndTime'] - last_description['TrainingStartTime']) \\\n                * instance_count\n            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))","method_path":"airflow\/contrib\/hooks\/sagemaker_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DataFlowPythonOperator.execute","method_code":"def execute(self, context):\n        bucket_helper = GoogleCloudBucketHelper(\n            self.gcp_conn_id, self.delegate_to)\n        self.py_file = bucket_helper.google_cloud_to_local(self.py_file)\n        hook = DataFlowHook(gcp_conn_id=self.gcp_conn_id,\n                            delegate_to=self.delegate_to,\n                            poll_sleep=self.poll_sleep)\n        dataflow_options = self.dataflow_default_options.copy()\n        dataflow_options.update(self.options)\n        \n        camel_to_snake = lambda name: re.sub(\n            r'[A-Z]', lambda x: '_' + x.group(0).lower(), name)\n        formatted_options = {camel_to_snake(key): dataflow_options[key]\n                             for key in dataflow_options}\n        hook.start_python_dataflow(\n            self.job_name, formatted_options,\n            self.py_file, self.py_options)","method_summary":"Execute the python dataflow job.","original_method_code":"def execute(self, context):\n        \"\"\"Execute the python dataflow job.\"\"\"\n        bucket_helper = GoogleCloudBucketHelper(\n            self.gcp_conn_id, self.delegate_to)\n        self.py_file = bucket_helper.google_cloud_to_local(self.py_file)\n        hook = DataFlowHook(gcp_conn_id=self.gcp_conn_id,\n                            delegate_to=self.delegate_to,\n                            poll_sleep=self.poll_sleep)\n        dataflow_options = self.dataflow_default_options.copy()\n        dataflow_options.update(self.options)\n        # Convert argument names from lowerCamelCase to snake case.\n        camel_to_snake = lambda name: re.sub(\n            r'[A-Z]', lambda x: '_' + x.group(0).lower(), name)\n        formatted_options = {camel_to_snake(key): dataflow_options[key]\n                             for key in dataflow_options}\n        hook.start_python_dataflow(\n            self.job_name, formatted_options,\n            self.py_file, self.py_options)","method_path":"airflow\/contrib\/operators\/dataflow_operator.py"}
{"repo_name":"apache\/airflow","method_name":"run_migrations_online","method_code":"def run_migrations_online():\n    connectable = settings.engine\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            transaction_per_migration=True,\n            target_metadata=target_metadata,\n            compare_type=COMPARE_TYPE,\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()","method_summary":"Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context.","original_method_code":"def run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = settings.engine\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            transaction_per_migration=True,\n            target_metadata=target_metadata,\n            compare_type=COMPARE_TYPE,\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()","method_path":"airflow\/migrations\/env.py"}
{"repo_name":"apache\/airflow","method_name":"BigtableHook.delete_instance","method_code":"def delete_instance(self, instance_id, project_id=None):\n        instance = self.get_instance(instance_id=instance_id, project_id=project_id)\n        if instance:\n            instance.delete()\n        else:\n            self.log.info(\"The instance '%s' does not exist in project '%s'. Exiting\", instance_id,\n                          project_id)","method_summary":"Deletes the specified Cloud Bigtable instance.","original_method_code":"def delete_instance(self, instance_id, project_id=None):\n        \"\"\"\n        Deletes the specified Cloud Bigtable instance.\n        Raises google.api_core.exceptions.NotFound if the Cloud Bigtable instance does\n        not exist.\n\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            BigTable exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Bigtable instance.\n        :type instance_id: str\n        \"\"\"\n        instance = self.get_instance(instance_id=instance_id, project_id=project_id)\n        if instance:\n            instance.delete()\n        else:\n            self.log.info(\"The instance '%s' does not exist in project '%s'. Exiting\", instance_id,\n                          project_id)","method_path":"airflow\/contrib\/hooks\/gcp_bigtable_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigtableHook.create_instance","method_code":"def create_instance(self,\n                        instance_id,\n                        main_cluster_id,\n                        main_cluster_zone,\n                        project_id=None,\n                        replica_cluster_id=None,\n                        replica_cluster_zone=None,\n                        instance_display_name=None,\n                        instance_type=enums.Instance.Type.TYPE_UNSPECIFIED,\n                        instance_labels=None,\n                        cluster_nodes=None,\n                        cluster_storage_type=enums.StorageType.STORAGE_TYPE_UNSPECIFIED,\n                        timeout=None):\n        cluster_storage_type = enums.StorageType(cluster_storage_type)\n        instance_type = enums.Instance.Type(instance_type)\n\n        instance = Instance(\n            instance_id,\n            self._get_client(project_id=project_id),\n            instance_display_name,\n            instance_type,\n            instance_labels,\n        )\n\n        clusters = [\n            instance.cluster(\n                main_cluster_id,\n                main_cluster_zone,\n                cluster_nodes,\n                cluster_storage_type\n            )\n        ]\n        if replica_cluster_id and replica_cluster_zone:\n            clusters.append(instance.cluster(\n                replica_cluster_id,\n                replica_cluster_zone,\n                cluster_nodes,\n                cluster_storage_type\n            ))\n        operation = instance.create(\n            clusters=clusters\n        )\n        operation.result(timeout)\n        return instance","method_summary":"Creates new instance.","original_method_code":"def create_instance(self,\n                        instance_id,\n                        main_cluster_id,\n                        main_cluster_zone,\n                        project_id=None,\n                        replica_cluster_id=None,\n                        replica_cluster_zone=None,\n                        instance_display_name=None,\n                        instance_type=enums.Instance.Type.TYPE_UNSPECIFIED,\n                        instance_labels=None,\n                        cluster_nodes=None,\n                        cluster_storage_type=enums.StorageType.STORAGE_TYPE_UNSPECIFIED,\n                        timeout=None):\n        \"\"\"\n        Creates new instance.\n\n        :type instance_id: str\n        :param instance_id: The ID for the new instance.\n        :type main_cluster_id: str\n        :param main_cluster_id: The ID for main cluster for the new instance.\n        :type main_cluster_zone: str\n        :param main_cluster_zone: The zone for main cluster.\n            See https:\/\/cloud.google.com\/bigtable\/docs\/locations for more details.\n        :type project_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            BigTable exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type replica_cluster_id: str\n        :param replica_cluster_id: (optional) The ID for replica cluster for the new\n            instance.\n        :type replica_cluster_zone: str\n        :param replica_cluster_zone: (optional)  The zone for replica cluster.\n        :type instance_type: enums.Instance.Type\n        :param instance_type: (optional) The type of the instance.\n        :type instance_display_name: str\n        :param instance_display_name: (optional) Human-readable name of the instance.\n                Defaults to ``instance_id``.\n        :type instance_labels: dict\n        :param instance_labels: (optional) Dictionary of labels to associate with the\n            instance.\n        :type cluster_nodes: int\n        :param cluster_nodes: (optional) Number of nodes for cluster.\n        :type cluster_storage_type: enums.StorageType\n        :param cluster_storage_type: (optional) The type of storage.\n        :type timeout: int\n        :param timeout: (optional) timeout (in seconds) for instance creation.\n                        If None is not specified, Operator will wait indefinitely.\n        \"\"\"\n        cluster_storage_type = enums.StorageType(cluster_storage_type)\n        instance_type = enums.Instance.Type(instance_type)\n\n        instance = Instance(\n            instance_id,\n            self._get_client(project_id=project_id),\n            instance_display_name,\n            instance_type,\n            instance_labels,\n        )\n\n        clusters = [\n            instance.cluster(\n                main_cluster_id,\n                main_cluster_zone,\n                cluster_nodes,\n                cluster_storage_type\n            )\n        ]\n        if replica_cluster_id and replica_cluster_zone:\n            clusters.append(instance.cluster(\n                replica_cluster_id,\n                replica_cluster_zone,\n                cluster_nodes,\n                cluster_storage_type\n            ))\n        operation = instance.create(\n            clusters=clusters\n        )\n        operation.result(timeout)\n        return instance","method_path":"airflow\/contrib\/hooks\/gcp_bigtable_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigtableHook.create_table","method_code":"def create_table(instance,\n                     table_id,\n                     initial_split_keys=None,\n                     column_families=None):\n        if column_families is None:\n            column_families = {}\n        if initial_split_keys is None:\n            initial_split_keys = []\n        table = Table(table_id, instance)\n        table.create(initial_split_keys, column_families)","method_summary":"Creates the specified Cloud Bigtable table.","original_method_code":"def create_table(instance,\n                     table_id,\n                     initial_split_keys=None,\n                     column_families=None):\n        \"\"\"\n        Creates the specified Cloud Bigtable table.\n        Raises ``google.api_core.exceptions.AlreadyExists`` if the table exists.\n\n        :type instance: Instance\n        :param instance: The Cloud Bigtable instance that owns the table.\n        :type table_id: str\n        :param table_id: The ID of the table to create in Cloud Bigtable.\n        :type initial_split_keys: list\n        :param initial_split_keys: (Optional) A list of row keys in bytes to use to\n            initially split the table.\n        :type column_families: dict\n        :param column_families: (Optional) A map of columns to create. The key is the\n            column_id str, and the value is a\n            :class:`google.cloud.bigtable.column_family.GarbageCollectionRule`.\n        \"\"\"\n        if column_families is None:\n            column_families = {}\n        if initial_split_keys is None:\n            initial_split_keys = []\n        table = Table(table_id, instance)\n        table.create(initial_split_keys, column_families)","method_path":"airflow\/contrib\/hooks\/gcp_bigtable_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigtableHook.delete_table","method_code":"def delete_table(self, instance_id, table_id, project_id=None):\n        table = self.get_instance(instance_id=instance_id, project_id=project_id).table(table_id=table_id)\n        table.delete()","method_summary":"Deletes the specified table in Cloud Bigtable.","original_method_code":"def delete_table(self, instance_id, table_id, project_id=None):\n        \"\"\"\n        Deletes the specified table in Cloud Bigtable.\n        Raises google.api_core.exceptions.NotFound if the table does not exist.\n\n        :type instance_id: str\n        :param instance_id: The ID of the Cloud Bigtable instance.\n        :type table_id: str\n        :param table_id: The ID of the table in Cloud Bigtable.\n        :type project_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            BigTable exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        \"\"\"\n        table = self.get_instance(instance_id=instance_id, project_id=project_id).table(table_id=table_id)\n        table.delete()","method_path":"airflow\/contrib\/hooks\/gcp_bigtable_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigtableHook.update_cluster","method_code":"def update_cluster(instance, cluster_id, nodes):\n        cluster = Cluster(cluster_id, instance)\n        cluster.serve_nodes = nodes\n        cluster.update()","method_summary":"Updates number of nodes in the specified Cloud Bigtable cluster.","original_method_code":"def update_cluster(instance, cluster_id, nodes):\n        \"\"\"\n        Updates number of nodes in the specified Cloud Bigtable cluster.\n        Raises google.api_core.exceptions.NotFound if the cluster does not exist.\n\n        :type instance: Instance\n        :param instance: The Cloud Bigtable instance that owns the cluster.\n        :type cluster_id: str\n        :param cluster_id: The ID of the cluster.\n        :type nodes: int\n        :param nodes: The desired number of nodes.\n        \"\"\"\n        cluster = Cluster(cluster_id, instance)\n        cluster.serve_nodes = nodes\n        cluster.update()","method_path":"airflow\/contrib\/hooks\/gcp_bigtable_hook.py"}
{"repo_name":"apache\/airflow","method_name":"HiveCliHook._prepare_cli_cmd","method_code":"def _prepare_cli_cmd(self):\n        conn = self.conn\n        hive_bin = 'hive'\n        cmd_extra = []\n\n        if self.use_beeline:\n            hive_bin = 'beeline'\n            jdbc_url = \"jdbc:hive2:\/\/{host}:{port}\/{schema}\".format(\n                host=conn.host, port=conn.port, schema=conn.schema)\n            if configuration.conf.get('core', 'security') == 'kerberos':\n                template = conn.extra_dejson.get(\n                    'principal', \"hive\/_HOST@EXAMPLE.COM\")\n                if \"_HOST\" in template:\n                    template = utils.replace_hostname_pattern(\n                        utils.get_components(template))\n\n                proxy_user = \"\"  \n                if conn.extra_dejson.get('proxy_user') == \"login\" and conn.login:\n                    proxy_user = \"hive.server2.proxy.user={0}\".format(conn.login)\n                elif conn.extra_dejson.get('proxy_user') == \"owner\" and self.run_as:\n                    proxy_user = \"hive.server2.proxy.user={0}\".format(self.run_as)\n\n                jdbc_url += \";principal={template};{proxy_user}\".format(\n                    template=template, proxy_user=proxy_user)\n            elif self.auth:\n                jdbc_url += \";auth=\" + self.auth\n\n            jdbc_url = '\"{}\"'.format(jdbc_url)\n\n            cmd_extra += ['-u', jdbc_url]\n            if conn.login:\n                cmd_extra += ['-n', conn.login]\n            if conn.password:\n                cmd_extra += ['-p', conn.password]\n\n        hive_params_list = self.hive_cli_params.split()\n\n        return [hive_bin] + cmd_extra + hive_params_list","method_summary":"This function creates the command list from available information","original_method_code":"def _prepare_cli_cmd(self):\n        \"\"\"\n        This function creates the command list from available information\n        \"\"\"\n        conn = self.conn\n        hive_bin = 'hive'\n        cmd_extra = []\n\n        if self.use_beeline:\n            hive_bin = 'beeline'\n            jdbc_url = \"jdbc:hive2:\/\/{host}:{port}\/{schema}\".format(\n                host=conn.host, port=conn.port, schema=conn.schema)\n            if configuration.conf.get('core', 'security') == 'kerberos':\n                template = conn.extra_dejson.get(\n                    'principal', \"hive\/_HOST@EXAMPLE.COM\")\n                if \"_HOST\" in template:\n                    template = utils.replace_hostname_pattern(\n                        utils.get_components(template))\n\n                proxy_user = \"\"  # noqa\n                if conn.extra_dejson.get('proxy_user') == \"login\" and conn.login:\n                    proxy_user = \"hive.server2.proxy.user={0}\".format(conn.login)\n                elif conn.extra_dejson.get('proxy_user') == \"owner\" and self.run_as:\n                    proxy_user = \"hive.server2.proxy.user={0}\".format(self.run_as)\n\n                jdbc_url += \";principal={template};{proxy_user}\".format(\n                    template=template, proxy_user=proxy_user)\n            elif self.auth:\n                jdbc_url += \";auth=\" + self.auth\n\n            jdbc_url = '\"{}\"'.format(jdbc_url)\n\n            cmd_extra += ['-u', jdbc_url]\n            if conn.login:\n                cmd_extra += ['-n', conn.login]\n            if conn.password:\n                cmd_extra += ['-p', conn.password]\n\n        hive_params_list = self.hive_cli_params.split()\n\n        return [hive_bin] + cmd_extra + hive_params_list","method_path":"airflow\/hooks\/hive_hooks.py"}
{"repo_name":"apache\/airflow","method_name":"HiveCliHook._prepare_hiveconf","method_code":"def _prepare_hiveconf(d):\n        if not d:\n            return []\n        return as_flattened_list(\n            zip([\"-hiveconf\"] * len(d),\n                [\"{}={}\".format(k, v) for k, v in d.items()])\n        )","method_summary":"This function prepares a list of hiveconf params from a dictionary of key value pairs.","original_method_code":"def _prepare_hiveconf(d):\n        \"\"\"\n        This function prepares a list of hiveconf params\n        from a dictionary of key value pairs.\n\n        :param d:\n        :type d: dict\n\n        >>> hh = HiveCliHook()\n        >>> hive_conf = {\"hive.exec.dynamic.partition\": \"true\",\n        ... \"hive.exec.dynamic.partition.mode\": \"nonstrict\"}\n        >>> hh._prepare_hiveconf(hive_conf)\n        [\"-hiveconf\", \"hive.exec.dynamic.partition=true\",\\\n \"-hiveconf\", \"hive.exec.dynamic.partition.mode=nonstrict\"]\n        \"\"\"\n        if not d:\n            return []\n        return as_flattened_list(\n            zip([\"-hiveconf\"] * len(d),\n                [\"{}={}\".format(k, v) for k, v in d.items()])\n        )","method_path":"airflow\/hooks\/hive_hooks.py"}
{"repo_name":"apache\/airflow","method_name":"HiveCliHook.load_df","method_code":"def load_df(\n            self,\n            df,\n            table,\n            field_dict=None,\n            delimiter=',',\n            encoding='utf8',\n            pandas_kwargs=None, **kwargs):\n        def _infer_field_types_from_df(df):\n            DTYPE_KIND_HIVE_TYPE = {\n                'b': 'BOOLEAN',    \n                'i': 'BIGINT',     \n                'u': 'BIGINT',     \n                'f': 'DOUBLE',     \n                'c': 'STRING',     \n                'M': 'TIMESTAMP',  \n                'O': 'STRING',     \n                'S': 'STRING',     \n                'U': 'STRING',     \n                'V': 'STRING'      \n            }\n\n            d = OrderedDict()\n            for col, dtype in df.dtypes.iteritems():\n                d[col] = DTYPE_KIND_HIVE_TYPE[dtype.kind]\n            return d\n\n        if pandas_kwargs is None:\n            pandas_kwargs = {}\n\n        with TemporaryDirectory(prefix='airflow_hiveop_') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir, mode=\"w\") as f:\n\n                if field_dict is None:\n                    field_dict = _infer_field_types_from_df(df)\n\n                df.to_csv(path_or_buf=f,\n                          sep=delimiter,\n                          header=False,\n                          index=False,\n                          encoding=encoding,\n                          date_format=\"%Y-%m-%d %H:%M:%S\",\n                          **pandas_kwargs)\n                f.flush()\n\n                return self.load_file(filepath=f.name,\n                                      table=table,\n                                      delimiter=delimiter,\n                                      field_dict=field_dict,\n                                      **kwargs)","method_summary":"Loads a pandas DataFrame into hive. Hive data types will be inferred if not passed but column names will not be sanitized.","original_method_code":"def load_df(\n            self,\n            df,\n            table,\n            field_dict=None,\n            delimiter=',',\n            encoding='utf8',\n            pandas_kwargs=None, **kwargs):\n        \"\"\"\n        Loads a pandas DataFrame into hive.\n\n        Hive data types will be inferred if not passed but column names will\n        not be sanitized.\n\n        :param df: DataFrame to load into a Hive table\n        :type df: pandas.DataFrame\n        :param table: target Hive table, use dot notation to target a\n            specific database\n        :type table: str\n        :param field_dict: mapping from column name to hive data type.\n            Note that it must be OrderedDict so as to keep columns' order.\n        :type field_dict: collections.OrderedDict\n        :param delimiter: field delimiter in the file\n        :type delimiter: str\n        :param encoding: str encoding to use when writing DataFrame to file\n        :type encoding: str\n        :param pandas_kwargs: passed to DataFrame.to_csv\n        :type pandas_kwargs: dict\n        :param kwargs: passed to self.load_file\n        \"\"\"\n\n        def _infer_field_types_from_df(df):\n            DTYPE_KIND_HIVE_TYPE = {\n                'b': 'BOOLEAN',    # boolean\n                'i': 'BIGINT',     # signed integer\n                'u': 'BIGINT',     # unsigned integer\n                'f': 'DOUBLE',     # floating-point\n                'c': 'STRING',     # complex floating-point\n                'M': 'TIMESTAMP',  # datetime\n                'O': 'STRING',     # object\n                'S': 'STRING',     # (byte-)string\n                'U': 'STRING',     # Unicode\n                'V': 'STRING'      # void\n            }\n\n            d = OrderedDict()\n            for col, dtype in df.dtypes.iteritems():\n                d[col] = DTYPE_KIND_HIVE_TYPE[dtype.kind]\n            return d\n\n        if pandas_kwargs is None:\n            pandas_kwargs = {}\n\n        with TemporaryDirectory(prefix='airflow_hiveop_') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir, mode=\"w\") as f:\n\n                if field_dict is None:\n                    field_dict = _infer_field_types_from_df(df)\n\n                df.to_csv(path_or_buf=f,\n                          sep=delimiter,\n                          header=False,\n                          index=False,\n                          encoding=encoding,\n                          date_format=\"%Y-%m-%d %H:%M:%S\",\n                          **pandas_kwargs)\n                f.flush()\n\n                return self.load_file(filepath=f.name,\n                                      table=table,\n                                      delimiter=delimiter,\n                                      field_dict=field_dict,\n                                      **kwargs)","method_path":"airflow\/hooks\/hive_hooks.py"}
{"repo_name":"apache\/airflow","method_name":"HiveMetastoreHook.check_for_named_partition","method_code":"def check_for_named_partition(self, schema, table, partition_name):\n        with self.metastore as client:\n            return client.check_for_named_partition(schema, table, partition_name)","method_summary":"Checks whether a partition with a given name exists","original_method_code":"def check_for_named_partition(self, schema, table, partition_name):\n        \"\"\"\n        Checks whether a partition with a given name exists\n\n        :param schema: Name of hive schema (database) @table belongs to\n        :type schema: str\n        :param table: Name of hive table @partition belongs to\n        :type schema: str\n        :partition: Name of the partitions to check for (eg `a=b\/c=d`)\n        :type schema: str\n        :rtype: bool\n\n        >>> hh = HiveMetastoreHook()\n        >>> t = 'static_babynames_partitioned'\n        >>> hh.check_for_named_partition('airflow', t, \"ds=2015-01-01\")\n        True\n        >>> hh.check_for_named_partition('airflow', t, \"ds=xxx\")\n        False\n        \"\"\"\n        with self.metastore as client:\n            return client.check_for_named_partition(schema, table, partition_name)","method_path":"airflow\/hooks\/hive_hooks.py"}
{"repo_name":"apache\/airflow","method_name":"HiveMetastoreHook.table_exists","method_code":"def table_exists(self, table_name, db='default'):\n        try:\n            self.get_table(table_name, db)\n            return True\n        except Exception:\n            return False","method_summary":"Check if table exists","original_method_code":"def table_exists(self, table_name, db='default'):\n        \"\"\"\n        Check if table exists\n\n        >>> hh = HiveMetastoreHook()\n        >>> hh.table_exists(db='airflow', table_name='static_babynames')\n        True\n        >>> hh.table_exists(db='airflow', table_name='does_not_exist')\n        False\n        \"\"\"\n        try:\n            self.get_table(table_name, db)\n            return True\n        except Exception:\n            return False","method_path":"airflow\/hooks\/hive_hooks.py"}
{"repo_name":"apache\/airflow","method_name":"HiveServer2Hook.get_results","method_code":"def get_results(self, hql, schema='default', fetch_size=None, hive_conf=None):\n        results_iter = self._get_results(hql, schema,\n                                         fetch_size=fetch_size, hive_conf=hive_conf)\n        header = next(results_iter)\n        results = {\n            'data': list(results_iter),\n            'header': header\n        }\n        return results","method_summary":"Get results of the provided hql in target schema.","original_method_code":"def get_results(self, hql, schema='default', fetch_size=None, hive_conf=None):\n        \"\"\"\n        Get results of the provided hql in target schema.\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :param fetch_size: max size of result to fetch.\n        :type fetch_size: int\n        :param hive_conf: hive_conf to execute alone with the hql.\n        :type hive_conf: dict\n        :return: results of hql execution, dict with data (list of results) and header\n        :rtype: dict\n        \"\"\"\n        results_iter = self._get_results(hql, schema,\n                                         fetch_size=fetch_size, hive_conf=hive_conf)\n        header = next(results_iter)\n        results = {\n            'data': list(results_iter),\n            'header': header\n        }\n        return results","method_path":"airflow\/hooks\/hive_hooks.py"}
{"repo_name":"apache\/airflow","method_name":"HiveServer2Hook.to_csv","method_code":"def to_csv(\n            self,\n            hql,\n            csv_filepath,\n            schema='default',\n            delimiter=',',\n            lineterminator='\\r\\n',\n            output_header=True,\n            fetch_size=1000,\n            hive_conf=None):\n        results_iter = self._get_results(hql, schema,\n                                         fetch_size=fetch_size, hive_conf=hive_conf)\n        header = next(results_iter)\n        message = None\n\n        i = 0\n        with open(csv_filepath, 'wb') as f:\n            writer = csv.writer(f,\n                                delimiter=delimiter,\n                                lineterminator=lineterminator,\n                                encoding='utf-8')\n            try:\n                if output_header:\n                    self.log.debug('Cursor description is %s', header)\n                    writer.writerow([c[0] for c in header])\n\n                for i, row in enumerate(results_iter, 1):\n                    writer.writerow(row)\n                    if i % fetch_size == 0:\n                        self.log.info(\"Written %s rows so far.\", i)\n            except ValueError as exception:\n                message = str(exception)\n\n        if message:\n            \n            os.remove(csv_filepath)\n            raise ValueError(message)\n\n        self.log.info(\"Done. Loaded a total of %s rows.\", i)","method_summary":"Execute hql in target schema and write results to a csv file.","original_method_code":"def to_csv(\n            self,\n            hql,\n            csv_filepath,\n            schema='default',\n            delimiter=',',\n            lineterminator='\\r\\n',\n            output_header=True,\n            fetch_size=1000,\n            hive_conf=None):\n        \"\"\"\n        Execute hql in target schema and write results to a csv file.\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param csv_filepath: filepath of csv to write results into.\n        :type csv_filepath: str\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :param delimiter: delimiter of the csv file, default to ','.\n        :type delimiter: str\n        :param lineterminator: lineterminator of the csv file.\n        :type lineterminator: str\n        :param output_header: header of the csv file, default to True.\n        :type output_header: bool\n        :param fetch_size: number of result rows to write into the csv file, default to 1000.\n        :type fetch_size: int\n        :param hive_conf: hive_conf to execute alone with the hql.\n        :type hive_conf: dict\n\n        \"\"\"\n\n        results_iter = self._get_results(hql, schema,\n                                         fetch_size=fetch_size, hive_conf=hive_conf)\n        header = next(results_iter)\n        message = None\n\n        i = 0\n        with open(csv_filepath, 'wb') as f:\n            writer = csv.writer(f,\n                                delimiter=delimiter,\n                                lineterminator=lineterminator,\n                                encoding='utf-8')\n            try:\n                if output_header:\n                    self.log.debug('Cursor description is %s', header)\n                    writer.writerow([c[0] for c in header])\n\n                for i, row in enumerate(results_iter, 1):\n                    writer.writerow(row)\n                    if i % fetch_size == 0:\n                        self.log.info(\"Written %s rows so far.\", i)\n            except ValueError as exception:\n                message = str(exception)\n\n        if message:\n            # need to clean up the file first\n            os.remove(csv_filepath)\n            raise ValueError(message)\n\n        self.log.info(\"Done. Loaded a total of %s rows.\", i)","method_path":"airflow\/hooks\/hive_hooks.py"}
{"repo_name":"apache\/airflow","method_name":"HiveServer2Hook.get_records","method_code":"def get_records(self, hql, schema='default', hive_conf=None):\n        return self.get_results(hql, schema=schema, hive_conf=hive_conf)['data']","method_summary":"Get a set of records from a Hive query.","original_method_code":"def get_records(self, hql, schema='default', hive_conf=None):\n        \"\"\"\n        Get a set of records from a Hive query.\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :param hive_conf: hive_conf to execute alone with the hql.\n        :type hive_conf: dict\n        :return: result of hive execution\n        :rtype: list\n\n        >>> hh = HiveServer2Hook()\n        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\"\n        >>> len(hh.get_records(sql))\n        100\n        \"\"\"\n        return self.get_results(hql, schema=schema, hive_conf=hive_conf)['data']","method_path":"airflow\/hooks\/hive_hooks.py"}
{"repo_name":"apache\/airflow","method_name":"HiveServer2Hook.get_pandas_df","method_code":"def get_pandas_df(self, hql, schema='default'):\n        import pandas as pd\n        res = self.get_results(hql, schema=schema)\n        df = pd.DataFrame(res['data'])\n        df.columns = [c[0] for c in res['header']]\n        return df","method_summary":"Get a pandas dataframe from a Hive query","original_method_code":"def get_pandas_df(self, hql, schema='default'):\n        \"\"\"\n        Get a pandas dataframe from a Hive query\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :return: result of hql execution\n        :rtype: DataFrame\n\n        >>> hh = HiveServer2Hook()\n        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\"\n        >>> df = hh.get_pandas_df(sql)\n        >>> len(df.index)\n        100\n\n        :return: pandas.DateFrame\n        \"\"\"\n        import pandas as pd\n        res = self.get_results(hql, schema=schema)\n        df = pd.DataFrame(res['data'])\n        df.columns = [c[0] for c in res['header']]\n        return df","method_path":"airflow\/hooks\/hive_hooks.py"}
{"repo_name":"apache\/airflow","method_name":"CloudVisionHook.get_conn","method_code":"def get_conn(self):\n        if not self._client:\n            self._client = ProductSearchClient(credentials=self._get_credentials())\n        return self._client","method_summary":"Retrieves connection to Cloud Vision.","original_method_code":"def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Vision.\n\n        :return: Google Cloud Vision client object.\n        :rtype: google.cloud.vision_v1.ProductSearchClient\n        \"\"\"\n        if not self._client:\n            self._client = ProductSearchClient(credentials=self._get_credentials())\n        return self._client","method_path":"airflow\/contrib\/hooks\/gcp_vision_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DingdingHook._get_endpoint","method_code":"def _get_endpoint(self):\n        conn = self.get_connection(self.http_conn_id)\n        token = conn.password\n        if not token:\n            raise AirflowException('Dingding token is requests but get nothing, '\n                                   'check you conn_id configuration.')\n        return 'robot\/send?access_token={}'.format(token)","method_summary":"Get Dingding endpoint for sending message.","original_method_code":"def _get_endpoint(self):\n        \"\"\"\n        Get Dingding endpoint for sending message.\n        \"\"\"\n        conn = self.get_connection(self.http_conn_id)\n        token = conn.password\n        if not token:\n            raise AirflowException('Dingding token is requests but get nothing, '\n                                   'check you conn_id configuration.')\n        return 'robot\/send?access_token={}'.format(token)","method_path":"airflow\/contrib\/hooks\/dingding_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DingdingHook.send","method_code":"def send(self):\n        support_type = ['text', 'link', 'markdown', 'actionCard', 'feedCard']\n        if self.message_type not in support_type:\n            raise ValueError('DingdingWebhookHook only support {} '\n                             'so far, but receive {}'.format(support_type, self.message_type))\n\n        data = self._build_message()\n        self.log.info('Sending Dingding type %s message %s', self.message_type, data)\n        resp = self.run(endpoint=self._get_endpoint(),\n                        data=data,\n                        headers={'Content-Type': 'application\/json'})\n\n        \n        if int(resp.json().get('errcode')) != 0:\n            raise AirflowException('Send Dingding message failed, receive error '\n                                   'message %s', resp.text)\n        self.log.info('Success Send Dingding message')","method_summary":"Send Dingding message","original_method_code":"def send(self):\n        \"\"\"\n        Send Dingding message\n        \"\"\"\n        support_type = ['text', 'link', 'markdown', 'actionCard', 'feedCard']\n        if self.message_type not in support_type:\n            raise ValueError('DingdingWebhookHook only support {} '\n                             'so far, but receive {}'.format(support_type, self.message_type))\n\n        data = self._build_message()\n        self.log.info('Sending Dingding type %s message %s', self.message_type, data)\n        resp = self.run(endpoint=self._get_endpoint(),\n                        data=data,\n                        headers={'Content-Type': 'application\/json'})\n\n        # Dingding success send message will with errcode equal to 0\n        if int(resp.json().get('errcode')) != 0:\n            raise AirflowException('Send Dingding message failed, receive error '\n                                   'message %s', resp.text)\n        self.log.info('Success Send Dingding message')","method_path":"airflow\/contrib\/hooks\/dingding_hook.py"}
{"repo_name":"apache\/airflow","method_name":"_bind_parameters","method_code":"def _bind_parameters(operation, parameters):\n    \n    string_parameters = {}\n    for (name, value) in iteritems(parameters):\n        if value is None:\n            string_parameters[name] = 'NULL'\n        elif isinstance(value, basestring):\n            string_parameters[name] = \"'\" + _escape(value) + \"'\"\n        else:\n            string_parameters[name] = str(value)\n    return operation % string_parameters","method_summary":"Helper method that binds parameters to a SQL query.","original_method_code":"def _bind_parameters(operation, parameters):\n    \"\"\" Helper method that binds parameters to a SQL query. \"\"\"\n    # inspired by MySQL Python Connector (conversion.py)\n    string_parameters = {}\n    for (name, value) in iteritems(parameters):\n        if value is None:\n            string_parameters[name] = 'NULL'\n        elif isinstance(value, basestring):\n            string_parameters[name] = \"'\" + _escape(value) + \"'\"\n        else:\n            string_parameters[name] = str(value)\n    return operation % string_parameters","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"_escape","method_code":"def _escape(s):\n    e = s\n    e = e.replace('\\\\', '\\\\\\\\')\n    e = e.replace('\\n', '\\\\n')\n    e = e.replace('\\r', '\\\\r')\n    e = e.replace(\"'\", \"\\\\'\")\n    e = e.replace('\"', '\\\\\"')\n    return e","method_summary":"Helper method that escapes parameters to a SQL query.","original_method_code":"def _escape(s):\n    \"\"\" Helper method that escapes parameters to a SQL query. \"\"\"\n    e = s\n    e = e.replace('\\\\', '\\\\\\\\')\n    e = e.replace('\\n', '\\\\n')\n    e = e.replace('\\r', '\\\\r')\n    e = e.replace(\"'\", \"\\\\'\")\n    e = e.replace('\"', '\\\\\"')\n    return e","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"_bq_cast","method_code":"def _bq_cast(string_field, bq_type):\n    if string_field is None:\n        return None\n    elif bq_type == 'INTEGER':\n        return int(string_field)\n    elif bq_type == 'FLOAT' or bq_type == 'TIMESTAMP':\n        return float(string_field)\n    elif bq_type == 'BOOLEAN':\n        if string_field not in ['true', 'false']:\n            raise ValueError(\"{} must have value 'true' or 'false'\".format(\n                string_field))\n        return string_field == 'true'\n    else:\n        return string_field","method_summary":"Helper method that casts a BigQuery row to the appropriate data types. This is useful because BigQuery returns all fields as strings.","original_method_code":"def _bq_cast(string_field, bq_type):\n    \"\"\"\n    Helper method that casts a BigQuery row to the appropriate data types.\n    This is useful because BigQuery returns all fields as strings.\n    \"\"\"\n    if string_field is None:\n        return None\n    elif bq_type == 'INTEGER':\n        return int(string_field)\n    elif bq_type == 'FLOAT' or bq_type == 'TIMESTAMP':\n        return float(string_field)\n    elif bq_type == 'BOOLEAN':\n        if string_field not in ['true', 'false']:\n            raise ValueError(\"{} must have value 'true' or 'false'\".format(\n                string_field))\n        return string_field == 'true'\n    else:\n        return string_field","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"_validate_value","method_code":"def _validate_value(key, value, expected_type):\n    if not isinstance(value, expected_type):\n        raise TypeError(\"{} argument must have a type {} not {}\".format(\n            key, expected_type, type(value)))","method_summary":"function to check expected type and raise error if type is not correct","original_method_code":"def _validate_value(key, value, expected_type):\n    \"\"\" function to check expected type and raise\n    error if type is not correct \"\"\"\n    if not isinstance(value, expected_type):\n        raise TypeError(\"{} argument must have a type {} not {}\".format(\n            key, expected_type, type(value)))","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryHook.table_exists","method_code":"def table_exists(self, project_id, dataset_id, table_id):\n        service = self.get_service()\n        try:\n            service.tables().get(\n                projectId=project_id, datasetId=dataset_id,\n                tableId=table_id).execute(num_retries=self.num_retries)\n            return True\n        except HttpError as e:\n            if e.resp['status'] == '404':\n                return False\n            raise","method_summary":"Checks for the existence of a table in Google BigQuery.","original_method_code":"def table_exists(self, project_id, dataset_id, table_id):\n        \"\"\"\n        Checks for the existence of a table in Google BigQuery.\n\n        :param project_id: The Google cloud project in which to look for the\n            table. The connection supplied to the hook must provide access to\n            the specified project.\n        :type project_id: str\n        :param dataset_id: The name of the dataset in which to look for the\n            table.\n        :type dataset_id: str\n        :param table_id: The name of the table to check the existence of.\n        :type table_id: str\n        \"\"\"\n        service = self.get_service()\n        try:\n            service.tables().get(\n                projectId=project_id, datasetId=dataset_id,\n                tableId=table_id).execute(num_retries=self.num_retries)\n            return True\n        except HttpError as e:\n            if e.resp['status'] == '404':\n                return False\n            raise","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryBaseCursor.create_empty_table","method_code":"def create_empty_table(self,\n                           project_id,\n                           dataset_id,\n                           table_id,\n                           schema_fields=None,\n                           time_partitioning=None,\n                           cluster_fields=None,\n                           labels=None,\n                           view=None,\n                           num_retries=None):\n        project_id = project_id if project_id is not None else self.project_id\n\n        table_resource = {\n            'tableReference': {\n                'tableId': table_id\n            }\n        }\n\n        if schema_fields:\n            table_resource['schema'] = {'fields': schema_fields}\n\n        if time_partitioning:\n            table_resource['timePartitioning'] = time_partitioning\n\n        if cluster_fields:\n            table_resource['clustering'] = {\n                'fields': cluster_fields\n            }\n\n        if labels:\n            table_resource['labels'] = labels\n\n        if view:\n            table_resource['view'] = view\n\n        num_retries = num_retries if num_retries else self.num_retries\n\n        self.log.info('Creating Table %s:%s.%s',\n                      project_id, dataset_id, table_id)\n\n        try:\n            self.service.tables().insert(\n                projectId=project_id,\n                datasetId=dataset_id,\n                body=table_resource).execute(num_retries=num_retries)\n\n            self.log.info('Table created successfully: %s:%s.%s',\n                          project_id, dataset_id, table_id)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )","method_summary":"Creates a new, empty table in the dataset. To create a view, which is defined by a SQL query, parse a dictionary to 'view' kwarg","original_method_code":"def create_empty_table(self,\n                           project_id,\n                           dataset_id,\n                           table_id,\n                           schema_fields=None,\n                           time_partitioning=None,\n                           cluster_fields=None,\n                           labels=None,\n                           view=None,\n                           num_retries=None):\n        \"\"\"\n        Creates a new, empty table in the dataset.\n        To create a view, which is defined by a SQL query, parse a dictionary to 'view' kwarg\n\n        :param project_id: The project to create the table into.\n        :type project_id: str\n        :param dataset_id: The dataset to create the table into.\n        :type dataset_id: str\n        :param table_id: The Name of the table to be created.\n        :type table_id: str\n        :param schema_fields: If set, the schema field list as defined here:\n            https:\/\/cloud.google.com\/bigquery\/docs\/reference\/rest\/v2\/jobs#configuration.load.schema\n        :type schema_fields: list\n        :param labels: a dictionary containing labels for the table, passed to BigQuery\n        :type labels: dict\n\n        **Example**: ::\n\n            schema_fields=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n                           {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}]\n\n        :param time_partitioning: configure optional time partitioning fields i.e.\n            partition by field, type and expiration as per API specifications.\n\n            .. seealso::\n                https:\/\/cloud.google.com\/bigquery\/docs\/reference\/rest\/v2\/tables#timePartitioning\n        :type time_partitioning: dict\n        :param cluster_fields: [Optional] The fields used for clustering.\n            Must be specified with time_partitioning, data in the table will be first\n            partitioned and subsequently clustered.\n            https:\/\/cloud.google.com\/bigquery\/docs\/reference\/rest\/v2\/tables#clustering.fields\n        :type cluster_fields: list\n        :param view: [Optional] A dictionary containing definition for the view.\n            If set, it will create a view instead of a table:\n            https:\/\/cloud.google.com\/bigquery\/docs\/reference\/rest\/v2\/tables#view\n        :type view: dict\n\n        **Example**: ::\n\n            view = {\n                \"query\": \"SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 1000\",\n                \"useLegacySql\": False\n            }\n\n        :return: None\n        \"\"\"\n\n        project_id = project_id if project_id is not None else self.project_id\n\n        table_resource = {\n            'tableReference': {\n                'tableId': table_id\n            }\n        }\n\n        if schema_fields:\n            table_resource['schema'] = {'fields': schema_fields}\n\n        if time_partitioning:\n            table_resource['timePartitioning'] = time_partitioning\n\n        if cluster_fields:\n            table_resource['clustering'] = {\n                'fields': cluster_fields\n            }\n\n        if labels:\n            table_resource['labels'] = labels\n\n        if view:\n            table_resource['view'] = view\n\n        num_retries = num_retries if num_retries else self.num_retries\n\n        self.log.info('Creating Table %s:%s.%s',\n                      project_id, dataset_id, table_id)\n\n        try:\n            self.service.tables().insert(\n                projectId=project_id,\n                datasetId=dataset_id,\n                body=table_resource).execute(num_retries=num_retries)\n\n            self.log.info('Table created successfully: %s:%s.%s',\n                          project_id, dataset_id, table_id)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryBaseCursor.patch_table","method_code":"def patch_table(self,\n                    dataset_id,\n                    table_id,\n                    project_id=None,\n                    description=None,\n                    expiration_time=None,\n                    external_data_configuration=None,\n                    friendly_name=None,\n                    labels=None,\n                    schema=None,\n                    time_partitioning=None,\n                    view=None,\n                    require_partition_filter=None):\n        project_id = project_id if project_id is not None else self.project_id\n\n        table_resource = {}\n\n        if description is not None:\n            table_resource['description'] = description\n        if expiration_time is not None:\n            table_resource['expirationTime'] = expiration_time\n        if external_data_configuration:\n            table_resource['externalDataConfiguration'] = external_data_configuration\n        if friendly_name is not None:\n            table_resource['friendlyName'] = friendly_name\n        if labels:\n            table_resource['labels'] = labels\n        if schema:\n            table_resource['schema'] = {'fields': schema}\n        if time_partitioning:\n            table_resource['timePartitioning'] = time_partitioning\n        if view:\n            table_resource['view'] = view\n        if require_partition_filter is not None:\n            table_resource['requirePartitionFilter'] = require_partition_filter\n\n        self.log.info('Patching Table %s:%s.%s',\n                      project_id, dataset_id, table_id)\n\n        try:\n            self.service.tables().patch(\n                projectId=project_id,\n                datasetId=dataset_id,\n                tableId=table_id,\n                body=table_resource).execute(num_retries=self.num_retries)\n\n            self.log.info('Table patched successfully: %s:%s.%s',\n                          project_id, dataset_id, table_id)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )","method_summary":"Patch information in an existing table. It only updates fileds that are provided in the request object.","original_method_code":"def patch_table(self,\n                    dataset_id,\n                    table_id,\n                    project_id=None,\n                    description=None,\n                    expiration_time=None,\n                    external_data_configuration=None,\n                    friendly_name=None,\n                    labels=None,\n                    schema=None,\n                    time_partitioning=None,\n                    view=None,\n                    require_partition_filter=None):\n        \"\"\"\n        Patch information in an existing table.\n        It only updates fileds that are provided in the request object.\n\n        Reference: https:\/\/cloud.google.com\/bigquery\/docs\/reference\/rest\/v2\/tables\/patch\n\n        :param dataset_id: The dataset containing the table to be patched.\n        :type dataset_id: str\n        :param table_id: The Name of the table to be patched.\n        :type table_id: str\n        :param project_id: The project containing the table to be patched.\n        :type project_id: str\n        :param description: [Optional] A user-friendly description of this table.\n        :type description: str\n        :param expiration_time: [Optional] The time when this table expires,\n            in milliseconds since the epoch.\n        :type expiration_time: int\n        :param external_data_configuration: [Optional] A dictionary containing\n            properties of a table stored outside of BigQuery.\n        :type external_data_configuration: dict\n        :param friendly_name: [Optional] A descriptive name for this table.\n        :type friendly_name: str\n        :param labels: [Optional] A dictionary containing labels associated with this table.\n        :type labels: dict\n        :param schema: [Optional] If set, the schema field list as defined here:\n            https:\/\/cloud.google.com\/bigquery\/docs\/reference\/rest\/v2\/jobs#configuration.load.schema\n            The supported schema modifications and unsupported schema modification are listed here:\n            https:\/\/cloud.google.com\/bigquery\/docs\/managing-table-schemas\n            **Example**: ::\n\n                schema=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n                               {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}]\n\n        :type schema: list\n        :param time_partitioning: [Optional] A dictionary containing time-based partitioning\n             definition for the table.\n        :type time_partitioning: dict\n        :param view: [Optional] A dictionary containing definition for the view.\n            If set, it will patch a view instead of a table:\n            https:\/\/cloud.google.com\/bigquery\/docs\/reference\/rest\/v2\/tables#view\n            **Example**: ::\n\n                view = {\n                    \"query\": \"SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 500\",\n                    \"useLegacySql\": False\n                }\n\n        :type view: dict\n        :param require_partition_filter: [Optional] If true, queries over the this table require a\n            partition filter. If false, queries over the table\n        :type require_partition_filter: bool\n\n        \"\"\"\n\n        project_id = project_id if project_id is not None else self.project_id\n\n        table_resource = {}\n\n        if description is not None:\n            table_resource['description'] = description\n        if expiration_time is not None:\n            table_resource['expirationTime'] = expiration_time\n        if external_data_configuration:\n            table_resource['externalDataConfiguration'] = external_data_configuration\n        if friendly_name is not None:\n            table_resource['friendlyName'] = friendly_name\n        if labels:\n            table_resource['labels'] = labels\n        if schema:\n            table_resource['schema'] = {'fields': schema}\n        if time_partitioning:\n            table_resource['timePartitioning'] = time_partitioning\n        if view:\n            table_resource['view'] = view\n        if require_partition_filter is not None:\n            table_resource['requirePartitionFilter'] = require_partition_filter\n\n        self.log.info('Patching Table %s:%s.%s',\n                      project_id, dataset_id, table_id)\n\n        try:\n            self.service.tables().patch(\n                projectId=project_id,\n                datasetId=dataset_id,\n                tableId=table_id,\n                body=table_resource).execute(num_retries=self.num_retries)\n\n            self.log.info('Table patched successfully: %s:%s.%s',\n                          project_id, dataset_id, table_id)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryBaseCursor.cancel_query","method_code":"def cancel_query(self):\n        jobs = self.service.jobs()\n        if (self.running_job_id and\n                not self.poll_job_complete(self.running_job_id)):\n            self.log.info('Attempting to cancel job : %s, %s', self.project_id,\n                          self.running_job_id)\n            if self.location:\n                jobs.cancel(\n                    projectId=self.project_id,\n                    jobId=self.running_job_id,\n                    location=self.location).execute(num_retries=self.num_retries)\n            else:\n                jobs.cancel(\n                    projectId=self.project_id,\n                    jobId=self.running_job_id).execute(num_retries=self.num_retries)\n        else:\n            self.log.info('No running BigQuery jobs to cancel.')\n            return\n\n        \n        max_polling_attempts = 12\n        polling_attempts = 0\n\n        job_complete = False\n        while polling_attempts < max_polling_attempts and not job_complete:\n            polling_attempts = polling_attempts + 1\n            job_complete = self.poll_job_complete(self.running_job_id)\n            if job_complete:\n                self.log.info('Job successfully canceled: %s, %s',\n                              self.project_id, self.running_job_id)\n            elif polling_attempts == max_polling_attempts:\n                self.log.info(\n                    \"Stopping polling due to timeout. Job with id %s \"\n                    \"has not completed cancel and may or may not finish.\",\n                    self.running_job_id)\n            else:\n                self.log.info('Waiting for canceled job with id %s to finish.',\n                              self.running_job_id)\n                time.sleep(5)","method_summary":"Cancel all started queries that have not yet completed","original_method_code":"def cancel_query(self):\n        \"\"\"\n        Cancel all started queries that have not yet completed\n        \"\"\"\n        jobs = self.service.jobs()\n        if (self.running_job_id and\n                not self.poll_job_complete(self.running_job_id)):\n            self.log.info('Attempting to cancel job : %s, %s', self.project_id,\n                          self.running_job_id)\n            if self.location:\n                jobs.cancel(\n                    projectId=self.project_id,\n                    jobId=self.running_job_id,\n                    location=self.location).execute(num_retries=self.num_retries)\n            else:\n                jobs.cancel(\n                    projectId=self.project_id,\n                    jobId=self.running_job_id).execute(num_retries=self.num_retries)\n        else:\n            self.log.info('No running BigQuery jobs to cancel.')\n            return\n\n        # Wait for all the calls to cancel to finish\n        max_polling_attempts = 12\n        polling_attempts = 0\n\n        job_complete = False\n        while polling_attempts < max_polling_attempts and not job_complete:\n            polling_attempts = polling_attempts + 1\n            job_complete = self.poll_job_complete(self.running_job_id)\n            if job_complete:\n                self.log.info('Job successfully canceled: %s, %s',\n                              self.project_id, self.running_job_id)\n            elif polling_attempts == max_polling_attempts:\n                self.log.info(\n                    \"Stopping polling due to timeout. Job with id %s \"\n                    \"has not completed cancel and may or may not finish.\",\n                    self.running_job_id)\n            else:\n                self.log.info('Waiting for canceled job with id %s to finish.',\n                              self.running_job_id)\n                time.sleep(5)","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryBaseCursor.run_table_delete","method_code":"def run_table_delete(self, deletion_dataset_table,\n                         ignore_if_missing=False):\n        deletion_project, deletion_dataset, deletion_table = \\\n            _split_tablename(table_input=deletion_dataset_table,\n                             default_project_id=self.project_id)\n\n        try:\n            self.service.tables() \\\n                .delete(projectId=deletion_project,\n                        datasetId=deletion_dataset,\n                        tableId=deletion_table) \\\n                .execute(num_retries=self.num_retries)\n            self.log.info('Deleted table %s:%s.%s.', deletion_project,\n                          deletion_dataset, deletion_table)\n        except HttpError:\n            if not ignore_if_missing:\n                raise Exception('Table deletion failed. Table does not exist.')\n            else:\n                self.log.info('Table does not exist. Skipping.')","method_summary":"Delete an existing table from the dataset; If the table does not exist, return an error unless ignore_if_missing is set to True.","original_method_code":"def run_table_delete(self, deletion_dataset_table,\n                         ignore_if_missing=False):\n        \"\"\"\n        Delete an existing table from the dataset;\n        If the table does not exist, return an error unless ignore_if_missing\n        is set to True.\n\n        :param deletion_dataset_table: A dotted\n            ``(<project>.|<project>:)<dataset>.<table>`` that indicates which table\n            will be deleted.\n        :type deletion_dataset_table: str\n        :param ignore_if_missing: if True, then return success even if the\n            requested table does not exist.\n        :type ignore_if_missing: bool\n        :return:\n        \"\"\"\n        deletion_project, deletion_dataset, deletion_table = \\\n            _split_tablename(table_input=deletion_dataset_table,\n                             default_project_id=self.project_id)\n\n        try:\n            self.service.tables() \\\n                .delete(projectId=deletion_project,\n                        datasetId=deletion_dataset,\n                        tableId=deletion_table) \\\n                .execute(num_retries=self.num_retries)\n            self.log.info('Deleted table %s:%s.%s.', deletion_project,\n                          deletion_dataset, deletion_table)\n        except HttpError:\n            if not ignore_if_missing:\n                raise Exception('Table deletion failed. Table does not exist.')\n            else:\n                self.log.info('Table does not exist. Skipping.')","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryBaseCursor.run_table_upsert","method_code":"def run_table_upsert(self, dataset_id, table_resource, project_id=None):\n        \n        table_id = table_resource['tableReference']['tableId']\n        project_id = project_id if project_id is not None else self.project_id\n        tables_list_resp = self.service.tables().list(\n            projectId=project_id, datasetId=dataset_id).execute(num_retries=self.num_retries)\n        while True:\n            for table in tables_list_resp.get('tables', []):\n                if table['tableReference']['tableId'] == table_id:\n                    \n                    self.log.info('Table %s:%s.%s exists, updating.',\n                                  project_id, dataset_id, table_id)\n                    return self.service.tables().update(\n                        projectId=project_id,\n                        datasetId=dataset_id,\n                        tableId=table_id,\n                        body=table_resource).execute(num_retries=self.num_retries)\n            \n            if 'nextPageToken' in tables_list_resp:\n                tables_list_resp = self.service.tables()\\\n                    .list(projectId=project_id,\n                          datasetId=dataset_id,\n                          pageToken=tables_list_resp['nextPageToken'])\\\n                    .execute(num_retries=self.num_retries)\n            \n            else:\n                \n                self.log.info('Table %s:%s.%s does not exist. creating.',\n                              project_id, dataset_id, table_id)\n                return self.service.tables().insert(\n                    projectId=project_id,\n                    datasetId=dataset_id,\n                    body=table_resource).execute(num_retries=self.num_retries)","method_summary":"creates a new, empty table in the dataset; If the table already exists, update the existing table. Since BigQuery does not natively allow table upserts, this is not an atomic operation.","original_method_code":"def run_table_upsert(self, dataset_id, table_resource, project_id=None):\n        \"\"\"\n        creates a new, empty table in the dataset;\n        If the table already exists, update the existing table.\n        Since BigQuery does not natively allow table upserts, this is not an\n        atomic operation.\n\n        :param dataset_id: the dataset to upsert the table into.\n        :type dataset_id: str\n        :param table_resource: a table resource. see\n            https:\/\/cloud.google.com\/bigquery\/docs\/reference\/v2\/tables#resource\n        :type table_resource: dict\n        :param project_id: the project to upsert the table into.  If None,\n            project will be self.project_id.\n        :return:\n        \"\"\"\n        # check to see if the table exists\n        table_id = table_resource['tableReference']['tableId']\n        project_id = project_id if project_id is not None else self.project_id\n        tables_list_resp = self.service.tables().list(\n            projectId=project_id, datasetId=dataset_id).execute(num_retries=self.num_retries)\n        while True:\n            for table in tables_list_resp.get('tables', []):\n                if table['tableReference']['tableId'] == table_id:\n                    # found the table, do update\n                    self.log.info('Table %s:%s.%s exists, updating.',\n                                  project_id, dataset_id, table_id)\n                    return self.service.tables().update(\n                        projectId=project_id,\n                        datasetId=dataset_id,\n                        tableId=table_id,\n                        body=table_resource).execute(num_retries=self.num_retries)\n            # If there is a next page, we need to check the next page.\n            if 'nextPageToken' in tables_list_resp:\n                tables_list_resp = self.service.tables()\\\n                    .list(projectId=project_id,\n                          datasetId=dataset_id,\n                          pageToken=tables_list_resp['nextPageToken'])\\\n                    .execute(num_retries=self.num_retries)\n            # If there is no next page, then the table doesn't exist.\n            else:\n                # do insert\n                self.log.info('Table %s:%s.%s does not exist. creating.',\n                              project_id, dataset_id, table_id)\n                return self.service.tables().insert(\n                    projectId=project_id,\n                    datasetId=dataset_id,\n                    body=table_resource).execute(num_retries=self.num_retries)","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryBaseCursor.get_dataset","method_code":"def get_dataset(self, dataset_id, project_id=None):\n        if not dataset_id or not isinstance(dataset_id, str):\n            raise ValueError(\"dataset_id argument must be provided and has \"\n                             \"a type 'str'. You provided: {}\".format(dataset_id))\n\n        dataset_project_id = project_id if project_id else self.project_id\n\n        try:\n            dataset_resource = self.service.datasets().get(\n                datasetId=dataset_id, projectId=dataset_project_id).execute(num_retries=self.num_retries)\n            self.log.info(\"Dataset Resource: %s\", dataset_resource)\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content))\n\n        return dataset_resource","method_summary":"Method returns dataset_resource if dataset exist and raised 404 error if dataset does not exist","original_method_code":"def get_dataset(self, dataset_id, project_id=None):\n        \"\"\"\n        Method returns dataset_resource if dataset exist\n        and raised 404 error if dataset does not exist\n\n        :param dataset_id: The BigQuery Dataset ID\n        :type dataset_id: str\n        :param project_id: The GCP Project ID\n        :type project_id: str\n        :return: dataset_resource\n\n            .. seealso::\n                For more information, see Dataset Resource content:\n                https:\/\/cloud.google.com\/bigquery\/docs\/reference\/rest\/v2\/datasets#resource\n        \"\"\"\n\n        if not dataset_id or not isinstance(dataset_id, str):\n            raise ValueError(\"dataset_id argument must be provided and has \"\n                             \"a type 'str'. You provided: {}\".format(dataset_id))\n\n        dataset_project_id = project_id if project_id else self.project_id\n\n        try:\n            dataset_resource = self.service.datasets().get(\n                datasetId=dataset_id, projectId=dataset_project_id).execute(num_retries=self.num_retries)\n            self.log.info(\"Dataset Resource: %s\", dataset_resource)\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content))\n\n        return dataset_resource","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryBaseCursor.get_datasets_list","method_code":"def get_datasets_list(self, project_id=None):\n        dataset_project_id = project_id if project_id else self.project_id\n\n        try:\n            datasets_list = self.service.datasets().list(\n                projectId=dataset_project_id).execute(num_retries=self.num_retries)['datasets']\n            self.log.info(\"Datasets List: %s\", datasets_list)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content))\n\n        return datasets_list","method_summary":"Method returns full list of BigQuery datasets in the current project","original_method_code":"def get_datasets_list(self, project_id=None):\n        \"\"\"\n        Method returns full list of BigQuery datasets in the current project\n\n        .. seealso::\n            For more information, see:\n            https:\/\/cloud.google.com\/bigquery\/docs\/reference\/rest\/v2\/datasets\/list\n\n        :param project_id: Google Cloud Project for which you\n            try to get all datasets\n        :type project_id: str\n        :return: datasets_list\n\n            Example of returned datasets_list: ::\n\n                   {\n                      \"kind\":\"bigquery#dataset\",\n                      \"location\":\"US\",\n                      \"id\":\"your-project:dataset_2_test\",\n                      \"datasetReference\":{\n                         \"projectId\":\"your-project\",\n                         \"datasetId\":\"dataset_2_test\"\n                      }\n                   },\n                   {\n                      \"kind\":\"bigquery#dataset\",\n                      \"location\":\"US\",\n                      \"id\":\"your-project:dataset_1_test\",\n                      \"datasetReference\":{\n                         \"projectId\":\"your-project\",\n                         \"datasetId\":\"dataset_1_test\"\n                      }\n                   }\n                ]\n        \"\"\"\n        dataset_project_id = project_id if project_id else self.project_id\n\n        try:\n            datasets_list = self.service.datasets().list(\n                projectId=dataset_project_id).execute(num_retries=self.num_retries)['datasets']\n            self.log.info(\"Datasets List: %s\", datasets_list)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content))\n\n        return datasets_list","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryBaseCursor.insert_all","method_code":"def insert_all(self, project_id, dataset_id, table_id,\n                   rows, ignore_unknown_values=False,\n                   skip_invalid_rows=False, fail_on_error=False):\n        dataset_project_id = project_id if project_id else self.project_id\n\n        body = {\n            \"rows\": rows,\n            \"ignoreUnknownValues\": ignore_unknown_values,\n            \"kind\": \"bigquery#tableDataInsertAllRequest\",\n            \"skipInvalidRows\": skip_invalid_rows,\n        }\n\n        try:\n            self.log.info(\n                'Inserting %s row(s) into Table %s:%s.%s',\n                len(rows), dataset_project_id, dataset_id, table_id\n            )\n\n            resp = self.service.tabledata().insertAll(\n                projectId=dataset_project_id, datasetId=dataset_id,\n                tableId=table_id, body=body\n            ).execute(num_retries=self.num_retries)\n\n            if 'insertErrors' not in resp:\n                self.log.info(\n                    'All row(s) inserted successfully: %s:%s.%s',\n                    dataset_project_id, dataset_id, table_id\n                )\n            else:\n                error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}'.format(\n                    len(resp['insertErrors']),\n                    dataset_project_id, dataset_id, table_id, resp['insertErrors'])\n                if fail_on_error:\n                    raise AirflowException(\n                        'BigQuery job failed. Error was: {}'.format(error_msg)\n                    )\n                self.log.info(error_msg)\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )","method_summary":"Method to stream data into BigQuery one record at a time without needing to run a load job","original_method_code":"def insert_all(self, project_id, dataset_id, table_id,\n                   rows, ignore_unknown_values=False,\n                   skip_invalid_rows=False, fail_on_error=False):\n        \"\"\"\n        Method to stream data into BigQuery one record at a time without needing\n        to run a load job\n\n        .. seealso::\n            For more information, see:\n            https:\/\/cloud.google.com\/bigquery\/docs\/reference\/rest\/v2\/tabledata\/insertAll\n\n        :param project_id: The name of the project where we have the table\n        :type project_id: str\n        :param dataset_id: The name of the dataset where we have the table\n        :type dataset_id: str\n        :param table_id: The name of the table\n        :type table_id: str\n        :param rows: the rows to insert\n        :type rows: list\n\n        **Example or rows**:\n            rows=[{\"json\": {\"a_key\": \"a_value_0\"}}, {\"json\": {\"a_key\": \"a_value_1\"}}]\n\n        :param ignore_unknown_values: [Optional] Accept rows that contain values\n            that do not match the schema. The unknown values are ignored.\n            The default value  is false, which treats unknown values as errors.\n        :type ignore_unknown_values: bool\n        :param skip_invalid_rows: [Optional] Insert all valid rows of a request,\n            even if invalid rows exist. The default value is false, which causes\n            the entire request to fail if any invalid rows exist.\n        :type skip_invalid_rows: bool\n        :param fail_on_error: [Optional] Force the task to fail if any errors occur.\n            The default value is false, which indicates the task should not fail\n            even if any insertion errors occur.\n        :type fail_on_error: bool\n        \"\"\"\n\n        dataset_project_id = project_id if project_id else self.project_id\n\n        body = {\n            \"rows\": rows,\n            \"ignoreUnknownValues\": ignore_unknown_values,\n            \"kind\": \"bigquery#tableDataInsertAllRequest\",\n            \"skipInvalidRows\": skip_invalid_rows,\n        }\n\n        try:\n            self.log.info(\n                'Inserting %s row(s) into Table %s:%s.%s',\n                len(rows), dataset_project_id, dataset_id, table_id\n            )\n\n            resp = self.service.tabledata().insertAll(\n                projectId=dataset_project_id, datasetId=dataset_id,\n                tableId=table_id, body=body\n            ).execute(num_retries=self.num_retries)\n\n            if 'insertErrors' not in resp:\n                self.log.info(\n                    'All row(s) inserted successfully: %s:%s.%s',\n                    dataset_project_id, dataset_id, table_id\n                )\n            else:\n                error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}'.format(\n                    len(resp['insertErrors']),\n                    dataset_project_id, dataset_id, table_id, resp['insertErrors'])\n                if fail_on_error:\n                    raise AirflowException(\n                        'BigQuery job failed. Error was: {}'.format(error_msg)\n                    )\n                self.log.info(error_msg)\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryCursor.execute","method_code":"def execute(self, operation, parameters=None):\n        sql = _bind_parameters(operation,\n                               parameters) if parameters else operation\n        self.job_id = self.run_query(sql)","method_summary":"Executes a BigQuery query, and returns the job ID.","original_method_code":"def execute(self, operation, parameters=None):\n        \"\"\"\n        Executes a BigQuery query, and returns the job ID.\n\n        :param operation: The query to execute.\n        :type operation: str\n        :param parameters: Parameters to substitute into the query.\n        :type parameters: dict\n        \"\"\"\n        sql = _bind_parameters(operation,\n                               parameters) if parameters else operation\n        self.job_id = self.run_query(sql)","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryCursor.executemany","method_code":"def executemany(self, operation, seq_of_parameters):\n        for parameters in seq_of_parameters:\n            self.execute(operation, parameters)","method_summary":"Execute a BigQuery query multiple times with different parameters.","original_method_code":"def executemany(self, operation, seq_of_parameters):\n        \"\"\"\n        Execute a BigQuery query multiple times with different parameters.\n\n        :param operation: The query to execute.\n        :type operation: str\n        :param seq_of_parameters: List of dictionary parameters to substitute into the\n            query.\n        :type seq_of_parameters: list\n        \"\"\"\n        for parameters in seq_of_parameters:\n            self.execute(operation, parameters)","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BigQueryCursor.next","method_code":"def next(self):\n        if not self.job_id:\n            return None\n\n        if len(self.buffer) == 0:\n            if self.all_pages_loaded:\n                return None\n\n            query_results = (self.service.jobs().getQueryResults(\n                projectId=self.project_id,\n                jobId=self.job_id,\n                pageToken=self.page_token).execute(num_retries=self.num_retries))\n\n            if 'rows' in query_results and query_results['rows']:\n                self.page_token = query_results.get('pageToken')\n                fields = query_results['schema']['fields']\n                col_types = [field['type'] for field in fields]\n                rows = query_results['rows']\n\n                for dict_row in rows:\n                    typed_row = ([\n                        _bq_cast(vs['v'], col_types[idx])\n                        for idx, vs in enumerate(dict_row['f'])\n                    ])\n                    self.buffer.append(typed_row)\n\n                if not self.page_token:\n                    self.all_pages_loaded = True\n\n            else:\n                \n                self.page_token = None\n                self.job_id = None\n                self.page_token = None\n                return None\n\n        return self.buffer.pop(0)","method_summary":"Helper method for fetchone, which returns the next row from a buffer. If the buffer is empty, attempts to paginate through the result set for the next page, and load it into the buffer.","original_method_code":"def next(self):\n        \"\"\"\n        Helper method for fetchone, which returns the next row from a buffer.\n        If the buffer is empty, attempts to paginate through the result set for\n        the next page, and load it into the buffer.\n        \"\"\"\n        if not self.job_id:\n            return None\n\n        if len(self.buffer) == 0:\n            if self.all_pages_loaded:\n                return None\n\n            query_results = (self.service.jobs().getQueryResults(\n                projectId=self.project_id,\n                jobId=self.job_id,\n                pageToken=self.page_token).execute(num_retries=self.num_retries))\n\n            if 'rows' in query_results and query_results['rows']:\n                self.page_token = query_results.get('pageToken')\n                fields = query_results['schema']['fields']\n                col_types = [field['type'] for field in fields]\n                rows = query_results['rows']\n\n                for dict_row in rows:\n                    typed_row = ([\n                        _bq_cast(vs['v'], col_types[idx])\n                        for idx, vs in enumerate(dict_row['f'])\n                    ])\n                    self.buffer.append(typed_row)\n\n                if not self.page_token:\n                    self.all_pages_loaded = True\n\n            else:\n                # Reset all state since we've exhausted the results.\n                self.page_token = None\n                self.job_id = None\n                self.page_token = None\n                return None\n\n        return self.buffer.pop(0)","method_path":"airflow\/contrib\/hooks\/bigquery_hook.py"}
{"repo_name":"apache\/airflow","method_name":"PostgresToGoogleCloudStorageOperator._query_postgres","method_code":"def _query_postgres(self):\n        postgres = PostgresHook(postgres_conn_id=self.postgres_conn_id)\n        conn = postgres.get_conn()\n        cursor = conn.cursor()\n        cursor.execute(self.sql, self.parameters)\n        return cursor","method_summary":"Queries Postgres and returns a cursor to the results.","original_method_code":"def _query_postgres(self):\n        \"\"\"\n        Queries Postgres and returns a cursor to the results.\n        \"\"\"\n        postgres = PostgresHook(postgres_conn_id=self.postgres_conn_id)\n        conn = postgres.get_conn()\n        cursor = conn.cursor()\n        cursor.execute(self.sql, self.parameters)\n        return cursor","method_path":"airflow\/contrib\/operators\/postgres_to_gcs_operator.py"}
{"repo_name":"apache\/airflow","method_name":"_make_intermediate_dirs","method_code":"def _make_intermediate_dirs(sftp_client, remote_directory):\n    if remote_directory == '\/':\n        sftp_client.chdir('\/')\n        return\n    if remote_directory == '':\n        return\n    try:\n        sftp_client.chdir(remote_directory)\n    except IOError:\n        dirname, basename = os.path.split(remote_directory.rstrip('\/'))\n        _make_intermediate_dirs(sftp_client, dirname)\n        sftp_client.mkdir(basename)\n        sftp_client.chdir(basename)\n        return","method_summary":"Create all the intermediate directories in a remote host","original_method_code":"def _make_intermediate_dirs(sftp_client, remote_directory):\n    \"\"\"\n    Create all the intermediate directories in a remote host\n\n    :param sftp_client: A Paramiko SFTP client.\n    :param remote_directory: Absolute Path of the directory containing the file\n    :return:\n    \"\"\"\n    if remote_directory == '\/':\n        sftp_client.chdir('\/')\n        return\n    if remote_directory == '':\n        return\n    try:\n        sftp_client.chdir(remote_directory)\n    except IOError:\n        dirname, basename = os.path.split(remote_directory.rstrip('\/'))\n        _make_intermediate_dirs(sftp_client, dirname)\n        sftp_client.mkdir(basename)\n        sftp_client.chdir(basename)\n        return","method_path":"airflow\/contrib\/operators\/sftp_operator.py"}
{"repo_name":"apache\/airflow","method_name":"SQSHook.create_queue","method_code":"def create_queue(self, queue_name, attributes=None):\n        return self.get_conn().create_queue(QueueName=queue_name, Attributes=attributes or {})","method_summary":"Create queue using connection object","original_method_code":"def create_queue(self, queue_name, attributes=None):\n        \"\"\"\n        Create queue using connection object\n\n        :param queue_name: name of the queue.\n        :type queue_name: str\n        :param attributes: additional attributes for the queue (default: None)\n            For details of the attributes parameter see :py:meth:`botocore.client.SQS.create_queue`\n        :type attributes: dict\n\n        :return: dict with the information about the queue\n            For details of the returned value see :py:meth:`botocore.client.SQS.create_queue`\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().create_queue(QueueName=queue_name, Attributes=attributes or {})","method_path":"airflow\/contrib\/hooks\/aws_sqs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SQSHook.send_message","method_code":"def send_message(self, queue_url, message_body, delay_seconds=0, message_attributes=None):\n        return self.get_conn().send_message(QueueUrl=queue_url,\n                                            MessageBody=message_body,\n                                            DelaySeconds=delay_seconds,\n                                            MessageAttributes=message_attributes or {})","method_summary":"Send message to the queue","original_method_code":"def send_message(self, queue_url, message_body, delay_seconds=0, message_attributes=None):\n        \"\"\"\n        Send message to the queue\n\n        :param queue_url: queue url\n        :type queue_url: str\n        :param message_body: the contents of the message\n        :type message_body: str\n        :param delay_seconds: seconds to delay the message\n        :type delay_seconds: int\n        :param message_attributes: additional attributes for the message (default: None)\n            For details of the attributes parameter see :py:meth:`botocore.client.SQS.send_message`\n        :type message_attributes: dict\n\n        :return: dict with the information about the message sent\n            For details of the returned value see :py:meth:`botocore.client.SQS.send_message`\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().send_message(QueueUrl=queue_url,\n                                            MessageBody=message_body,\n                                            DelaySeconds=delay_seconds,\n                                            MessageAttributes=message_attributes or {})","method_path":"airflow\/contrib\/hooks\/aws_sqs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"BaseTaskRunner.run_command","method_code":"def run_command(self, run_with=None, join_args=False):\n        run_with = run_with or []\n        cmd = [\" \".join(self._command)] if join_args else self._command\n        full_cmd = run_with + cmd\n\n        self.log.info('Running: %s', full_cmd)\n        proc = subprocess.Popen(\n            full_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            universal_newlines=True,\n            close_fds=True,\n            env=os.environ.copy(),\n            preexec_fn=os.setsid\n        )\n\n        \n        log_reader = threading.Thread(\n            target=self._read_task_logs,\n            args=(proc.stdout,),\n        )\n        log_reader.daemon = True\n        log_reader.start()\n        return proc","method_summary":"Run the task command.","original_method_code":"def run_command(self, run_with=None, join_args=False):\n        \"\"\"\n        Run the task command.\n\n        :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``\n        :type run_with: list\n        :param join_args: whether to concatenate the list of command tokens e.g. ``['airflow', 'run']`` vs\n            ``['airflow run']``\n        :param join_args: bool\n        :return: the process that was run\n        :rtype: subprocess.Popen\n        \"\"\"\n        run_with = run_with or []\n        cmd = [\" \".join(self._command)] if join_args else self._command\n        full_cmd = run_with + cmd\n\n        self.log.info('Running: %s', full_cmd)\n        proc = subprocess.Popen(\n            full_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            universal_newlines=True,\n            close_fds=True,\n            env=os.environ.copy(),\n            preexec_fn=os.setsid\n        )\n\n        # Start daemon thread to read subprocess logging output\n        log_reader = threading.Thread(\n            target=self._read_task_logs,\n            args=(proc.stdout,),\n        )\n        log_reader.daemon = True\n        log_reader.start()\n        return proc","method_path":"airflow\/task\/task_runner\/base_task_runner.py"}
{"repo_name":"apache\/airflow","method_name":"BaseTaskRunner.on_finish","method_code":"def on_finish(self):\n        if self._cfg_path and os.path.isfile(self._cfg_path):\n            if self.run_as_user:\n                subprocess.call(['sudo', 'rm', self._cfg_path], close_fds=True)\n            else:\n                os.remove(self._cfg_path)","method_summary":"A callback that should be called when this is done running.","original_method_code":"def on_finish(self):\n        \"\"\"\n        A callback that should be called when this is done running.\n        \"\"\"\n        if self._cfg_path and os.path.isfile(self._cfg_path):\n            if self.run_as_user:\n                subprocess.call(['sudo', 'rm', self._cfg_path], close_fds=True)\n            else:\n                os.remove(self._cfg_path)","method_path":"airflow\/task\/task_runner\/base_task_runner.py"}
{"repo_name":"apache\/airflow","method_name":"_main","method_code":"def _main():\n    \n    usage = \"usage: nvd3.py [options]\"\n    parser = OptionParser(usage=usage,\n                          version=(\"python-nvd3 - Charts generator with \"\n                                   \"nvd3.js and d3.js\"))\n    parser.add_option(\"-q\", \"--quiet\",\n                      action=\"store_false\", dest=\"verbose\", default=True,\n                      help=\"don't print messages to stdout\")\n\n    (options, args) = parser.parse_args()","method_summary":"Parse options and process commands","original_method_code":"def _main():\n    \"\"\"\n    Parse options and process commands\n    \"\"\"\n    # Parse arguments\n    usage = \"usage: nvd3.py [options]\"\n    parser = OptionParser(usage=usage,\n                          version=(\"python-nvd3 - Charts generator with \"\n                                   \"nvd3.js and d3.js\"))\n    parser.add_option(\"-q\", \"--quiet\",\n                      action=\"store_false\", dest=\"verbose\", default=True,\n                      help=\"don't print messages to stdout\")\n\n    (options, args) = parser.parse_args()","method_path":"airflow\/_vendor\/nvd3\/NVD3Chart.py"}
{"repo_name":"apache\/airflow","method_name":"NVD3Chart.buildhtmlheader","method_code":"def buildhtmlheader(self):\n        self.htmlheader = ''\n        \n        global _js_initialized\n        if '_js_initialized' not in globals() or not _js_initialized:\n            for css in self.header_css:\n                self.htmlheader += css\n            for js in self.header_js:\n                self.htmlheader += js","method_summary":"generate HTML header content","original_method_code":"def buildhtmlheader(self):\n        \"\"\"generate HTML header content\"\"\"\n        self.htmlheader = ''\n        # If the JavaScript assets have already been injected, don't bother re-sourcing them.\n        global _js_initialized\n        if '_js_initialized' not in globals() or not _js_initialized:\n            for css in self.header_css:\n                self.htmlheader += css\n            for js in self.header_js:\n                self.htmlheader += js","method_path":"airflow\/_vendor\/nvd3\/NVD3Chart.py"}
{"repo_name":"apache\/airflow","method_name":"NVD3Chart.buildcontainer","method_code":"def buildcontainer(self):\n        if self.container:\n            return\n\n        \n        if self.width:\n            if self.width[-1] != '%':\n                self.style += 'width:%spx;' % self.width\n            else:\n                self.style += 'width:%s;' % self.width\n        if self.height:\n            if self.height[-1] != '%':\n                self.style += 'height:%spx;' % self.height\n            else:\n                self.style += 'height:%s;' % self.height\n        if self.style:\n            self.style = 'style=\"%s\"' % self.style\n\n        self.container = self.containerheader + \\\n            '<div id=\"%s\"><svg %s><\/svg><\/div>\\n' % (self.name, self.style)","method_summary":"generate HTML div","original_method_code":"def buildcontainer(self):\n        \"\"\"generate HTML div\"\"\"\n        if self.container:\n            return\n\n        # Create SVG div with style\n        if self.width:\n            if self.width[-1] != '%':\n                self.style += 'width:%spx;' % self.width\n            else:\n                self.style += 'width:%s;' % self.width\n        if self.height:\n            if self.height[-1] != '%':\n                self.style += 'height:%spx;' % self.height\n            else:\n                self.style += 'height:%s;' % self.height\n        if self.style:\n            self.style = 'style=\"%s\"' % self.style\n\n        self.container = self.containerheader + \\\n            '<div id=\"%s\"><svg %s><\/svg><\/div>\\n' % (self.name, self.style)","method_path":"airflow\/_vendor\/nvd3\/NVD3Chart.py"}
{"repo_name":"apache\/airflow","method_name":"NVD3Chart.buildjschart","method_code":"def buildjschart(self):\n        self.jschart = ''\n\n        \n        \n        if self.tooltip_condition_string == '':\n            self.tooltip_condition_string = 'var y = String(graph.point.y);\\n'\n\n        \n        self.series_js = json.dumps(self.series)","method_summary":"generate javascript code for the chart","original_method_code":"def buildjschart(self):\n        \"\"\"generate javascript code for the chart\"\"\"\n        self.jschart = ''\n\n        # add custom tooltip string in jschart\n        # default condition (if build_custom_tooltip is not called explicitly with date_flag=True)\n        if self.tooltip_condition_string == '':\n            self.tooltip_condition_string = 'var y = String(graph.point.y);\\n'\n\n        # Include data\n        self.series_js = json.dumps(self.series)","method_path":"airflow\/_vendor\/nvd3\/NVD3Chart.py"}
{"repo_name":"apache\/airflow","method_name":"NVD3Chart.create_x_axis","method_code":"def create_x_axis(self, name, label=None, format=None, date=False, custom_format=False):\n        axis = {}\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:\n            if format == 'AM_PM':\n                axis['tickFormat'] = \"function(d) { return get_am_pm(parseInt(d)); }\"\n            else:\n                axis['tickFormat'] = \"d3.format(',%s')\" % format\n\n        if label:\n            axis['axisLabel'] = \"'\" + label + \"'\"\n\n        \n        if date:\n            self.dateformat = format\n            axis['tickFormat'] = (\"function(d) { return d3.time.format('%s')\"\n                                  \"(new Date(parseInt(d))) }\\n\"\n                                  \"\" % self.dateformat)\n            \n            if name[0] == 'x':\n                self.x_axis_date = True\n\n        \n        self.axislist[name] = axis\n\n        \n        if name == \"xAxis\" and self.focus_enable:\n            self.axislist['x2Axis'] = axis","method_summary":"Create X-axis","original_method_code":"def create_x_axis(self, name, label=None, format=None, date=False, custom_format=False):\n        \"\"\"Create X-axis\"\"\"\n        axis = {}\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:\n            if format == 'AM_PM':\n                axis['tickFormat'] = \"function(d) { return get_am_pm(parseInt(d)); }\"\n            else:\n                axis['tickFormat'] = \"d3.format(',%s')\" % format\n\n        if label:\n            axis['axisLabel'] = \"'\" + label + \"'\"\n\n        # date format : see https:\/\/github.com\/mbostock\/d3\/wiki\/Time-Formatting\n        if date:\n            self.dateformat = format\n            axis['tickFormat'] = (\"function(d) { return d3.time.format('%s')\"\n                                  \"(new Date(parseInt(d))) }\\n\"\n                                  \"\" % self.dateformat)\n            # flag is the x Axis is a date\n            if name[0] == 'x':\n                self.x_axis_date = True\n\n        # Add new axis to list of axis\n        self.axislist[name] = axis\n\n        # Create x2Axis if focus_enable\n        if name == \"xAxis\" and self.focus_enable:\n            self.axislist['x2Axis'] = axis","method_path":"airflow\/_vendor\/nvd3\/NVD3Chart.py"}
{"repo_name":"apache\/airflow","method_name":"NVD3Chart.create_y_axis","method_code":"def create_y_axis(self, name, label=None, format=None, custom_format=False):\n        axis = {}\n\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:\n            axis['tickFormat'] = \"d3.format(',%s')\" % format\n\n        if label:\n            axis['axisLabel'] = \"'\" + label + \"'\"\n\n        \n        self.axislist[name] = axis","method_summary":"Create Y-axis","original_method_code":"def create_y_axis(self, name, label=None, format=None, custom_format=False):\n        \"\"\"\n        Create Y-axis\n        \"\"\"\n        axis = {}\n\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:\n            axis['tickFormat'] = \"d3.format(',%s')\" % format\n\n        if label:\n            axis['axisLabel'] = \"'\" + label + \"'\"\n\n        # Add new axis to list of axis\n        self.axislist[name] = axis","method_path":"airflow\/_vendor\/nvd3\/NVD3Chart.py"}
{"repo_name":"apache\/airflow","method_name":"action_logging","method_code":"def action_logging(f):\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n\n        with create_session() as session:\n            if g.user.is_anonymous:\n                user = 'anonymous'\n            else:\n                user = g.user.username\n\n            log = Log(\n                event=f.__name__,\n                task_instance=None,\n                owner=user,\n                extra=str(list(request.args.items())),\n                task_id=request.args.get('task_id'),\n                dag_id=request.args.get('dag_id'))\n\n            if 'execution_date' in request.args:\n                log.execution_date = pendulum.parse(\n                    request.args.get('execution_date'))\n\n            session.add(log)\n\n        return f(*args, **kwargs)\n\n    return wrapper","method_summary":"Decorator to log user actions","original_method_code":"def action_logging(f):\n    \"\"\"\n    Decorator to log user actions\n    \"\"\"\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n\n        with create_session() as session:\n            if g.user.is_anonymous:\n                user = 'anonymous'\n            else:\n                user = g.user.username\n\n            log = Log(\n                event=f.__name__,\n                task_instance=None,\n                owner=user,\n                extra=str(list(request.args.items())),\n                task_id=request.args.get('task_id'),\n                dag_id=request.args.get('dag_id'))\n\n            if 'execution_date' in request.args:\n                log.execution_date = pendulum.parse(\n                    request.args.get('execution_date'))\n\n            session.add(log)\n\n        return f(*args, **kwargs)\n\n    return wrapper","method_path":"airflow\/www\/decorators.py"}
{"repo_name":"apache\/airflow","method_name":"gzipped","method_code":"def gzipped(f):\n    @functools.wraps(f)\n    def view_func(*args, **kwargs):\n        @after_this_request\n        def zipper(response):\n            accept_encoding = request.headers.get('Accept-Encoding', '')\n\n            if 'gzip' not in accept_encoding.lower():\n                return response\n\n            response.direct_passthrough = False\n\n            if (response.status_code < 200 or response.status_code >= 300 or\n                    'Content-Encoding' in response.headers):\n                return response\n            gzip_buffer = IO()\n            gzip_file = gzip.GzipFile(mode='wb',\n                                      fileobj=gzip_buffer)\n            gzip_file.write(response.data)\n            gzip_file.close()\n\n            response.data = gzip_buffer.getvalue()\n            response.headers['Content-Encoding'] = 'gzip'\n            response.headers['Vary'] = 'Accept-Encoding'\n            response.headers['Content-Length'] = len(response.data)\n\n            return response\n\n        return f(*args, **kwargs)\n\n    return view_func","method_summary":"Decorator to make a view compressed","original_method_code":"def gzipped(f):\n    \"\"\"\n    Decorator to make a view compressed\n    \"\"\"\n    @functools.wraps(f)\n    def view_func(*args, **kwargs):\n        @after_this_request\n        def zipper(response):\n            accept_encoding = request.headers.get('Accept-Encoding', '')\n\n            if 'gzip' not in accept_encoding.lower():\n                return response\n\n            response.direct_passthrough = False\n\n            if (response.status_code < 200 or response.status_code >= 300 or\n                    'Content-Encoding' in response.headers):\n                return response\n            gzip_buffer = IO()\n            gzip_file = gzip.GzipFile(mode='wb',\n                                      fileobj=gzip_buffer)\n            gzip_file.write(response.data)\n            gzip_file.close()\n\n            response.data = gzip_buffer.getvalue()\n            response.headers['Content-Encoding'] = 'gzip'\n            response.headers['Vary'] = 'Accept-Encoding'\n            response.headers['Content-Length'] = len(response.data)\n\n            return response\n\n        return f(*args, **kwargs)\n\n    return view_func","method_path":"airflow\/www\/decorators.py"}
{"repo_name":"apache\/airflow","method_name":"DagModel.create_dagrun","method_code":"def create_dagrun(self,\n                      run_id,\n                      state,\n                      execution_date,\n                      start_date=None,\n                      external_trigger=False,\n                      conf=None,\n                      session=None):\n        return self.get_dag().create_dagrun(run_id=run_id,\n                                            state=state,\n                                            execution_date=execution_date,\n                                            start_date=start_date,\n                                            external_trigger=external_trigger,\n                                            conf=conf,\n                                            session=session)","method_summary":"Creates a dag run from this dag including the tasks associated with this dag.","original_method_code":"def create_dagrun(self,\n                      run_id,\n                      state,\n                      execution_date,\n                      start_date=None,\n                      external_trigger=False,\n                      conf=None,\n                      session=None):\n        \"\"\"\n        Creates a dag run from this dag including the tasks associated with this dag.\n        Returns the dag run.\n\n        :param run_id: defines the the run id for this dag run\n        :type run_id: str\n        :param execution_date: the execution date of this dag run\n        :type execution_date: datetime.datetime\n        :param state: the state of the dag run\n        :type state: airflow.utils.state.State\n        :param start_date: the date this dag run should be evaluated\n        :type start_date: datetime.datetime\n        :param external_trigger: whether this dag run is externally triggered\n        :type external_trigger: bool\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n\n        return self.get_dag().create_dagrun(run_id=run_id,\n                                            state=state,\n                                            execution_date=execution_date,\n                                            start_date=start_date,\n                                            external_trigger=external_trigger,\n                                            conf=conf,\n                                            session=session)","method_path":"airflow\/models\/dag.py"}
{"repo_name":"apache\/airflow","method_name":"SQSPublishOperator.execute","method_code":"def execute(self, context):\n        hook = SQSHook(aws_conn_id=self.aws_conn_id)\n\n        result = hook.send_message(queue_url=self.sqs_queue,\n                                   message_body=self.message_content,\n                                   delay_seconds=self.delay_seconds,\n                                   message_attributes=self.message_attributes)\n\n        self.log.info('result is send_message is %s', result)\n\n        return result","method_summary":"Publish the message to SQS queue","original_method_code":"def execute(self, context):\n        \"\"\"\n        Publish the message to SQS queue\n\n        :param context: the context object\n        :type context: dict\n        :return: dict with information about the message sent\n            For details of the returned dict see :py:meth:`botocore.client.SQS.send_message`\n        :rtype: dict\n        \"\"\"\n\n        hook = SQSHook(aws_conn_id=self.aws_conn_id)\n\n        result = hook.send_message(queue_url=self.sqs_queue,\n                                   message_body=self.message_content,\n                                   delay_seconds=self.delay_seconds,\n                                   message_attributes=self.message_attributes)\n\n        self.log.info('result is send_message is %s', result)\n\n        return result","method_path":"airflow\/contrib\/operators\/aws_sqs_publish_operator.py"}
{"repo_name":"apache\/airflow","method_name":"json_response","method_code":"def json_response(obj):\n    return Response(\n        response=json.dumps(\n            obj, indent=4, cls=AirflowJsonEncoder),\n        status=200,\n        mimetype=\"application\/json\")","method_summary":"returns a json response from a json serializable python object","original_method_code":"def json_response(obj):\n    \"\"\"\n    returns a json response from a json serializable python object\n    \"\"\"\n    return Response(\n        response=json.dumps(\n            obj, indent=4, cls=AirflowJsonEncoder),\n        status=200,\n        mimetype=\"application\/json\")","method_path":"airflow\/www\/utils.py"}
{"repo_name":"apache\/airflow","method_name":"open_maybe_zipped","method_code":"def open_maybe_zipped(f, mode='r'):\n    _, archive, filename = ZIP_REGEX.search(f).groups()\n    if archive and zipfile.is_zipfile(archive):\n        return zipfile.ZipFile(archive, mode=mode).open(filename)\n    else:\n        return io.open(f, mode=mode)","method_summary":"Opens the given file. If the path contains a folder with a .zip suffix, then the folder is treated as a zip archive, opening the file inside the archive.","original_method_code":"def open_maybe_zipped(f, mode='r'):\n    \"\"\"\n    Opens the given file. If the path contains a folder with a .zip suffix, then\n    the folder is treated as a zip archive, opening the file inside the archive.\n\n    :return: a file object, as in `open`, or as in `ZipFile.open`.\n    \"\"\"\n\n    _, archive, filename = ZIP_REGEX.search(f).groups()\n    if archive and zipfile.is_zipfile(archive):\n        return zipfile.ZipFile(archive, mode=mode).open(filename)\n    else:\n        return io.open(f, mode=mode)","method_path":"airflow\/www\/utils.py"}
{"repo_name":"apache\/airflow","method_name":"make_cache_key","method_code":"def make_cache_key(*args, **kwargs):\n    path = request.path\n    args = str(hash(frozenset(request.args.items())))\n    return (path + args).encode('ascii', 'ignore')","method_summary":"Used by cache to get a unique key per URL","original_method_code":"def make_cache_key(*args, **kwargs):\n    \"\"\"\n    Used by cache to get a unique key per URL\n    \"\"\"\n    path = request.path\n    args = str(hash(frozenset(request.args.items())))\n    return (path + args).encode('ascii', 'ignore')","method_path":"airflow\/www\/utils.py"}
{"repo_name":"apache\/airflow","method_name":"CloudVideoIntelligenceHook.annotate_video","method_code":"def annotate_video(\n        self,\n        input_uri=None,\n        input_content=None,\n        features=None,\n        video_context=None,\n        output_uri=None,\n        location=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        client = self.get_conn()\n        return client.annotate_video(\n            input_uri=input_uri,\n            input_content=input_content,\n            features=features,\n            video_context=video_context,\n            output_uri=output_uri,\n            location_id=location,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )","method_summary":"Performs video annotation.","original_method_code":"def annotate_video(\n        self,\n        input_uri=None,\n        input_content=None,\n        features=None,\n        video_context=None,\n        output_uri=None,\n        location=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        Performs video annotation.\n\n        :param input_uri: Input video location. Currently, only Google Cloud Storage URIs are supported,\n            which must be specified in the following format: ``gs:\/\/bucket-id\/object-id``.\n        :type input_uri: str\n        :param input_content: The video data bytes.\n            If unset, the input video(s) should be specified via ``input_uri``.\n            If set, ``input_uri`` should be unset.\n        :type input_content: bytes\n        :param features: Requested video annotation features.\n        :type features: list[google.cloud.videointelligence_v1.VideoIntelligenceServiceClient.enums.Feature]\n        :param output_uri: Optional, location where the output (in JSON format) should be stored. Currently,\n            only Google Cloud Storage URIs are supported, which must be specified in the following format:\n            ``gs:\/\/bucket-id\/object-id``.\n        :type output_uri: str\n        :param video_context: Optional, Additional video context and\/or feature-specific parameters.\n        :type video_context: dict or google.cloud.videointelligence_v1.types.VideoContext\n        :param location: Optional, cloud region where annotation should take place. Supported cloud regions:\n            us-east1, us-west1, europe-west1, asia-east1.\n            If no region is specified, a region will be determined based on video file location.\n        :type location: str\n        :param retry: Retry object used to determine when\/if to retry requests.\n            If None is specified, requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: Optional, The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Optional, Additional metadata that is provided to the method.\n        :type metadata: seq[tuple[str, str]]\n        \"\"\"\n        client = self.get_conn()\n        return client.annotate_video(\n            input_uri=input_uri,\n            input_content=input_content,\n            features=features,\n            video_context=video_context,\n            output_uri=output_uri,\n            location_id=location,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )","method_path":"airflow\/contrib\/hooks\/gcp_video_intelligence_hook.py"}
{"repo_name":"apache\/airflow","method_name":"OpsgenieAlertHook._get_api_key","method_code":"def _get_api_key(self):\n        conn = self.get_connection(self.http_conn_id)\n        api_key = conn.password\n        if not api_key:\n            raise AirflowException('Opsgenie API Key is required for this hook, '\n                                   'please check your conn_id configuration.')\n        return api_key","method_summary":"Get Opsgenie api_key for creating alert","original_method_code":"def _get_api_key(self):\n        \"\"\"\n        Get Opsgenie api_key for creating alert\n        \"\"\"\n        conn = self.get_connection(self.http_conn_id)\n        api_key = conn.password\n        if not api_key:\n            raise AirflowException('Opsgenie API Key is required for this hook, '\n                                   'please check your conn_id configuration.')\n        return api_key","method_path":"airflow\/contrib\/hooks\/opsgenie_alert_hook.py"}
{"repo_name":"apache\/airflow","method_name":"OpsgenieAlertHook.get_conn","method_code":"def get_conn(self, headers=None):\n        conn = self.get_connection(self.http_conn_id)\n        self.base_url = conn.host if conn.host else 'https:\/\/api.opsgenie.com'\n        session = requests.Session()\n        if headers:\n            session.headers.update(headers)\n        return session","method_summary":"Overwrite HttpHook get_conn because this hook just needs base_url and headers, and does not need generic params","original_method_code":"def get_conn(self, headers=None):\n        \"\"\"\n        Overwrite HttpHook get_conn because this hook just needs base_url\n        and headers, and does not need generic params\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"\n        conn = self.get_connection(self.http_conn_id)\n        self.base_url = conn.host if conn.host else 'https:\/\/api.opsgenie.com'\n        session = requests.Session()\n        if headers:\n            session.headers.update(headers)\n        return session","method_path":"airflow\/contrib\/hooks\/opsgenie_alert_hook.py"}
{"repo_name":"apache\/airflow","method_name":"OpsgenieAlertHook.execute","method_code":"def execute(self, payload={}):\n        api_key = self._get_api_key()\n        return self.run(endpoint='v2\/alerts',\n                        data=json.dumps(payload),\n                        headers={'Content-Type': 'application\/json',\n                                 'Authorization': 'GenieKey %s' % api_key})","method_summary":"Execute the Opsgenie Alert call","original_method_code":"def execute(self, payload={}):\n        \"\"\"\n        Execute the Opsgenie Alert call\n\n        :param payload: Opsgenie API Create Alert payload values\n            See https:\/\/docs.opsgenie.com\/docs\/alert-api#section-create-alert\n        :type payload: dict\n        \"\"\"\n        api_key = self._get_api_key()\n        return self.run(endpoint='v2\/alerts',\n                        data=json.dumps(payload),\n                        headers={'Content-Type': 'application\/json',\n                                 'Authorization': 'GenieKey %s' % api_key})","method_path":"airflow\/contrib\/hooks\/opsgenie_alert_hook.py"}
{"repo_name":"apache\/airflow","method_name":"OpsgenieAlertOperator._build_opsgenie_payload","method_code":"def _build_opsgenie_payload(self):\n        payload = {}\n\n        for key in [\n            \"message\", \"alias\", \"description\", \"responders\",\n            \"visibleTo\", \"actions\", \"tags\", \"details\", \"entity\",\n            \"source\", \"priority\", \"user\", \"note\"\n        ]:\n            val = getattr(self, key)\n            if val:\n                payload[key] = val\n        return payload","method_summary":"Construct the Opsgenie JSON payload. All relevant parameters are combined here to a valid Opsgenie JSON payload.","original_method_code":"def _build_opsgenie_payload(self):\n        \"\"\"\n        Construct the Opsgenie JSON payload. All relevant parameters are combined here\n        to a valid Opsgenie JSON payload.\n\n        :return: Opsgenie payload (dict) to send\n        \"\"\"\n        payload = {}\n\n        for key in [\n            \"message\", \"alias\", \"description\", \"responders\",\n            \"visibleTo\", \"actions\", \"tags\", \"details\", \"entity\",\n            \"source\", \"priority\", \"user\", \"note\"\n        ]:\n            val = getattr(self, key)\n            if val:\n                payload[key] = val\n        return payload","method_path":"airflow\/contrib\/operators\/opsgenie_alert_operator.py"}
{"repo_name":"apache\/airflow","method_name":"OpsgenieAlertOperator.execute","method_code":"def execute(self, context):\n        self.hook = OpsgenieAlertHook(self.opsgenie_conn_id)\n        self.hook.execute(self._build_opsgenie_payload())","method_summary":"Call the OpsgenieAlertHook to post message","original_method_code":"def execute(self, context):\n        \"\"\"\n        Call the OpsgenieAlertHook to post message\n        \"\"\"\n        self.hook = OpsgenieAlertHook(self.opsgenie_conn_id)\n        self.hook.execute(self._build_opsgenie_payload())","method_path":"airflow\/contrib\/operators\/opsgenie_alert_operator.py"}
{"repo_name":"apache\/airflow","method_name":"AWSAthenaHook.get_conn","method_code":"def get_conn(self):\n        if not self.conn:\n            self.conn = self.get_client_type('athena')\n        return self.conn","method_summary":"check if aws conn exists already or create one and return it","original_method_code":"def get_conn(self):\n        \"\"\"\n        check if aws conn exists already or create one and return it\n\n        :return: boto3 session\n        \"\"\"\n        if not self.conn:\n            self.conn = self.get_client_type('athena')\n        return self.conn","method_path":"airflow\/contrib\/hooks\/aws_athena_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AWSAthenaHook.run_query","method_code":"def run_query(self, query, query_context, result_configuration, client_request_token=None):\n        response = self.conn.start_query_execution(QueryString=query,\n                                                   ClientRequestToken=client_request_token,\n                                                   QueryExecutionContext=query_context,\n                                                   ResultConfiguration=result_configuration)\n        query_execution_id = response['QueryExecutionId']\n        return query_execution_id","method_summary":"Run Presto query on athena with provided config and return submitted query_execution_id","original_method_code":"def run_query(self, query, query_context, result_configuration, client_request_token=None):\n        \"\"\"\n        Run Presto query on athena with provided config and return submitted query_execution_id\n\n        :param query: Presto query to run\n        :type query: str\n        :param query_context: Context in which query need to be run\n        :type query_context: dict\n        :param result_configuration: Dict with path to store results in and config related to encryption\n        :type result_configuration: dict\n        :param client_request_token: Unique token created by user to avoid multiple executions of same query\n        :type client_request_token: str\n        :return: str\n        \"\"\"\n        response = self.conn.start_query_execution(QueryString=query,\n                                                   ClientRequestToken=client_request_token,\n                                                   QueryExecutionContext=query_context,\n                                                   ResultConfiguration=result_configuration)\n        query_execution_id = response['QueryExecutionId']\n        return query_execution_id","method_path":"airflow\/contrib\/hooks\/aws_athena_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AWSAthenaHook.check_query_status","method_code":"def check_query_status(self, query_execution_id):\n        response = self.conn.get_query_execution(QueryExecutionId=query_execution_id)\n        state = None\n        try:\n            state = response['QueryExecution']['Status']['State']\n        except Exception as ex:\n            self.log.error('Exception while getting query state', ex)\n        finally:\n            return state","method_summary":"Fetch the status of submitted athena query.","original_method_code":"def check_query_status(self, query_execution_id):\n        \"\"\"\n        Fetch the status of submitted athena query. Returns None or one of valid query states.\n\n        :param query_execution_id: Id of submitted athena query\n        :type query_execution_id: str\n        :return: str\n        \"\"\"\n        response = self.conn.get_query_execution(QueryExecutionId=query_execution_id)\n        state = None\n        try:\n            state = response['QueryExecution']['Status']['State']\n        except Exception as ex:\n            self.log.error('Exception while getting query state', ex)\n        finally:\n            return state","method_path":"airflow\/contrib\/hooks\/aws_athena_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AWSAthenaHook.poll_query_status","method_code":"def poll_query_status(self, query_execution_id, max_tries=None):\n        try_number = 1\n        final_query_state = None  \n        while True:\n            query_state = self.check_query_status(query_execution_id)\n            if query_state is None:\n                self.log.info('Trial {try_number}: Invalid query state. Retrying again'.format(\n                    try_number=try_number))\n            elif query_state in self.INTERMEDIATE_STATES:\n                self.log.info('Trial {try_number}: Query is still in an intermediate state - {state}'\n                              .format(try_number=try_number, state=query_state))\n            else:\n                self.log.info('Trial {try_number}: Query execution completed. Final state is {state}'\n                              .format(try_number=try_number, state=query_state))\n                final_query_state = query_state\n                break\n            if max_tries and try_number >= max_tries:  \n                final_query_state = query_state\n                break\n            try_number += 1\n            sleep(self.sleep_time)\n        return final_query_state","method_summary":"Poll the status of submitted athena query until query state reaches final state.","original_method_code":"def poll_query_status(self, query_execution_id, max_tries=None):\n        \"\"\"\n        Poll the status of submitted athena query until query state reaches final state.\n        Returns one of the final states\n\n        :param query_execution_id: Id of submitted athena query\n        :type query_execution_id: str\n        :param max_tries: Number of times to poll for query state before function exits\n        :type max_tries: int\n        :return: str\n        \"\"\"\n        try_number = 1\n        final_query_state = None  # Query state when query reaches final state or max_tries reached\n        while True:\n            query_state = self.check_query_status(query_execution_id)\n            if query_state is None:\n                self.log.info('Trial {try_number}: Invalid query state. Retrying again'.format(\n                    try_number=try_number))\n            elif query_state in self.INTERMEDIATE_STATES:\n                self.log.info('Trial {try_number}: Query is still in an intermediate state - {state}'\n                              .format(try_number=try_number, state=query_state))\n            else:\n                self.log.info('Trial {try_number}: Query execution completed. Final state is {state}'\n                              .format(try_number=try_number, state=query_state))\n                final_query_state = query_state\n                break\n            if max_tries and try_number >= max_tries:  # Break loop if max_tries reached\n                final_query_state = query_state\n                break\n            try_number += 1\n            sleep(self.sleep_time)\n        return final_query_state","method_path":"airflow\/contrib\/hooks\/aws_athena_hook.py"}
{"repo_name":"apache\/airflow","method_name":"ZendeskHook.__handle_rate_limit_exception","method_code":"def __handle_rate_limit_exception(self, rate_limit_exception):\n        retry_after = int(\n            rate_limit_exception.response.headers.get('Retry-After', 60))\n        self.log.info(\n            \"Hit Zendesk API rate limit. Pausing for %s seconds\",\n            retry_after\n        )\n        time.sleep(retry_after)","method_summary":"Sleep for the time specified in the exception. If not specified, wait for 60 seconds.","original_method_code":"def __handle_rate_limit_exception(self, rate_limit_exception):\n        \"\"\"\n        Sleep for the time specified in the exception. If not specified, wait\n        for 60 seconds.\n        \"\"\"\n        retry_after = int(\n            rate_limit_exception.response.headers.get('Retry-After', 60))\n        self.log.info(\n            \"Hit Zendesk API rate limit. Pausing for %s seconds\",\n            retry_after\n        )\n        time.sleep(retry_after)","method_path":"airflow\/hooks\/zendesk_hook.py"}
{"repo_name":"apache\/airflow","method_name":"ZendeskHook.call","method_code":"def call(self, path, query=None, get_all_pages=True, side_loading=False):\n        zendesk = self.get_conn()\n        first_request_successful = False\n\n        while not first_request_successful:\n            try:\n                results = zendesk.call(path, query)\n                first_request_successful = True\n            except RateLimitError as rle:\n                self.__handle_rate_limit_exception(rle)\n\n        \n        keys = [path.split(\"\/\")[-1].split(\".json\")[0]]\n        next_page = results['next_page']\n        if side_loading:\n            keys += query['include'].split(',')\n        results = {key: results[key] for key in keys}\n\n        if get_all_pages:\n            while next_page is not None:\n                try:\n                    \n                    \n                    \n                    next_url = next_page.split(self.__url)[1]\n                    self.log.info(\"Calling %s\", next_url)\n                    more_res = zendesk.call(next_url)\n                    for key in results:\n                        results[key].extend(more_res[key])\n                    if next_page == more_res['next_page']:\n                        \n                        \n                        \n                        \n                        break\n                    else:\n                        next_page = more_res['next_page']\n                except RateLimitError as rle:\n                    self.__handle_rate_limit_exception(rle)\n                except ZendeskError as ze:\n                    if b\"Use a start_time older than 5 minutes\" in ze.msg:\n                        \n                        break\n                    else:\n                        raise ze\n\n        return results","method_summary":"Call Zendesk API and return results","original_method_code":"def call(self, path, query=None, get_all_pages=True, side_loading=False):\n        \"\"\"\n        Call Zendesk API and return results\n\n        :param path: The Zendesk API to call\n        :param query: Query parameters\n        :param get_all_pages: Accumulate results over all pages before\n               returning. Due to strict rate limiting, this can often timeout.\n               Waits for recommended period between tries after a timeout.\n        :param side_loading: Retrieve related records as part of a single\n               request. In order to enable side-loading, add an 'include'\n               query parameter containing a comma-separated list of resources\n               to load. For more information on side-loading see\n               https:\/\/developer.zendesk.com\/rest_api\/docs\/core\/side_loading\n        \"\"\"\n        zendesk = self.get_conn()\n        first_request_successful = False\n\n        while not first_request_successful:\n            try:\n                results = zendesk.call(path, query)\n                first_request_successful = True\n            except RateLimitError as rle:\n                self.__handle_rate_limit_exception(rle)\n\n        # Find the key with the results\n        keys = [path.split(\"\/\")[-1].split(\".json\")[0]]\n        next_page = results['next_page']\n        if side_loading:\n            keys += query['include'].split(',')\n        results = {key: results[key] for key in keys}\n\n        if get_all_pages:\n            while next_page is not None:\n                try:\n                    # Need to split because the next page URL has\n                    # `github.zendesk...`\n                    # in it, but the call function needs it removed.\n                    next_url = next_page.split(self.__url)[1]\n                    self.log.info(\"Calling %s\", next_url)\n                    more_res = zendesk.call(next_url)\n                    for key in results:\n                        results[key].extend(more_res[key])\n                    if next_page == more_res['next_page']:\n                        # Unfortunately zdesk doesn't always throw ZendeskError\n                        # when we are done getting all the data. Sometimes the\n                        # next just refers to the current set of results.\n                        # Hence, need to deal with this special case\n                        break\n                    else:\n                        next_page = more_res['next_page']\n                except RateLimitError as rle:\n                    self.__handle_rate_limit_exception(rle)\n                except ZendeskError as ze:\n                    if b\"Use a start_time older than 5 minutes\" in ze.msg:\n                        # We have pretty up to date data\n                        break\n                    else:\n                        raise ze\n\n        return results","method_path":"airflow\/hooks\/zendesk_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AwsGlueCatalogHook.get_partitions","method_code":"def get_partitions(self,\n                       database_name,\n                       table_name,\n                       expression='',\n                       page_size=None,\n                       max_items=None):\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('get_partitions')\n        response = paginator.paginate(\n            DatabaseName=database_name,\n            TableName=table_name,\n            Expression=expression,\n            PaginationConfig=config\n        )\n\n        partitions = set()\n        for page in response:\n            for p in page['Partitions']:\n                partitions.add(tuple(p['Values']))\n\n        return partitions","method_summary":"Retrieves the partition values for a table.","original_method_code":"def get_partitions(self,\n                       database_name,\n                       table_name,\n                       expression='',\n                       page_size=None,\n                       max_items=None):\n        \"\"\"\n        Retrieves the partition values for a table.\n\n        :param database_name: The name of the catalog database where the partitions reside.\n        :type database_name: str\n        :param table_name: The name of the partitions' table.\n        :type table_name: str\n        :param expression: An expression filtering the partitions to be returned.\n            Please see official AWS documentation for further information.\n            https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/aws-glue-api-catalog-partitions.html#aws-glue-api-catalog-partitions-GetPartitions\n        :type expression: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        :return: set of partition values where each value is a tuple since\n            a partition may be composed of multiple columns. For example:\n            ``{('2018-01-01','1'), ('2018-01-01','2')}``\n        \"\"\"\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('get_partitions')\n        response = paginator.paginate(\n            DatabaseName=database_name,\n            TableName=table_name,\n            Expression=expression,\n            PaginationConfig=config\n        )\n\n        partitions = set()\n        for page in response:\n            for p in page['Partitions']:\n                partitions.add(tuple(p['Values']))\n\n        return partitions","method_path":"airflow\/contrib\/hooks\/aws_glue_catalog_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AwsGlueCatalogHook.get_table","method_code":"def get_table(self, database_name, table_name):\n        result = self.get_conn().get_table(DatabaseName=database_name, Name=table_name)\n\n        return result['Table']","method_summary":"Get the information of the table","original_method_code":"def get_table(self, database_name, table_name):\n        \"\"\"\n        Get the information of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :rtype: dict\n\n        >>> hook = AwsGlueCatalogHook()\n        >>> r = hook.get_table('db', 'table_foo')\n        >>> r['Name'] = 'table_foo'\n        \"\"\"\n\n        result = self.get_conn().get_table(DatabaseName=database_name, Name=table_name)\n\n        return result['Table']","method_path":"airflow\/contrib\/hooks\/aws_glue_catalog_hook.py"}
{"repo_name":"apache\/airflow","method_name":"AwsGlueCatalogHook.get_table_location","method_code":"def get_table_location(self, database_name, table_name):\n        table = self.get_table(database_name, table_name)\n\n        return table['StorageDescriptor']['Location']","method_summary":"Get the physical location of the table","original_method_code":"def get_table_location(self, database_name, table_name):\n        \"\"\"\n        Get the physical location of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :return: str\n        \"\"\"\n\n        table = self.get_table(database_name, table_name)\n\n        return table['StorageDescriptor']['Location']","method_path":"airflow\/contrib\/hooks\/aws_glue_catalog_hook.py"}
{"repo_name":"apache\/airflow","method_name":"RedshiftHook.cluster_status","method_code":"def cluster_status(self, cluster_identifier):\n        conn = self.get_conn()\n        try:\n            response = conn.describe_clusters(\n                ClusterIdentifier=cluster_identifier)['Clusters']\n            return response[0]['ClusterStatus'] if response else None\n        except conn.exceptions.ClusterNotFoundFault:\n            return 'cluster_not_found'","method_summary":"Return status of a cluster","original_method_code":"def cluster_status(self, cluster_identifier):\n        \"\"\"\n        Return status of a cluster\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        \"\"\"\n        conn = self.get_conn()\n        try:\n            response = conn.describe_clusters(\n                ClusterIdentifier=cluster_identifier)['Clusters']\n            return response[0]['ClusterStatus'] if response else None\n        except conn.exceptions.ClusterNotFoundFault:\n            return 'cluster_not_found'","method_path":"airflow\/contrib\/hooks\/redshift_hook.py"}
{"repo_name":"apache\/airflow","method_name":"RedshiftHook.delete_cluster","method_code":"def delete_cluster(\n            self,\n            cluster_identifier,\n            skip_final_cluster_snapshot=True,\n            final_cluster_snapshot_identifier=''):\n        response = self.get_conn().delete_cluster(\n            ClusterIdentifier=cluster_identifier,\n            SkipFinalClusterSnapshot=skip_final_cluster_snapshot,\n            FinalClusterSnapshotIdentifier=final_cluster_snapshot_identifier\n        )\n        return response['Cluster'] if response['Cluster'] else None","method_summary":"Delete a cluster and optionally create a snapshot","original_method_code":"def delete_cluster(\n            self,\n            cluster_identifier,\n            skip_final_cluster_snapshot=True,\n            final_cluster_snapshot_identifier=''):\n        \"\"\"\n        Delete a cluster and optionally create a snapshot\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        :param skip_final_cluster_snapshot: determines cluster snapshot creation\n        :type skip_final_cluster_snapshot: bool\n        :param final_cluster_snapshot_identifier: name of final cluster snapshot\n        :type final_cluster_snapshot_identifier: str\n        \"\"\"\n        response = self.get_conn().delete_cluster(\n            ClusterIdentifier=cluster_identifier,\n            SkipFinalClusterSnapshot=skip_final_cluster_snapshot,\n            FinalClusterSnapshotIdentifier=final_cluster_snapshot_identifier\n        )\n        return response['Cluster'] if response['Cluster'] else None","method_path":"airflow\/contrib\/hooks\/redshift_hook.py"}
{"repo_name":"apache\/airflow","method_name":"RedshiftHook.describe_cluster_snapshots","method_code":"def describe_cluster_snapshots(self, cluster_identifier):\n        response = self.get_conn().describe_cluster_snapshots(\n            ClusterIdentifier=cluster_identifier\n        )\n        if 'Snapshots' not in response:\n            return None\n        snapshots = response['Snapshots']\n        snapshots = filter(lambda x: x['Status'], snapshots)\n        snapshots.sort(key=lambda x: x['SnapshotCreateTime'], reverse=True)\n        return snapshots","method_summary":"Gets a list of snapshots for a cluster","original_method_code":"def describe_cluster_snapshots(self, cluster_identifier):\n        \"\"\"\n        Gets a list of snapshots for a cluster\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        \"\"\"\n        response = self.get_conn().describe_cluster_snapshots(\n            ClusterIdentifier=cluster_identifier\n        )\n        if 'Snapshots' not in response:\n            return None\n        snapshots = response['Snapshots']\n        snapshots = filter(lambda x: x['Status'], snapshots)\n        snapshots.sort(key=lambda x: x['SnapshotCreateTime'], reverse=True)\n        return snapshots","method_path":"airflow\/contrib\/hooks\/redshift_hook.py"}
{"repo_name":"apache\/airflow","method_name":"RedshiftHook.restore_from_cluster_snapshot","method_code":"def restore_from_cluster_snapshot(self, cluster_identifier, snapshot_identifier):\n        response = self.get_conn().restore_from_cluster_snapshot(\n            ClusterIdentifier=cluster_identifier,\n            SnapshotIdentifier=snapshot_identifier\n        )\n        return response['Cluster'] if response['Cluster'] else None","method_summary":"Restores a cluster from its snapshot","original_method_code":"def restore_from_cluster_snapshot(self, cluster_identifier, snapshot_identifier):\n        \"\"\"\n        Restores a cluster from its snapshot\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        :param snapshot_identifier: unique identifier for a snapshot of a cluster\n        :type snapshot_identifier: str\n        \"\"\"\n        response = self.get_conn().restore_from_cluster_snapshot(\n            ClusterIdentifier=cluster_identifier,\n            SnapshotIdentifier=snapshot_identifier\n        )\n        return response['Cluster'] if response['Cluster'] else None","method_path":"airflow\/contrib\/hooks\/redshift_hook.py"}
{"repo_name":"apache\/airflow","method_name":"RedshiftHook.create_cluster_snapshot","method_code":"def create_cluster_snapshot(self, snapshot_identifier, cluster_identifier):\n        response = self.get_conn().create_cluster_snapshot(\n            SnapshotIdentifier=snapshot_identifier,\n            ClusterIdentifier=cluster_identifier,\n        )\n        return response['Snapshot'] if response['Snapshot'] else None","method_summary":"Creates a snapshot of a cluster","original_method_code":"def create_cluster_snapshot(self, snapshot_identifier, cluster_identifier):\n        \"\"\"\n        Creates a snapshot of a cluster\n\n        :param snapshot_identifier: unique identifier for a snapshot of a cluster\n        :type snapshot_identifier: str\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        \"\"\"\n        response = self.get_conn().create_cluster_snapshot(\n            SnapshotIdentifier=snapshot_identifier,\n            ClusterIdentifier=cluster_identifier,\n        )\n        return response['Snapshot'] if response['Snapshot'] else None","method_path":"airflow\/contrib\/hooks\/redshift_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SlackAPIOperator.execute","method_code":"def execute(self, **kwargs):\n        if not self.api_params:\n            self.construct_api_call_params()\n        slack = SlackHook(token=self.token, slack_conn_id=self.slack_conn_id)\n        slack.call(self.method, self.api_params)","method_summary":"SlackAPIOperator calls will not fail even if the call is not unsuccessful. It should not prevent a DAG from completing in success","original_method_code":"def execute(self, **kwargs):\n        \"\"\"\n        SlackAPIOperator calls will not fail even if the call is not unsuccessful.\n        It should not prevent a DAG from completing in success\n        \"\"\"\n        if not self.api_params:\n            self.construct_api_call_params()\n        slack = SlackHook(token=self.token, slack_conn_id=self.slack_conn_id)\n        slack.call(self.method, self.api_params)","method_path":"airflow\/operators\/slack_operator.py"}
{"repo_name":"apache\/airflow","method_name":"HdfsSensor.filter_for_filesize","method_code":"def filter_for_filesize(result, size=None):\n        if size:\n            log = LoggingMixin().log\n            log.debug(\n                'Filtering for file size >= %s in files: %s',\n                size, map(lambda x: x['path'], result)\n            )\n            size *= settings.MEGABYTE\n            result = [x for x in result if x['length'] >= size]\n            log.debug('HdfsSensor.poke: after size filter result is %s', result)\n        return result","method_summary":"Will test the filepath result and test if its size is at least self.filesize","original_method_code":"def filter_for_filesize(result, size=None):\n        \"\"\"\n        Will test the filepath result and test if its size is at least self.filesize\n\n        :param result: a list of dicts returned by Snakebite ls\n        :param size: the file size in MB a file should be at least to trigger True\n        :return: (bool) depending on the matching criteria\n        \"\"\"\n        if size:\n            log = LoggingMixin().log\n            log.debug(\n                'Filtering for file size >= %s in files: %s',\n                size, map(lambda x: x['path'], result)\n            )\n            size *= settings.MEGABYTE\n            result = [x for x in result if x['length'] >= size]\n            log.debug('HdfsSensor.poke: after size filter result is %s', result)\n        return result","method_path":"airflow\/sensors\/hdfs_sensor.py"}
{"repo_name":"apache\/airflow","method_name":"HdfsSensor.filter_for_ignored_ext","method_code":"def filter_for_ignored_ext(result, ignored_ext, ignore_copying):\n        if ignore_copying:\n            log = LoggingMixin().log\n            regex_builder = r\"^.*\\.(%s$)$\" % '$|'.join(ignored_ext)\n            ignored_extensions_regex = re.compile(regex_builder)\n            log.debug(\n                'Filtering result for ignored extensions: %s in files %s',\n                ignored_extensions_regex.pattern, map(lambda x: x['path'], result)\n            )\n            result = [x for x in result if not ignored_extensions_regex.match(x['path'])]\n            log.debug('HdfsSensor.poke: after ext filter result is %s', result)\n        return result","method_summary":"Will filter if instructed to do so the result to remove matching criteria","original_method_code":"def filter_for_ignored_ext(result, ignored_ext, ignore_copying):\n        \"\"\"\n        Will filter if instructed to do so the result to remove matching criteria\n\n        :param result: list of dicts returned by Snakebite ls\n        :type result: list[dict]\n        :param ignored_ext: list of ignored extensions\n        :type ignored_ext: list\n        :param ignore_copying: shall we ignore ?\n        :type ignore_copying: bool\n        :return: list of dicts which were not removed\n        :rtype: list[dict]\n        \"\"\"\n        if ignore_copying:\n            log = LoggingMixin().log\n            regex_builder = r\"^.*\\.(%s$)$\" % '$|'.join(ignored_ext)\n            ignored_extensions_regex = re.compile(regex_builder)\n            log.debug(\n                'Filtering result for ignored extensions: %s in files %s',\n                ignored_extensions_regex.pattern, map(lambda x: x['path'], result)\n            )\n            result = [x for x in result if not ignored_extensions_regex.match(x['path'])]\n            log.debug('HdfsSensor.poke: after ext filter result is %s', result)\n        return result","method_path":"airflow\/sensors\/hdfs_sensor.py"}
{"repo_name":"apache\/airflow","method_name":"MongoToS3Operator.execute","method_code":"def execute(self, context):\n        s3_conn = S3Hook(self.s3_conn_id)\n\n        \n        if self.is_pipeline:\n            results = MongoHook(self.mongo_conn_id).aggregate(\n                mongo_collection=self.mongo_collection,\n                aggregate_query=self.mongo_query,\n                mongo_db=self.mongo_db\n            )\n\n        else:\n            results = MongoHook(self.mongo_conn_id).find(\n                mongo_collection=self.mongo_collection,\n                query=self.mongo_query,\n                mongo_db=self.mongo_db\n            )\n\n        \n        docs_str = self._stringify(self.transform(results))\n\n        \n        s3_conn.load_string(\n            string_data=docs_str,\n            key=self.s3_key,\n            bucket_name=self.s3_bucket,\n            replace=self.replace\n        )\n\n        return True","method_summary":"Executed by task_instance at runtime","original_method_code":"def execute(self, context):\n        \"\"\"\n        Executed by task_instance at runtime\n        \"\"\"\n        s3_conn = S3Hook(self.s3_conn_id)\n\n        # Grab collection and execute query according to whether or not it is a pipeline\n        if self.is_pipeline:\n            results = MongoHook(self.mongo_conn_id).aggregate(\n                mongo_collection=self.mongo_collection,\n                aggregate_query=self.mongo_query,\n                mongo_db=self.mongo_db\n            )\n\n        else:\n            results = MongoHook(self.mongo_conn_id).find(\n                mongo_collection=self.mongo_collection,\n                query=self.mongo_query,\n                mongo_db=self.mongo_db\n            )\n\n        # Performs transform then stringifies the docs results into json format\n        docs_str = self._stringify(self.transform(results))\n\n        # Load Into S3\n        s3_conn.load_string(\n            string_data=docs_str,\n            key=self.s3_key,\n            bucket_name=self.s3_bucket,\n            replace=self.replace\n        )\n\n        return True","method_path":"airflow\/contrib\/operators\/mongo_to_s3.py"}
{"repo_name":"apache\/airflow","method_name":"get_pool","method_code":"def get_pool(name, session=None):\n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    pool = session.query(Pool).filter_by(pool=name).first()\n    if pool is None:\n        raise PoolNotFound(\"Pool '%s' doesn't exist\" % name)\n\n    return pool","method_summary":"Get pool by a given name.","original_method_code":"def get_pool(name, session=None):\n    \"\"\"Get pool by a given name.\"\"\"\n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    pool = session.query(Pool).filter_by(pool=name).first()\n    if pool is None:\n        raise PoolNotFound(\"Pool '%s' doesn't exist\" % name)\n\n    return pool","method_path":"airflow\/api\/common\/experimental\/pool.py"}
{"repo_name":"apache\/airflow","method_name":"create_pool","method_code":"def create_pool(name, slots, description, session=None):\n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    try:\n        slots = int(slots)\n    except ValueError:\n        raise AirflowBadRequest(\"Bad value for `slots`: %s\" % slots)\n\n    session.expire_on_commit = False\n    pool = session.query(Pool).filter_by(pool=name).first()\n    if pool is None:\n        pool = Pool(pool=name, slots=slots, description=description)\n        session.add(pool)\n    else:\n        pool.slots = slots\n        pool.description = description\n\n    session.commit()\n\n    return pool","method_summary":"Create a pool with a given parameters.","original_method_code":"def create_pool(name, slots, description, session=None):\n    \"\"\"Create a pool with a given parameters.\"\"\"\n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    try:\n        slots = int(slots)\n    except ValueError:\n        raise AirflowBadRequest(\"Bad value for `slots`: %s\" % slots)\n\n    session.expire_on_commit = False\n    pool = session.query(Pool).filter_by(pool=name).first()\n    if pool is None:\n        pool = Pool(pool=name, slots=slots, description=description)\n        session.add(pool)\n    else:\n        pool.slots = slots\n        pool.description = description\n\n    session.commit()\n\n    return pool","method_path":"airflow\/api\/common\/experimental\/pool.py"}
{"repo_name":"apache\/airflow","method_name":"delete_pool","method_code":"def delete_pool(name, session=None):\n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    pool = session.query(Pool).filter_by(pool=name).first()\n    if pool is None:\n        raise PoolNotFound(\"Pool '%s' doesn't exist\" % name)\n\n    session.delete(pool)\n    session.commit()\n\n    return pool","method_summary":"Delete pool by a given name.","original_method_code":"def delete_pool(name, session=None):\n    \"\"\"Delete pool by a given name.\"\"\"\n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    pool = session.query(Pool).filter_by(pool=name).first()\n    if pool is None:\n        raise PoolNotFound(\"Pool '%s' doesn't exist\" % name)\n\n    session.delete(pool)\n    session.commit()\n\n    return pool","method_path":"airflow\/api\/common\/experimental\/pool.py"}
{"repo_name":"apache\/airflow","method_name":"GKEClusterHook._dict_to_proto","method_code":"def _dict_to_proto(py_dict, proto):\n        dict_json_str = json.dumps(py_dict)\n        return json_format.Parse(dict_json_str, proto)","method_summary":"Converts a python dictionary to the proto supplied","original_method_code":"def _dict_to_proto(py_dict, proto):\n        \"\"\"\n        Converts a python dictionary to the proto supplied\n\n        :param py_dict: The dictionary to convert\n        :type py_dict: dict\n        :param proto: The proto object to merge with dictionary\n        :type proto: protobuf\n        :return: A parsed python dictionary in provided proto format\n        :raises:\n            ParseError: On JSON parsing problems.\n        \"\"\"\n        dict_json_str = json.dumps(py_dict)\n        return json_format.Parse(dict_json_str, proto)","method_path":"airflow\/contrib\/hooks\/gcp_container_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GKEClusterHook.wait_for_operation","method_code":"def wait_for_operation(self, operation, project_id=None):\n        self.log.info(\"Waiting for OPERATION_NAME %s\", operation.name)\n        time.sleep(OPERATIONAL_POLL_INTERVAL)\n        while operation.status != Operation.Status.DONE:\n            if operation.status == Operation.Status.RUNNING or operation.status == \\\n                    Operation.Status.PENDING:\n                time.sleep(OPERATIONAL_POLL_INTERVAL)\n            else:\n                raise exceptions.GoogleCloudError(\n                    \"Operation has failed with status: %s\" % operation.status)\n            \n            operation = self.get_operation(operation.name, project_id=project_id or self.project_id)\n        return operation","method_summary":"Given an operation, continuously fetches the status from Google Cloud until either completion or an error occurring","original_method_code":"def wait_for_operation(self, operation, project_id=None):\n        \"\"\"\n        Given an operation, continuously fetches the status from Google Cloud until either\n        completion or an error occurring\n\n        :param operation: The Operation to wait for\n        :type operation: google.cloud.container_V1.gapic.enums.Operation\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :return: A new, updated operation fetched from Google Cloud\n        \"\"\"\n        self.log.info(\"Waiting for OPERATION_NAME %s\", operation.name)\n        time.sleep(OPERATIONAL_POLL_INTERVAL)\n        while operation.status != Operation.Status.DONE:\n            if operation.status == Operation.Status.RUNNING or operation.status == \\\n                    Operation.Status.PENDING:\n                time.sleep(OPERATIONAL_POLL_INTERVAL)\n            else:\n                raise exceptions.GoogleCloudError(\n                    \"Operation has failed with status: %s\" % operation.status)\n            # To update status of operation\n            operation = self.get_operation(operation.name, project_id=project_id or self.project_id)\n        return operation","method_path":"airflow\/contrib\/hooks\/gcp_container_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GKEClusterHook.get_operation","method_code":"def get_operation(self, operation_name, project_id=None):\n        return self.get_client().get_operation(project_id=project_id or self.project_id,\n                                               zone=self.location,\n                                               operation_id=operation_name)","method_summary":"Fetches the operation from Google Cloud","original_method_code":"def get_operation(self, operation_name, project_id=None):\n        \"\"\"\n        Fetches the operation from Google Cloud\n\n        :param operation_name: Name of operation to fetch\n        :type operation_name: str\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :return: The new, updated operation from Google Cloud\n        \"\"\"\n        return self.get_client().get_operation(project_id=project_id or self.project_id,\n                                               zone=self.location,\n                                               operation_id=operation_name)","method_path":"airflow\/contrib\/hooks\/gcp_container_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GKEClusterHook._append_label","method_code":"def _append_label(cluster_proto, key, val):\n        val = val.replace('.', '-').replace('+', '-')\n        cluster_proto.resource_labels.update({key: val})\n        return cluster_proto","method_summary":"Append labels to provided Cluster Protobuf Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current airflow version string follows semantic versioning","original_method_code":"def _append_label(cluster_proto, key, val):\n        \"\"\"\n        Append labels to provided Cluster Protobuf\n\n        Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current\n         airflow version string follows semantic versioning spec: x.y.z).\n\n        :param cluster_proto: The proto to append resource_label airflow\n            version to\n        :type cluster_proto: google.cloud.container_v1.types.Cluster\n        :param key: The key label\n        :type key: str\n        :param val:\n        :type val: str\n        :return: The cluster proto updated with new label\n        \"\"\"\n        val = val.replace('.', '-').replace('+', '-')\n        cluster_proto.resource_labels.update({key: val})\n        return cluster_proto","method_path":"airflow\/contrib\/hooks\/gcp_container_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GKEClusterHook.create_cluster","method_code":"def create_cluster(self, cluster, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        if isinstance(cluster, dict):\n            cluster_proto = Cluster()\n            cluster = self._dict_to_proto(py_dict=cluster, proto=cluster_proto)\n        elif not isinstance(cluster, Cluster):\n            raise AirflowException(\n                \"cluster is not instance of Cluster proto or python dict\")\n\n        self._append_label(cluster, 'airflow-version', 'v' + version.version)\n\n        self.log.info(\n            \"Creating (project_id=%s, zone=%s, cluster_name=%s)\",\n            self.project_id, self.location, cluster.name\n        )\n        try:\n            op = self.get_client().create_cluster(project_id=project_id or self.project_id,\n                                                  zone=self.location,\n                                                  cluster=cluster,\n                                                  retry=retry,\n                                                  timeout=timeout)\n            op = self.wait_for_operation(op)\n\n            return op.target_link\n        except AlreadyExists as error:\n            self.log.info('Assuming Success: %s', error.message)\n            return self.get_cluster(name=cluster.name).self_link","method_summary":"Creates a cluster, consisting of the specified number and type of Google Compute Engine instances.","original_method_code":"def create_cluster(self, cluster, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        \"\"\"\n        Creates a cluster, consisting of the specified number and type of Google Compute\n        Engine instances.\n\n        :param cluster: A Cluster protobuf or dict. If dict is provided, it must\n            be of the same form as the protobuf message\n            :class:`google.cloud.container_v1.types.Cluster`\n        :type cluster: dict or google.cloud.container_v1.types.Cluster\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object (``google.api_core.retry.Retry``) used to\n            retry requests.\n            If None is specified, requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: The full url to the new, or existing, cluster\n        :raises:\n            ParseError: On JSON parsing problems when trying to convert dict\n            AirflowException: cluster is not dict type nor Cluster proto type\n        \"\"\"\n\n        if isinstance(cluster, dict):\n            cluster_proto = Cluster()\n            cluster = self._dict_to_proto(py_dict=cluster, proto=cluster_proto)\n        elif not isinstance(cluster, Cluster):\n            raise AirflowException(\n                \"cluster is not instance of Cluster proto or python dict\")\n\n        self._append_label(cluster, 'airflow-version', 'v' + version.version)\n\n        self.log.info(\n            \"Creating (project_id=%s, zone=%s, cluster_name=%s)\",\n            self.project_id, self.location, cluster.name\n        )\n        try:\n            op = self.get_client().create_cluster(project_id=project_id or self.project_id,\n                                                  zone=self.location,\n                                                  cluster=cluster,\n                                                  retry=retry,\n                                                  timeout=timeout)\n            op = self.wait_for_operation(op)\n\n            return op.target_link\n        except AlreadyExists as error:\n            self.log.info('Assuming Success: %s', error.message)\n            return self.get_cluster(name=cluster.name).self_link","method_path":"airflow\/contrib\/hooks\/gcp_container_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GKEClusterHook.get_cluster","method_code":"def get_cluster(self, name, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        self.log.info(\n            \"Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)\",\n            project_id or self.project_id, self.location, name\n        )\n\n        return self.get_client().get_cluster(project_id=project_id or self.project_id,\n                                             zone=self.location,\n                                             cluster_id=name,\n                                             retry=retry,\n                                             timeout=timeout).self_link","method_summary":"Gets details of specified cluster","original_method_code":"def get_cluster(self, name, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        \"\"\"\n        Gets details of specified cluster\n\n        :param name: The name of the cluster to retrieve\n        :type name: str\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: google.cloud.container_v1.types.Cluster\n        \"\"\"\n        self.log.info(\n            \"Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)\",\n            project_id or self.project_id, self.location, name\n        )\n\n        return self.get_client().get_cluster(project_id=project_id or self.project_id,\n                                             zone=self.location,\n                                             cluster_id=name,\n                                             retry=retry,\n                                             timeout=timeout).self_link","method_path":"airflow\/contrib\/hooks\/gcp_container_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DiscordWebhookHook._get_webhook_endpoint","method_code":"def _get_webhook_endpoint(self, http_conn_id, webhook_endpoint):\n        if webhook_endpoint:\n            endpoint = webhook_endpoint\n        elif http_conn_id:\n            conn = self.get_connection(http_conn_id)\n            extra = conn.extra_dejson\n            endpoint = extra.get('webhook_endpoint', '')\n        else:\n            raise AirflowException('Cannot get webhook endpoint: No valid Discord '\n                                   'webhook endpoint or http_conn_id supplied.')\n\n        \n        if not re.match('^webhooks\/[0-9]+\/[a-zA-Z0-9_-]+$', endpoint):\n            raise AirflowException('Expected Discord webhook endpoint in the form '\n                                   'of \"webhooks\/{webhook.id}\/{webhook.token}\".')\n\n        return endpoint","method_summary":"Given a Discord http_conn_id, return the default webhook endpoint or override if a webhook_endpoint is manually supplied.","original_method_code":"def _get_webhook_endpoint(self, http_conn_id, webhook_endpoint):\n        \"\"\"\n        Given a Discord http_conn_id, return the default webhook endpoint or override if a\n        webhook_endpoint is manually supplied.\n\n        :param http_conn_id: The provided connection ID\n        :param webhook_endpoint: The manually provided webhook endpoint\n        :return: Webhook endpoint (str) to use\n        \"\"\"\n        if webhook_endpoint:\n            endpoint = webhook_endpoint\n        elif http_conn_id:\n            conn = self.get_connection(http_conn_id)\n            extra = conn.extra_dejson\n            endpoint = extra.get('webhook_endpoint', '')\n        else:\n            raise AirflowException('Cannot get webhook endpoint: No valid Discord '\n                                   'webhook endpoint or http_conn_id supplied.')\n\n        # make sure endpoint matches the expected Discord webhook format\n        if not re.match('^webhooks\/[0-9]+\/[a-zA-Z0-9_-]+$', endpoint):\n            raise AirflowException('Expected Discord webhook endpoint in the form '\n                                   'of \"webhooks\/{webhook.id}\/{webhook.token}\".')\n\n        return endpoint","method_path":"airflow\/contrib\/hooks\/discord_webhook_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DiscordWebhookHook._build_discord_payload","method_code":"def _build_discord_payload(self):\n        payload = {}\n\n        if self.username:\n            payload['username'] = self.username\n        if self.avatar_url:\n            payload['avatar_url'] = self.avatar_url\n\n        payload['tts'] = self.tts\n\n        if len(self.message) <= 2000:\n            payload['content'] = self.message\n        else:\n            raise AirflowException('Discord message length must be 2000 or fewer '\n                                   'characters.')\n\n        return json.dumps(payload)","method_summary":"Construct the Discord JSON payload. All relevant parameters are combined here to a valid Discord JSON payload.","original_method_code":"def _build_discord_payload(self):\n        \"\"\"\n        Construct the Discord JSON payload. All relevant parameters are combined here\n        to a valid Discord JSON payload.\n\n        :return: Discord payload (str) to send\n        \"\"\"\n        payload = {}\n\n        if self.username:\n            payload['username'] = self.username\n        if self.avatar_url:\n            payload['avatar_url'] = self.avatar_url\n\n        payload['tts'] = self.tts\n\n        if len(self.message) <= 2000:\n            payload['content'] = self.message\n        else:\n            raise AirflowException('Discord message length must be 2000 or fewer '\n                                   'characters.')\n\n        return json.dumps(payload)","method_path":"airflow\/contrib\/hooks\/discord_webhook_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DiscordWebhookHook.execute","method_code":"def execute(self):\n        proxies = {}\n        if self.proxy:\n            \n            proxies = {'https': self.proxy}\n\n        discord_payload = self._build_discord_payload()\n\n        self.run(endpoint=self.webhook_endpoint,\n                 data=discord_payload,\n                 headers={'Content-type': 'application\/json'},\n                 extra_options={'proxies': proxies})","method_summary":"Execute the Discord webhook call","original_method_code":"def execute(self):\n        \"\"\"\n        Execute the Discord webhook call\n        \"\"\"\n        proxies = {}\n        if self.proxy:\n            # we only need https proxy for Discord\n            proxies = {'https': self.proxy}\n\n        discord_payload = self._build_discord_payload()\n\n        self.run(endpoint=self.webhook_endpoint,\n                 data=discord_payload,\n                 headers={'Content-type': 'application\/json'},\n                 extra_options={'proxies': proxies})","method_path":"airflow\/contrib\/hooks\/discord_webhook_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudKMSHook.encrypt","method_code":"def encrypt(self, key_name, plaintext, authenticated_data=None):\n        keys = self.get_conn().projects().locations().keyRings().cryptoKeys()\n        body = {'plaintext': _b64encode(plaintext)}\n        if authenticated_data:\n            body['additionalAuthenticatedData'] = _b64encode(authenticated_data)\n\n        request = keys.encrypt(name=key_name, body=body)\n        response = request.execute(num_retries=self.num_retries)\n\n        ciphertext = response['ciphertext']\n        return ciphertext","method_summary":"Encrypts a plaintext message using Google Cloud KMS.","original_method_code":"def encrypt(self, key_name, plaintext, authenticated_data=None):\n        \"\"\"\n        Encrypts a plaintext message using Google Cloud KMS.\n\n        :param key_name: The Resource Name for the key (or key version)\n                         to be used for encyption. Of the form\n                         ``projects\/*\/locations\/*\/keyRings\/*\/cryptoKeys\/**``\n        :type key_name: str\n        :param plaintext: The message to be encrypted.\n        :type plaintext: bytes\n        :param authenticated_data: Optional additional authenticated data that\n                                   must also be provided to decrypt the message.\n        :type authenticated_data: bytes\n        :return: The base 64 encoded ciphertext of the original message.\n        :rtype: str\n        \"\"\"\n        keys = self.get_conn().projects().locations().keyRings().cryptoKeys()\n        body = {'plaintext': _b64encode(plaintext)}\n        if authenticated_data:\n            body['additionalAuthenticatedData'] = _b64encode(authenticated_data)\n\n        request = keys.encrypt(name=key_name, body=body)\n        response = request.execute(num_retries=self.num_retries)\n\n        ciphertext = response['ciphertext']\n        return ciphertext","method_path":"airflow\/contrib\/hooks\/gcp_kms_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SqoopHook.import_table","method_code":"def import_table(self, table, target_dir=None, append=False, file_type=\"text\",\n                     columns=None, split_by=None, where=None, direct=False,\n                     driver=None, extra_import_options=None):\n        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,\n                               driver, extra_import_options)\n\n        cmd += [\"--table\", table]\n\n        if columns:\n            cmd += [\"--columns\", columns]\n        if where:\n            cmd += [\"--where\", where]\n\n        self.Popen(cmd)","method_summary":"Imports table from remote location to target dir. Arguments are copies of direct sqoop command line arguments","original_method_code":"def import_table(self, table, target_dir=None, append=False, file_type=\"text\",\n                     columns=None, split_by=None, where=None, direct=False,\n                     driver=None, extra_import_options=None):\n        \"\"\"\n        Imports table from remote location to target dir. Arguments are\n        copies of direct sqoop command line arguments\n\n        :param table: Table to read\n        :param target_dir: HDFS destination dir\n        :param append: Append data to an existing dataset in HDFS\n        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\".\n            Imports data to into the specified format. Defaults to text.\n        :param columns: <col,col,col\u2026> Columns to import from table\n        :param split_by: Column of the table used to split work units\n        :param where: WHERE clause to use during import\n        :param direct: Use direct connector if exists for the database\n        :param driver: Manually specify JDBC driver class to use\n        :param extra_import_options: Extra import options to pass as dict.\n            If a key doesn't have a value, just pass an empty string to it.\n            Don't include prefix of -- for sqoop options.\n        \"\"\"\n        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,\n                               driver, extra_import_options)\n\n        cmd += [\"--table\", table]\n\n        if columns:\n            cmd += [\"--columns\", columns]\n        if where:\n            cmd += [\"--where\", where]\n\n        self.Popen(cmd)","method_path":"airflow\/contrib\/hooks\/sqoop_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SqoopHook.import_query","method_code":"def import_query(self, query, target_dir, append=False, file_type=\"text\",\n                     split_by=None, direct=None, driver=None, extra_import_options=None):\n        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,\n                               driver, extra_import_options)\n        cmd += [\"--query\", query]\n\n        self.Popen(cmd)","method_summary":"Imports a specific query from the rdbms to hdfs","original_method_code":"def import_query(self, query, target_dir, append=False, file_type=\"text\",\n                     split_by=None, direct=None, driver=None, extra_import_options=None):\n        \"\"\"\n        Imports a specific query from the rdbms to hdfs\n\n        :param query: Free format query to run\n        :param target_dir: HDFS destination dir\n        :param append: Append data to an existing dataset in HDFS\n        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\"\n            Imports data to hdfs into the specified format. Defaults to text.\n        :param split_by: Column of the table used to split work units\n        :param direct: Use direct import fast path\n        :param driver: Manually specify JDBC driver class to use\n        :param extra_import_options: Extra import options to pass as dict.\n            If a key doesn't have a value, just pass an empty string to it.\n            Don't include prefix of -- for sqoop options.\n        \"\"\"\n        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,\n                               driver, extra_import_options)\n        cmd += [\"--query\", query]\n\n        self.Popen(cmd)","method_path":"airflow\/contrib\/hooks\/sqoop_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SqoopHook.export_table","method_code":"def export_table(self, table, export_dir, input_null_string,\n                     input_null_non_string, staging_table,\n                     clear_staging_table, enclosed_by,\n                     escaped_by, input_fields_terminated_by,\n                     input_lines_terminated_by,\n                     input_optionally_enclosed_by, batch,\n                     relaxed_isolation, extra_export_options=None):\n        cmd = self._export_cmd(table, export_dir, input_null_string,\n                               input_null_non_string, staging_table,\n                               clear_staging_table, enclosed_by, escaped_by,\n                               input_fields_terminated_by,\n                               input_lines_terminated_by,\n                               input_optionally_enclosed_by, batch,\n                               relaxed_isolation, extra_export_options)\n\n        self.Popen(cmd)","method_summary":"Exports Hive table to remote location. Arguments are copies of direct sqoop command line Arguments","original_method_code":"def export_table(self, table, export_dir, input_null_string,\n                     input_null_non_string, staging_table,\n                     clear_staging_table, enclosed_by,\n                     escaped_by, input_fields_terminated_by,\n                     input_lines_terminated_by,\n                     input_optionally_enclosed_by, batch,\n                     relaxed_isolation, extra_export_options=None):\n        \"\"\"\n        Exports Hive table to remote location. Arguments are copies of direct\n        sqoop command line Arguments\n\n        :param table: Table remote destination\n        :param export_dir: Hive table to export\n        :param input_null_string: The string to be interpreted as null for\n            string columns\n        :param input_null_non_string: The string to be interpreted as null\n            for non-string columns\n        :param staging_table: The table in which data will be staged before\n            being inserted into the destination table\n        :param clear_staging_table: Indicate that any data present in the\n            staging table can be deleted\n        :param enclosed_by: Sets a required field enclosing character\n        :param escaped_by: Sets the escape character\n        :param input_fields_terminated_by: Sets the field separator character\n        :param input_lines_terminated_by: Sets the end-of-line character\n        :param input_optionally_enclosed_by: Sets a field enclosing character\n        :param batch: Use batch mode for underlying statement execution\n        :param relaxed_isolation: Transaction isolation to read uncommitted\n            for the mappers\n        :param extra_export_options: Extra export options to pass as dict.\n            If a key doesn't have a value, just pass an empty string to it.\n            Don't include prefix of -- for sqoop options.\n        \"\"\"\n        cmd = self._export_cmd(table, export_dir, input_null_string,\n                               input_null_non_string, staging_table,\n                               clear_staging_table, enclosed_by, escaped_by,\n                               input_fields_terminated_by,\n                               input_lines_terminated_by,\n                               input_optionally_enclosed_by, batch,\n                               relaxed_isolation, extra_export_options)\n\n        self.Popen(cmd)","method_path":"airflow\/contrib\/hooks\/sqoop_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GCPTextToSpeechHook.get_conn","method_code":"def get_conn(self):\n        if not self._client:\n            self._client = TextToSpeechClient(credentials=self._get_credentials())\n        return self._client","method_summary":"Retrieves connection to Cloud Text to Speech.","original_method_code":"def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Text to Speech.\n\n        :return: Google Cloud Text to Speech client object.\n        :rtype: google.cloud.texttospeech_v1.TextToSpeechClient\n        \"\"\"\n        if not self._client:\n            self._client = TextToSpeechClient(credentials=self._get_credentials())\n        return self._client","method_path":"airflow\/contrib\/hooks\/gcp_text_to_speech_hook.py"}
{"repo_name":"apache\/airflow","method_name":"GCPTextToSpeechHook.synthesize_speech","method_code":"def synthesize_speech(self, input_data, voice, audio_config, retry=None, timeout=None):\n        client = self.get_conn()\n        self.log.info(\"Synthesizing input: %s\" % input_data)\n        return client.synthesize_speech(\n            input_=input_data, voice=voice, audio_config=audio_config, retry=retry, timeout=timeout\n        )","method_summary":"Synthesizes text input","original_method_code":"def synthesize_speech(self, input_data, voice, audio_config, retry=None, timeout=None):\n        \"\"\"\n        Synthesizes text input\n\n        :param input_data: text input to be synthesized. See more:\n            https:\/\/googleapis.github.io\/google-cloud-python\/latest\/texttospeech\/gapic\/v1\/types.html#google.cloud.texttospeech_v1.types.SynthesisInput\n        :type input_data: dict or google.cloud.texttospeech_v1.types.SynthesisInput\n        :param voice: configuration of voice to be used in synthesis. See more:\n            https:\/\/googleapis.github.io\/google-cloud-python\/latest\/texttospeech\/gapic\/v1\/types.html#google.cloud.texttospeech_v1.types.VoiceSelectionParams\n        :type voice: dict or google.cloud.texttospeech_v1.types.VoiceSelectionParams\n        :param audio_config: configuration of the synthesized audio. See more:\n            https:\/\/googleapis.github.io\/google-cloud-python\/latest\/texttospeech\/gapic\/v1\/types.html#google.cloud.texttospeech_v1.types.AudioConfig\n        :type audio_config: dict or google.cloud.texttospeech_v1.types.AudioConfig\n        :return: SynthesizeSpeechResponse See more:\n            https:\/\/googleapis.github.io\/google-cloud-python\/latest\/texttospeech\/gapic\/v1\/types.html#google.cloud.texttospeech_v1.types.SynthesizeSpeechResponse\n        :rtype: object\n        :param retry: (Optional) A retry object used to retry requests. If None is specified,\n                requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        \"\"\"\n        client = self.get_conn()\n        self.log.info(\"Synthesizing input: %s\" % input_data)\n        return client.synthesize_speech(\n            input_=input_data, voice=voice, audio_config=audio_config, retry=retry, timeout=timeout\n        )","method_path":"airflow\/contrib\/hooks\/gcp_text_to_speech_hook.py"}
{"repo_name":"apache\/airflow","method_name":"S3TaskHandler.close","method_code":"def close(self):\n        \n        \n        \n        \n        if self.closed:\n            return\n\n        super().close()\n\n        if not self.upload_on_close:\n            return\n\n        local_loc = os.path.join(self.local_base, self.log_relative_path)\n        remote_loc = os.path.join(self.remote_base, self.log_relative_path)\n        if os.path.exists(local_loc):\n            \n            with open(local_loc, 'r') as logfile:\n                log = logfile.read()\n            self.s3_write(log, remote_loc)\n\n        \n        self.closed = True","method_summary":"Close and upload local log file to remote storage S3.","original_method_code":"def close(self):\n        \"\"\"\n        Close and upload local log file to remote storage S3.\n        \"\"\"\n        # When application exit, system shuts down all handlers by\n        # calling close method. Here we check if logger is already\n        # closed to prevent uploading the log to remote storage multiple\n        # times when `logging.shutdown` is called.\n        if self.closed:\n            return\n\n        super().close()\n\n        if not self.upload_on_close:\n            return\n\n        local_loc = os.path.join(self.local_base, self.log_relative_path)\n        remote_loc = os.path.join(self.remote_base, self.log_relative_path)\n        if os.path.exists(local_loc):\n            # read log and remove old logs to get just the latest additions\n            with open(local_loc, 'r') as logfile:\n                log = logfile.read()\n            self.s3_write(log, remote_loc)\n\n        # Mark closed so we don't double write if close is called twice\n        self.closed = True","method_path":"airflow\/utils\/log\/s3_task_handler.py"}
{"repo_name":"apache\/airflow","method_name":"WorkerConfiguration._get_init_containers","method_code":"def _get_init_containers(self):\n        \n        if self.kube_config.dags_volume_claim or \\\n           self.kube_config.dags_volume_host or self.kube_config.dags_in_image:\n            return []\n\n        \n        init_environment = [{\n            'name': 'GIT_SYNC_REPO',\n            'value': self.kube_config.git_repo\n        }, {\n            'name': 'GIT_SYNC_BRANCH',\n            'value': self.kube_config.git_branch\n        }, {\n            'name': 'GIT_SYNC_ROOT',\n            'value': self.kube_config.git_sync_root\n        }, {\n            'name': 'GIT_SYNC_DEST',\n            'value': self.kube_config.git_sync_dest\n        }, {\n            'name': 'GIT_SYNC_DEPTH',\n            'value': '1'\n        }, {\n            'name': 'GIT_SYNC_ONE_TIME',\n            'value': 'true'\n        }]\n        if self.kube_config.git_user:\n            init_environment.append({\n                'name': 'GIT_SYNC_USERNAME',\n                'value': self.kube_config.git_user\n            })\n        if self.kube_config.git_password:\n            init_environment.append({\n                'name': 'GIT_SYNC_PASSWORD',\n                'value': self.kube_config.git_password\n            })\n\n        volume_mounts = [{\n            'mountPath': self.kube_config.git_sync_root,\n            'name': self.dags_volume_name,\n            'readOnly': False\n        }]\n        if self.kube_config.git_ssh_key_secret_name:\n            volume_mounts.append({\n                'name': self.git_sync_ssh_secret_volume_name,\n                'mountPath': '\/etc\/git-secret\/ssh',\n                'subPath': 'ssh'\n            })\n            init_environment.extend([\n                {\n                    'name': 'GIT_SSH_KEY_FILE',\n                    'value': '\/etc\/git-secret\/ssh'\n                },\n                {\n                    'name': 'GIT_SYNC_SSH',\n                    'value': 'true'\n                }])\n        if self.kube_config.git_ssh_known_hosts_configmap_name:\n            volume_mounts.append({\n                'name': self.git_sync_ssh_known_hosts_volume_name,\n                'mountPath': '\/etc\/git-secret\/known_hosts',\n                'subPath': 'known_hosts'\n            })\n            init_environment.extend([\n                {\n                    'name': 'GIT_KNOWN_HOSTS',\n                    'value': 'true'\n                },\n                {\n                    'name': 'GIT_SSH_KNOWN_HOSTS_FILE',\n                    'value': '\/etc\/git-secret\/known_hosts'\n                }\n            ])\n        else:\n            init_environment.append({\n                'name': 'GIT_KNOWN_HOSTS',\n                'value': 'false'\n            })\n\n        return [{\n            'name': self.kube_config.git_sync_init_container_name,\n            'image': self.kube_config.git_sync_container,\n            'securityContext': {'runAsUser': 65533},  \n            'env': init_environment,\n            'volumeMounts': volume_mounts\n        }]","method_summary":"When using git to retrieve the DAGs, use the GitSync Init Container","original_method_code":"def _get_init_containers(self):\n        \"\"\"When using git to retrieve the DAGs, use the GitSync Init Container\"\"\"\n        # If we're using volume claims to mount the dags, no init container is needed\n        if self.kube_config.dags_volume_claim or \\\n           self.kube_config.dags_volume_host or self.kube_config.dags_in_image:\n            return []\n\n        # Otherwise, define a git-sync init container\n        init_environment = [{\n            'name': 'GIT_SYNC_REPO',\n            'value': self.kube_config.git_repo\n        }, {\n            'name': 'GIT_SYNC_BRANCH',\n            'value': self.kube_config.git_branch\n        }, {\n            'name': 'GIT_SYNC_ROOT',\n            'value': self.kube_config.git_sync_root\n        }, {\n            'name': 'GIT_SYNC_DEST',\n            'value': self.kube_config.git_sync_dest\n        }, {\n            'name': 'GIT_SYNC_DEPTH',\n            'value': '1'\n        }, {\n            'name': 'GIT_SYNC_ONE_TIME',\n            'value': 'true'\n        }]\n        if self.kube_config.git_user:\n            init_environment.append({\n                'name': 'GIT_SYNC_USERNAME',\n                'value': self.kube_config.git_user\n            })\n        if self.kube_config.git_password:\n            init_environment.append({\n                'name': 'GIT_SYNC_PASSWORD',\n                'value': self.kube_config.git_password\n            })\n\n        volume_mounts = [{\n            'mountPath': self.kube_config.git_sync_root,\n            'name': self.dags_volume_name,\n            'readOnly': False\n        }]\n        if self.kube_config.git_ssh_key_secret_name:\n            volume_mounts.append({\n                'name': self.git_sync_ssh_secret_volume_name,\n                'mountPath': '\/etc\/git-secret\/ssh',\n                'subPath': 'ssh'\n            })\n            init_environment.extend([\n                {\n                    'name': 'GIT_SSH_KEY_FILE',\n                    'value': '\/etc\/git-secret\/ssh'\n                },\n                {\n                    'name': 'GIT_SYNC_SSH',\n                    'value': 'true'\n                }])\n        if self.kube_config.git_ssh_known_hosts_configmap_name:\n            volume_mounts.append({\n                'name': self.git_sync_ssh_known_hosts_volume_name,\n                'mountPath': '\/etc\/git-secret\/known_hosts',\n                'subPath': 'known_hosts'\n            })\n            init_environment.extend([\n                {\n                    'name': 'GIT_KNOWN_HOSTS',\n                    'value': 'true'\n                },\n                {\n                    'name': 'GIT_SSH_KNOWN_HOSTS_FILE',\n                    'value': '\/etc\/git-secret\/known_hosts'\n                }\n            ])\n        else:\n            init_environment.append({\n                'name': 'GIT_KNOWN_HOSTS',\n                'value': 'false'\n            })\n\n        return [{\n            'name': self.kube_config.git_sync_init_container_name,\n            'image': self.kube_config.git_sync_container,\n            'securityContext': {'runAsUser': 65533},  # git-sync user\n            'env': init_environment,\n            'volumeMounts': volume_mounts\n        }]","method_path":"airflow\/contrib\/kubernetes\/worker_configuration.py"}
{"repo_name":"apache\/airflow","method_name":"WorkerConfiguration._get_environment","method_code":"def _get_environment(self):\n        env = {}\n\n        for env_var_name, env_var_val in six.iteritems(self.kube_config.kube_env_vars):\n            env[env_var_name] = env_var_val\n\n        env[\"AIRFLOW__CORE__EXECUTOR\"] = \"LocalExecutor\"\n\n        if self.kube_config.airflow_configmap:\n            env['AIRFLOW_HOME'] = self.worker_airflow_home\n            env['AIRFLOW__CORE__DAGS_FOLDER'] = self.worker_airflow_dags\n        if (not self.kube_config.airflow_configmap and\n                'AIRFLOW__CORE__SQL_ALCHEMY_CONN' not in self.kube_config.kube_secrets):\n            env['AIRFLOW__CORE__SQL_ALCHEMY_CONN'] = conf.get(\"core\", \"SQL_ALCHEMY_CONN\")\n        if self.kube_config.git_dags_folder_mount_point:\n            \n            dag_volume_mount_path = os.path.join(\n                self.kube_config.git_dags_folder_mount_point,\n                self.kube_config.git_sync_dest,  \n                self.kube_config.git_subpath     \n            )\n            env['AIRFLOW__CORE__DAGS_FOLDER'] = dag_volume_mount_path\n        return env","method_summary":"Defines any necessary environment variables for the pod executor","original_method_code":"def _get_environment(self):\n        \"\"\"Defines any necessary environment variables for the pod executor\"\"\"\n        env = {}\n\n        for env_var_name, env_var_val in six.iteritems(self.kube_config.kube_env_vars):\n            env[env_var_name] = env_var_val\n\n        env[\"AIRFLOW__CORE__EXECUTOR\"] = \"LocalExecutor\"\n\n        if self.kube_config.airflow_configmap:\n            env['AIRFLOW_HOME'] = self.worker_airflow_home\n            env['AIRFLOW__CORE__DAGS_FOLDER'] = self.worker_airflow_dags\n        if (not self.kube_config.airflow_configmap and\n                'AIRFLOW__CORE__SQL_ALCHEMY_CONN' not in self.kube_config.kube_secrets):\n            env['AIRFLOW__CORE__SQL_ALCHEMY_CONN'] = conf.get(\"core\", \"SQL_ALCHEMY_CONN\")\n        if self.kube_config.git_dags_folder_mount_point:\n            # \/root\/airflow\/dags\/repo\/dags\n            dag_volume_mount_path = os.path.join(\n                self.kube_config.git_dags_folder_mount_point,\n                self.kube_config.git_sync_dest,  # repo\n                self.kube_config.git_subpath     # dags\n            )\n            env['AIRFLOW__CORE__DAGS_FOLDER'] = dag_volume_mount_path\n        return env","method_path":"airflow\/contrib\/kubernetes\/worker_configuration.py"}
{"repo_name":"apache\/airflow","method_name":"WorkerConfiguration._get_secrets","method_code":"def _get_secrets(self):\n        worker_secrets = []\n\n        for env_var_name, obj_key_pair in six.iteritems(self.kube_config.kube_secrets):\n            k8s_secret_obj, k8s_secret_key = obj_key_pair.split('=')\n            worker_secrets.append(\n                Secret('env', env_var_name, k8s_secret_obj, k8s_secret_key)\n            )\n\n        if self.kube_config.env_from_secret_ref:\n            for secret_ref in self.kube_config.env_from_secret_ref.split(','):\n                worker_secrets.append(\n                    Secret('env', None, secret_ref)\n                )\n\n        return worker_secrets","method_summary":"Defines any necessary secrets for the pod executor","original_method_code":"def _get_secrets(self):\n        \"\"\"Defines any necessary secrets for the pod executor\"\"\"\n        worker_secrets = []\n\n        for env_var_name, obj_key_pair in six.iteritems(self.kube_config.kube_secrets):\n            k8s_secret_obj, k8s_secret_key = obj_key_pair.split('=')\n            worker_secrets.append(\n                Secret('env', env_var_name, k8s_secret_obj, k8s_secret_key)\n            )\n\n        if self.kube_config.env_from_secret_ref:\n            for secret_ref in self.kube_config.env_from_secret_ref.split(','):\n                worker_secrets.append(\n                    Secret('env', None, secret_ref)\n                )\n\n        return worker_secrets","method_path":"airflow\/contrib\/kubernetes\/worker_configuration.py"}
{"repo_name":"apache\/airflow","method_name":"WorkerConfiguration._get_security_context","method_code":"def _get_security_context(self):\n        security_context = {}\n\n        if self.kube_config.worker_run_as_user:\n            security_context['runAsUser'] = self.kube_config.worker_run_as_user\n\n        if self.kube_config.worker_fs_group:\n            security_context['fsGroup'] = self.kube_config.worker_fs_group\n\n        \n        if self.kube_config.git_ssh_key_secret_name and security_context.get('fsGroup') is None:\n            security_context['fsGroup'] = 65533\n\n        return security_context","method_summary":"Defines the security context","original_method_code":"def _get_security_context(self):\n        \"\"\"Defines the security context\"\"\"\n        security_context = {}\n\n        if self.kube_config.worker_run_as_user:\n            security_context['runAsUser'] = self.kube_config.worker_run_as_user\n\n        if self.kube_config.worker_fs_group:\n            security_context['fsGroup'] = self.kube_config.worker_fs_group\n\n        # set fs_group to 65533 if not explicitly specified and using git ssh keypair auth\n        if self.kube_config.git_ssh_key_secret_name and security_context.get('fsGroup') is None:\n            security_context['fsGroup'] = 65533\n\n        return security_context","method_path":"airflow\/contrib\/kubernetes\/worker_configuration.py"}
{"repo_name":"apache\/airflow","method_name":"QuboleHook.get_extra_links","method_code":"def get_extra_links(self, operator, dttm):\n        conn = BaseHook.get_connection(operator.kwargs['qubole_conn_id'])\n        if conn and conn.host:\n            host = re.sub(r'api$', 'v2\/analyze?command_id=', conn.host)\n        else:\n            host = 'https:\/\/api.qubole.com\/v2\/analyze?command_id='\n\n        ti = TaskInstance(task=operator, execution_date=dttm)\n        qds_command_id = ti.xcom_pull(task_ids=operator.task_id, key='qbol_cmd_id')\n        url = host + str(qds_command_id) if qds_command_id else ''\n        return url","method_summary":"Get link to qubole command result page.","original_method_code":"def get_extra_links(self, operator, dttm):\n        \"\"\"\n        Get link to qubole command result page.\n\n        :param operator: operator\n        :param dttm: datetime\n        :return: url link\n        \"\"\"\n        conn = BaseHook.get_connection(operator.kwargs['qubole_conn_id'])\n        if conn and conn.host:\n            host = re.sub(r'api$', 'v2\/analyze?command_id=', conn.host)\n        else:\n            host = 'https:\/\/api.qubole.com\/v2\/analyze?command_id='\n\n        ti = TaskInstance(task=operator, execution_date=dttm)\n        qds_command_id = ti.xcom_pull(task_ids=operator.task_id, key='qbol_cmd_id')\n        url = host + str(qds_command_id) if qds_command_id else ''\n        return url","method_path":"airflow\/contrib\/hooks\/qubole_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessor._launch_process","method_code":"def _launch_process(result_queue,\n                        file_path,\n                        pickle_dags,\n                        dag_id_white_list,\n                        thread_name,\n                        zombies):\n        def helper():\n            \n            log = logging.getLogger(\"airflow.processor\")\n\n            stdout = StreamLogWriter(log, logging.INFO)\n            stderr = StreamLogWriter(log, logging.WARN)\n\n            set_context(log, file_path)\n\n            try:\n                \n                sys.stdout = stdout\n                sys.stderr = stderr\n\n                \n                settings.configure_orm()\n\n                \n                \n                \n                threading.current_thread().name = thread_name\n                start_time = time.time()\n\n                log.info(\"Started process (PID=%s) to work on %s\",\n                         os.getpid(), file_path)\n                scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)\n                result = scheduler_job.process_file(file_path,\n                                                    zombies,\n                                                    pickle_dags)\n                result_queue.put(result)\n                end_time = time.time()\n                log.info(\n                    \"Processing %s took %.3f seconds\", file_path, end_time - start_time\n                )\n            except Exception:\n                \n                log.exception(\"Got an exception! Propagating...\")\n                raise\n            finally:\n                sys.stdout = sys.__stdout__\n                sys.stderr = sys.__stderr__\n                \n                \n                settings.dispose_orm()\n\n        p = multiprocessing.Process(target=helper,\n                                    args=(),\n                                    name=\"{}-Process\".format(thread_name))\n        p.start()\n        return p","method_summary":"Launch a process to process the given file.","original_method_code":"def _launch_process(result_queue,\n                        file_path,\n                        pickle_dags,\n                        dag_id_white_list,\n                        thread_name,\n                        zombies):\n        \"\"\"\n        Launch a process to process the given file.\n\n        :param result_queue: the queue to use for passing back the result\n        :type result_queue: multiprocessing.Queue\n        :param file_path: the file to process\n        :type file_path: unicode\n        :param pickle_dags: whether to pickle the DAGs found in the file and\n            save them to the DB\n        :type pickle_dags: bool\n        :param dag_id_white_list: if specified, only examine DAG ID's that are\n            in this list\n        :type dag_id_white_list: list[unicode]\n        :param thread_name: the name to use for the process that is launched\n        :type thread_name: unicode\n        :return: the process that was launched\n        :rtype: multiprocessing.Process\n        :param zombies: zombie task instances to kill\n        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]\n        \"\"\"\n        def helper():\n            # This helper runs in the newly created process\n            log = logging.getLogger(\"airflow.processor\")\n\n            stdout = StreamLogWriter(log, logging.INFO)\n            stderr = StreamLogWriter(log, logging.WARN)\n\n            set_context(log, file_path)\n\n            try:\n                # redirect stdout\/stderr to log\n                sys.stdout = stdout\n                sys.stderr = stderr\n\n                # Re-configure the ORM engine as there are issues with multiple processes\n                settings.configure_orm()\n\n                # Change the thread name to differentiate log lines. This is\n                # really a separate process, but changing the name of the\n                # process doesn't work, so changing the thread name instead.\n                threading.current_thread().name = thread_name\n                start_time = time.time()\n\n                log.info(\"Started process (PID=%s) to work on %s\",\n                         os.getpid(), file_path)\n                scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)\n                result = scheduler_job.process_file(file_path,\n                                                    zombies,\n                                                    pickle_dags)\n                result_queue.put(result)\n                end_time = time.time()\n                log.info(\n                    \"Processing %s took %.3f seconds\", file_path, end_time - start_time\n                )\n            except Exception:\n                # Log exceptions through the logging framework.\n                log.exception(\"Got an exception! Propagating...\")\n                raise\n            finally:\n                sys.stdout = sys.__stdout__\n                sys.stderr = sys.__stderr__\n                # We re-initialized the ORM within this Process above so we need to\n                # tear it down manually here\n                settings.dispose_orm()\n\n        p = multiprocessing.Process(target=helper,\n                                    args=(),\n                                    name=\"{}-Process\".format(thread_name))\n        p.start()\n        return p","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessor.start","method_code":"def start(self):\n        self._process = DagFileProcessor._launch_process(\n            self._result_queue,\n            self.file_path,\n            self._pickle_dags,\n            self._dag_id_white_list,\n            \"DagFileProcessor{}\".format(self._instance_id),\n            self._zombies)\n        self._start_time = timezone.utcnow()","method_summary":"Launch the process and start processing the DAG.","original_method_code":"def start(self):\n        \"\"\"\n        Launch the process and start processing the DAG.\n        \"\"\"\n        self._process = DagFileProcessor._launch_process(\n            self._result_queue,\n            self.file_path,\n            self._pickle_dags,\n            self._dag_id_white_list,\n            \"DagFileProcessor{}\".format(self._instance_id),\n            self._zombies)\n        self._start_time = timezone.utcnow()","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"DagFileProcessor.done","method_code":"def done(self):\n        if self._process is None:\n            raise AirflowException(\"Tried to see if it's done before starting!\")\n\n        if self._done:\n            return True\n\n        \n        if self._result_queue and not self._result_queue.empty():\n            self._result = self._result_queue.get_nowait()\n            self._done = True\n            self.log.debug(\"Waiting for %s\", self._process)\n            self._process.join()\n            return True\n\n        \n        if self._result_queue and not self._process.is_alive():\n            self._done = True\n            \n            if not self._result_queue.empty():\n                self._result = self._result_queue.get_nowait()\n            self.log.debug(\"Waiting for %s\", self._process)\n            self._process.join()\n            return True\n\n        return False","method_summary":"Check if the process launched to process this file is done.","original_method_code":"def done(self):\n        \"\"\"\n        Check if the process launched to process this file is done.\n\n        :return: whether the process is finished running\n        :rtype: bool\n        \"\"\"\n        if self._process is None:\n            raise AirflowException(\"Tried to see if it's done before starting!\")\n\n        if self._done:\n            return True\n\n        # In case result queue is corrupted.\n        if self._result_queue and not self._result_queue.empty():\n            self._result = self._result_queue.get_nowait()\n            self._done = True\n            self.log.debug(\"Waiting for %s\", self._process)\n            self._process.join()\n            return True\n\n        # Potential error case when process dies\n        if self._result_queue and not self._process.is_alive():\n            self._done = True\n            # Get the object from the queue or else join() can hang.\n            if not self._result_queue.empty():\n                self._result = self._result_queue.get_nowait()\n            self.log.debug(\"Waiting for %s\", self._process)\n            self._process.join()\n            return True\n\n        return False","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"SchedulerJob._exit_gracefully","method_code":"def _exit_gracefully(self, signum, frame):\n        self.log.info(\"Exiting gracefully upon receiving signal %s\", signum)\n        if self.processor_agent:\n            self.processor_agent.end()\n        sys.exit(os.EX_OK)","method_summary":"Helper method to clean up processor_agent to avoid leaving orphan processes.","original_method_code":"def _exit_gracefully(self, signum, frame):\n        \"\"\"\n        Helper method to clean up processor_agent to avoid leaving orphan processes.\n        \"\"\"\n        self.log.info(\"Exiting gracefully upon receiving signal %s\", signum)\n        if self.processor_agent:\n            self.processor_agent.end()\n        sys.exit(os.EX_OK)","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"SchedulerJob._process_task_instances","method_code":"def _process_task_instances(self, dag, queue, session=None):\n        \n        dag_runs = DagRun.find(dag_id=dag.dag_id, state=State.RUNNING, session=session)\n        active_dag_runs = []\n        for run in dag_runs:\n            self.log.info(\"Examining DAG run %s\", run)\n            \n            if run.execution_date > timezone.utcnow():\n                self.log.error(\n                    \"Execution date is in future: %s\",\n                    run.execution_date\n                )\n                continue\n\n            if len(active_dag_runs) >= dag.max_active_runs:\n                self.log.info(\"Number of active dag runs reached max_active_run.\")\n                break\n\n            \n            if run.is_backfill:\n                continue\n\n            \n            run.dag = dag\n            \n            run.verify_integrity(session=session)\n            run.update_state(session=session)\n            if run.state == State.RUNNING:\n                make_transient(run)\n                active_dag_runs.append(run)\n\n        for run in active_dag_runs:\n            self.log.debug(\"Examining active DAG run: %s\", run)\n            \n            tis = run.get_task_instances(state=(State.NONE,\n                                                State.UP_FOR_RETRY,\n                                                State.UP_FOR_RESCHEDULE))\n\n            \n            \n            \n            for ti in tis:\n                task = dag.get_task(ti.task_id)\n\n                \n                ti.task = task\n\n                if ti.are_dependencies_met(\n                        dep_context=DepContext(flag_upstream_failed=True),\n                        session=session):\n                    self.log.debug('Queuing task: %s', ti)\n                    queue.append(ti.key)","method_summary":"This method schedules the tasks for a single DAG by looking at the active DAG runs and adding task instances that should run to the queue.","original_method_code":"def _process_task_instances(self, dag, queue, session=None):\n        \"\"\"\n        This method schedules the tasks for a single DAG by looking at the\n        active DAG runs and adding task instances that should run to the\n        queue.\n        \"\"\"\n\n        # update the state of the previously active dag runs\n        dag_runs = DagRun.find(dag_id=dag.dag_id, state=State.RUNNING, session=session)\n        active_dag_runs = []\n        for run in dag_runs:\n            self.log.info(\"Examining DAG run %s\", run)\n            # don't consider runs that are executed in the future\n            if run.execution_date > timezone.utcnow():\n                self.log.error(\n                    \"Execution date is in future: %s\",\n                    run.execution_date\n                )\n                continue\n\n            if len(active_dag_runs) >= dag.max_active_runs:\n                self.log.info(\"Number of active dag runs reached max_active_run.\")\n                break\n\n            # skip backfill dagruns for now as long as they are not really scheduled\n            if run.is_backfill:\n                continue\n\n            # todo: run.dag is transient but needs to be set\n            run.dag = dag\n            # todo: preferably the integrity check happens at dag collection time\n            run.verify_integrity(session=session)\n            run.update_state(session=session)\n            if run.state == State.RUNNING:\n                make_transient(run)\n                active_dag_runs.append(run)\n\n        for run in active_dag_runs:\n            self.log.debug(\"Examining active DAG run: %s\", run)\n            # this needs a fresh session sometimes tis get detached\n            tis = run.get_task_instances(state=(State.NONE,\n                                                State.UP_FOR_RETRY,\n                                                State.UP_FOR_RESCHEDULE))\n\n            # this loop is quite slow as it uses are_dependencies_met for\n            # every task (in ti.is_runnable). This is also called in\n            # update_state above which has already checked these tasks\n            for ti in tis:\n                task = dag.get_task(ti.task_id)\n\n                # fixme: ti.task is transient but needs to be set\n                ti.task = task\n\n                if ti.are_dependencies_met(\n                        dep_context=DepContext(flag_upstream_failed=True),\n                        session=session):\n                    self.log.debug('Queuing task: %s', ti)\n                    queue.append(ti.key)","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"SchedulerJob.__get_concurrency_maps","method_code":"def __get_concurrency_maps(self, states, session=None):\n        TI = models.TaskInstance\n        ti_concurrency_query = (\n            session\n            .query(TI.task_id, TI.dag_id, func.count('*'))\n            .filter(TI.state.in_(states))\n            .group_by(TI.task_id, TI.dag_id)\n        ).all()\n        dag_map = defaultdict(int)\n        task_map = defaultdict(int)\n        for result in ti_concurrency_query:\n            task_id, dag_id, count = result\n            dag_map[dag_id] += count\n            task_map[(dag_id, task_id)] = count\n        return dag_map, task_map","method_summary":"Get the concurrency maps.","original_method_code":"def __get_concurrency_maps(self, states, session=None):\n        \"\"\"\n        Get the concurrency maps.\n\n        :param states: List of states to query for\n        :type states: list[airflow.utils.state.State]\n        :return: A map from (dag_id, task_id) to # of task instances and\n         a map from (dag_id, task_id) to # of task instances in the given state list\n        :rtype: dict[tuple[str, str], int]\n\n        \"\"\"\n        TI = models.TaskInstance\n        ti_concurrency_query = (\n            session\n            .query(TI.task_id, TI.dag_id, func.count('*'))\n            .filter(TI.state.in_(states))\n            .group_by(TI.task_id, TI.dag_id)\n        ).all()\n        dag_map = defaultdict(int)\n        task_map = defaultdict(int)\n        for result in ti_concurrency_query:\n            task_id, dag_id, count = result\n            dag_map[dag_id] += count\n            task_map[(dag_id, task_id)] = count\n        return dag_map, task_map","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"SchedulerJob._change_state_for_executable_task_instances","method_code":"def _change_state_for_executable_task_instances(self, task_instances,\n                                                    acceptable_states, session=None):\n        if len(task_instances) == 0:\n            session.commit()\n            return []\n\n        TI = models.TaskInstance\n        filter_for_ti_state_change = (\n            [and_(\n                TI.dag_id == ti.dag_id,\n                TI.task_id == ti.task_id,\n                TI.execution_date == ti.execution_date)\n                for ti in task_instances])\n        ti_query = (\n            session\n            .query(TI)\n            .filter(or_(*filter_for_ti_state_change)))\n\n        if None in acceptable_states:\n            ti_query = ti_query.filter(\n                or_(TI.state == None, TI.state.in_(acceptable_states))  \n            )\n        else:\n            ti_query = ti_query.filter(TI.state.in_(acceptable_states))\n\n        tis_to_set_to_queued = (\n            ti_query\n            .with_for_update()\n            .all())\n        if len(tis_to_set_to_queued) == 0:\n            self.log.info(\"No tasks were able to have their state changed to queued.\")\n            session.commit()\n            return []\n\n        \n        for task_instance in tis_to_set_to_queued:\n            task_instance.state = State.QUEUED\n            task_instance.queued_dttm = (timezone.utcnow()\n                                         if not task_instance.queued_dttm\n                                         else task_instance.queued_dttm)\n            session.merge(task_instance)\n\n        \n        \n        simple_task_instances = [SimpleTaskInstance(ti) for ti in\n                                 tis_to_set_to_queued]\n\n        task_instance_str = \"\\n\\t\".join(\n            [repr(x) for x in tis_to_set_to_queued])\n\n        session.commit()\n        self.log.info(\"Setting the following %s tasks to queued state:\\n\\t%s\",\n                      len(tis_to_set_to_queued), task_instance_str)\n        return simple_task_instances","method_summary":"Changes the state of task instances in the list with one of the given states to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.","original_method_code":"def _change_state_for_executable_task_instances(self, task_instances,\n                                                    acceptable_states, session=None):\n        \"\"\"\n        Changes the state of task instances in the list with one of the given states\n        to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.\n\n        :param task_instances: TaskInstances to change the state of\n        :type task_instances: list[airflow.models.TaskInstance]\n        :param acceptable_states: Filters the TaskInstances updated to be in these states\n        :type acceptable_states: Iterable[State]\n        :rtype: list[airflow.utils.dag_processing.SimpleTaskInstance]\n        \"\"\"\n        if len(task_instances) == 0:\n            session.commit()\n            return []\n\n        TI = models.TaskInstance\n        filter_for_ti_state_change = (\n            [and_(\n                TI.dag_id == ti.dag_id,\n                TI.task_id == ti.task_id,\n                TI.execution_date == ti.execution_date)\n                for ti in task_instances])\n        ti_query = (\n            session\n            .query(TI)\n            .filter(or_(*filter_for_ti_state_change)))\n\n        if None in acceptable_states:\n            ti_query = ti_query.filter(\n                or_(TI.state == None, TI.state.in_(acceptable_states))  # noqa: E711\n            )\n        else:\n            ti_query = ti_query.filter(TI.state.in_(acceptable_states))\n\n        tis_to_set_to_queued = (\n            ti_query\n            .with_for_update()\n            .all())\n        if len(tis_to_set_to_queued) == 0:\n            self.log.info(\"No tasks were able to have their state changed to queued.\")\n            session.commit()\n            return []\n\n        # set TIs to queued state\n        for task_instance in tis_to_set_to_queued:\n            task_instance.state = State.QUEUED\n            task_instance.queued_dttm = (timezone.utcnow()\n                                         if not task_instance.queued_dttm\n                                         else task_instance.queued_dttm)\n            session.merge(task_instance)\n\n        # Generate a list of SimpleTaskInstance for the use of queuing\n        # them in the executor.\n        simple_task_instances = [SimpleTaskInstance(ti) for ti in\n                                 tis_to_set_to_queued]\n\n        task_instance_str = \"\\n\\t\".join(\n            [repr(x) for x in tis_to_set_to_queued])\n\n        session.commit()\n        self.log.info(\"Setting the following %s tasks to queued state:\\n\\t%s\",\n                      len(tis_to_set_to_queued), task_instance_str)\n        return simple_task_instances","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"SchedulerJob._enqueue_task_instances_with_queued_state","method_code":"def _enqueue_task_instances_with_queued_state(self, simple_dag_bag,\n                                                  simple_task_instances):\n        TI = models.TaskInstance\n        \n        for simple_task_instance in simple_task_instances:\n            simple_dag = simple_dag_bag.get_dag(simple_task_instance.dag_id)\n            command = TI.generate_command(\n                simple_task_instance.dag_id,\n                simple_task_instance.task_id,\n                simple_task_instance.execution_date,\n                local=True,\n                mark_success=False,\n                ignore_all_deps=False,\n                ignore_depends_on_past=False,\n                ignore_task_deps=False,\n                ignore_ti_state=False,\n                pool=simple_task_instance.pool,\n                file_path=simple_dag.full_filepath,\n                pickle_id=simple_dag.pickle_id)\n\n            priority = simple_task_instance.priority_weight\n            queue = simple_task_instance.queue\n            self.log.info(\n                \"Sending %s to executor with priority %s and queue %s\",\n                simple_task_instance.key, priority, queue\n            )\n\n            self.executor.queue_command(\n                simple_task_instance,\n                command,\n                priority=priority,\n                queue=queue)","method_summary":"Takes task_instances, which should have been set to queued, and enqueues them with the executor.","original_method_code":"def _enqueue_task_instances_with_queued_state(self, simple_dag_bag,\n                                                  simple_task_instances):\n        \"\"\"\n        Takes task_instances, which should have been set to queued, and enqueues them\n        with the executor.\n\n        :param simple_task_instances: TaskInstances to enqueue\n        :type simple_task_instances: list[SimpleTaskInstance]\n        :param simple_dag_bag: Should contains all of the task_instances' dags\n        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag\n        \"\"\"\n        TI = models.TaskInstance\n        # actually enqueue them\n        for simple_task_instance in simple_task_instances:\n            simple_dag = simple_dag_bag.get_dag(simple_task_instance.dag_id)\n            command = TI.generate_command(\n                simple_task_instance.dag_id,\n                simple_task_instance.task_id,\n                simple_task_instance.execution_date,\n                local=True,\n                mark_success=False,\n                ignore_all_deps=False,\n                ignore_depends_on_past=False,\n                ignore_task_deps=False,\n                ignore_ti_state=False,\n                pool=simple_task_instance.pool,\n                file_path=simple_dag.full_filepath,\n                pickle_id=simple_dag.pickle_id)\n\n            priority = simple_task_instance.priority_weight\n            queue = simple_task_instance.queue\n            self.log.info(\n                \"Sending %s to executor with priority %s and queue %s\",\n                simple_task_instance.key, priority, queue\n            )\n\n            self.executor.queue_command(\n                simple_task_instance,\n                command,\n                priority=priority,\n                queue=queue)","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"SchedulerJob._execute_task_instances","method_code":"def _execute_task_instances(self,\n                                simple_dag_bag,\n                                states,\n                                session=None):\n        executable_tis = self._find_executable_task_instances(simple_dag_bag, states,\n                                                              session=session)\n\n        def query(result, items):\n            simple_tis_with_state_changed = \\\n                self._change_state_for_executable_task_instances(items,\n                                                                 states,\n                                                                 session=session)\n            self._enqueue_task_instances_with_queued_state(\n                simple_dag_bag,\n                simple_tis_with_state_changed)\n            session.commit()\n            return result + len(simple_tis_with_state_changed)\n\n        return helpers.reduce_in_chunks(query, executable_tis, 0, self.max_tis_per_query)","method_summary":"Attempts to execute TaskInstances that should be executed by the scheduler. There are three","original_method_code":"def _execute_task_instances(self,\n                                simple_dag_bag,\n                                states,\n                                session=None):\n        \"\"\"\n        Attempts to execute TaskInstances that should be executed by the scheduler.\n\n        There are three steps:\n        1. Pick TIs by priority with the constraint that they are in the expected states\n        and that we do exceed max_active_runs or pool limits.\n        2. Change the state for the TIs above atomically.\n        3. Enqueue the TIs in the executor.\n\n        :param simple_dag_bag: TaskInstances associated with DAGs in the\n            simple_dag_bag will be fetched from the DB and executed\n        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag\n        :param states: Execute TaskInstances in these states\n        :type states: tuple[airflow.utils.state.State]\n        :return: Number of task instance with state changed.\n        \"\"\"\n        executable_tis = self._find_executable_task_instances(simple_dag_bag, states,\n                                                              session=session)\n\n        def query(result, items):\n            simple_tis_with_state_changed = \\\n                self._change_state_for_executable_task_instances(items,\n                                                                 states,\n                                                                 session=session)\n            self._enqueue_task_instances_with_queued_state(\n                simple_dag_bag,\n                simple_tis_with_state_changed)\n            session.commit()\n            return result + len(simple_tis_with_state_changed)\n\n        return helpers.reduce_in_chunks(query, executable_tis, 0, self.max_tis_per_query)","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"SchedulerJob._change_state_for_tasks_failed_to_execute","method_code":"def _change_state_for_tasks_failed_to_execute(self, session):\n        if self.executor.queued_tasks:\n            TI = models.TaskInstance\n            filter_for_ti_state_change = (\n                [and_(\n                    TI.dag_id == dag_id,\n                    TI.task_id == task_id,\n                    TI.execution_date == execution_date,\n                    \n                    \n                    TI._try_number == try_number - 1,\n                    TI.state == State.QUEUED)\n                    for dag_id, task_id, execution_date, try_number\n                    in self.executor.queued_tasks.keys()])\n            ti_query = (session.query(TI)\n                        .filter(or_(*filter_for_ti_state_change)))\n            tis_to_set_to_scheduled = (ti_query\n                                       .with_for_update()\n                                       .all())\n            if len(tis_to_set_to_scheduled) == 0:\n                session.commit()\n                return\n\n            \n            for task_instance in tis_to_set_to_scheduled:\n                task_instance.state = State.SCHEDULED\n\n            task_instance_str = \"\\n\\t\".join(\n                [repr(x) for x in tis_to_set_to_scheduled])\n\n            session.commit()\n            self.log.info(\"Set the following tasks to scheduled state:\\n\\t%s\", task_instance_str)","method_summary":"If there are tasks left over in the executor, we set them back to SCHEDULED to avoid creating hanging tasks.","original_method_code":"def _change_state_for_tasks_failed_to_execute(self, session):\n        \"\"\"\n        If there are tasks left over in the executor,\n        we set them back to SCHEDULED to avoid creating hanging tasks.\n\n        :param session: session for ORM operations\n        \"\"\"\n        if self.executor.queued_tasks:\n            TI = models.TaskInstance\n            filter_for_ti_state_change = (\n                [and_(\n                    TI.dag_id == dag_id,\n                    TI.task_id == task_id,\n                    TI.execution_date == execution_date,\n                    # The TI.try_number will return raw try_number+1 since the\n                    # ti is not running. And we need to -1 to match the DB record.\n                    TI._try_number == try_number - 1,\n                    TI.state == State.QUEUED)\n                    for dag_id, task_id, execution_date, try_number\n                    in self.executor.queued_tasks.keys()])\n            ti_query = (session.query(TI)\n                        .filter(or_(*filter_for_ti_state_change)))\n            tis_to_set_to_scheduled = (ti_query\n                                       .with_for_update()\n                                       .all())\n            if len(tis_to_set_to_scheduled) == 0:\n                session.commit()\n                return\n\n            # set TIs to queued state\n            for task_instance in tis_to_set_to_scheduled:\n                task_instance.state = State.SCHEDULED\n\n            task_instance_str = \"\\n\\t\".join(\n                [repr(x) for x in tis_to_set_to_scheduled])\n\n            session.commit()\n            self.log.info(\"Set the following tasks to scheduled state:\\n\\t%s\", task_instance_str)","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"SchedulerJob._process_executor_events","method_code":"def _process_executor_events(self, simple_dag_bag, session=None):\n        \n\n        TI = models.TaskInstance\n        for key, state in list(self.executor.get_event_buffer(simple_dag_bag.dag_ids)\n                                   .items()):\n            dag_id, task_id, execution_date, try_number = key\n            self.log.info(\n                \"Executor reports execution of %s.%s execution_date=%s \"\n                \"exited with status %s for try_number %s\",\n                dag_id, task_id, execution_date, state, try_number\n            )\n            if state == State.FAILED or state == State.SUCCESS:\n                qry = session.query(TI).filter(TI.dag_id == dag_id,\n                                               TI.task_id == task_id,\n                                               TI.execution_date == execution_date)\n                ti = qry.first()\n                if not ti:\n                    self.log.warning(\"TaskInstance %s went missing from the database\", ti)\n                    continue\n\n                \n                if ti.try_number == try_number and ti.state == State.QUEUED:\n                    msg = (\"Executor reports task instance {} finished ({}) \"\n                           \"although the task says its {}. Was the task \"\n                           \"killed externally?\".format(ti, state, ti.state))\n                    self.log.error(msg)\n                    try:\n                        simple_dag = simple_dag_bag.get_dag(dag_id)\n                        dagbag = models.DagBag(simple_dag.full_filepath)\n                        dag = dagbag.get_dag(dag_id)\n                        ti.task = dag.get_task(task_id)\n                        ti.handle_failure(msg)\n                    except Exception:\n                        self.log.error(\"Cannot load the dag bag to handle failure for %s\"\n                                       \". Setting task to FAILED without callbacks or \"\n                                       \"retries. Do you have enough resources?\", ti)\n                        ti.state = State.FAILED\n                        session.merge(ti)\n                        session.commit()","method_summary":"Respond to executor events.","original_method_code":"def _process_executor_events(self, simple_dag_bag, session=None):\n        \"\"\"\n        Respond to executor events.\n        \"\"\"\n        # TODO: this shares quite a lot of code with _manage_executor_state\n\n        TI = models.TaskInstance\n        for key, state in list(self.executor.get_event_buffer(simple_dag_bag.dag_ids)\n                                   .items()):\n            dag_id, task_id, execution_date, try_number = key\n            self.log.info(\n                \"Executor reports execution of %s.%s execution_date=%s \"\n                \"exited with status %s for try_number %s\",\n                dag_id, task_id, execution_date, state, try_number\n            )\n            if state == State.FAILED or state == State.SUCCESS:\n                qry = session.query(TI).filter(TI.dag_id == dag_id,\n                                               TI.task_id == task_id,\n                                               TI.execution_date == execution_date)\n                ti = qry.first()\n                if not ti:\n                    self.log.warning(\"TaskInstance %s went missing from the database\", ti)\n                    continue\n\n                # TODO: should we fail RUNNING as well, as we do in Backfills?\n                if ti.try_number == try_number and ti.state == State.QUEUED:\n                    msg = (\"Executor reports task instance {} finished ({}) \"\n                           \"although the task says its {}. Was the task \"\n                           \"killed externally?\".format(ti, state, ti.state))\n                    self.log.error(msg)\n                    try:\n                        simple_dag = simple_dag_bag.get_dag(dag_id)\n                        dagbag = models.DagBag(simple_dag.full_filepath)\n                        dag = dagbag.get_dag(dag_id)\n                        ti.task = dag.get_task(task_id)\n                        ti.handle_failure(msg)\n                    except Exception:\n                        self.log.error(\"Cannot load the dag bag to handle failure for %s\"\n                                       \". Setting task to FAILED without callbacks or \"\n                                       \"retries. Do you have enough resources?\", ti)\n                        ti.state = State.FAILED\n                        session.merge(ti)\n                        session.commit()","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"SchedulerJob.process_file","method_code":"def process_file(self, file_path, zombies, pickle_dags=False, session=None):\n        self.log.info(\"Processing file %s for tasks to queue\", file_path)\n        \n        simple_dags = []\n\n        try:\n            dagbag = models.DagBag(file_path, include_examples=False)\n        except Exception:\n            self.log.exception(\"Failed at reloading the DAG file %s\", file_path)\n            Stats.incr('dag_file_refresh_error', 1, 1)\n            return []\n\n        if len(dagbag.dags) > 0:\n            self.log.info(\"DAG(s) %s retrieved from %s\", dagbag.dags.keys(), file_path)\n        else:\n            self.log.warning(\"No viable dags retrieved from %s\", file_path)\n            self.update_import_errors(session, dagbag)\n            return []\n\n        \n        for dag in dagbag.dags.values():\n            dag.sync_to_db()\n\n        paused_dag_ids = [dag.dag_id for dag in dagbag.dags.values()\n                          if dag.is_paused]\n\n        \n        for dag_id in dagbag.dags:\n            \n            if dag_id not in paused_dag_ids:\n                dag = dagbag.get_dag(dag_id)\n                pickle_id = None\n                if pickle_dags:\n                    pickle_id = dag.pickle(session).id\n                simple_dags.append(SimpleDag(dag, pickle_id=pickle_id))\n\n        if len(self.dag_ids) > 0:\n            dags = [dag for dag in dagbag.dags.values()\n                    if dag.dag_id in self.dag_ids and\n                    dag.dag_id not in paused_dag_ids]\n        else:\n            dags = [dag for dag in dagbag.dags.values()\n                    if not dag.parent_dag and\n                    dag.dag_id not in paused_dag_ids]\n\n        \n        \n        \n        ti_keys_to_schedule = []\n\n        self._process_dags(dagbag, dags, ti_keys_to_schedule)\n\n        for ti_key in ti_keys_to_schedule:\n            dag = dagbag.dags[ti_key[0]]\n            task = dag.get_task(ti_key[1])\n            ti = models.TaskInstance(task, ti_key[2])\n\n            ti.refresh_from_db(session=session, lock_for_update=True)\n            \n            \n            dep_context = DepContext(deps=QUEUE_DEPS, ignore_task_deps=True)\n\n            \n            \n            \n            \n            \n            \n            if ti.are_dependencies_met(\n                    dep_context=dep_context,\n                    session=session,\n                    verbose=True):\n                \n                \n                ti.state = State.SCHEDULED\n\n            \n            self.log.info(\"Creating \/ updating %s in ORM\", ti)\n            session.merge(ti)\n        \n        session.commit()\n\n        \n        try:\n            self.update_import_errors(session, dagbag)\n        except Exception:\n            self.log.exception(\"Error logging import errors!\")\n        try:\n            dagbag.kill_zombies(zombies)\n        except Exception:\n            self.log.exception(\"Error killing zombies!\")\n\n        return simple_dags","method_summary":"Process a Python file containing Airflow DAGs. This","original_method_code":"def process_file(self, file_path, zombies, pickle_dags=False, session=None):\n        \"\"\"\n        Process a Python file containing Airflow DAGs.\n\n        This includes:\n\n        1. Execute the file and look for DAG objects in the namespace.\n        2. Pickle the DAG and save it to the DB (if necessary).\n        3. For each DAG, see what tasks should run and create appropriate task\n        instances in the DB.\n        4. Record any errors importing the file into ORM\n        5. Kill (in ORM) any task instances belonging to the DAGs that haven't\n        issued a heartbeat in a while.\n\n        Returns a list of SimpleDag objects that represent the DAGs found in\n        the file\n\n        :param file_path: the path to the Python file that should be executed\n        :type file_path: unicode\n        :param zombies: zombie task instances to kill.\n        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]\n        :param pickle_dags: whether serialize the DAGs found in the file and\n            save them to the db\n        :type pickle_dags: bool\n        :return: a list of SimpleDags made from the Dags found in the file\n        :rtype: list[airflow.utils.dag_processing.SimpleDagBag]\n        \"\"\"\n        self.log.info(\"Processing file %s for tasks to queue\", file_path)\n        # As DAGs are parsed from this file, they will be converted into SimpleDags\n        simple_dags = []\n\n        try:\n            dagbag = models.DagBag(file_path, include_examples=False)\n        except Exception:\n            self.log.exception(\"Failed at reloading the DAG file %s\", file_path)\n            Stats.incr('dag_file_refresh_error', 1, 1)\n            return []\n\n        if len(dagbag.dags) > 0:\n            self.log.info(\"DAG(s) %s retrieved from %s\", dagbag.dags.keys(), file_path)\n        else:\n            self.log.warning(\"No viable dags retrieved from %s\", file_path)\n            self.update_import_errors(session, dagbag)\n            return []\n\n        # Save individual DAGs in the ORM and update DagModel.last_scheduled_time\n        for dag in dagbag.dags.values():\n            dag.sync_to_db()\n\n        paused_dag_ids = [dag.dag_id for dag in dagbag.dags.values()\n                          if dag.is_paused]\n\n        # Pickle the DAGs (if necessary) and put them into a SimpleDag\n        for dag_id in dagbag.dags:\n            # Only return DAGs that are not paused\n            if dag_id not in paused_dag_ids:\n                dag = dagbag.get_dag(dag_id)\n                pickle_id = None\n                if pickle_dags:\n                    pickle_id = dag.pickle(session).id\n                simple_dags.append(SimpleDag(dag, pickle_id=pickle_id))\n\n        if len(self.dag_ids) > 0:\n            dags = [dag for dag in dagbag.dags.values()\n                    if dag.dag_id in self.dag_ids and\n                    dag.dag_id not in paused_dag_ids]\n        else:\n            dags = [dag for dag in dagbag.dags.values()\n                    if not dag.parent_dag and\n                    dag.dag_id not in paused_dag_ids]\n\n        # Not using multiprocessing.Queue() since it's no longer a separate\n        # process and due to some unusual behavior. (empty() incorrectly\n        # returns true?)\n        ti_keys_to_schedule = []\n\n        self._process_dags(dagbag, dags, ti_keys_to_schedule)\n\n        for ti_key in ti_keys_to_schedule:\n            dag = dagbag.dags[ti_key[0]]\n            task = dag.get_task(ti_key[1])\n            ti = models.TaskInstance(task, ti_key[2])\n\n            ti.refresh_from_db(session=session, lock_for_update=True)\n            # We can defer checking the task dependency checks to the worker themselves\n            # since they can be expensive to run in the scheduler.\n            dep_context = DepContext(deps=QUEUE_DEPS, ignore_task_deps=True)\n\n            # Only schedule tasks that have their dependencies met, e.g. to avoid\n            # a task that recently got its state changed to RUNNING from somewhere\n            # other than the scheduler from getting its state overwritten.\n            # TODO(aoen): It's not great that we have to check all the task instance\n            # dependencies twice; once to get the task scheduled, and again to actually\n            # run the task. We should try to come up with a way to only check them once.\n            if ti.are_dependencies_met(\n                    dep_context=dep_context,\n                    session=session,\n                    verbose=True):\n                # Task starts out in the scheduled state. All tasks in the\n                # scheduled state will be sent to the executor\n                ti.state = State.SCHEDULED\n\n            # Also save this task instance to the DB.\n            self.log.info(\"Creating \/ updating %s in ORM\", ti)\n            session.merge(ti)\n        # commit batch\n        session.commit()\n\n        # Record import errors into the ORM\n        try:\n            self.update_import_errors(session, dagbag)\n        except Exception:\n            self.log.exception(\"Error logging import errors!\")\n        try:\n            dagbag.kill_zombies(zombies)\n        except Exception:\n            self.log.exception(\"Error killing zombies!\")\n\n        return simple_dags","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"BackfillJob._update_counters","method_code":"def _update_counters(self, ti_status):\n        for key, ti in list(ti_status.running.items()):\n            ti.refresh_from_db()\n            if ti.state == State.SUCCESS:\n                ti_status.succeeded.add(key)\n                self.log.debug(\"Task instance %s succeeded. Don't rerun.\", ti)\n                ti_status.running.pop(key)\n                continue\n            elif ti.state == State.SKIPPED:\n                ti_status.skipped.add(key)\n                self.log.debug(\"Task instance %s skipped. Don't rerun.\", ti)\n                ti_status.running.pop(key)\n                continue\n            elif ti.state == State.FAILED:\n                self.log.error(\"Task instance %s failed\", ti)\n                ti_status.failed.add(key)\n                ti_status.running.pop(key)\n                continue\n            \n            elif ti.state == State.UP_FOR_RETRY:\n                self.log.warning(\"Task instance %s is up for retry\", ti)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti\n            \n            elif ti.state == State.UP_FOR_RESCHEDULE:\n                self.log.warning(\"Task instance %s is up for reschedule\", ti)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti\n            \n            \n            \n            \n            \n            elif ti.state == State.NONE:\n                self.log.warning(\n                    \"FIXME: task instance %s state was set to none externally or \"\n                    \"reaching concurrency limits. Re-adding task to queue.\",\n                    ti\n                )\n                ti.set_state(State.SCHEDULED)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti","method_summary":"Updates the counters per state of the tasks that were running. Can re-add to tasks to run in case required.","original_method_code":"def _update_counters(self, ti_status):\n        \"\"\"\n        Updates the counters per state of the tasks that were running. Can re-add\n        to tasks to run in case required.\n\n        :param ti_status: the internal status of the backfill job tasks\n        :type ti_status: BackfillJob._DagRunTaskStatus\n        \"\"\"\n        for key, ti in list(ti_status.running.items()):\n            ti.refresh_from_db()\n            if ti.state == State.SUCCESS:\n                ti_status.succeeded.add(key)\n                self.log.debug(\"Task instance %s succeeded. Don't rerun.\", ti)\n                ti_status.running.pop(key)\n                continue\n            elif ti.state == State.SKIPPED:\n                ti_status.skipped.add(key)\n                self.log.debug(\"Task instance %s skipped. Don't rerun.\", ti)\n                ti_status.running.pop(key)\n                continue\n            elif ti.state == State.FAILED:\n                self.log.error(\"Task instance %s failed\", ti)\n                ti_status.failed.add(key)\n                ti_status.running.pop(key)\n                continue\n            # special case: if the task needs to run again put it back\n            elif ti.state == State.UP_FOR_RETRY:\n                self.log.warning(\"Task instance %s is up for retry\", ti)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti\n            # special case: if the task needs to be rescheduled put it back\n            elif ti.state == State.UP_FOR_RESCHEDULE:\n                self.log.warning(\"Task instance %s is up for reschedule\", ti)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti\n            # special case: The state of the task can be set to NONE by the task itself\n            # when it reaches concurrency limits. It could also happen when the state\n            # is changed externally, e.g. by clearing tasks from the ui. We need to cover\n            # for that as otherwise those tasks would fall outside of the scope of\n            # the backfill suddenly.\n            elif ti.state == State.NONE:\n                self.log.warning(\n                    \"FIXME: task instance %s state was set to none externally or \"\n                    \"reaching concurrency limits. Re-adding task to queue.\",\n                    ti\n                )\n                ti.set_state(State.SCHEDULED)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"BackfillJob._manage_executor_state","method_code":"def _manage_executor_state(self, running):\n        executor = self.executor\n\n        for key, state in list(executor.get_event_buffer().items()):\n            if key not in running:\n                self.log.warning(\n                    \"%s state %s not in running=%s\",\n                    key, state, running.values()\n                )\n                continue\n\n            ti = running[key]\n            ti.refresh_from_db()\n\n            self.log.debug(\"Executor state: %s task %s\", state, ti)\n\n            if state == State.FAILED or state == State.SUCCESS:\n                if ti.state == State.RUNNING or ti.state == State.QUEUED:\n                    msg = (\"Executor reports task instance {} finished ({}) \"\n                           \"although the task says its {}. Was the task \"\n                           \"killed externally?\".format(ti, state, ti.state))\n                    self.log.error(msg)\n                    ti.handle_failure(msg)","method_summary":"Checks if the executor agrees with the state of task instances that are running","original_method_code":"def _manage_executor_state(self, running):\n        \"\"\"\n        Checks if the executor agrees with the state of task instances\n        that are running\n\n        :param running: dict of key, task to verify\n        \"\"\"\n        executor = self.executor\n\n        for key, state in list(executor.get_event_buffer().items()):\n            if key not in running:\n                self.log.warning(\n                    \"%s state %s not in running=%s\",\n                    key, state, running.values()\n                )\n                continue\n\n            ti = running[key]\n            ti.refresh_from_db()\n\n            self.log.debug(\"Executor state: %s task %s\", state, ti)\n\n            if state == State.FAILED or state == State.SUCCESS:\n                if ti.state == State.RUNNING or ti.state == State.QUEUED:\n                    msg = (\"Executor reports task instance {} finished ({}) \"\n                           \"although the task says its {}. Was the task \"\n                           \"killed externally?\".format(ti, state, ti.state))\n                    self.log.error(msg)\n                    ti.handle_failure(msg)","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"BackfillJob._execute_for_run_dates","method_code":"def _execute_for_run_dates(self, run_dates, ti_status, executor, pickle_id,\n                               start_date, session=None):\n        for next_run_date in run_dates:\n            dag_run = self._get_dag_run(next_run_date, session=session)\n            tis_map = self._task_instances_for_dag_run(dag_run,\n                                                       session=session)\n            if dag_run is None:\n                continue\n\n            ti_status.active_runs.append(dag_run)\n            ti_status.to_run.update(tis_map or {})\n\n        processed_dag_run_dates = self._process_backfill_task_instances(\n            ti_status=ti_status,\n            executor=executor,\n            pickle_id=pickle_id,\n            start_date=start_date,\n            session=session)\n\n        ti_status.executed_dag_run_dates.update(processed_dag_run_dates)","method_summary":"Computes the dag runs and their respective task instances for the given run dates and executes the task instances.","original_method_code":"def _execute_for_run_dates(self, run_dates, ti_status, executor, pickle_id,\n                               start_date, session=None):\n        \"\"\"\n        Computes the dag runs and their respective task instances for\n        the given run dates and executes the task instances.\n        Returns a list of execution dates of the dag runs that were executed.\n\n        :param run_dates: Execution dates for dag runs\n        :type run_dates: list\n        :param ti_status: internal BackfillJob status structure to tis track progress\n        :type ti_status: BackfillJob._DagRunTaskStatus\n        :param executor: the executor to use, it must be previously started\n        :type executor: BaseExecutor\n        :param pickle_id: numeric id of the pickled dag, None if not pickled\n        :type pickle_id: int\n        :param start_date: backfill start date\n        :type start_date: datetime.datetime\n        :param session: the current session object\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n        for next_run_date in run_dates:\n            dag_run = self._get_dag_run(next_run_date, session=session)\n            tis_map = self._task_instances_for_dag_run(dag_run,\n                                                       session=session)\n            if dag_run is None:\n                continue\n\n            ti_status.active_runs.append(dag_run)\n            ti_status.to_run.update(tis_map or {})\n\n        processed_dag_run_dates = self._process_backfill_task_instances(\n            ti_status=ti_status,\n            executor=executor,\n            pickle_id=pickle_id,\n            start_date=start_date,\n            session=session)\n\n        ti_status.executed_dag_run_dates.update(processed_dag_run_dates)","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"BackfillJob._set_unfinished_dag_runs_to_failed","method_code":"def _set_unfinished_dag_runs_to_failed(self, dag_runs, session=None):\n        for dag_run in dag_runs:\n            dag_run.update_state()\n            if dag_run.state not in State.finished():\n                dag_run.set_state(State.FAILED)\n            session.merge(dag_run)","method_summary":"Go through the dag_runs and update the state based on the task_instance state. Then set DAG runs that are not finished to failed.","original_method_code":"def _set_unfinished_dag_runs_to_failed(self, dag_runs, session=None):\n        \"\"\"\n        Go through the dag_runs and update the state based on the task_instance state.\n        Then set DAG runs that are not finished to failed.\n\n        :param dag_runs: DAG runs\n        :param session: session\n        :return: None\n        \"\"\"\n        for dag_run in dag_runs:\n            dag_run.update_state()\n            if dag_run.state not in State.finished():\n                dag_run.set_state(State.FAILED)\n            session.merge(dag_run)","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"BackfillJob._execute","method_code":"def _execute(self, session=None):\n        ti_status = BackfillJob._DagRunTaskStatus()\n\n        start_date = self.bf_start_date\n\n        \n        run_dates = self.dag.get_run_dates(start_date=start_date,\n                                           end_date=self.bf_end_date)\n        if self.run_backwards:\n            tasks_that_depend_on_past = [t.task_id for t in self.dag.task_dict.values() if t.depends_on_past]\n            if tasks_that_depend_on_past:\n                raise AirflowException(\n                    'You cannot backfill backwards because one or more tasks depend_on_past: {}'.format(\n                        \",\".join(tasks_that_depend_on_past)))\n            run_dates = run_dates[::-1]\n\n        if len(run_dates) == 0:\n            self.log.info(\"No run dates were found for the given dates and dag interval.\")\n            return\n\n        \n        pickle_id = None\n        if not self.donot_pickle and self.executor.__class__ not in (\n                executors.LocalExecutor, executors.SequentialExecutor):\n            pickle = DagPickle(self.dag)\n            session.add(pickle)\n            session.commit()\n            pickle_id = pickle.id\n\n        executor = self.executor\n        executor.start()\n\n        ti_status.total_runs = len(run_dates)  \n\n        try:\n            remaining_dates = ti_status.total_runs\n            while remaining_dates > 0:\n                dates_to_process = [run_date for run_date in run_dates\n                                    if run_date not in ti_status.executed_dag_run_dates]\n\n                self._execute_for_run_dates(run_dates=dates_to_process,\n                                            ti_status=ti_status,\n                                            executor=executor,\n                                            pickle_id=pickle_id,\n                                            start_date=start_date,\n                                            session=session)\n\n                remaining_dates = (\n                    ti_status.total_runs - len(ti_status.executed_dag_run_dates)\n                )\n                err = self._collect_errors(ti_status=ti_status, session=session)\n                if err:\n                    raise AirflowException(err)\n\n                if remaining_dates > 0:\n                    self.log.info(\n                        \"max_active_runs limit for dag %s has been reached \"\n                        \" - waiting for other dag runs to finish\",\n                        self.dag_id\n                    )\n                    time.sleep(self.delay_on_limit_secs)\n        except (KeyboardInterrupt, SystemExit):\n            self.log.warning(\"Backfill terminated by user.\")\n\n            \n            \n            self._set_unfinished_dag_runs_to_failed(ti_status.active_runs)\n        finally:\n            session.commit()\n            executor.end()\n\n        self.log.info(\"Backfill done. Exiting.\")","method_summary":"Initializes all components required to run a dag for a specified date range and calls helper method to execute the tasks.","original_method_code":"def _execute(self, session=None):\n        \"\"\"\n        Initializes all components required to run a dag for a specified date range and\n        calls helper method to execute the tasks.\n        \"\"\"\n        ti_status = BackfillJob._DagRunTaskStatus()\n\n        start_date = self.bf_start_date\n\n        # Get intervals between the start\/end dates, which will turn into dag runs\n        run_dates = self.dag.get_run_dates(start_date=start_date,\n                                           end_date=self.bf_end_date)\n        if self.run_backwards:\n            tasks_that_depend_on_past = [t.task_id for t in self.dag.task_dict.values() if t.depends_on_past]\n            if tasks_that_depend_on_past:\n                raise AirflowException(\n                    'You cannot backfill backwards because one or more tasks depend_on_past: {}'.format(\n                        \",\".join(tasks_that_depend_on_past)))\n            run_dates = run_dates[::-1]\n\n        if len(run_dates) == 0:\n            self.log.info(\"No run dates were found for the given dates and dag interval.\")\n            return\n\n        # picklin'\n        pickle_id = None\n        if not self.donot_pickle and self.executor.__class__ not in (\n                executors.LocalExecutor, executors.SequentialExecutor):\n            pickle = DagPickle(self.dag)\n            session.add(pickle)\n            session.commit()\n            pickle_id = pickle.id\n\n        executor = self.executor\n        executor.start()\n\n        ti_status.total_runs = len(run_dates)  # total dag runs in backfill\n\n        try:\n            remaining_dates = ti_status.total_runs\n            while remaining_dates > 0:\n                dates_to_process = [run_date for run_date in run_dates\n                                    if run_date not in ti_status.executed_dag_run_dates]\n\n                self._execute_for_run_dates(run_dates=dates_to_process,\n                                            ti_status=ti_status,\n                                            executor=executor,\n                                            pickle_id=pickle_id,\n                                            start_date=start_date,\n                                            session=session)\n\n                remaining_dates = (\n                    ti_status.total_runs - len(ti_status.executed_dag_run_dates)\n                )\n                err = self._collect_errors(ti_status=ti_status, session=session)\n                if err:\n                    raise AirflowException(err)\n\n                if remaining_dates > 0:\n                    self.log.info(\n                        \"max_active_runs limit for dag %s has been reached \"\n                        \" - waiting for other dag runs to finish\",\n                        self.dag_id\n                    )\n                    time.sleep(self.delay_on_limit_secs)\n        except (KeyboardInterrupt, SystemExit):\n            self.log.warning(\"Backfill terminated by user.\")\n\n            # TODO: we will need to terminate running task instances and set the\n            # state to failed.\n            self._set_unfinished_dag_runs_to_failed(ti_status.active_runs)\n        finally:\n            session.commit()\n            executor.end()\n\n        self.log.info(\"Backfill done. Exiting.\")","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"LocalTaskJob.heartbeat_callback","method_code":"def heartbeat_callback(self, session=None):\n        if self.terminating:\n            \n            self.task_runner.terminate()\n            return\n\n        self.task_instance.refresh_from_db()\n        ti = self.task_instance\n\n        fqdn = get_hostname()\n        same_hostname = fqdn == ti.hostname\n        same_process = ti.pid == os.getpid()\n\n        if ti.state == State.RUNNING:\n            if not same_hostname:\n                self.log.warning(\"The recorded hostname %s \"\n                                 \"does not match this instance's hostname \"\n                                 \"%s\", ti.hostname, fqdn)\n                raise AirflowException(\"Hostname of job runner does not match\")\n            elif not same_process:\n                current_pid = os.getpid()\n                self.log.warning(\"Recorded pid %s does not match \"\n                                 \"the current pid %s\", ti.pid, current_pid)\n                raise AirflowException(\"PID of job runner does not match\")\n        elif (\n                self.task_runner.return_code() is None and\n                hasattr(self.task_runner, 'process')\n        ):\n            self.log.warning(\n                \"State of this instance has been externally set to %s. \"\n                \"Taking the poison pill.\",\n                ti.state\n            )\n            self.task_runner.terminate()\n            self.terminating = True","method_summary":"Self destruct task if state has been moved away from running externally","original_method_code":"def heartbeat_callback(self, session=None):\n        \"\"\"Self destruct task if state has been moved away from running externally\"\"\"\n\n        if self.terminating:\n            # ensure termination if processes are created later\n            self.task_runner.terminate()\n            return\n\n        self.task_instance.refresh_from_db()\n        ti = self.task_instance\n\n        fqdn = get_hostname()\n        same_hostname = fqdn == ti.hostname\n        same_process = ti.pid == os.getpid()\n\n        if ti.state == State.RUNNING:\n            if not same_hostname:\n                self.log.warning(\"The recorded hostname %s \"\n                                 \"does not match this instance's hostname \"\n                                 \"%s\", ti.hostname, fqdn)\n                raise AirflowException(\"Hostname of job runner does not match\")\n            elif not same_process:\n                current_pid = os.getpid()\n                self.log.warning(\"Recorded pid %s does not match \"\n                                 \"the current pid %s\", ti.pid, current_pid)\n                raise AirflowException(\"PID of job runner does not match\")\n        elif (\n                self.task_runner.return_code() is None and\n                hasattr(self.task_runner, 'process')\n        ):\n            self.log.warning(\n                \"State of this instance has been externally set to %s. \"\n                \"Taking the poison pill.\",\n                ti.state\n            )\n            self.task_runner.terminate()\n            self.terminating = True","method_path":"airflow\/jobs.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSpannerHook._get_client","method_code":"def _get_client(self, project_id):\n        if not self._client:\n            self._client = Client(project=project_id, credentials=self._get_credentials())\n        return self._client","method_summary":"Provides a client for interacting with the Cloud Spanner API.","original_method_code":"def _get_client(self, project_id):\n        \"\"\"\n        Provides a client for interacting with the Cloud Spanner API.\n\n        :param project_id: The ID of the  GCP project.\n        :type project_id: str\n        :return: google.cloud.spanner_v1.client.Client\n        :rtype: object\n        \"\"\"\n        if not self._client:\n            self._client = Client(project=project_id, credentials=self._get_credentials())\n        return self._client","method_path":"airflow\/contrib\/hooks\/gcp_spanner_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSpannerHook.get_instance","method_code":"def get_instance(self, instance_id, project_id=None):\n        instance = self._get_client(project_id=project_id).instance(instance_id=instance_id)\n        if not instance.exists():\n            return None\n        return instance","method_summary":"Gets information about a particular instance.","original_method_code":"def get_instance(self, instance_id, project_id=None):\n        \"\"\"\n        Gets information about a particular instance.\n\n        :param project_id: Optional, The ID of the  GCP project that owns the Cloud Spanner\n            database.  If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :return: google.cloud.spanner_v1.instance.Instance\n        :rtype: object\n        \"\"\"\n        instance = self._get_client(project_id=project_id).instance(instance_id=instance_id)\n        if not instance.exists():\n            return None\n        return instance","method_path":"airflow\/contrib\/hooks\/gcp_spanner_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSpannerHook._apply_to_instance","method_code":"def _apply_to_instance(self, project_id, instance_id, configuration_name, node_count,\n                           display_name, func):\n        \n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id, configuration_name=configuration_name,\n            node_count=node_count, display_name=display_name)\n        try:\n            operation = func(instance)  \n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)","method_summary":"Invokes a method on a given instance by applying a specified Callable.","original_method_code":"def _apply_to_instance(self, project_id, instance_id, configuration_name, node_count,\n                           display_name, func):\n        \"\"\"\n        Invokes a method on a given instance by applying a specified Callable.\n\n        :param project_id: The ID of the  GCP project that owns the Cloud Spanner\n            database.\n        :type project_id: str\n        :param instance_id: The ID of the instance.\n        :type instance_id: str\n        :param configuration_name: Name of the instance configuration defining how the\n            instance will be created. Required for instances which do not yet exist.\n        :type configuration_name: str\n        :param node_count: (Optional) Number of nodes allocated to the instance.\n        :type node_count: int\n        :param display_name: (Optional) The display name for the instance in the Cloud\n            Console UI. (Must be between 4 and 30 characters.) If this value is not set\n            in the constructor, will fall back to the instance ID.\n        :type display_name: str\n        :param func: Method of the instance to be called.\n        :type func: Callable\n        \"\"\"\n        # noinspection PyUnresolvedReferences\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id, configuration_name=configuration_name,\n            node_count=node_count, display_name=display_name)\n        try:\n            operation = func(instance)  # type: Operation\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)","method_path":"airflow\/contrib\/hooks\/gcp_spanner_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSpannerHook.create_instance","method_code":"def create_instance(self, instance_id, configuration_name, node_count,\n                        display_name, project_id=None):\n        self._apply_to_instance(project_id, instance_id, configuration_name,\n                                node_count, display_name, lambda x: x.create())","method_summary":"Creates a new Cloud Spanner instance.","original_method_code":"def create_instance(self, instance_id, configuration_name, node_count,\n                        display_name, project_id=None):\n        \"\"\"\n        Creates a new Cloud Spanner instance.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param configuration_name: The name of the instance configuration defining how the\n            instance will be created. Possible configuration values can be retrieved via\n            https:\/\/cloud.google.com\/spanner\/docs\/reference\/rest\/v1\/projects.instanceConfigs\/list\n        :type configuration_name: str\n        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner\n            instance.\n        :type node_count: int\n        :param display_name: (Optional) The display name for the instance in the GCP\n            Console. Must be between 4 and 30 characters.  If this value is not set in\n            the constructor, the name falls back to the instance ID.\n        :type display_name: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        self._apply_to_instance(project_id, instance_id, configuration_name,\n                                node_count, display_name, lambda x: x.create())","method_path":"airflow\/contrib\/hooks\/gcp_spanner_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSpannerHook.update_instance","method_code":"def update_instance(self, instance_id, configuration_name, node_count,\n                        display_name, project_id=None):\n        return self._apply_to_instance(project_id, instance_id, configuration_name,\n                                       node_count, display_name, lambda x: x.update())","method_summary":"Updates an existing Cloud Spanner instance.","original_method_code":"def update_instance(self, instance_id, configuration_name, node_count,\n                        display_name, project_id=None):\n        \"\"\"\n        Updates an existing Cloud Spanner instance.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param configuration_name: The name of the instance configuration defining how the\n            instance will be created. Possible configuration values can be retrieved via\n            https:\/\/cloud.google.com\/spanner\/docs\/reference\/rest\/v1\/projects.instanceConfigs\/list\n        :type configuration_name: str\n        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner\n            instance.\n        :type node_count: int\n        :param display_name: (Optional) The display name for the instance in the GCP\n            Console. Must be between 4 and 30 characters. If this value is not set in\n            the constructor, the name falls back to the instance ID.\n        :type display_name: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        return self._apply_to_instance(project_id, instance_id, configuration_name,\n                                       node_count, display_name, lambda x: x.update())","method_path":"airflow\/contrib\/hooks\/gcp_spanner_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSpannerHook.delete_instance","method_code":"def delete_instance(self, instance_id, project_id=None):\n        instance = self._get_client(project_id=project_id).instance(instance_id)\n        try:\n            instance.delete()\n            return\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e","method_summary":"Deletes an existing Cloud Spanner instance.","original_method_code":"def delete_instance(self, instance_id, project_id=None):\n        \"\"\"\n        Deletes an existing Cloud Spanner instance.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).instance(instance_id)\n        try:\n            instance.delete()\n            return\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e","method_path":"airflow\/contrib\/hooks\/gcp_spanner_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSpannerHook.get_database","method_code":"def get_database(self, instance_id, database_id, project_id=None):\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        if not database.exists():\n            return None\n        else:\n            return database","method_summary":"Retrieves a database in Cloud Spanner. If the database does not exist in the specified instance, it returns None.","original_method_code":"def get_database(self, instance_id, database_id, project_id=None):\n        \"\"\"\n        Retrieves a database in Cloud Spanner. If the database does not exist\n        in the specified instance, it returns None.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: Database object or None if database does not exist\n        :rtype: google.cloud.spanner_v1.database.Database or None\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        if not database.exists():\n            return None\n        else:\n            return database","method_path":"airflow\/contrib\/hooks\/gcp_spanner_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSpannerHook.create_database","method_code":"def create_database(self, instance_id, database_id, ddl_statements, project_id=None):\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id,\n                                     ddl_statements=ddl_statements)\n        try:\n            operation = database.create()  \n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)\n        return","method_summary":"Creates a new database in Cloud Spanner.","original_method_code":"def create_database(self, instance_id, database_id, ddl_statements, project_id=None):\n        \"\"\"\n        Creates a new database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database to create in Cloud Spanner.\n        :type database_id: str\n        :param ddl_statements: The string list containing DDL for the new database.\n        :type ddl_statements: list[str]\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :return: None\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id,\n                                     ddl_statements=ddl_statements)\n        try:\n            operation = database.create()  # type: Operation\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)\n        return","method_path":"airflow\/contrib\/hooks\/gcp_spanner_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSpannerHook.update_database","method_code":"def update_database(self, instance_id, database_id, ddl_statements,\n                        project_id=None,\n                        operation_id=None):\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        try:\n            operation = database.update_ddl(\n                ddl_statements=ddl_statements, operation_id=operation_id)\n            if operation:\n                result = operation.result()\n                self.log.info(result)\n            return\n        except AlreadyExists as e:\n            if e.code == 409 and operation_id in e.message:\n                self.log.info(\"Replayed update_ddl message - the operation id %s \"\n                              \"was already done before.\", operation_id)\n                return\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e","method_summary":"Updates DDL of a database in Cloud Spanner.","original_method_code":"def update_database(self, instance_id, database_id, ddl_statements,\n                        project_id=None,\n                        operation_id=None):\n        \"\"\"\n        Updates DDL of a database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param ddl_statements: The string list containing DDL for the new database.\n        :type ddl_statements: list[str]\n        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :param operation_id: (Optional) The unique per database operation ID that can be\n            specified to implement idempotency check.\n        :type operation_id: str\n        :return: None\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        try:\n            operation = database.update_ddl(\n                ddl_statements=ddl_statements, operation_id=operation_id)\n            if operation:\n                result = operation.result()\n                self.log.info(result)\n            return\n        except AlreadyExists as e:\n            if e.code == 409 and operation_id in e.message:\n                self.log.info(\"Replayed update_ddl message - the operation id %s \"\n                              \"was already done before.\", operation_id)\n                return\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e","method_path":"airflow\/contrib\/hooks\/gcp_spanner_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSpannerHook.delete_database","method_code":"def delete_database(self, instance_id, database_id, project_id=None):\n        instance = self._get_client(project_id=project_id).\\\n            instance(instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        if not database.exists():\n            self.log.info(\"The database {} is already deleted from instance {}. \"\n                          \"Exiting.\".format(database_id, instance_id))\n            return\n        try:\n            operation = database.drop()  \n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)\n        return","method_summary":"Drops a database in Cloud Spanner.","original_method_code":"def delete_database(self, instance_id, database_id, project_id=None):\n        \"\"\"\n        Drops a database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :return: True if everything succeeded\n        :rtype: bool\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).\\\n            instance(instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        if not database.exists():\n            self.log.info(\"The database {} is already deleted from instance {}. \"\n                          \"Exiting.\".format(database_id, instance_id))\n            return\n        try:\n            operation = database.drop()  # type: Operation\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)\n        return","method_path":"airflow\/contrib\/hooks\/gcp_spanner_hook.py"}
{"repo_name":"apache\/airflow","method_name":"ImapAttachmentSensor.poke","method_code":"def poke(self, context):\n        self.log.info('Poking for %s', self.attachment_name)\n\n        with ImapHook(imap_conn_id=self.conn_id) as imap_hook:\n            return imap_hook.has_mail_attachment(\n                name=self.attachment_name,\n                mail_folder=self.mail_folder,\n                check_regex=self.check_regex\n            )","method_summary":"Pokes for a mail attachment on the mail server.","original_method_code":"def poke(self, context):\n        \"\"\"\n        Pokes for a mail attachment on the mail server.\n\n        :param context: The context that is being provided when poking.\n        :type context: dict\n        :return: True if attachment with the given name is present and False if not.\n        :rtype: bool\n        \"\"\"\n        self.log.info('Poking for %s', self.attachment_name)\n\n        with ImapHook(imap_conn_id=self.conn_id) as imap_hook:\n            return imap_hook.has_mail_attachment(\n                name=self.attachment_name,\n                mail_folder=self.mail_folder,\n                check_regex=self.check_regex\n            )","method_path":"airflow\/contrib\/sensors\/imap_attachment_sensor.py"}
{"repo_name":"apache\/airflow","method_name":"prepare_additional_parameters","method_code":"def prepare_additional_parameters(additional_properties, language_hints, web_detection_params):\n    if language_hints is None and web_detection_params is None:\n        return additional_properties\n\n    if additional_properties is None:\n        return {}\n\n    merged_additional_parameters = deepcopy(additional_properties)\n\n    if 'image_context' not in merged_additional_parameters:\n        merged_additional_parameters['image_context'] = {}\n\n    merged_additional_parameters['image_context']['language_hints'] = merged_additional_parameters[\n        'image_context'\n    ].get('language_hints', language_hints)\n    merged_additional_parameters['image_context']['web_detection_params'] = merged_additional_parameters[\n        'image_context'\n    ].get('web_detection_params', web_detection_params)\n\n    return merged_additional_parameters","method_summary":"Creates additional_properties parameter based on language_hints, web_detection_params and additional_properties parameters specified by the user","original_method_code":"def prepare_additional_parameters(additional_properties, language_hints, web_detection_params):\n    \"\"\"\n    Creates additional_properties parameter based on language_hints, web_detection_params and\n    additional_properties parameters specified by the user\n    \"\"\"\n    if language_hints is None and web_detection_params is None:\n        return additional_properties\n\n    if additional_properties is None:\n        return {}\n\n    merged_additional_parameters = deepcopy(additional_properties)\n\n    if 'image_context' not in merged_additional_parameters:\n        merged_additional_parameters['image_context'] = {}\n\n    merged_additional_parameters['image_context']['language_hints'] = merged_additional_parameters[\n        'image_context'\n    ].get('language_hints', language_hints)\n    merged_additional_parameters['image_context']['web_detection_params'] = merged_additional_parameters[\n        'image_context'\n    ].get('web_detection_params', web_detection_params)\n\n    return merged_additional_parameters","method_path":"airflow\/contrib\/operators\/gcp_vision_operator.py"}
{"repo_name":"apache\/airflow","method_name":"CassandraHook.table_exists","method_code":"def table_exists(self, table):\n        keyspace = self.keyspace\n        if '.' in table:\n            keyspace, table = table.split('.', 1)\n        cluster_metadata = self.get_conn().cluster.metadata\n        return (keyspace in cluster_metadata.keyspaces and\n                table in cluster_metadata.keyspaces[keyspace].tables)","method_summary":"Checks if a table exists in Cassandra","original_method_code":"def table_exists(self, table):\n        \"\"\"\n        Checks if a table exists in Cassandra\n\n        :param table: Target Cassandra table.\n                      Use dot notation to target a specific keyspace.\n        :type table: str\n        \"\"\"\n        keyspace = self.keyspace\n        if '.' in table:\n            keyspace, table = table.split('.', 1)\n        cluster_metadata = self.get_conn().cluster.metadata\n        return (keyspace in cluster_metadata.keyspaces and\n                table in cluster_metadata.keyspaces[keyspace].tables)","method_path":"airflow\/contrib\/hooks\/cassandra_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CassandraHook.record_exists","method_code":"def record_exists(self, table, keys):\n        keyspace = self.keyspace\n        if '.' in table:\n            keyspace, table = table.split('.', 1)\n        ks = \" AND \".join(\"{}=%({})s\".format(key, key) for key in keys.keys())\n        cql = \"SELECT * FROM {keyspace}.{table} WHERE {keys}\".format(\n            keyspace=keyspace, table=table, keys=ks)\n\n        try:\n            rs = self.get_conn().execute(cql, keys)\n            return rs.one() is not None\n        except Exception:\n            return False","method_summary":"Checks if a record exists in Cassandra","original_method_code":"def record_exists(self, table, keys):\n        \"\"\"\n        Checks if a record exists in Cassandra\n\n        :param table: Target Cassandra table.\n                      Use dot notation to target a specific keyspace.\n        :type table: str\n        :param keys: The keys and their values to check the existence.\n        :type keys: dict\n        \"\"\"\n        keyspace = self.keyspace\n        if '.' in table:\n            keyspace, table = table.split('.', 1)\n        ks = \" AND \".join(\"{}=%({})s\".format(key, key) for key in keys.keys())\n        cql = \"SELECT * FROM {keyspace}.{table} WHERE {keys}\".format(\n            keyspace=keyspace, table=table, keys=ks)\n\n        try:\n            rs = self.get_conn().execute(cql, keys)\n            return rs.one() is not None\n        except Exception:\n            return False","method_path":"airflow\/contrib\/hooks\/cassandra_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SparkSubmitHook._build_track_driver_status_command","method_code":"def _build_track_driver_status_command(self):\n        connection_cmd = self._get_spark_binary_path()\n\n        \n        connection_cmd += [\"--master\", self._connection['master']]\n\n        \n        if self._driver_id:\n            connection_cmd += [\"--status\", self._driver_id]\n        else:\n            raise AirflowException(\n                \"Invalid status: attempted to poll driver \" +\n                \"status but no driver id is known. Giving up.\")\n\n        self.log.debug(\"Poll driver status cmd: %s\", connection_cmd)\n\n        return connection_cmd","method_summary":"Construct the command to poll the driver status.","original_method_code":"def _build_track_driver_status_command(self):\n        \"\"\"\n        Construct the command to poll the driver status.\n\n        :return: full command to be executed\n        \"\"\"\n        connection_cmd = self._get_spark_binary_path()\n\n        # The url ot the spark master\n        connection_cmd += [\"--master\", self._connection['master']]\n\n        # The driver id so we can poll for its status\n        if self._driver_id:\n            connection_cmd += [\"--status\", self._driver_id]\n        else:\n            raise AirflowException(\n                \"Invalid status: attempted to poll driver \" +\n                \"status but no driver id is known. Giving up.\")\n\n        self.log.debug(\"Poll driver status cmd: %s\", connection_cmd)\n\n        return connection_cmd","method_path":"airflow\/contrib\/hooks\/spark_submit_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SparkSubmitHook.submit","method_code":"def submit(self, application=\"\", **kwargs):\n        spark_submit_cmd = self._build_spark_submit_command(application)\n\n        if hasattr(self, '_env'):\n            env = os.environ.copy()\n            env.update(self._env)\n            kwargs[\"env\"] = env\n\n        self._submit_sp = subprocess.Popen(spark_submit_cmd,\n                                           stdout=subprocess.PIPE,\n                                           stderr=subprocess.STDOUT,\n                                           bufsize=-1,\n                                           universal_newlines=True,\n                                           **kwargs)\n\n        self._process_spark_submit_log(iter(self._submit_sp.stdout.readline, ''))\n        returncode = self._submit_sp.wait()\n\n        \n        \n        if returncode or (self._is_kubernetes and self._spark_exit_code != 0):\n            raise AirflowException(\n                \"Cannot execute: {}. Error code is: {}.\".format(\n                    spark_submit_cmd, returncode\n                )\n            )\n\n        self.log.debug(\"Should track driver: {}\".format(self._should_track_driver_status))\n\n        \n        if self._should_track_driver_status:\n            if self._driver_id is None:\n                raise AirflowException(\n                    \"No driver id is known: something went wrong when executing \" +\n                    \"the spark submit command\"\n                )\n\n            \n            self._driver_status = \"SUBMITTED\"\n\n            \n            self._start_driver_status_tracking()\n\n            if self._driver_status != \"FINISHED\":\n                raise AirflowException(\n                    \"ERROR : Driver {} badly exited with status {}\"\n                    .format(self._driver_id, self._driver_status)\n                )","method_summary":"Remote Popen to execute the spark-submit job","original_method_code":"def submit(self, application=\"\", **kwargs):\n        \"\"\"\n        Remote Popen to execute the spark-submit job\n\n        :param application: Submitted application, jar or py file\n        :type application: str\n        :param kwargs: extra arguments to Popen (see subprocess.Popen)\n        \"\"\"\n        spark_submit_cmd = self._build_spark_submit_command(application)\n\n        if hasattr(self, '_env'):\n            env = os.environ.copy()\n            env.update(self._env)\n            kwargs[\"env\"] = env\n\n        self._submit_sp = subprocess.Popen(spark_submit_cmd,\n                                           stdout=subprocess.PIPE,\n                                           stderr=subprocess.STDOUT,\n                                           bufsize=-1,\n                                           universal_newlines=True,\n                                           **kwargs)\n\n        self._process_spark_submit_log(iter(self._submit_sp.stdout.readline, ''))\n        returncode = self._submit_sp.wait()\n\n        # Check spark-submit return code. In Kubernetes mode, also check the value\n        # of exit code in the log, as it may differ.\n        if returncode or (self._is_kubernetes and self._spark_exit_code != 0):\n            raise AirflowException(\n                \"Cannot execute: {}. Error code is: {}.\".format(\n                    spark_submit_cmd, returncode\n                )\n            )\n\n        self.log.debug(\"Should track driver: {}\".format(self._should_track_driver_status))\n\n        # We want the Airflow job to wait until the Spark driver is finished\n        if self._should_track_driver_status:\n            if self._driver_id is None:\n                raise AirflowException(\n                    \"No driver id is known: something went wrong when executing \" +\n                    \"the spark submit command\"\n                )\n\n            # We start with the SUBMITTED status as initial status\n            self._driver_status = \"SUBMITTED\"\n\n            # Start tracking the driver status (blocking function)\n            self._start_driver_status_tracking()\n\n            if self._driver_status != \"FINISHED\":\n                raise AirflowException(\n                    \"ERROR : Driver {} badly exited with status {}\"\n                    .format(self._driver_id, self._driver_status)\n                )","method_path":"airflow\/contrib\/hooks\/spark_submit_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SparkSubmitHook._process_spark_submit_log","method_code":"def _process_spark_submit_log(self, itr):\n        \n        for line in itr:\n            line = line.strip()\n            \n            \n            if self._is_yarn and self._connection['deploy_mode'] == 'cluster':\n                match = re.search('(application[0-9_]+)', line)\n                if match:\n                    self._yarn_application_id = match.groups()[0]\n                    self.log.info(\"Identified spark driver id: %s\",\n                                  self._yarn_application_id)\n\n            \n            \n            elif self._is_kubernetes:\n                match = re.search(r'\\s*pod name: ((.+?)-([a-z0-9]+)-driver)', line)\n                if match:\n                    self._kubernetes_driver_pod = match.groups()[0]\n                    self.log.info(\"Identified spark driver pod: %s\",\n                                  self._kubernetes_driver_pod)\n\n                \n                match_exit_code = re.search(r'\\s*Exit code: (\\d+)', line)\n                if match_exit_code:\n                    self._spark_exit_code = int(match_exit_code.groups()[0])\n\n            \n            \n            \n            elif self._should_track_driver_status and not self._driver_id:\n                match_driver_id = re.search(r'(driver-[0-9\\-]+)', line)\n                if match_driver_id:\n                    self._driver_id = match_driver_id.groups()[0]\n                    self.log.info(\"identified spark driver id: {}\"\n                                  .format(self._driver_id))\n\n            else:\n                self.log.info(line)\n\n            self.log.debug(\"spark submit log: {}\".format(line))","method_summary":"Processes the log files and extracts useful information out of it. If the deploy-mode is 'client', log the output of the submit command as those are the output logs of the Spark worker directly.","original_method_code":"def _process_spark_submit_log(self, itr):\n        \"\"\"\n        Processes the log files and extracts useful information out of it.\n\n        If the deploy-mode is 'client', log the output of the submit command as those\n        are the output logs of the Spark worker directly.\n\n        Remark: If the driver needs to be tracked for its status, the log-level of the\n        spark deploy needs to be at least INFO (log4j.logger.org.apache.spark.deploy=INFO)\n\n        :param itr: An iterator which iterates over the input of the subprocess\n        \"\"\"\n        # Consume the iterator\n        for line in itr:\n            line = line.strip()\n            # If we run yarn cluster mode, we want to extract the application id from\n            # the logs so we can kill the application when we stop it unexpectedly\n            if self._is_yarn and self._connection['deploy_mode'] == 'cluster':\n                match = re.search('(application[0-9_]+)', line)\n                if match:\n                    self._yarn_application_id = match.groups()[0]\n                    self.log.info(\"Identified spark driver id: %s\",\n                                  self._yarn_application_id)\n\n            # If we run Kubernetes cluster mode, we want to extract the driver pod id\n            # from the logs so we can kill the application when we stop it unexpectedly\n            elif self._is_kubernetes:\n                match = re.search(r'\\s*pod name: ((.+?)-([a-z0-9]+)-driver)', line)\n                if match:\n                    self._kubernetes_driver_pod = match.groups()[0]\n                    self.log.info(\"Identified spark driver pod: %s\",\n                                  self._kubernetes_driver_pod)\n\n                # Store the Spark Exit code\n                match_exit_code = re.search(r'\\s*Exit code: (\\d+)', line)\n                if match_exit_code:\n                    self._spark_exit_code = int(match_exit_code.groups()[0])\n\n            # if we run in standalone cluster mode and we want to track the driver status\n            # we need to extract the driver id from the logs. This allows us to poll for\n            # the status using the driver id. Also, we can kill the driver when needed.\n            elif self._should_track_driver_status and not self._driver_id:\n                match_driver_id = re.search(r'(driver-[0-9\\-]+)', line)\n                if match_driver_id:\n                    self._driver_id = match_driver_id.groups()[0]\n                    self.log.info(\"identified spark driver id: {}\"\n                                  .format(self._driver_id))\n\n            else:\n                self.log.info(line)\n\n            self.log.debug(\"spark submit log: {}\".format(line))","method_path":"airflow\/contrib\/hooks\/spark_submit_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SparkSubmitHook._process_spark_status_log","method_code":"def _process_spark_status_log(self, itr):\n        \n        for line in itr:\n            line = line.strip()\n\n            \n            if \"driverState\" in line:\n                self._driver_status = line.split(' : ')[1] \\\n                    .replace(',', '').replace('\\\"', '').strip()\n\n            self.log.debug(\"spark driver status log: {}\".format(line))","method_summary":"parses the logs of the spark driver status query process","original_method_code":"def _process_spark_status_log(self, itr):\n        \"\"\"\n        parses the logs of the spark driver status query process\n\n        :param itr: An iterator which iterates over the input of the subprocess\n        \"\"\"\n        # Consume the iterator\n        for line in itr:\n            line = line.strip()\n\n            # Check if the log line is about the driver status and extract the status.\n            if \"driverState\" in line:\n                self._driver_status = line.split(' : ')[1] \\\n                    .replace(',', '').replace('\\\"', '').strip()\n\n            self.log.debug(\"spark driver status log: {}\".format(line))","method_path":"airflow\/contrib\/hooks\/spark_submit_hook.py"}
{"repo_name":"apache\/airflow","method_name":"get_task_runner","method_code":"def get_task_runner(local_task_job):\n    if _TASK_RUNNER == \"StandardTaskRunner\":\n        return StandardTaskRunner(local_task_job)\n    elif _TASK_RUNNER == \"CgroupTaskRunner\":\n        from airflow.contrib.task_runner.cgroup_task_runner import CgroupTaskRunner\n        return CgroupTaskRunner(local_task_job)\n    else:\n        raise AirflowException(\"Unknown task runner type {}\".format(_TASK_RUNNER))","method_summary":"Get the task runner that can be used to run the given job.","original_method_code":"def get_task_runner(local_task_job):\n    \"\"\"\n    Get the task runner that can be used to run the given job.\n\n    :param local_task_job: The LocalTaskJob associated with the TaskInstance\n        that needs to be executed.\n    :type local_task_job: airflow.jobs.LocalTaskJob\n    :return: The task runner to use to run the task.\n    :rtype: airflow.task.task_runner.base_task_runner.BaseTaskRunner\n    \"\"\"\n    if _TASK_RUNNER == \"StandardTaskRunner\":\n        return StandardTaskRunner(local_task_job)\n    elif _TASK_RUNNER == \"CgroupTaskRunner\":\n        from airflow.contrib.task_runner.cgroup_task_runner import CgroupTaskRunner\n        return CgroupTaskRunner(local_task_job)\n    else:\n        raise AirflowException(\"Unknown task runner type {}\".format(_TASK_RUNNER))","method_path":"airflow\/task\/task_runner\/__init__.py"}
{"repo_name":"apache\/airflow","method_name":"AWSBatchOperator._wait_for_task_ended","method_code":"def _wait_for_task_ended(self):\n        try:\n            waiter = self.client.get_waiter('job_execution_complete')\n            waiter.config.max_attempts = sys.maxsize  \n            waiter.wait(jobs=[self.jobId])\n        except ValueError:\n            \n            retry = True\n            retries = 0\n\n            while retries < self.max_retries and retry:\n                self.log.info('AWS Batch retry in the next %s seconds', retries)\n                response = self.client.describe_jobs(\n                    jobs=[self.jobId]\n                )\n                if response['jobs'][-1]['status'] in ['SUCCEEDED', 'FAILED']:\n                    retry = False\n\n                sleep(1 + pow(retries * 0.1, 2))\n                retries += 1","method_summary":"Try to use a waiter from the below pull request","original_method_code":"def _wait_for_task_ended(self):\n        \"\"\"\n        Try to use a waiter from the below pull request\n\n            * https:\/\/github.com\/boto\/botocore\/pull\/1307\n\n        If the waiter is not available apply a exponential backoff\n\n            * docs.aws.amazon.com\/general\/latest\/gr\/api-retries.html\n        \"\"\"\n        try:\n            waiter = self.client.get_waiter('job_execution_complete')\n            waiter.config.max_attempts = sys.maxsize  # timeout is managed by airflow\n            waiter.wait(jobs=[self.jobId])\n        except ValueError:\n            # If waiter not available use expo\n            retry = True\n            retries = 0\n\n            while retries < self.max_retries and retry:\n                self.log.info('AWS Batch retry in the next %s seconds', retries)\n                response = self.client.describe_jobs(\n                    jobs=[self.jobId]\n                )\n                if response['jobs'][-1]['status'] in ['SUCCEEDED', 'FAILED']:\n                    retry = False\n\n                sleep(1 + pow(retries * 0.1, 2))\n                retries += 1","method_path":"airflow\/contrib\/operators\/awsbatch_operator.py"}
{"repo_name":"apache\/airflow","method_name":"MySqlToGoogleCloudStorageOperator._query_mysql","method_code":"def _query_mysql(self):\n        mysql = MySqlHook(mysql_conn_id=self.mysql_conn_id)\n        conn = mysql.get_conn()\n        cursor = conn.cursor()\n        cursor.execute(self.sql)\n        return cursor","method_summary":"Queries mysql and returns a cursor to the results.","original_method_code":"def _query_mysql(self):\n        \"\"\"\n        Queries mysql and returns a cursor to the results.\n        \"\"\"\n        mysql = MySqlHook(mysql_conn_id=self.mysql_conn_id)\n        conn = mysql.get_conn()\n        cursor = conn.cursor()\n        cursor.execute(self.sql)\n        return cursor","method_path":"airflow\/contrib\/operators\/mysql_to_gcs.py"}
{"repo_name":"apache\/airflow","method_name":"MySqlToGoogleCloudStorageOperator._configure_csv_file","method_code":"def _configure_csv_file(self, file_handle, schema):\n        csv_writer = csv.writer(file_handle, encoding='utf-8',\n                                delimiter=self.field_delimiter)\n        csv_writer.writerow(schema)\n        return csv_writer","method_summary":"Configure a csv writer with the file_handle and write schema as headers for the new file.","original_method_code":"def _configure_csv_file(self, file_handle, schema):\n        \"\"\"Configure a csv writer with the file_handle and write schema\n        as headers for the new file.\n        \"\"\"\n        csv_writer = csv.writer(file_handle, encoding='utf-8',\n                                delimiter=self.field_delimiter)\n        csv_writer.writerow(schema)\n        return csv_writer","method_path":"airflow\/contrib\/operators\/mysql_to_gcs.py"}
{"repo_name":"apache\/airflow","method_name":"MySqlToGoogleCloudStorageOperator._write_local_schema_file","method_code":"def _write_local_schema_file(self, cursor):\n        schema_str = None\n        schema_file_mime_type = 'application\/json'\n        tmp_schema_file_handle = NamedTemporaryFile(delete=True)\n        if self.schema is not None and isinstance(self.schema, string_types):\n            schema_str = self.schema.encode('utf-8')\n        elif self.schema is not None and isinstance(self.schema, list):\n            schema_str = json.dumps(self.schema).encode('utf-8')\n        else:\n            schema = []\n            for field in cursor.description:\n                \n                field_name = field[0]\n                field_type = self.type_map(field[1])\n                \n                \n                \n                if field[6] or field_type == 'TIMESTAMP':\n                    field_mode = 'NULLABLE'\n                else:\n                    field_mode = 'REQUIRED'\n                schema.append({\n                    'name': field_name,\n                    'type': field_type,\n                    'mode': field_mode,\n                })\n            schema_str = json.dumps(schema, sort_keys=True).encode('utf-8')\n        tmp_schema_file_handle.write(schema_str)\n\n        self.log.info('Using schema for %s: %s', self.schema_filename, schema_str)\n        schema_file_to_upload = {\n            'file_name': self.schema_filename,\n            'file_handle': tmp_schema_file_handle,\n            'file_mime_type': schema_file_mime_type\n        }\n        return schema_file_to_upload","method_summary":"Takes a cursor, and writes the BigQuery schema in .json format for the results to a local file system.","original_method_code":"def _write_local_schema_file(self, cursor):\n        \"\"\"\n        Takes a cursor, and writes the BigQuery schema in .json format for the\n        results to a local file system.\n\n        :return: A dictionary where key is a filename to be used as an object\n            name in GCS, and values are file handles to local files that\n            contains the BigQuery schema fields in .json format.\n        \"\"\"\n        schema_str = None\n        schema_file_mime_type = 'application\/json'\n        tmp_schema_file_handle = NamedTemporaryFile(delete=True)\n        if self.schema is not None and isinstance(self.schema, string_types):\n            schema_str = self.schema.encode('utf-8')\n        elif self.schema is not None and isinstance(self.schema, list):\n            schema_str = json.dumps(self.schema).encode('utf-8')\n        else:\n            schema = []\n            for field in cursor.description:\n                # See PEP 249 for details about the description tuple.\n                field_name = field[0]\n                field_type = self.type_map(field[1])\n                # Always allow TIMESTAMP to be nullable. MySQLdb returns None types\n                # for required fields because some MySQL timestamps can't be\n                # represented by Python's datetime (e.g. 0000-00-00 00:00:00).\n                if field[6] or field_type == 'TIMESTAMP':\n                    field_mode = 'NULLABLE'\n                else:\n                    field_mode = 'REQUIRED'\n                schema.append({\n                    'name': field_name,\n                    'type': field_type,\n                    'mode': field_mode,\n                })\n            schema_str = json.dumps(schema, sort_keys=True).encode('utf-8')\n        tmp_schema_file_handle.write(schema_str)\n\n        self.log.info('Using schema for %s: %s', self.schema_filename, schema_str)\n        schema_file_to_upload = {\n            'file_name': self.schema_filename,\n            'file_handle': tmp_schema_file_handle,\n            'file_mime_type': schema_file_mime_type\n        }\n        return schema_file_to_upload","method_path":"airflow\/contrib\/operators\/mysql_to_gcs.py"}
{"repo_name":"apache\/airflow","method_name":"MySqlToGoogleCloudStorageOperator._get_col_type_dict","method_code":"def _get_col_type_dict(self):\n        schema = []\n        if isinstance(self.schema, string_types):\n            schema = json.loads(self.schema)\n        elif isinstance(self.schema, list):\n            schema = self.schema\n        elif self.schema is not None:\n            self.log.warn('Using default schema due to unexpected type.'\n                          'Should be a string or list.')\n\n        col_type_dict = {}\n        try:\n            col_type_dict = {col['name']: col['type'] for col in schema}\n        except KeyError:\n            self.log.warn('Using default schema due to missing name or type. Please '\n                          'refer to: https:\/\/cloud.google.com\/bigquery\/docs\/schemas'\n                          '#specifying_a_json_schema_file')\n        return col_type_dict","method_summary":"Return a dict of column name and column type based on self.schema if not None.","original_method_code":"def _get_col_type_dict(self):\n        \"\"\"\n        Return a dict of column name and column type based on self.schema if not None.\n        \"\"\"\n        schema = []\n        if isinstance(self.schema, string_types):\n            schema = json.loads(self.schema)\n        elif isinstance(self.schema, list):\n            schema = self.schema\n        elif self.schema is not None:\n            self.log.warn('Using default schema due to unexpected type.'\n                          'Should be a string or list.')\n\n        col_type_dict = {}\n        try:\n            col_type_dict = {col['name']: col['type'] for col in schema}\n        except KeyError:\n            self.log.warn('Using default schema due to missing name or type. Please '\n                          'refer to: https:\/\/cloud.google.com\/bigquery\/docs\/schemas'\n                          '#specifying_a_json_schema_file')\n        return col_type_dict","method_path":"airflow\/contrib\/operators\/mysql_to_gcs.py"}
{"repo_name":"apache\/airflow","method_name":"MySqlToGoogleCloudStorageOperator.type_map","method_code":"def type_map(cls, mysql_type):\n        d = {\n            FIELD_TYPE.INT24: 'INTEGER',\n            FIELD_TYPE.TINY: 'INTEGER',\n            FIELD_TYPE.BIT: 'INTEGER',\n            FIELD_TYPE.DATETIME: 'TIMESTAMP',\n            FIELD_TYPE.DATE: 'TIMESTAMP',\n            FIELD_TYPE.DECIMAL: 'FLOAT',\n            FIELD_TYPE.NEWDECIMAL: 'FLOAT',\n            FIELD_TYPE.DOUBLE: 'FLOAT',\n            FIELD_TYPE.FLOAT: 'FLOAT',\n            FIELD_TYPE.LONG: 'INTEGER',\n            FIELD_TYPE.LONGLONG: 'INTEGER',\n            FIELD_TYPE.SHORT: 'INTEGER',\n            FIELD_TYPE.TIMESTAMP: 'TIMESTAMP',\n            FIELD_TYPE.YEAR: 'INTEGER',\n        }\n        return d[mysql_type] if mysql_type in d else 'STRING'","method_summary":"Helper function that maps from MySQL fields to BigQuery fields. Used when a schema_filename is set.","original_method_code":"def type_map(cls, mysql_type):\n        \"\"\"\n        Helper function that maps from MySQL fields to BigQuery fields. Used\n        when a schema_filename is set.\n        \"\"\"\n        d = {\n            FIELD_TYPE.INT24: 'INTEGER',\n            FIELD_TYPE.TINY: 'INTEGER',\n            FIELD_TYPE.BIT: 'INTEGER',\n            FIELD_TYPE.DATETIME: 'TIMESTAMP',\n            FIELD_TYPE.DATE: 'TIMESTAMP',\n            FIELD_TYPE.DECIMAL: 'FLOAT',\n            FIELD_TYPE.NEWDECIMAL: 'FLOAT',\n            FIELD_TYPE.DOUBLE: 'FLOAT',\n            FIELD_TYPE.FLOAT: 'FLOAT',\n            FIELD_TYPE.LONG: 'INTEGER',\n            FIELD_TYPE.LONGLONG: 'INTEGER',\n            FIELD_TYPE.SHORT: 'INTEGER',\n            FIELD_TYPE.TIMESTAMP: 'TIMESTAMP',\n            FIELD_TYPE.YEAR: 'INTEGER',\n        }\n        return d[mysql_type] if mysql_type in d else 'STRING'","method_path":"airflow\/contrib\/operators\/mysql_to_gcs.py"}
{"repo_name":"apache\/airflow","method_name":"SqoopOperator.execute","method_code":"def execute(self, context):\n        self.hook = SqoopHook(\n            conn_id=self.conn_id,\n            verbose=self.verbose,\n            num_mappers=self.num_mappers,\n            hcatalog_database=self.hcatalog_database,\n            hcatalog_table=self.hcatalog_table,\n            properties=self.properties\n        )\n\n        if self.cmd_type == 'export':\n            self.hook.export_table(\n                table=self.table,\n                export_dir=self.export_dir,\n                input_null_string=self.input_null_string,\n                input_null_non_string=self.input_null_non_string,\n                staging_table=self.staging_table,\n                clear_staging_table=self.clear_staging_table,\n                enclosed_by=self.enclosed_by,\n                escaped_by=self.escaped_by,\n                input_fields_terminated_by=self.input_fields_terminated_by,\n                input_lines_terminated_by=self.input_lines_terminated_by,\n                input_optionally_enclosed_by=self.input_optionally_enclosed_by,\n                batch=self.batch,\n                relaxed_isolation=self.relaxed_isolation,\n                extra_export_options=self.extra_export_options)\n        elif self.cmd_type == 'import':\n            \n            \n            \n            if self.create_hcatalog_table:\n                self.extra_import_options['create-hcatalog-table'] = ''\n\n            if self.table and self.query:\n                raise AirflowException(\n                    'Cannot specify query and table together. Need to specify either or.'\n                )\n\n            if self.table:\n                self.hook.import_table(\n                    table=self.table,\n                    target_dir=self.target_dir,\n                    append=self.append,\n                    file_type=self.file_type,\n                    columns=self.columns,\n                    split_by=self.split_by,\n                    where=self.where,\n                    direct=self.direct,\n                    driver=self.driver,\n                    extra_import_options=self.extra_import_options)\n            elif self.query:\n                self.hook.import_query(\n                    query=self.query,\n                    target_dir=self.target_dir,\n                    append=self.append,\n                    file_type=self.file_type,\n                    split_by=self.split_by,\n                    direct=self.direct,\n                    driver=self.driver,\n                    extra_import_options=self.extra_import_options)\n            else:\n                raise AirflowException(\n                    \"Provide query or table parameter to import using Sqoop\"\n                )\n        else:\n            raise AirflowException(\"cmd_type should be 'import' or 'export'\")","method_summary":"Execute sqoop job","original_method_code":"def execute(self, context):\n        \"\"\"\n        Execute sqoop job\n        \"\"\"\n        self.hook = SqoopHook(\n            conn_id=self.conn_id,\n            verbose=self.verbose,\n            num_mappers=self.num_mappers,\n            hcatalog_database=self.hcatalog_database,\n            hcatalog_table=self.hcatalog_table,\n            properties=self.properties\n        )\n\n        if self.cmd_type == 'export':\n            self.hook.export_table(\n                table=self.table,\n                export_dir=self.export_dir,\n                input_null_string=self.input_null_string,\n                input_null_non_string=self.input_null_non_string,\n                staging_table=self.staging_table,\n                clear_staging_table=self.clear_staging_table,\n                enclosed_by=self.enclosed_by,\n                escaped_by=self.escaped_by,\n                input_fields_terminated_by=self.input_fields_terminated_by,\n                input_lines_terminated_by=self.input_lines_terminated_by,\n                input_optionally_enclosed_by=self.input_optionally_enclosed_by,\n                batch=self.batch,\n                relaxed_isolation=self.relaxed_isolation,\n                extra_export_options=self.extra_export_options)\n        elif self.cmd_type == 'import':\n            # add create hcatalog table to extra import options if option passed\n            # if new params are added to constructor can pass them in here\n            # so don't modify sqoop_hook for each param\n            if self.create_hcatalog_table:\n                self.extra_import_options['create-hcatalog-table'] = ''\n\n            if self.table and self.query:\n                raise AirflowException(\n                    'Cannot specify query and table together. Need to specify either or.'\n                )\n\n            if self.table:\n                self.hook.import_table(\n                    table=self.table,\n                    target_dir=self.target_dir,\n                    append=self.append,\n                    file_type=self.file_type,\n                    columns=self.columns,\n                    split_by=self.split_by,\n                    where=self.where,\n                    direct=self.direct,\n                    driver=self.driver,\n                    extra_import_options=self.extra_import_options)\n            elif self.query:\n                self.hook.import_query(\n                    query=self.query,\n                    target_dir=self.target_dir,\n                    append=self.append,\n                    file_type=self.file_type,\n                    split_by=self.split_by,\n                    direct=self.direct,\n                    driver=self.driver,\n                    extra_import_options=self.extra_import_options)\n            else:\n                raise AirflowException(\n                    \"Provide query or table parameter to import using Sqoop\"\n                )\n        else:\n            raise AirflowException(\"cmd_type should be 'import' or 'export'\")","method_path":"airflow\/contrib\/operators\/sqoop_operator.py"}
{"repo_name":"apache\/airflow","method_name":"apply_lineage","method_code":"def apply_lineage(func):\n    backend = _get_backend()\n\n    @wraps(func)\n    def wrapper(self, context, *args, **kwargs):\n        self.log.debug(\"Backend: %s, Lineage called with inlets: %s, outlets: %s\",\n                       backend, self.inlets, self.outlets)\n        ret_val = func(self, context, *args, **kwargs)\n\n        outlets = [x.as_dict() for x in self.outlets]\n        inlets = [x.as_dict() for x in self.inlets]\n\n        if len(self.outlets) > 0:\n            self.xcom_push(context,\n                           key=PIPELINE_OUTLETS,\n                           value=outlets,\n                           execution_date=context['ti'].execution_date)\n\n        if len(self.inlets) > 0:\n            self.xcom_push(context,\n                           key=PIPELINE_INLETS,\n                           value=inlets,\n                           execution_date=context['ti'].execution_date)\n\n        if backend:\n            backend.send_lineage(operator=self, inlets=self.inlets,\n                                 outlets=self.outlets, context=context)\n\n        return ret_val\n\n    return wrapper","method_summary":"Saves the lineage to XCom and if configured to do so sends it to the backend.","original_method_code":"def apply_lineage(func):\n    \"\"\"\n    Saves the lineage to XCom and if configured to do so sends it\n    to the backend.\n    \"\"\"\n    backend = _get_backend()\n\n    @wraps(func)\n    def wrapper(self, context, *args, **kwargs):\n        self.log.debug(\"Backend: %s, Lineage called with inlets: %s, outlets: %s\",\n                       backend, self.inlets, self.outlets)\n        ret_val = func(self, context, *args, **kwargs)\n\n        outlets = [x.as_dict() for x in self.outlets]\n        inlets = [x.as_dict() for x in self.inlets]\n\n        if len(self.outlets) > 0:\n            self.xcom_push(context,\n                           key=PIPELINE_OUTLETS,\n                           value=outlets,\n                           execution_date=context['ti'].execution_date)\n\n        if len(self.inlets) > 0:\n            self.xcom_push(context,\n                           key=PIPELINE_INLETS,\n                           value=inlets,\n                           execution_date=context['ti'].execution_date)\n\n        if backend:\n            backend.send_lineage(operator=self, inlets=self.inlets,\n                                 outlets=self.outlets, context=context)\n\n        return ret_val\n\n    return wrapper","method_path":"airflow\/lineage\/__init__.py"}
{"repo_name":"apache\/airflow","method_name":"date_range","method_code":"def date_range(start_date, end_date=None, num=None, delta=None):\n    if not delta:\n        return []\n    if end_date and start_date > end_date:\n        raise Exception(\"Wait. start_date needs to be before end_date\")\n    if end_date and num:\n        raise Exception(\"Wait. Either specify end_date OR num\")\n    if not end_date and not num:\n        end_date = timezone.utcnow()\n\n    delta_iscron = False\n    tz = start_date.tzinfo\n    if isinstance(delta, six.string_types):\n        delta_iscron = True\n        start_date = timezone.make_naive(start_date, tz)\n        cron = croniter(delta, start_date)\n    elif isinstance(delta, timedelta):\n        delta = abs(delta)\n    dates = []\n    if end_date:\n        if timezone.is_naive(start_date):\n            end_date = timezone.make_naive(end_date, tz)\n        while start_date <= end_date:\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                start_date = cron.get_next(datetime)\n            else:\n                start_date += delta\n    else:\n        for _ in range(abs(num)):\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                if num > 0:\n                    start_date = cron.get_next(datetime)\n                else:\n                    start_date = cron.get_prev(datetime)\n            else:\n                if num > 0:\n                    start_date += delta\n                else:\n                    start_date -= delta\n    return sorted(dates)","method_summary":"Get a set of dates as a list based on a start, end and delta, delta can be something that can be added to `datetime.datetime` or a cron expression as a `str`","original_method_code":"def date_range(start_date, end_date=None, num=None, delta=None):\n    \"\"\"\n    Get a set of dates as a list based on a start, end and delta, delta\n    can be something that can be added to `datetime.datetime`\n    or a cron expression as a `str`\n\n    :Example::\n\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta=timedelta(1))\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta='0 0 * * *')\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 3, 3), delta=\"0 0 0 * *\")\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 2, 1, 0, 0),\n            datetime.datetime(2016, 3, 1, 0, 0)]\n\n    :param start_date: anchor date to start the series from\n    :type start_date: datetime.datetime\n    :param end_date: right boundary for the date range\n    :type end_date: datetime.datetime\n    :param num: alternatively to end_date, you can specify the number of\n        number of entries you want in the range. This number can be negative,\n        output will always be sorted regardless\n    :type num: int\n    \"\"\"\n    if not delta:\n        return []\n    if end_date and start_date > end_date:\n        raise Exception(\"Wait. start_date needs to be before end_date\")\n    if end_date and num:\n        raise Exception(\"Wait. Either specify end_date OR num\")\n    if not end_date and not num:\n        end_date = timezone.utcnow()\n\n    delta_iscron = False\n    tz = start_date.tzinfo\n    if isinstance(delta, six.string_types):\n        delta_iscron = True\n        start_date = timezone.make_naive(start_date, tz)\n        cron = croniter(delta, start_date)\n    elif isinstance(delta, timedelta):\n        delta = abs(delta)\n    dates = []\n    if end_date:\n        if timezone.is_naive(start_date):\n            end_date = timezone.make_naive(end_date, tz)\n        while start_date <= end_date:\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                start_date = cron.get_next(datetime)\n            else:\n                start_date += delta\n    else:\n        for _ in range(abs(num)):\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                if num > 0:\n                    start_date = cron.get_next(datetime)\n                else:\n                    start_date = cron.get_prev(datetime)\n            else:\n                if num > 0:\n                    start_date += delta\n                else:\n                    start_date -= delta\n    return sorted(dates)","method_path":"airflow\/utils\/dates.py"}
{"repo_name":"apache\/airflow","method_name":"scale_time_units","method_code":"def scale_time_units(time_seconds_arr, unit):\n    if unit == 'minutes':\n        return list(map(lambda x: x * 1.0 \/ 60, time_seconds_arr))\n    elif unit == 'hours':\n        return list(map(lambda x: x * 1.0 \/ (60 * 60), time_seconds_arr))\n    elif unit == 'days':\n        return list(map(lambda x: x * 1.0 \/ (24 * 60 * 60), time_seconds_arr))\n    return time_seconds_arr","method_summary":"Convert an array of time durations in seconds to the specified time unit.","original_method_code":"def scale_time_units(time_seconds_arr, unit):\n    \"\"\"\n    Convert an array of time durations in seconds to the specified time unit.\n    \"\"\"\n    if unit == 'minutes':\n        return list(map(lambda x: x * 1.0 \/ 60, time_seconds_arr))\n    elif unit == 'hours':\n        return list(map(lambda x: x * 1.0 \/ (60 * 60), time_seconds_arr))\n    elif unit == 'days':\n        return list(map(lambda x: x * 1.0 \/ (24 * 60 * 60), time_seconds_arr))\n    return time_seconds_arr","method_path":"airflow\/utils\/dates.py"}
{"repo_name":"apache\/airflow","method_name":"days_ago","method_code":"def days_ago(n, hour=0, minute=0, second=0, microsecond=0):\n    today = timezone.utcnow().replace(\n        hour=hour,\n        minute=minute,\n        second=second,\n        microsecond=microsecond)\n    return today - timedelta(days=n)","method_summary":"Get a datetime object representing `n` days ago. By default the time is set to midnight.","original_method_code":"def days_ago(n, hour=0, minute=0, second=0, microsecond=0):\n    \"\"\"\n    Get a datetime object representing `n` days ago. By default the time is\n    set to midnight.\n    \"\"\"\n    today = timezone.utcnow().replace(\n        hour=hour,\n        minute=minute,\n        second=second,\n        microsecond=microsecond)\n    return today - timedelta(days=n)","method_path":"airflow\/utils\/dates.py"}
{"repo_name":"apache\/airflow","method_name":"AirflowSecurityManager.init_role","method_code":"def init_role(self, role_name, role_vms, role_perms):\n        pvms = self.get_session.query(sqla_models.PermissionView).all()\n        pvms = [p for p in pvms if p.permission and p.view_menu]\n\n        role = self.find_role(role_name)\n        if not role:\n            role = self.add_role(role_name)\n\n        if len(role.permissions) == 0:\n            self.log.info('Initializing permissions for role:%s in the database.', role_name)\n            role_pvms = set()\n            for pvm in pvms:\n                if pvm.view_menu.name in role_vms and pvm.permission.name in role_perms:\n                    role_pvms.add(pvm)\n            role.permissions = list(role_pvms)\n            self.get_session.merge(role)\n            self.get_session.commit()\n        else:\n            self.log.debug('Existing permissions for the role:%s '\n                           'within the database will persist.', role_name)","method_summary":"Initialize the role with the permissions and related view-menus.","original_method_code":"def init_role(self, role_name, role_vms, role_perms):\n        \"\"\"\n        Initialize the role with the permissions and related view-menus.\n\n        :param role_name:\n        :param role_vms:\n        :param role_perms:\n        :return:\n        \"\"\"\n        pvms = self.get_session.query(sqla_models.PermissionView).all()\n        pvms = [p for p in pvms if p.permission and p.view_menu]\n\n        role = self.find_role(role_name)\n        if not role:\n            role = self.add_role(role_name)\n\n        if len(role.permissions) == 0:\n            self.log.info('Initializing permissions for role:%s in the database.', role_name)\n            role_pvms = set()\n            for pvm in pvms:\n                if pvm.view_menu.name in role_vms and pvm.permission.name in role_perms:\n                    role_pvms.add(pvm)\n            role.permissions = list(role_pvms)\n            self.get_session.merge(role)\n            self.get_session.commit()\n        else:\n            self.log.debug('Existing permissions for the role:%s '\n                           'within the database will persist.', role_name)","method_path":"airflow\/www\/security.py"}
{"repo_name":"apache\/airflow","method_name":"AirflowSecurityManager.delete_role","method_code":"def delete_role(self, role_name):\n        session = self.get_session\n        role = session.query(sqla_models.Role)\\\n                      .filter(sqla_models.Role.name == role_name)\\\n                      .first()\n        if role:\n            self.log.info(\"Deleting role '%s'\", role_name)\n            session.delete(role)\n            session.commit()\n        else:\n            raise AirflowException(\"Role named '{}' does not exist\".format(\n                role_name))","method_summary":"Delete the given Role","original_method_code":"def delete_role(self, role_name):\n        \"\"\"Delete the given Role\n\n        :param role_name: the name of a role in the ab_role table\n        \"\"\"\n        session = self.get_session\n        role = session.query(sqla_models.Role)\\\n                      .filter(sqla_models.Role.name == role_name)\\\n                      .first()\n        if role:\n            self.log.info(\"Deleting role '%s'\", role_name)\n            session.delete(role)\n            session.commit()\n        else:\n            raise AirflowException(\"Role named '{}' does not exist\".format(\n                role_name))","method_path":"airflow\/www\/security.py"}
{"repo_name":"apache\/airflow","method_name":"AirflowSecurityManager.get_user_roles","method_code":"def get_user_roles(self, user=None):\n        if user is None:\n            user = g.user\n        if user.is_anonymous:\n            public_role = appbuilder.config.get('AUTH_ROLE_PUBLIC')\n            return [appbuilder.security_manager.find_role(public_role)] \\\n                if public_role else []\n        return user.roles","method_summary":"Get all the roles associated with the user.","original_method_code":"def get_user_roles(self, user=None):\n        \"\"\"\n        Get all the roles associated with the user.\n\n        :param user: the ab_user in FAB model.\n        :return: a list of roles associated with the user.\n        \"\"\"\n        if user is None:\n            user = g.user\n        if user.is_anonymous:\n            public_role = appbuilder.config.get('AUTH_ROLE_PUBLIC')\n            return [appbuilder.security_manager.find_role(public_role)] \\\n                if public_role else []\n        return user.roles","method_path":"airflow\/www\/security.py"}
{"repo_name":"apache\/airflow","method_name":"AirflowSecurityManager._has_role","method_code":"def _has_role(self, role_name_or_list):\n        if not isinstance(role_name_or_list, list):\n            role_name_or_list = [role_name_or_list]\n        return any(\n            [r.name in role_name_or_list for r in self.get_user_roles()])","method_summary":"Whether the user has this role name","original_method_code":"def _has_role(self, role_name_or_list):\n        \"\"\"\n        Whether the user has this role name\n        \"\"\"\n        if not isinstance(role_name_or_list, list):\n            role_name_or_list = [role_name_or_list]\n        return any(\n            [r.name in role_name_or_list for r in self.get_user_roles()])","method_path":"airflow\/www\/security.py"}
{"repo_name":"apache\/airflow","method_name":"AirflowSecurityManager._has_perm","method_code":"def _has_perm(self, permission_name, view_menu_name):\n        if hasattr(self, 'perms'):\n            if (permission_name, view_menu_name) in self.perms:\n                return True\n        \n        self._get_and_cache_perms()\n        return (permission_name, view_menu_name) in self.perms","method_summary":"Whether the user has this perm","original_method_code":"def _has_perm(self, permission_name, view_menu_name):\n        \"\"\"\n        Whether the user has this perm\n        \"\"\"\n        if hasattr(self, 'perms'):\n            if (permission_name, view_menu_name) in self.perms:\n                return True\n        # rebuild the permissions set\n        self._get_and_cache_perms()\n        return (permission_name, view_menu_name) in self.perms","method_path":"airflow\/www\/security.py"}
{"repo_name":"apache\/airflow","method_name":"AirflowSecurityManager.clean_perms","method_code":"def clean_perms(self):\n        self.log.debug('Cleaning faulty perms')\n        sesh = self.get_session\n        pvms = (\n            sesh.query(sqla_models.PermissionView)\n            .filter(or_(\n                sqla_models.PermissionView.permission == None,  \n                sqla_models.PermissionView.view_menu == None,  \n            ))\n        )\n        deleted_count = pvms.delete()\n        sesh.commit()\n        if deleted_count:\n            self.log.info('Deleted %s faulty permissions', deleted_count)","method_summary":"FAB leaves faulty permissions that need to be cleaned up","original_method_code":"def clean_perms(self):\n        \"\"\"\n        FAB leaves faulty permissions that need to be cleaned up\n        \"\"\"\n        self.log.debug('Cleaning faulty perms')\n        sesh = self.get_session\n        pvms = (\n            sesh.query(sqla_models.PermissionView)\n            .filter(or_(\n                sqla_models.PermissionView.permission == None,  # NOQA\n                sqla_models.PermissionView.view_menu == None,  # NOQA\n            ))\n        )\n        deleted_count = pvms.delete()\n        sesh.commit()\n        if deleted_count:\n            self.log.info('Deleted %s faulty permissions', deleted_count)","method_path":"airflow\/www\/security.py"}
{"repo_name":"apache\/airflow","method_name":"AirflowSecurityManager._merge_perm","method_code":"def _merge_perm(self, permission_name, view_menu_name):\n        permission = self.find_permission(permission_name)\n        view_menu = self.find_view_menu(view_menu_name)\n        pv = None\n        if permission and view_menu:\n            pv = self.get_session.query(self.permissionview_model).filter_by(\n                permission=permission, view_menu=view_menu).first()\n        if not pv and permission_name and view_menu_name:\n            self.add_permission_view_menu(permission_name, view_menu_name)","method_summary":"Add the new permission , view_menu to ab_permission_view_role if not exists. It will add the related entry to ab_permission and ab_view_menu two meta tables as well.","original_method_code":"def _merge_perm(self, permission_name, view_menu_name):\n        \"\"\"\n        Add the new permission , view_menu to ab_permission_view_role if not exists.\n        It will add the related entry to ab_permission\n        and ab_view_menu two meta tables as well.\n\n        :param permission_name: Name of the permission.\n        :type permission_name: str\n        :param view_menu_name: Name of the view-menu\n        :type view_menu_name: str\n        :return:\n        \"\"\"\n        permission = self.find_permission(permission_name)\n        view_menu = self.find_view_menu(view_menu_name)\n        pv = None\n        if permission and view_menu:\n            pv = self.get_session.query(self.permissionview_model).filter_by(\n                permission=permission, view_menu=view_menu).first()\n        if not pv and permission_name and view_menu_name:\n            self.add_permission_view_menu(permission_name, view_menu_name)","method_path":"airflow\/www\/security.py"}
{"repo_name":"apache\/airflow","method_name":"AirflowSecurityManager.update_admin_perm_view","method_code":"def update_admin_perm_view(self):\n        pvms = self.get_session.query(sqla_models.PermissionView).all()\n        pvms = [p for p in pvms if p.permission and p.view_menu]\n\n        admin = self.find_role('Admin')\n        admin.permissions = list(set(admin.permissions) | set(pvms))\n\n        self.get_session.commit()","method_summary":"Admin should have all the permission-views. Add the missing ones to the table for admin.","original_method_code":"def update_admin_perm_view(self):\n        \"\"\"\n        Admin should have all the permission-views.\n        Add the missing ones to the table for admin.\n\n        :return: None.\n        \"\"\"\n        pvms = self.get_session.query(sqla_models.PermissionView).all()\n        pvms = [p for p in pvms if p.permission and p.view_menu]\n\n        admin = self.find_role('Admin')\n        admin.permissions = list(set(admin.permissions) | set(pvms))\n\n        self.get_session.commit()","method_path":"airflow\/www\/security.py"}
{"repo_name":"apache\/airflow","method_name":"AirflowSecurityManager._sync_dag_view_permissions","method_code":"def _sync_dag_view_permissions(self, dag_id, access_control):\n        def _get_or_create_dag_permission(perm_name):\n            dag_perm = self.find_permission_view_menu(perm_name, dag_id)\n            if not dag_perm:\n                self.log.info(\n                    \"Creating new permission '%s' on view '%s'\",\n                    perm_name, dag_id\n                )\n                dag_perm = self.add_permission_view_menu(perm_name, dag_id)\n\n            return dag_perm\n\n        def _revoke_stale_permissions(dag_view):\n            existing_dag_perms = self.find_permissions_view_menu(dag_view)\n            for perm in existing_dag_perms:\n                non_admin_roles = [role for role in perm.role\n                                   if role.name != 'Admin']\n                for role in non_admin_roles:\n                    target_perms_for_role = access_control.get(role.name, {})\n                    if perm.permission.name not in target_perms_for_role:\n                        self.log.info(\n                            \"Revoking '%s' on DAG '%s' for role '%s'\",\n                            perm.permission, dag_id, role.name\n                        )\n                        self.del_permission_role(role, perm)\n\n        dag_view = self.find_view_menu(dag_id)\n        if dag_view:\n            _revoke_stale_permissions(dag_view)\n\n        for rolename, perms in access_control.items():\n            role = self.find_role(rolename)\n            if not role:\n                raise AirflowException(\n                    \"The access_control mapping for DAG '{}' includes a role \"\n                    \"named '{}', but that role does not exist\".format(\n                        dag_id,\n                        rolename))\n\n            perms = set(perms)\n            invalid_perms = perms - self.DAG_PERMS\n            if invalid_perms:\n                raise AirflowException(\n                    \"The access_control map for DAG '{}' includes the following \"\n                    \"invalid permissions: {}; The set of valid permissions \"\n                    \"is: {}\".format(dag_id,\n                                    (perms - self.DAG_PERMS),\n                                    self.DAG_PERMS))\n\n            for perm_name in perms:\n                dag_perm = _get_or_create_dag_permission(perm_name)\n                self.add_permission_role(role, dag_perm)","method_summary":"Set the access policy on the given DAG's ViewModel.","original_method_code":"def _sync_dag_view_permissions(self, dag_id, access_control):\n        \"\"\"Set the access policy on the given DAG's ViewModel.\n\n        :param dag_id: the ID of the DAG whose permissions should be updated\n        :type dag_id: string\n        :param access_control: a dict where each key is a rolename and\n            each value is a set() of permission names (e.g.,\n            {'can_dag_read'}\n        :type access_control: dict\n        \"\"\"\n        def _get_or_create_dag_permission(perm_name):\n            dag_perm = self.find_permission_view_menu(perm_name, dag_id)\n            if not dag_perm:\n                self.log.info(\n                    \"Creating new permission '%s' on view '%s'\",\n                    perm_name, dag_id\n                )\n                dag_perm = self.add_permission_view_menu(perm_name, dag_id)\n\n            return dag_perm\n\n        def _revoke_stale_permissions(dag_view):\n            existing_dag_perms = self.find_permissions_view_menu(dag_view)\n            for perm in existing_dag_perms:\n                non_admin_roles = [role for role in perm.role\n                                   if role.name != 'Admin']\n                for role in non_admin_roles:\n                    target_perms_for_role = access_control.get(role.name, {})\n                    if perm.permission.name not in target_perms_for_role:\n                        self.log.info(\n                            \"Revoking '%s' on DAG '%s' for role '%s'\",\n                            perm.permission, dag_id, role.name\n                        )\n                        self.del_permission_role(role, perm)\n\n        dag_view = self.find_view_menu(dag_id)\n        if dag_view:\n            _revoke_stale_permissions(dag_view)\n\n        for rolename, perms in access_control.items():\n            role = self.find_role(rolename)\n            if not role:\n                raise AirflowException(\n                    \"The access_control mapping for DAG '{}' includes a role \"\n                    \"named '{}', but that role does not exist\".format(\n                        dag_id,\n                        rolename))\n\n            perms = set(perms)\n            invalid_perms = perms - self.DAG_PERMS\n            if invalid_perms:\n                raise AirflowException(\n                    \"The access_control map for DAG '{}' includes the following \"\n                    \"invalid permissions: {}; The set of valid permissions \"\n                    \"is: {}\".format(dag_id,\n                                    (perms - self.DAG_PERMS),\n                                    self.DAG_PERMS))\n\n            for perm_name in perms:\n                dag_perm = _get_or_create_dag_permission(perm_name)\n                self.add_permission_role(role, dag_perm)","method_path":"airflow\/www\/security.py"}
{"repo_name":"apache\/airflow","method_name":"AirflowSecurityManager.create_perm_vm_for_all_dag","method_code":"def create_perm_vm_for_all_dag(self):\n        \n        for dag_vm in self.DAG_VMS:\n            for perm in self.DAG_PERMS:\n                self._merge_perm(permission_name=perm,\n                                 view_menu_name=dag_vm)","method_summary":"Create perm-vm if not exist and insert into FAB security model for all-dags.","original_method_code":"def create_perm_vm_for_all_dag(self):\n        \"\"\"\n        Create perm-vm if not exist and insert into FAB security model for all-dags.\n        \"\"\"\n        # create perm for global logical dag\n        for dag_vm in self.DAG_VMS:\n            for perm in self.DAG_PERMS:\n                self._merge_perm(permission_name=perm,\n                                 view_menu_name=dag_vm)","method_path":"airflow\/www\/security.py"}
{"repo_name":"apache\/airflow","method_name":"get_fernet","method_code":"def get_fernet():\n    global _fernet\n    log = LoggingMixin().log\n\n    if _fernet:\n        return _fernet\n    try:\n        from cryptography.fernet import Fernet, MultiFernet, InvalidToken\n        global InvalidFernetToken\n        InvalidFernetToken = InvalidToken\n\n    except BuiltinImportError:\n        log.warning(\n            \"cryptography not found - values will not be stored encrypted.\"\n        )\n        _fernet = NullFernet()\n        return _fernet\n\n    try:\n        fernet_key = configuration.conf.get('core', 'FERNET_KEY')\n        if not fernet_key:\n            log.warning(\n                \"empty cryptography key - values will not be stored encrypted.\"\n            )\n            _fernet = NullFernet()\n        else:\n            _fernet = MultiFernet([\n                Fernet(fernet_part.encode('utf-8'))\n                for fernet_part in fernet_key.split(',')\n            ])\n            _fernet.is_encrypted = True\n    except (ValueError, TypeError) as ve:\n        raise AirflowException(\"Could not create Fernet object: {}\".format(ve))\n\n    return _fernet","method_summary":"Deferred load of Fernet key. This function could fail either because Cryptography is not installed or because the Fernet key is invalid.","original_method_code":"def get_fernet():\n    \"\"\"\n    Deferred load of Fernet key.\n\n    This function could fail either because Cryptography is not installed\n    or because the Fernet key is invalid.\n\n    :return: Fernet object\n    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet\n    \"\"\"\n    global _fernet\n    log = LoggingMixin().log\n\n    if _fernet:\n        return _fernet\n    try:\n        from cryptography.fernet import Fernet, MultiFernet, InvalidToken\n        global InvalidFernetToken\n        InvalidFernetToken = InvalidToken\n\n    except BuiltinImportError:\n        log.warning(\n            \"cryptography not found - values will not be stored encrypted.\"\n        )\n        _fernet = NullFernet()\n        return _fernet\n\n    try:\n        fernet_key = configuration.conf.get('core', 'FERNET_KEY')\n        if not fernet_key:\n            log.warning(\n                \"empty cryptography key - values will not be stored encrypted.\"\n            )\n            _fernet = NullFernet()\n        else:\n            _fernet = MultiFernet([\n                Fernet(fernet_part.encode('utf-8'))\n                for fernet_part in fernet_key.split(',')\n            ])\n            _fernet.is_encrypted = True\n    except (ValueError, TypeError) as ve:\n        raise AirflowException(\"Could not create Fernet object: {}\".format(ve))\n\n    return _fernet","method_path":"airflow\/models\/crypto.py"}
{"repo_name":"apache\/airflow","method_name":"AwsGlueCatalogPartitionSensor.poke","method_code":"def poke(self, context):\n        if '.' in self.table_name:\n            self.database_name, self.table_name = self.table_name.split('.')\n        self.log.info(\n            'Poking for table %s. %s, expression %s', self.database_name, self.table_name, self.expression\n        )\n\n        return self.get_hook().check_for_partition(\n            self.database_name, self.table_name, self.expression)","method_summary":"Checks for existence of the partition in the AWS Glue Catalog table","original_method_code":"def poke(self, context):\n        \"\"\"\n        Checks for existence of the partition in the AWS Glue Catalog table\n        \"\"\"\n        if '.' in self.table_name:\n            self.database_name, self.table_name = self.table_name.split('.')\n        self.log.info(\n            'Poking for table %s. %s, expression %s', self.database_name, self.table_name, self.expression\n        )\n\n        return self.get_hook().check_for_partition(\n            self.database_name, self.table_name, self.expression)","method_path":"airflow\/contrib\/sensors\/aws_glue_catalog_partition_sensor.py"}
{"repo_name":"apache\/airflow","method_name":"AwsGlueCatalogPartitionSensor.get_hook","method_code":"def get_hook(self):\n        if not hasattr(self, 'hook'):\n            from airflow.contrib.hooks.aws_glue_catalog_hook import AwsGlueCatalogHook\n            self.hook = AwsGlueCatalogHook(\n                aws_conn_id=self.aws_conn_id,\n                region_name=self.region_name)\n\n        return self.hook","method_summary":"Gets the AwsGlueCatalogHook","original_method_code":"def get_hook(self):\n        \"\"\"\n        Gets the AwsGlueCatalogHook\n        \"\"\"\n        if not hasattr(self, 'hook'):\n            from airflow.contrib.hooks.aws_glue_catalog_hook import AwsGlueCatalogHook\n            self.hook = AwsGlueCatalogHook(\n                aws_conn_id=self.aws_conn_id,\n                region_name=self.region_name)\n\n        return self.hook","method_path":"airflow\/contrib\/sensors\/aws_glue_catalog_partition_sensor.py"}
{"repo_name":"apache\/airflow","method_name":"SQSSensor.poke","method_code":"def poke(self, context):\n        sqs_hook = SQSHook(aws_conn_id=self.aws_conn_id)\n        sqs_conn = sqs_hook.get_conn()\n\n        self.log.info('SQSSensor checking for message on queue: %s', self.sqs_queue)\n\n        messages = sqs_conn.receive_message(QueueUrl=self.sqs_queue,\n                                            MaxNumberOfMessages=self.max_messages,\n                                            WaitTimeSeconds=self.wait_time_seconds)\n\n        self.log.info(\"reveived message %s\", str(messages))\n\n        if 'Messages' in messages and len(messages['Messages']) > 0:\n\n            entries = [{'Id': message['MessageId'], 'ReceiptHandle': message['ReceiptHandle']}\n                       for message in messages['Messages']]\n\n            result = sqs_conn.delete_message_batch(QueueUrl=self.sqs_queue,\n                                                   Entries=entries)\n\n            if 'Successful' in result:\n                context['ti'].xcom_push(key='messages', value=messages)\n                return True\n            else:\n                raise AirflowException(\n                    'Delete SQS Messages failed ' + str(result) + ' for messages ' + str(messages))\n\n        return False","method_summary":"Check for message on subscribed queue and write to xcom the message with key ``messages``","original_method_code":"def poke(self, context):\n        \"\"\"\n        Check for message on subscribed queue and write to xcom the message with key ``messages``\n\n        :param context: the context object\n        :type context: dict\n        :return: ``True`` if message is available or ``False``\n        \"\"\"\n\n        sqs_hook = SQSHook(aws_conn_id=self.aws_conn_id)\n        sqs_conn = sqs_hook.get_conn()\n\n        self.log.info('SQSSensor checking for message on queue: %s', self.sqs_queue)\n\n        messages = sqs_conn.receive_message(QueueUrl=self.sqs_queue,\n                                            MaxNumberOfMessages=self.max_messages,\n                                            WaitTimeSeconds=self.wait_time_seconds)\n\n        self.log.info(\"reveived message %s\", str(messages))\n\n        if 'Messages' in messages and len(messages['Messages']) > 0:\n\n            entries = [{'Id': message['MessageId'], 'ReceiptHandle': message['ReceiptHandle']}\n                       for message in messages['Messages']]\n\n            result = sqs_conn.delete_message_batch(QueueUrl=self.sqs_queue,\n                                                   Entries=entries)\n\n            if 'Successful' in result:\n                context['ti'].xcom_push(key='messages', value=messages)\n                return True\n            else:\n                raise AirflowException(\n                    'Delete SQS Messages failed ' + str(result) + ' for messages ' + str(messages))\n\n        return False","method_path":"airflow\/contrib\/sensors\/aws_sqs_sensor.py"}
{"repo_name":"apache\/airflow","method_name":"WebHDFSHook.get_conn","method_code":"def get_conn(self):\n        connections = self.get_connections(self.webhdfs_conn_id)\n\n        for connection in connections:\n            try:\n                self.log.debug('Trying namenode %s', connection.host)\n                client = self._get_client(connection)\n                client.status('\/')\n                self.log.debug('Using namenode %s for hook', connection.host)\n                return client\n            except HdfsError as hdfs_error:\n                self.log.debug('Read operation on namenode %s failed with error: %s',\n                               connection.host, hdfs_error)\n\n        hosts = [connection.host for connection in connections]\n        error_message = 'Read operations failed on the namenodes below:\\n{hosts}'.format(\n            hosts='\\n'.join(hosts))\n        raise AirflowWebHDFSHookException(error_message)","method_summary":"Establishes a connection depending on the security mode set via config or environment variable.","original_method_code":"def get_conn(self):\n        \"\"\"\n        Establishes a connection depending on the security mode set via config or environment variable.\n\n        :return: a hdfscli InsecureClient or KerberosClient object.\n        :rtype: hdfs.InsecureClient or hdfs.ext.kerberos.KerberosClient\n        \"\"\"\n        connections = self.get_connections(self.webhdfs_conn_id)\n\n        for connection in connections:\n            try:\n                self.log.debug('Trying namenode %s', connection.host)\n                client = self._get_client(connection)\n                client.status('\/')\n                self.log.debug('Using namenode %s for hook', connection.host)\n                return client\n            except HdfsError as hdfs_error:\n                self.log.debug('Read operation on namenode %s failed with error: %s',\n                               connection.host, hdfs_error)\n\n        hosts = [connection.host for connection in connections]\n        error_message = 'Read operations failed on the namenodes below:\\n{hosts}'.format(\n            hosts='\\n'.join(hosts))\n        raise AirflowWebHDFSHookException(error_message)","method_path":"airflow\/hooks\/webhdfs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"WebHDFSHook.check_for_path","method_code":"def check_for_path(self, hdfs_path):\n        conn = self.get_conn()\n\n        status = conn.status(hdfs_path, strict=False)\n        return bool(status)","method_summary":"Check for the existence of a path in HDFS by querying FileStatus.","original_method_code":"def check_for_path(self, hdfs_path):\n        \"\"\"\n        Check for the existence of a path in HDFS by querying FileStatus.\n\n        :param hdfs_path: The path to check.\n        :type hdfs_path: str\n        :return: True if the path exists and False if not.\n        :rtype: bool\n        \"\"\"\n        conn = self.get_conn()\n\n        status = conn.status(hdfs_path, strict=False)\n        return bool(status)","method_path":"airflow\/hooks\/webhdfs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"WebHDFSHook.load_file","method_code":"def load_file(self, source, destination, overwrite=True, parallelism=1, **kwargs):\n        rconn = self.get_conn()\n\n        conn.upload(hdfs_path=destination,\n                    local_path=source,\n                    overwrite=overwrite,\n                    n_threads=parallelism,\n                    **kwargs)\n        self.log.debug(\"Uploaded file %s to %s\", source, destination)","method_summary":"r\"\"\" Uploads a file to HDFS.","original_method_code":"def load_file(self, source, destination, overwrite=True, parallelism=1, **kwargs):\n        r\"\"\"\n        Uploads a file to HDFS.\n\n        :param source: Local path to file or folder.\n            If it's a folder, all the files inside of it will be uploaded.\n            .. note:: This implies that folders empty of files will not be created remotely.\n\n        :type source: str\n        :param destination: PTarget HDFS path.\n            If it already exists and is a directory, files will be uploaded inside.\n        :type destination: str\n        :param overwrite: Overwrite any existing file or directory.\n        :type overwrite: bool\n        :param parallelism: Number of threads to use for parallelization.\n            A value of `0` (or negative) uses as many threads as there are files.\n        :type parallelism: int\n        :param \\**kwargs: Keyword arguments forwarded to :meth:`hdfs.client.Client.upload`.\n        \"\"\"\n        conn = self.get_conn()\n\n        conn.upload(hdfs_path=destination,\n                    local_path=source,\n                    overwrite=overwrite,\n                    n_threads=parallelism,\n                    **kwargs)\n        self.log.debug(\"Uploaded file %s to %s\", source, destination)","method_path":"airflow\/hooks\/webhdfs_hook.py"}
{"repo_name":"apache\/airflow","method_name":"PinotDbApiHook.get_conn","method_code":"def get_conn(self):\n        conn = self.get_connection(self.pinot_broker_conn_id)\n        pinot_broker_conn = connect(\n            host=conn.host,\n            port=conn.port,\n            path=conn.extra_dejson.get('endpoint', '\/pql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to pinot '\n                      'broker on {host}'.format(host=conn.host))\n        return pinot_broker_conn","method_summary":"Establish a connection to pinot broker through pinot dbqpi.","original_method_code":"def get_conn(self):\n        \"\"\"\n        Establish a connection to pinot broker through pinot dbqpi.\n        \"\"\"\n        conn = self.get_connection(self.pinot_broker_conn_id)\n        pinot_broker_conn = connect(\n            host=conn.host,\n            port=conn.port,\n            path=conn.extra_dejson.get('endpoint', '\/pql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to pinot '\n                      'broker on {host}'.format(host=conn.host))\n        return pinot_broker_conn","method_path":"airflow\/contrib\/hooks\/pinot_hook.py"}
{"repo_name":"apache\/airflow","method_name":"PinotDbApiHook.get_uri","method_code":"def get_uri(self):\n        conn = self.get_connection(getattr(self, self.conn_name_attr))\n        host = conn.host\n        if conn.port is not None:\n            host += ':{port}'.format(port=conn.port)\n        conn_type = 'http' if not conn.conn_type else conn.conn_type\n        endpoint = conn.extra_dejson.get('endpoint', 'pql')\n        return '{conn_type}:\/\/{host}\/{endpoint}'.format(\n            conn_type=conn_type, host=host, endpoint=endpoint)","method_summary":"Get the connection uri for pinot broker. e.g:","original_method_code":"def get_uri(self):\n        \"\"\"\n        Get the connection uri for pinot broker.\n\n        e.g: http:\/\/localhost:9000\/pql\n        \"\"\"\n        conn = self.get_connection(getattr(self, self.conn_name_attr))\n        host = conn.host\n        if conn.port is not None:\n            host += ':{port}'.format(port=conn.port)\n        conn_type = 'http' if not conn.conn_type else conn.conn_type\n        endpoint = conn.extra_dejson.get('endpoint', 'pql')\n        return '{conn_type}:\/\/{host}\/{endpoint}'.format(\n            conn_type=conn_type, host=host, endpoint=endpoint)","method_path":"airflow\/contrib\/hooks\/pinot_hook.py"}
{"repo_name":"apache\/airflow","method_name":"TransferJobPreprocessor._convert_date_to_dict","method_code":"def _convert_date_to_dict(field_date):\n        return {DAY: field_date.day, MONTH: field_date.month, YEAR: field_date.year}","method_summary":"Convert native python ``datetime.date`` object to a format supported by the API","original_method_code":"def _convert_date_to_dict(field_date):\n        \"\"\"\n        Convert native python ``datetime.date`` object  to a format supported by the API\n        \"\"\"\n        return {DAY: field_date.day, MONTH: field_date.month, YEAR: field_date.year}","method_path":"airflow\/contrib\/operators\/gcp_transfer_operator.py"}
{"repo_name":"apache\/airflow","method_name":"TransferJobPreprocessor._convert_time_to_dict","method_code":"def _convert_time_to_dict(time):\n        return {HOURS: time.hour, MINUTES: time.minute, SECONDS: time.second}","method_summary":"Convert native python ``datetime.time`` object to a format supported by the API","original_method_code":"def _convert_time_to_dict(time):\n        \"\"\"\n        Convert native python ``datetime.time`` object  to a format supported by the API\n        \"\"\"\n        return {HOURS: time.hour, MINUTES: time.minute, SECONDS: time.second}","method_path":"airflow\/contrib\/operators\/gcp_transfer_operator.py"}
{"repo_name":"apache\/airflow","method_name":"DbApiHook.get_pandas_df","method_code":"def get_pandas_df(self, sql, parameters=None):\n        import pandas.io.sql as psql\n\n        with closing(self.get_conn()) as conn:\n            return psql.read_sql(sql, con=conn, params=parameters)","method_summary":"Executes the sql and returns a pandas dataframe","original_method_code":"def get_pandas_df(self, sql, parameters=None):\n        \"\"\"\n        Executes the sql and returns a pandas dataframe\n\n        :param sql: the sql statement to be executed (str) or a list of\n            sql statements to execute\n        :type sql: str or list\n        :param parameters: The parameters to render the SQL query with.\n        :type parameters: mapping or iterable\n        \"\"\"\n        import pandas.io.sql as psql\n\n        with closing(self.get_conn()) as conn:\n            return psql.read_sql(sql, con=conn, params=parameters)","method_path":"airflow\/hooks\/dbapi_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DbApiHook.run","method_code":"def run(self, sql, autocommit=False, parameters=None):\n        if isinstance(sql, basestring):\n            sql = [sql]\n\n        with closing(self.get_conn()) as conn:\n            if self.supports_autocommit:\n                self.set_autocommit(conn, autocommit)\n\n            with closing(conn.cursor()) as cur:\n                for s in sql:\n                    if parameters is not None:\n                        self.log.info(\"{} with parameters {}\".format(s, parameters))\n                        cur.execute(s, parameters)\n                    else:\n                        self.log.info(s)\n                        cur.execute(s)\n\n            \n            \n            if not self.get_autocommit(conn):\n                conn.commit()","method_summary":"Runs a command or a list of commands. Pass a list of sql statements to the sql parameter to get them to execute sequentially","original_method_code":"def run(self, sql, autocommit=False, parameters=None):\n        \"\"\"\n        Runs a command or a list of commands. Pass a list of sql\n        statements to the sql parameter to get them to execute\n        sequentially\n\n        :param sql: the sql statement to be executed (str) or a list of\n            sql statements to execute\n        :type sql: str or list\n        :param autocommit: What to set the connection's autocommit setting to\n            before executing the query.\n        :type autocommit: bool\n        :param parameters: The parameters to render the SQL query with.\n        :type parameters: mapping or iterable\n        \"\"\"\n        if isinstance(sql, basestring):\n            sql = [sql]\n\n        with closing(self.get_conn()) as conn:\n            if self.supports_autocommit:\n                self.set_autocommit(conn, autocommit)\n\n            with closing(conn.cursor()) as cur:\n                for s in sql:\n                    if parameters is not None:\n                        self.log.info(\"{} with parameters {}\".format(s, parameters))\n                        cur.execute(s, parameters)\n                    else:\n                        self.log.info(s)\n                        cur.execute(s)\n\n            # If autocommit was set to False for db that supports autocommit,\n            # or if db does not supports autocommit, we do a manual commit.\n            if not self.get_autocommit(conn):\n                conn.commit()","method_path":"airflow\/hooks\/dbapi_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DbApiHook.set_autocommit","method_code":"def set_autocommit(self, conn, autocommit):\n        if not self.supports_autocommit and autocommit:\n            self.log.warn(\n                (\"%s connection doesn't support \"\n                 \"autocommit but autocommit activated.\"),\n                getattr(self, self.conn_name_attr))\n        conn.autocommit = autocommit","method_summary":"Sets the autocommit flag on the connection","original_method_code":"def set_autocommit(self, conn, autocommit):\n        \"\"\"\n        Sets the autocommit flag on the connection\n        \"\"\"\n        if not self.supports_autocommit and autocommit:\n            self.log.warn(\n                (\"%s connection doesn't support \"\n                 \"autocommit but autocommit activated.\"),\n                getattr(self, self.conn_name_attr))\n        conn.autocommit = autocommit","method_path":"airflow\/hooks\/dbapi_hook.py"}
{"repo_name":"apache\/airflow","method_name":"DbApiHook.insert_rows","method_code":"def insert_rows(self, table, rows, target_fields=None, commit_every=1000,\n                    replace=False):\n        if target_fields:\n            target_fields = \", \".join(target_fields)\n            target_fields = \"({})\".format(target_fields)\n        else:\n            target_fields = ''\n        i = 0\n        with closing(self.get_conn()) as conn:\n            if self.supports_autocommit:\n                self.set_autocommit(conn, False)\n\n            conn.commit()\n\n            with closing(conn.cursor()) as cur:\n                for i, row in enumerate(rows, 1):\n                    lst = []\n                    for cell in row:\n                        lst.append(self._serialize_cell(cell, conn))\n                    values = tuple(lst)\n                    placeholders = [\"%s\", ] * len(values)\n                    if not replace:\n                        sql = \"INSERT INTO \"\n                    else:\n                        sql = \"REPLACE INTO \"\n                    sql += \"{0} {1} VALUES ({2})\".format(\n                        table,\n                        target_fields,\n                        \",\".join(placeholders))\n                    cur.execute(sql, values)\n                    if commit_every and i % commit_every == 0:\n                        conn.commit()\n                        self.log.info(\n                            \"Loaded %s into %s rows so far\", i, table\n                        )\n\n            conn.commit()\n        self.log.info(\"Done loading. Loaded a total of %s rows\", i)","method_summary":"A generic way to insert a set of tuples into a table, a new transaction is created every commit_every rows","original_method_code":"def insert_rows(self, table, rows, target_fields=None, commit_every=1000,\n                    replace=False):\n        \"\"\"\n        A generic way to insert a set of tuples into a table,\n        a new transaction is created every commit_every rows\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        :param commit_every: The maximum number of rows to insert in one\n            transaction. Set to 0 to insert all rows in one transaction.\n        :type commit_every: int\n        :param replace: Whether to replace instead of insert\n        :type replace: bool\n        \"\"\"\n        if target_fields:\n            target_fields = \", \".join(target_fields)\n            target_fields = \"({})\".format(target_fields)\n        else:\n            target_fields = ''\n        i = 0\n        with closing(self.get_conn()) as conn:\n            if self.supports_autocommit:\n                self.set_autocommit(conn, False)\n\n            conn.commit()\n\n            with closing(conn.cursor()) as cur:\n                for i, row in enumerate(rows, 1):\n                    lst = []\n                    for cell in row:\n                        lst.append(self._serialize_cell(cell, conn))\n                    values = tuple(lst)\n                    placeholders = [\"%s\", ] * len(values)\n                    if not replace:\n                        sql = \"INSERT INTO \"\n                    else:\n                        sql = \"REPLACE INTO \"\n                    sql += \"{0} {1} VALUES ({2})\".format(\n                        table,\n                        target_fields,\n                        \",\".join(placeholders))\n                    cur.execute(sql, values)\n                    if commit_every and i % commit_every == 0:\n                        conn.commit()\n                        self.log.info(\n                            \"Loaded %s into %s rows so far\", i, table\n                        )\n\n            conn.commit()\n        self.log.info(\"Done loading. Loaded a total of %s rows\", i)","method_path":"airflow\/hooks\/dbapi_hook.py"}
{"repo_name":"apache\/airflow","method_name":"Airflow.health","method_code":"def health(self, session=None):\n        BJ = jobs.BaseJob\n        payload = {}\n        scheduler_health_check_threshold = timedelta(seconds=conf.getint('scheduler',\n                                                                         'scheduler_health_check_threshold'\n                                                                         ))\n\n        latest_scheduler_heartbeat = None\n        payload['metadatabase'] = {'status': 'healthy'}\n        try:\n            latest_scheduler_heartbeat = session.query(func.max(BJ.latest_heartbeat)).\\\n                filter(BJ.state == 'running', BJ.job_type == 'SchedulerJob').\\\n                scalar()\n        except Exception:\n            payload['metadatabase']['status'] = 'unhealthy'\n\n        if not latest_scheduler_heartbeat:\n            scheduler_status = 'unhealthy'\n        else:\n            if timezone.utcnow() - latest_scheduler_heartbeat <= scheduler_health_check_threshold:\n                scheduler_status = 'healthy'\n            else:\n                scheduler_status = 'unhealthy'\n\n        payload['scheduler'] = {'status': scheduler_status,\n                                'latest_scheduler_heartbeat': str(latest_scheduler_heartbeat)}\n\n        return wwwutils.json_response(payload)","method_summary":"An endpoint helping check the health status of the Airflow instance, including metadatabase and scheduler.","original_method_code":"def health(self, session=None):\n        \"\"\"\n        An endpoint helping check the health status of the Airflow instance,\n        including metadatabase and scheduler.\n        \"\"\"\n\n        BJ = jobs.BaseJob\n        payload = {}\n        scheduler_health_check_threshold = timedelta(seconds=conf.getint('scheduler',\n                                                                         'scheduler_health_check_threshold'\n                                                                         ))\n\n        latest_scheduler_heartbeat = None\n        payload['metadatabase'] = {'status': 'healthy'}\n        try:\n            latest_scheduler_heartbeat = session.query(func.max(BJ.latest_heartbeat)).\\\n                filter(BJ.state == 'running', BJ.job_type == 'SchedulerJob').\\\n                scalar()\n        except Exception:\n            payload['metadatabase']['status'] = 'unhealthy'\n\n        if not latest_scheduler_heartbeat:\n            scheduler_status = 'unhealthy'\n        else:\n            if timezone.utcnow() - latest_scheduler_heartbeat <= scheduler_health_check_threshold:\n                scheduler_status = 'healthy'\n            else:\n                scheduler_status = 'unhealthy'\n\n        payload['scheduler'] = {'status': scheduler_status,\n                                'latest_scheduler_heartbeat': str(latest_scheduler_heartbeat)}\n\n        return wwwutils.json_response(payload)","method_path":"airflow\/www\/views.py"}
{"repo_name":"apache\/airflow","method_name":"Airflow.extra_links","method_code":"def extra_links(self):\n        dag_id = request.args.get('dag_id')\n        task_id = request.args.get('task_id')\n        execution_date = request.args.get('execution_date')\n        link_name = request.args.get('link_name')\n        dttm = airflow.utils.timezone.parse(execution_date)\n        dag = dagbag.get_dag(dag_id)\n\n        if not dag or task_id not in dag.task_ids:\n            response = jsonify(\n                {'url': None,\n                 'error': \"can't find dag {dag} or task_id {task_id}\".format(\n                     dag=dag,\n                     task_id=task_id\n                 )}\n            )\n            response.status_code = 404\n            return response\n\n        task = dag.get_task(task_id)\n\n        try:\n            url = task.get_extra_links(dttm, link_name)\n        except ValueError as err:\n            response = jsonify({'url': None, 'error': str(err)})\n            response.status_code = 404\n            return response\n        if url:\n            response = jsonify({'error': None, 'url': url})\n            response.status_code = 200\n            return response\n        else:\n            response = jsonify(\n                {'url': None, 'error': 'No URL found for {dest}'.format(dest=link_name)})\n            response.status_code = 404\n            return response","method_summary":"A restful endpoint that returns external links for a given Operator It queries the operator that sent the request for the links it wishes to provide for a given external link name.","original_method_code":"def extra_links(self):\n        \"\"\"\n        A restful endpoint that returns external links for a given Operator\n\n        It queries the operator that sent the request for the links it wishes\n        to provide for a given external link name.\n\n        API: GET\n        Args: dag_id: The id of the dag containing the task in question\n              task_id: The id of the task in question\n              execution_date: The date of execution of the task\n              link_name: The name of the link reference to find the actual URL for\n\n        Returns:\n            200: {url: <url of link>, error: None} - returned when there was no problem\n                finding the URL\n            404: {url: None, error: <error message>} - returned when the operator does\n                not return a URL\n        \"\"\"\n        dag_id = request.args.get('dag_id')\n        task_id = request.args.get('task_id')\n        execution_date = request.args.get('execution_date')\n        link_name = request.args.get('link_name')\n        dttm = airflow.utils.timezone.parse(execution_date)\n        dag = dagbag.get_dag(dag_id)\n\n        if not dag or task_id not in dag.task_ids:\n            response = jsonify(\n                {'url': None,\n                 'error': \"can't find dag {dag} or task_id {task_id}\".format(\n                     dag=dag,\n                     task_id=task_id\n                 )}\n            )\n            response.status_code = 404\n            return response\n\n        task = dag.get_task(task_id)\n\n        try:\n            url = task.get_extra_links(dttm, link_name)\n        except ValueError as err:\n            response = jsonify({'url': None, 'error': str(err)})\n            response.status_code = 404\n            return response\n        if url:\n            response = jsonify({'error': None, 'url': url})\n            response.status_code = 200\n            return response\n        else:\n            response = jsonify(\n                {'url': None, 'error': 'No URL found for {dest}'.format(dest=link_name)})\n            response.status_code = 404\n            return response","method_path":"airflow\/www\/views.py"}
{"repo_name":"apache\/airflow","method_name":"CloudantHook.get_conn","method_code":"def get_conn(self):\n        conn = self.get_connection(self.cloudant_conn_id)\n\n        self._validate_connection(conn)\n\n        cloudant_session = cloudant(user=conn.login, passwd=conn.password, account=conn.host)\n\n        return cloudant_session","method_summary":"Opens a connection to the cloudant service and closes it automatically if used as context manager.","original_method_code":"def get_conn(self):\n        \"\"\"\n        Opens a connection to the cloudant service and closes it automatically if used as context manager.\n\n        .. note::\n            In the connection form:\n            - 'host' equals the 'Account' (optional)\n            - 'login' equals the 'Username (or API Key)' (required)\n            - 'password' equals the 'Password' (required)\n\n        :return: an authorized cloudant session context manager object.\n        :rtype: cloudant\n        \"\"\"\n        conn = self.get_connection(self.cloudant_conn_id)\n\n        self._validate_connection(conn)\n\n        cloudant_session = cloudant(user=conn.login, passwd=conn.password, account=conn.host)\n\n        return cloudant_session","method_path":"airflow\/contrib\/hooks\/cloudant_hook.py"}
{"repo_name":"apache\/airflow","method_name":"SlackWebhookOperator.execute","method_code":"def execute(self, context):\n        self.hook = SlackWebhookHook(\n            self.http_conn_id,\n            self.webhook_token,\n            self.message,\n            self.attachments,\n            self.channel,\n            self.username,\n            self.icon_emoji,\n            self.link_names,\n            self.proxy\n        )\n        self.hook.execute()","method_summary":"Call the SlackWebhookHook to post the provided Slack message","original_method_code":"def execute(self, context):\n        \"\"\"\n        Call the SlackWebhookHook to post the provided Slack message\n        \"\"\"\n        self.hook = SlackWebhookHook(\n            self.http_conn_id,\n            self.webhook_token,\n            self.message,\n            self.attachments,\n            self.channel,\n            self.username,\n            self.icon_emoji,\n            self.link_names,\n            self.proxy\n        )\n        self.hook.execute()","method_path":"airflow\/contrib\/operators\/slack_webhook_operator.py"}
{"repo_name":"apache\/airflow","method_name":"GoogleCloudBaseHook.catch_http_exception","method_code":"def catch_http_exception(func):\n        @functools.wraps(func)\n        def wrapper_decorator(self, *args, **kwargs):\n            try:\n                return func(self, *args, **kwargs)\n            except GoogleAPICallError as e:\n                if isinstance(e, AlreadyExists):\n                    raise e\n                else:\n                    self.log.error('The request failed:\\n%s', str(e))\n                    raise AirflowException(e)\n            except RetryError as e:\n                self.log.error('The request failed due to a retryable error and retry attempts failed.')\n                raise AirflowException(e)\n            except ValueError as e:\n                self.log.error('The request failed, the parameters are invalid.')\n                raise AirflowException(e)\n            except HttpError as e:\n                self.log.error('The request failed:\\n%s', str(e))\n                raise AirflowException(e)\n\n        return wrapper_decorator","method_summary":"Function decorator that intercepts HTTP Errors and raises AirflowException with more informative message.","original_method_code":"def catch_http_exception(func):\n        \"\"\"\n        Function decorator that intercepts HTTP Errors and raises AirflowException\n        with more informative message.\n        \"\"\"\n\n        @functools.wraps(func)\n        def wrapper_decorator(self, *args, **kwargs):\n            try:\n                return func(self, *args, **kwargs)\n            except GoogleAPICallError as e:\n                if isinstance(e, AlreadyExists):\n                    raise e\n                else:\n                    self.log.error('The request failed:\\n%s', str(e))\n                    raise AirflowException(e)\n            except RetryError as e:\n                self.log.error('The request failed due to a retryable error and retry attempts failed.')\n                raise AirflowException(e)\n            except ValueError as e:\n                self.log.error('The request failed, the parameters are invalid.')\n                raise AirflowException(e)\n            except HttpError as e:\n                self.log.error('The request failed:\\n%s', str(e))\n                raise AirflowException(e)\n\n        return wrapper_decorator","method_path":"airflow\/contrib\/hooks\/gcp_api_base_hook.py"}
{"repo_name":"apache\/airflow","method_name":"State.unfinished","method_code":"def unfinished(cls):\n        return [\n            cls.NONE,\n            cls.SCHEDULED,\n            cls.QUEUED,\n            cls.RUNNING,\n            cls.SHUTDOWN,\n            cls.UP_FOR_RETRY,\n            cls.UP_FOR_RESCHEDULE\n        ]","method_summary":"A list of states indicating that a task either has not completed a run or has not even started.","original_method_code":"def unfinished(cls):\n        \"\"\"\n        A list of states indicating that a task either has not completed\n        a run or has not even started.\n        \"\"\"\n        return [\n            cls.NONE,\n            cls.SCHEDULED,\n            cls.QUEUED,\n            cls.RUNNING,\n            cls.SHUTDOWN,\n            cls.UP_FOR_RETRY,\n            cls.UP_FOR_RESCHEDULE\n        ]","method_path":"airflow\/utils\/state.py"}
{"repo_name":"apache\/airflow","method_name":"SparkSqlHook._prepare_command","method_code":"def _prepare_command(self, cmd):\n        connection_cmd = [\"spark-sql\"]\n        if self._conf:\n            for conf_el in self._conf.split(\",\"):\n                connection_cmd += [\"--conf\", conf_el]\n        if self._total_executor_cores:\n            connection_cmd += [\"--total-executor-cores\", str(self._total_executor_cores)]\n        if self._executor_cores:\n            connection_cmd += [\"--executor-cores\", str(self._executor_cores)]\n        if self._executor_memory:\n            connection_cmd += [\"--executor-memory\", self._executor_memory]\n        if self._keytab:\n            connection_cmd += [\"--keytab\", self._keytab]\n        if self._principal:\n            connection_cmd += [\"--principal\", self._principal]\n        if self._num_executors:\n            connection_cmd += [\"--num-executors\", str(self._num_executors)]\n        if self._sql:\n            sql = self._sql.strip()\n            if sql.endswith(\".sql\") or sql.endswith(\".hql\"):\n                connection_cmd += [\"-f\", sql]\n            else:\n                connection_cmd += [\"-e\", sql]\n        if self._master:\n            connection_cmd += [\"--master\", self._master]\n        if self._name:\n            connection_cmd += [\"--name\", self._name]\n        if self._verbose:\n            connection_cmd += [\"--verbose\"]\n        if self._yarn_queue:\n            connection_cmd += [\"--queue\", self._yarn_queue]\n\n        connection_cmd += cmd\n        self.log.debug(\"Spark-Sql cmd: %s\", connection_cmd)\n\n        return connection_cmd","method_summary":"Construct the spark-sql command to execute. Verbose output is enabled as default.","original_method_code":"def _prepare_command(self, cmd):\n        \"\"\"\n        Construct the spark-sql command to execute. Verbose output is enabled\n        as default.\n\n        :param cmd: command to append to the spark-sql command\n        :type cmd: str\n        :return: full command to be executed\n        \"\"\"\n        connection_cmd = [\"spark-sql\"]\n        if self._conf:\n            for conf_el in self._conf.split(\",\"):\n                connection_cmd += [\"--conf\", conf_el]\n        if self._total_executor_cores:\n            connection_cmd += [\"--total-executor-cores\", str(self._total_executor_cores)]\n        if self._executor_cores:\n            connection_cmd += [\"--executor-cores\", str(self._executor_cores)]\n        if self._executor_memory:\n            connection_cmd += [\"--executor-memory\", self._executor_memory]\n        if self._keytab:\n            connection_cmd += [\"--keytab\", self._keytab]\n        if self._principal:\n            connection_cmd += [\"--principal\", self._principal]\n        if self._num_executors:\n            connection_cmd += [\"--num-executors\", str(self._num_executors)]\n        if self._sql:\n            sql = self._sql.strip()\n            if sql.endswith(\".sql\") or sql.endswith(\".hql\"):\n                connection_cmd += [\"-f\", sql]\n            else:\n                connection_cmd += [\"-e\", sql]\n        if self._master:\n            connection_cmd += [\"--master\", self._master]\n        if self._name:\n            connection_cmd += [\"--name\", self._name]\n        if self._verbose:\n            connection_cmd += [\"--verbose\"]\n        if self._yarn_queue:\n            connection_cmd += [\"--queue\", self._yarn_queue]\n\n        connection_cmd += cmd\n        self.log.debug(\"Spark-Sql cmd: %s\", connection_cmd)\n\n        return connection_cmd","method_path":"airflow\/contrib\/hooks\/spark_sql_hook.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"Message.schedule","method_code":"def schedule(self, schedule_time):\n        if not self.properties.message_id:\n            self.properties.message_id = str(uuid.uuid4())\n        if not self.message.annotations:\n            self.message.annotations = {}\n        self.message.annotations[types.AMQPSymbol(self._x_OPT_SCHEDULED_ENQUEUE_TIME)] = schedule_time","method_summary":"Add a specific enqueue time to the message.","original_method_code":"def schedule(self, schedule_time):\n        \"\"\"Add a specific enqueue time to the message.\n\n        :param schedule_time: The scheduled time to enqueue the message.\n        :type schedule_time: ~datetime.datetime\n        \"\"\"\n        if not self.properties.message_id:\n            self.properties.message_id = str(uuid.uuid4())\n        if not self.message.annotations:\n            self.message.annotations = {}\n        self.message.annotations[types.AMQPSymbol(self._x_OPT_SCHEDULED_ENQUEUE_TIME)] = schedule_time","method_path":"azure-servicebus\/azure\/servicebus\/common\/message.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"VpnSitesConfigurationOperations.download","method_code":"def download(\n            self, resource_group_name, virtual_wan_name, vpn_sites=None, output_blob_sas_url=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._download_initial(\n            resource_group_name=resource_group_name,\n            virtual_wan_name=virtual_wan_name,\n            vpn_sites=vpn_sites,\n            output_blob_sas_url=output_blob_sas_url,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Gives the sas-url to download the configurations for vpn-sites in a resource group.","original_method_code":"def download(\n            self, resource_group_name, virtual_wan_name, vpn_sites=None, output_blob_sas_url=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Gives the sas-url to download the configurations for vpn-sites in a\n        resource group.\n\n        :param resource_group_name: The resource group name.\n        :type resource_group_name: str\n        :param virtual_wan_name: The name of the VirtualWAN for which\n         configuration of all vpn-sites is needed.\n        :type virtual_wan_name: str\n        :param vpn_sites: List of resource-ids of the vpn-sites for which\n         config is to be downloaded.\n        :type vpn_sites:\n         list[~azure.mgmt.network.v2018_04_01.models.SubResource]\n        :param output_blob_sas_url: The sas-url to download the configurations\n         for vpn-sites\n        :type output_blob_sas_url: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorException<azure.mgmt.network.v2018_04_01.models.ErrorException>`\n        \"\"\"\n        raw_result = self._download_initial(\n            resource_group_name=resource_group_name,\n            virtual_wan_name=virtual_wan_name,\n            vpn_sites=vpn_sites,\n            output_blob_sas_url=output_blob_sas_url,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-network\/azure\/mgmt\/network\/v2018_04_01\/operations\/vpn_sites_configuration_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"guess_service_info_from_path","method_code":"def guess_service_info_from_path(spec_path):\n    spec_path = spec_path.lower()\n    spec_path = spec_path[spec_path.index(\"specification\"):] \n    split_spec_path = spec_path.split(\"\/\")\n\n    rp_name = split_spec_path[1]\n    is_arm = split_spec_path[2] == \"resource-manager\"\n\n    return {\n        \"rp_name\": rp_name,\n        \"is_arm\": is_arm\n    }","method_summary":"Guess Python Autorest options based on the spec path. Expected","original_method_code":"def guess_service_info_from_path(spec_path):\n    \"\"\"Guess Python Autorest options based on the spec path.\n\n    Expected path:\n    specification\/compute\/resource-manager\/readme.md\n    \"\"\"\n    spec_path = spec_path.lower()\n    spec_path = spec_path[spec_path.index(\"specification\"):] # Might raise and it's ok\n    split_spec_path = spec_path.split(\"\/\")\n\n    rp_name = split_spec_path[1]\n    is_arm = split_spec_path[2] == \"resource-manager\"\n\n    return {\n        \"rp_name\": rp_name,\n        \"is_arm\": is_arm\n    }","method_path":"scripts\/build_sdk.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"PowerShellOperations.update_command","method_code":"def update_command(\n            self, resource_group_name, node_name, session, pssession, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._update_command_initial(\n            resource_group_name=resource_group_name,\n            node_name=node_name,\n            session=session,\n            pssession=pssession,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('PowerShellCommandResults', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Updates a running PowerShell command with more data.","original_method_code":"def update_command(\n            self, resource_group_name, node_name, session, pssession, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Updates a running PowerShell command with more data.\n\n        :param resource_group_name: The resource group name uniquely\n         identifies the resource group within the user subscriptionId.\n        :type resource_group_name: str\n        :param node_name: The node name (256 characters maximum).\n        :type node_name: str\n        :param session: The sessionId from the user.\n        :type session: str\n        :param pssession: The PowerShell sessionId from the user.\n        :type pssession: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns\n         PowerShellCommandResults or\n         ClientRawResponse<PowerShellCommandResults> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.servermanager.models.PowerShellCommandResults]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.servermanager.models.PowerShellCommandResults]]\n        :raises:\n         :class:`ErrorException<azure.mgmt.servermanager.models.ErrorException>`\n        \"\"\"\n        raw_result = self._update_command_initial(\n            resource_group_name=resource_group_name,\n            node_name=node_name,\n            session=session,\n            pssession=pssession,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('PowerShellCommandResults', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-servermanager\/azure\/mgmt\/servermanager\/operations\/power_shell_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ApplicationDefinitionsOperations.delete_by_id","method_code":"def delete_by_id(\n            self, application_definition_id, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._delete_by_id_initial(\n            application_definition_id=application_definition_id,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Deletes the managed application definition.","original_method_code":"def delete_by_id(\n            self, application_definition_id, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Deletes the managed application definition.\n\n        :param application_definition_id: The fully qualified ID of the\n         managed application definition, including the managed application name\n         and the managed application definition resource type. Use the format,\n         \/subscriptions\/{guid}\/resourceGroups\/{resource-group-name}\/Microsoft.Solutions\/applicationDefinitions\/{applicationDefinition-name}\n        :type application_definition_id: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.resource.managedapplications.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._delete_by_id_initial(\n            application_definition_id=application_definition_id,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-resource\/azure\/mgmt\/resource\/managedapplications\/operations\/application_definitions_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ApplicationDefinitionsOperations.create_or_update_by_id","method_code":"def create_or_update_by_id(\n            self, application_definition_id, parameters, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._create_or_update_by_id_initial(\n            application_definition_id=application_definition_id,\n            parameters=parameters,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('ApplicationDefinition', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Creates a new managed application definition.","original_method_code":"def create_or_update_by_id(\n            self, application_definition_id, parameters, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Creates a new managed application definition.\n\n        :param application_definition_id: The fully qualified ID of the\n         managed application definition, including the managed application name\n         and the managed application definition resource type. Use the format,\n         \/subscriptions\/{guid}\/resourceGroups\/{resource-group-name}\/Microsoft.Solutions\/applicationDefinitions\/{applicationDefinition-name}\n        :type application_definition_id: str\n        :param parameters: Parameters supplied to the create or update a\n         managed application definition.\n        :type parameters:\n         ~azure.mgmt.resource.managedapplications.models.ApplicationDefinition\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns ApplicationDefinition\n         or ClientRawResponse<ApplicationDefinition> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.resource.managedapplications.models.ApplicationDefinition]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.resource.managedapplications.models.ApplicationDefinition]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.resource.managedapplications.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._create_or_update_by_id_initial(\n            application_definition_id=application_definition_id,\n            parameters=parameters,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('ApplicationDefinition', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-resource\/azure\/mgmt\/resource\/managedapplications\/operations\/application_definitions_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_HTTPClient.get_uri","method_code":"def get_uri(self, request):\n        protocol = request.protocol_override \\\n            if request.protocol_override else self.protocol\n        protocol = protocol.lower()\n        port = HTTP_PORT if protocol == 'http' else HTTPS_PORT\n        return protocol + ':\/\/' + request.host + ':' + str(port) + request.path","method_summary":"Return the target uri for the request.","original_method_code":"def get_uri(self, request):\n        ''' Return the target uri for the request.'''\n        protocol = request.protocol_override \\\n            if request.protocol_override else self.protocol\n        protocol = protocol.lower()\n        port = HTTP_PORT if protocol == 'http' else HTTPS_PORT\n        return protocol + ':\/\/' + request.host + ':' + str(port) + request.path","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_http\/httpclient.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_HTTPClient.get_connection","method_code":"def get_connection(self, request):\n        protocol = request.protocol_override \\\n            if request.protocol_override else self.protocol\n        protocol = protocol.lower()\n        target_host = request.host\n        \n\n        connection = _RequestsConnection(\n            target_host, protocol, self.request_session, self.timeout)\n        proxy_host = self.proxy_host\n        proxy_port = self.proxy_port\n\n        if self.proxy_host:\n            headers = None\n            if self.proxy_user and self.proxy_password:\n                auth = base64.b64encode(\"{0}:{1}\".format(self.proxy_user, self.proxy_password).encode())\n                headers = {'Proxy-Authorization': 'Basic {0}'.format(auth.decode())}\n            connection.set_tunnel(proxy_host, int(proxy_port), headers)\n\n        return connection","method_summary":"Create connection for the request.","original_method_code":"def get_connection(self, request):\n        ''' Create connection for the request. '''\n        protocol = request.protocol_override \\\n            if request.protocol_override else self.protocol\n        protocol = protocol.lower()\n        target_host = request.host\n        # target_port = HTTP_PORT if protocol == 'http' else HTTPS_PORT\n\n        connection = _RequestsConnection(\n            target_host, protocol, self.request_session, self.timeout)\n        proxy_host = self.proxy_host\n        proxy_port = self.proxy_port\n\n        if self.proxy_host:\n            headers = None\n            if self.proxy_user and self.proxy_password:\n                auth = base64.b64encode(\"{0}:{1}\".format(self.proxy_user, self.proxy_password).encode())\n                headers = {'Proxy-Authorization': 'Basic {0}'.format(auth.decode())}\n            connection.set_tunnel(proxy_host, int(proxy_port), headers)\n\n        return connection","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_http\/httpclient.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_HTTPClient.perform_request","method_code":"def perform_request(self, request):\n        connection = self.get_connection(request)\n        try:\n            connection.putrequest(request.method, request.path)\n\n            self.send_request_headers(connection, request.headers)\n            self.send_request_body(connection, request.body)\n\n            if DEBUG_REQUESTS and request.body:\n                print('request:')\n                try:\n                    print(request.body)\n                except:  \n                    pass\n\n            resp = connection.getresponse()\n            status = int(resp.status)\n            message = resp.reason\n            respheaders = resp.getheaders()\n\n            \n            for i, value in enumerate(respheaders):\n                respheaders[i] = (value[0].lower(), value[1])\n\n            respbody = None\n            if resp.length is None:\n                respbody = resp.read()\n            elif resp.length > 0:\n                respbody = resp.read(resp.length)\n\n            if DEBUG_RESPONSES and respbody:\n                print('response:')\n                try:\n                    print(respbody)\n                except:  \n                    pass\n\n            response = HTTPResponse(\n                status, resp.reason, respheaders, respbody)\n            if status == 307:\n                new_url = urlparse(dict(respheaders)['location'])\n                request.host = new_url.hostname\n                request.path = new_url.path\n                request.path, request.query = self._update_request_uri_query(request)\n                return self.perform_request(request)\n            if status >= 300:\n                raise HTTPError(status, message, respheaders, respbody)\n\n            return response\n        finally:\n            connection.close()","method_summary":"Sends request to cloud service server and return the response.","original_method_code":"def perform_request(self, request):\n        ''' Sends request to cloud service server and return the response. '''\n        connection = self.get_connection(request)\n        try:\n            connection.putrequest(request.method, request.path)\n\n            self.send_request_headers(connection, request.headers)\n            self.send_request_body(connection, request.body)\n\n            if DEBUG_REQUESTS and request.body:\n                print('request:')\n                try:\n                    print(request.body)\n                except:  # pylint: disable=bare-except\n                    pass\n\n            resp = connection.getresponse()\n            status = int(resp.status)\n            message = resp.reason\n            respheaders = resp.getheaders()\n\n            # for consistency across platforms, make header names lowercase\n            for i, value in enumerate(respheaders):\n                respheaders[i] = (value[0].lower(), value[1])\n\n            respbody = None\n            if resp.length is None:\n                respbody = resp.read()\n            elif resp.length > 0:\n                respbody = resp.read(resp.length)\n\n            if DEBUG_RESPONSES and respbody:\n                print('response:')\n                try:\n                    print(respbody)\n                except:  # pylint: disable=bare-except\n                    pass\n\n            response = HTTPResponse(\n                status, resp.reason, respheaders, respbody)\n            if status == 307:\n                new_url = urlparse(dict(respheaders)['location'])\n                request.host = new_url.hostname\n                request.path = new_url.path\n                request.path, request.query = self._update_request_uri_query(request)\n                return self.perform_request(request)\n            if status >= 300:\n                raise HTTPError(status, message, respheaders, respbody)\n\n            return response\n        finally:\n            connection.close()","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_http\/httpclient.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ClustersOperations.execute_script_actions","method_code":"def execute_script_actions(\n            self, resource_group_name, cluster_name, persist_on_success, script_actions=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._execute_script_actions_initial(\n            resource_group_name=resource_group_name,\n            cluster_name=cluster_name,\n            persist_on_success=persist_on_success,\n            script_actions=script_actions,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Executes script actions on the specified HDInsight cluster.","original_method_code":"def execute_script_actions(\n            self, resource_group_name, cluster_name, persist_on_success, script_actions=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Executes script actions on the specified HDInsight cluster.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param cluster_name: The name of the cluster.\n        :type cluster_name: str\n        :param persist_on_success: Gets or sets if the scripts needs to be\n         persisted.\n        :type persist_on_success: bool\n        :param script_actions: The list of run time script actions.\n        :type script_actions:\n         list[~azure.mgmt.hdinsight.models.RuntimeScriptAction]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.hdinsight.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._execute_script_actions_initial(\n            resource_group_name=resource_group_name,\n            cluster_name=cluster_name,\n            persist_on_success=persist_on_success,\n            script_actions=script_actions,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-hdinsight\/azure\/mgmt\/hdinsight\/operations\/clusters_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"FrontDoorManagementClient.check_front_door_name_availability","method_code":"def check_front_door_name_availability(\n            self, name, type, custom_headers=None, raw=False, **operation_config):\n        check_front_door_name_availability_input = models.CheckNameAvailabilityInput(name=name, type=type)\n\n        api_version = \"2018-08-01\"\n\n        \n        url = self.check_front_door_name_availability.metadata['url']\n\n        \n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n\n        \n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        \n        body_content = self._serialize.body(check_front_door_name_availability_input, 'CheckNameAvailabilityInput')\n\n        \n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorResponseException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('CheckNameAvailabilityOutput', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_summary":"Check the availability of a Front Door resource name.","original_method_code":"def check_front_door_name_availability(\n            self, name, type, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Check the availability of a Front Door resource name.\n\n        :param name: The resource name to validate.\n        :type name: str\n        :param type: The type of the resource whose name is to be validated.\n         Possible values include: 'Microsoft.Network\/frontDoors',\n         'Microsoft.Network\/frontDoors\/frontendEndpoints'\n        :type type: str or ~azure.mgmt.frontdoor.models.ResourceType\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: CheckNameAvailabilityOutput or ClientRawResponse if raw=true\n        :rtype: ~azure.mgmt.frontdoor.models.CheckNameAvailabilityOutput or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.frontdoor.models.ErrorResponseException>`\n        \"\"\"\n        check_front_door_name_availability_input = models.CheckNameAvailabilityInput(name=name, type=type)\n\n        api_version = \"2018-08-01\"\n\n        # Construct URL\n        url = self.check_front_door_name_availability.metadata['url']\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        # Construct body\n        body_content = self._serialize.body(check_front_door_name_availability_input, 'CheckNameAvailabilityInput')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorResponseException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('CheckNameAvailabilityOutput', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-mgmt-frontdoor\/azure\/mgmt\/frontdoor\/front_door_management_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"VaultsOperations.purge_deleted","method_code":"def purge_deleted(\n            self, vault_name, location, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._purge_deleted_initial(\n            vault_name=vault_name,\n            location=location,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Permanently deletes the specified vault. aka Purges the deleted Azure key vault.","original_method_code":"def purge_deleted(\n            self, vault_name, location, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Permanently deletes the specified vault. aka Purges the deleted Azure\n        key vault.\n\n        :param vault_name: The name of the soft-deleted vault.\n        :type vault_name: str\n        :param location: The location of the soft-deleted vault.\n        :type location: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._purge_deleted_initial(\n            vault_name=vault_name,\n            location=location,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-keyvault\/azure\/mgmt\/keyvault\/v2016_10_01\/operations\/vaults_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"HttpChallenge._validate_request_uri","method_code":"def _validate_request_uri(self, uri):\n        if not uri:\n            raise ValueError('request_uri cannot be empty')\n\n        uri = parse.urlparse(uri)\n        if not uri.netloc:\n            raise ValueError('request_uri must be an absolute URI')\n\n        if uri.scheme.lower() not in ['http', 'https']:\n            raise ValueError('request_uri must be HTTP or HTTPS')\n\n        return uri.netloc","method_summary":"Extracts the host authority from the given URI.","original_method_code":"def _validate_request_uri(self, uri):\n        \"\"\" Extracts the host authority from the given URI. \"\"\"\n        if not uri:\n            raise ValueError('request_uri cannot be empty')\n\n        uri = parse.urlparse(uri)\n        if not uri.netloc:\n            raise ValueError('request_uri must be an absolute URI')\n\n        if uri.scheme.lower() not in ['http', 'https']:\n            raise ValueError('request_uri must be HTTP or HTTPS')\n\n        return uri.netloc","method_path":"azure-keyvault\/azure\/keyvault\/http_challenge.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"get_cli_profile","method_code":"def get_cli_profile():\n    try:\n        from azure.cli.core._profile import Profile\n        from azure.cli.core._session import ACCOUNT\n        from azure.cli.core._environment import get_config_dir\n    except ImportError:\n        raise ImportError(\"You need to install 'azure-cli-core' to load CLI credentials\")\n\n\n    azure_folder = get_config_dir()\n    ACCOUNT.load(os.path.join(azure_folder, 'azureProfile.json'))\n    return Profile(storage=ACCOUNT)","method_summary":"Return a CLI profile class.","original_method_code":"def get_cli_profile():\n    \"\"\"Return a CLI profile class.\n\n    .. versionadded:: 1.1.6\n\n    :return: A CLI Profile\n    :rtype: azure.cli.core._profile.Profile\n    :raises: ImportError if azure-cli-core package is not available\n    \"\"\"\n\n    try:\n        from azure.cli.core._profile import Profile\n        from azure.cli.core._session import ACCOUNT\n        from azure.cli.core._environment import get_config_dir\n    except ImportError:\n        raise ImportError(\"You need to install 'azure-cli-core' to load CLI credentials\")\n\n\n    azure_folder = get_config_dir()\n    ACCOUNT.load(os.path.join(azure_folder, 'azureProfile.json'))\n    return Profile(storage=ACCOUNT)","method_path":"azure-common\/azure\/common\/credentials.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"get_azure_cli_credentials","method_code":"def get_azure_cli_credentials(resource=None, with_tenant=False):\n    profile = get_cli_profile()\n    cred, subscription_id, tenant_id = profile.get_login_credentials(resource=resource)\n    if with_tenant:\n        return cred, subscription_id, tenant_id\n    else:\n        return cred, subscription_id","method_summary":"Return Credentials and default SubscriptionID of current loaded profile of the CLI. Credentials will be the \"az login\"","original_method_code":"def get_azure_cli_credentials(resource=None, with_tenant=False):\n    \"\"\"Return Credentials and default SubscriptionID of current loaded profile of the CLI.\n\n    Credentials will be the \"az login\" command:\n    https:\/\/docs.microsoft.com\/cli\/azure\/authenticate-azure-cli\n\n    Default subscription ID is either the only one you have, or you can define it:\n    https:\/\/docs.microsoft.com\/cli\/azure\/manage-azure-subscriptions-azure-cli\n\n    .. versionadded:: 1.1.6\n\n    :param str resource: The alternative resource for credentials if not ARM (GraphRBac, etc.)\n    :param bool with_tenant: If True, return a three-tuple with last as tenant ID\n    :return: tuple of Credentials and SubscriptionID (and tenant ID if with_tenant)\n    :rtype: tuple\n    \"\"\"\n    profile = get_cli_profile()\n    cred, subscription_id, tenant_id = profile.get_login_credentials(resource=resource)\n    if with_tenant:\n        return cred, subscription_id, tenant_id\n    else:\n        return cred, subscription_id","method_path":"azure-common\/azure\/common\/credentials.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"PredictionOperations.resolve","method_code":"def resolve(\n            self, app_id, query, timezone_offset=None, verbose=None, staging=None, spell_check=None, bing_spell_check_subscription_key=None, log=None, custom_headers=None, raw=False, **operation_config):\n        \n        url = self.resolve.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True),\n            'appId': self._serialize.url(\"app_id\", app_id, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n        if timezone_offset is not None:\n            query_parameters['timezoneOffset'] = self._serialize.query(\"timezone_offset\", timezone_offset, 'float')\n        if verbose is not None:\n            query_parameters['verbose'] = self._serialize.query(\"verbose\", verbose, 'bool')\n        if staging is not None:\n            query_parameters['staging'] = self._serialize.query(\"staging\", staging, 'bool')\n        if spell_check is not None:\n            query_parameters['spellCheck'] = self._serialize.query(\"spell_check\", spell_check, 'bool')\n        if bing_spell_check_subscription_key is not None:\n            query_parameters['bing-spell-check-subscription-key'] = self._serialize.query(\"bing_spell_check_subscription_key\", bing_spell_check_subscription_key, 'str')\n        if log is not None:\n            query_parameters['log'] = self._serialize.query(\"log\", log, 'bool')\n\n        \n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        \n        body_content = self._serialize.body(query, 'str')\n\n        \n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.APIErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('LuisResult', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_summary":"Gets predictions for a given utterance, in the form of intents and entities. The current maximum query size is 500 characters.","original_method_code":"def resolve(\n            self, app_id, query, timezone_offset=None, verbose=None, staging=None, spell_check=None, bing_spell_check_subscription_key=None, log=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Gets predictions for a given utterance, in the form of intents and\n        entities. The current maximum query size is 500 characters.\n\n        :param app_id: The LUIS application ID (Guid).\n        :type app_id: str\n        :param query: The utterance to predict.\n        :type query: str\n        :param timezone_offset: The timezone offset for the location of the\n         request.\n        :type timezone_offset: float\n        :param verbose: If true, return all intents instead of just the top\n         scoring intent.\n        :type verbose: bool\n        :param staging: Use the staging endpoint slot.\n        :type staging: bool\n        :param spell_check: Enable spell checking.\n        :type spell_check: bool\n        :param bing_spell_check_subscription_key: The subscription key to use\n         when enabling Bing spell check\n        :type bing_spell_check_subscription_key: str\n        :param log: Log query (default is true)\n        :type log: bool\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: LuisResult or ClientRawResponse if raw=true\n        :rtype:\n         ~azure.cognitiveservices.language.luis.runtime.models.LuisResult or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`APIErrorException<azure.cognitiveservices.language.luis.runtime.models.APIErrorException>`\n        \"\"\"\n        # Construct URL\n        url = self.resolve.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True),\n            'appId': self._serialize.url(\"app_id\", app_id, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        if timezone_offset is not None:\n            query_parameters['timezoneOffset'] = self._serialize.query(\"timezone_offset\", timezone_offset, 'float')\n        if verbose is not None:\n            query_parameters['verbose'] = self._serialize.query(\"verbose\", verbose, 'bool')\n        if staging is not None:\n            query_parameters['staging'] = self._serialize.query(\"staging\", staging, 'bool')\n        if spell_check is not None:\n            query_parameters['spellCheck'] = self._serialize.query(\"spell_check\", spell_check, 'bool')\n        if bing_spell_check_subscription_key is not None:\n            query_parameters['bing-spell-check-subscription-key'] = self._serialize.query(\"bing_spell_check_subscription_key\", bing_spell_check_subscription_key, 'str')\n        if log is not None:\n            query_parameters['log'] = self._serialize.query(\"log\", log, 'bool')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(query, 'str')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.APIErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('LuisResult', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-cognitiveservices-language-luis\/azure\/cognitiveservices\/language\/luis\/runtime\/operations\/prediction_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"MixedRealityClient.check_name_availability_local","method_code":"def check_name_availability_local(\n            self, location, name, type, custom_headers=None, raw=False, **operation_config):\n        check_name_availability = models.CheckNameAvailabilityRequest(name=name, type=type)\n\n        \n        url = self.check_name_availability_local.metadata['url']\n        path_format_arguments = {\n            'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str'),\n            'location': self._serialize.url(\"location\", location, 'str', max_length=90, min_length=1, pattern=r'^[-\\w\\._\\(\\)]+$')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        \n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        \n        body_content = self._serialize.body(check_name_availability, 'CheckNameAvailabilityRequest')\n\n        \n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorResponseException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('CheckNameAvailabilityResponse', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_summary":"Check Name Availability for global uniqueness.","original_method_code":"def check_name_availability_local(\n            self, location, name, type, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Check Name Availability for global uniqueness.\n\n        :param location: The location in which uniqueness will be verified.\n        :type location: str\n        :param name: Resource Name To Verify\n        :type name: str\n        :param type: Fully qualified resource type which includes provider\n         namespace\n        :type type: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: CheckNameAvailabilityResponse or ClientRawResponse if\n         raw=true\n        :rtype: ~azure.mgmt.mixedreality.models.CheckNameAvailabilityResponse\n         or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.mixedreality.models.ErrorResponseException>`\n        \"\"\"\n        check_name_availability = models.CheckNameAvailabilityRequest(name=name, type=type)\n\n        # Construct URL\n        url = self.check_name_availability_local.metadata['url']\n        path_format_arguments = {\n            'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str'),\n            'location': self._serialize.url(\"location\", location, 'str', max_length=90, min_length=1, pattern=r'^[-\\w\\._\\(\\)]+$')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        # Construct body\n        body_content = self._serialize.body(check_name_availability, 'CheckNameAvailabilityRequest')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorResponseException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('CheckNameAvailabilityResponse', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-mgmt-mixedreality\/azure\/mgmt\/mixedreality\/mixed_reality_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_WinHttpRequest.open","method_code":"def open(self, method, url):\n        flag = VARIANT.create_bool_false()\n        _method = BSTR(method)\n        _url = BSTR(url)\n        _WinHttpRequest._Open(self, _method, _url, flag)","method_summary":"Opens the request.","original_method_code":"def open(self, method, url):\n        '''\n        Opens the request.\n\n        method:\n            the request VERB 'GET', 'POST', etc.\n        url:\n            the url to connect\n        '''\n        flag = VARIANT.create_bool_false()\n        _method = BSTR(method)\n        _url = BSTR(url)\n        _WinHttpRequest._Open(self, _method, _url, flag)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_WinHttpRequest.set_timeout","method_code":"def set_timeout(self, timeout_in_seconds):\n        timeout_in_ms = int(timeout_in_seconds * 1000)\n        _WinHttpRequest._SetTimeouts(\n            self, 0, timeout_in_ms, timeout_in_ms, timeout_in_ms)","method_summary":"Sets up the timeout for the request.","original_method_code":"def set_timeout(self, timeout_in_seconds):\n        ''' Sets up the timeout for the request. '''\n        timeout_in_ms = int(timeout_in_seconds * 1000)\n        _WinHttpRequest._SetTimeouts(\n            self, 0, timeout_in_ms, timeout_in_ms, timeout_in_ms)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_WinHttpRequest.set_request_header","method_code":"def set_request_header(self, name, value):\n        _name = BSTR(name)\n        _value = BSTR(value)\n        _WinHttpRequest._SetRequestHeader(self, _name, _value)","method_summary":"Sets the request header.","original_method_code":"def set_request_header(self, name, value):\n        ''' Sets the request header. '''\n\n        _name = BSTR(name)\n        _value = BSTR(value)\n        _WinHttpRequest._SetRequestHeader(self, _name, _value)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_WinHttpRequest.get_all_response_headers","method_code":"def get_all_response_headers(self):\n        bstr_headers = c_void_p()\n        _WinHttpRequest._GetAllResponseHeaders(self, byref(bstr_headers))\n        bstr_headers = ctypes.cast(bstr_headers, c_wchar_p)\n        headers = bstr_headers.value\n        _SysFreeString(bstr_headers)\n        return headers","method_summary":"Gets back all response headers.","original_method_code":"def get_all_response_headers(self):\n        ''' Gets back all response headers. '''\n\n        bstr_headers = c_void_p()\n        _WinHttpRequest._GetAllResponseHeaders(self, byref(bstr_headers))\n        bstr_headers = ctypes.cast(bstr_headers, c_wchar_p)\n        headers = bstr_headers.value\n        _SysFreeString(bstr_headers)\n        return headers","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_WinHttpRequest.send","method_code":"def send(self, request=None):\n        \n        if request is None:\n            var_empty = VARIANT.create_empty()\n            _WinHttpRequest._Send(self, var_empty)\n        else:  \n            _request = VARIANT.create_safearray_from_str(request)\n            _WinHttpRequest._Send(self, _request)","method_summary":"Sends the request body.","original_method_code":"def send(self, request=None):\n        ''' Sends the request body. '''\n\n        # Sends VT_EMPTY if it is GET, HEAD request.\n        if request is None:\n            var_empty = VARIANT.create_empty()\n            _WinHttpRequest._Send(self, var_empty)\n        else:  # Sends request body as SAFEArray.\n            _request = VARIANT.create_safearray_from_str(request)\n            _WinHttpRequest._Send(self, _request)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_WinHttpRequest.status","method_code":"def status(self):\n        status = c_long()\n        _WinHttpRequest._Status(self, byref(status))\n        return int(status.value)","method_summary":"Gets status of response.","original_method_code":"def status(self):\n        ''' Gets status of response. '''\n\n        status = c_long()\n        _WinHttpRequest._Status(self, byref(status))\n        return int(status.value)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_WinHttpRequest.status_text","method_code":"def status_text(self):\n        bstr_status_text = c_void_p()\n        _WinHttpRequest._StatusText(self, byref(bstr_status_text))\n        bstr_status_text = ctypes.cast(bstr_status_text, c_wchar_p)\n        status_text = bstr_status_text.value\n        _SysFreeString(bstr_status_text)\n        return status_text","method_summary":"Gets status text of response.","original_method_code":"def status_text(self):\n        ''' Gets status text of response. '''\n\n        bstr_status_text = c_void_p()\n        _WinHttpRequest._StatusText(self, byref(bstr_status_text))\n        bstr_status_text = ctypes.cast(bstr_status_text, c_wchar_p)\n        status_text = bstr_status_text.value\n        _SysFreeString(bstr_status_text)\n        return status_text","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_WinHttpRequest.response_body","method_code":"def response_body(self):\n        var_respbody = VARIANT()\n        _WinHttpRequest._ResponseBody(self, byref(var_respbody))\n        if var_respbody.is_safearray_of_bytes():\n            respbody = var_respbody.str_from_safearray()\n            return respbody\n        else:\n            return ''","method_summary":"Gets response body as a SAFEARRAY and converts the SAFEARRAY to str.","original_method_code":"def response_body(self):\n        '''\n        Gets response body as a SAFEARRAY and converts the SAFEARRAY to str.\n        '''\n        var_respbody = VARIANT()\n        _WinHttpRequest._ResponseBody(self, byref(var_respbody))\n        if var_respbody.is_safearray_of_bytes():\n            respbody = var_respbody.str_from_safearray()\n            return respbody\n        else:\n            return ''","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_WinHttpRequest.set_client_certificate","method_code":"def set_client_certificate(self, certificate):\n        _certificate = BSTR(certificate)\n        _WinHttpRequest._SetClientCertificate(self, _certificate)","method_summary":"Sets client certificate for the request.","original_method_code":"def set_client_certificate(self, certificate):\n        '''Sets client certificate for the request. '''\n        _certificate = BSTR(certificate)\n        _WinHttpRequest._SetClientCertificate(self, _certificate)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_HTTPConnection.putrequest","method_code":"def putrequest(self, method, uri):\n        protocol = unicode(self.protocol + ':\/\/')\n        url = protocol + self.host + unicode(uri)\n        self._httprequest.set_timeout(self.timeout)\n        self._httprequest.open(unicode(method), url)\n\n        \n        if self.cert_file is not None:\n            self._httprequest.set_client_certificate(unicode(self.cert_file))","method_summary":"Connects to host and sends the request.","original_method_code":"def putrequest(self, method, uri):\n        ''' Connects to host and sends the request. '''\n\n        protocol = unicode(self.protocol + ':\/\/')\n        url = protocol + self.host + unicode(uri)\n        self._httprequest.set_timeout(self.timeout)\n        self._httprequest.open(unicode(method), url)\n\n        # sets certificate for the connection if cert_file is set.\n        if self.cert_file is not None:\n            self._httprequest.set_client_certificate(unicode(self.cert_file))","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_HTTPConnection.putheader","method_code":"def putheader(self, name, value):\n        if sys.version_info < (3,):\n            name = str(name).decode('utf-8')\n            value = str(value).decode('utf-8')\n        self._httprequest.set_request_header(name, value)","method_summary":"Sends the headers of request.","original_method_code":"def putheader(self, name, value):\n        ''' Sends the headers of request. '''\n        if sys.version_info < (3,):\n            name = str(name).decode('utf-8')\n            value = str(value).decode('utf-8')\n        self._httprequest.set_request_header(name, value)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_HTTPConnection.send","method_code":"def send(self, request_body):\n        if not request_body:\n            self._httprequest.send()\n        else:\n            self._httprequest.send(request_body)","method_summary":"Sends request body.","original_method_code":"def send(self, request_body):\n        ''' Sends request body. '''\n        if not request_body:\n            self._httprequest.send()\n        else:\n            self._httprequest.send(request_body)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_HTTPConnection.getresponse","method_code":"def getresponse(self):\n        status = self._httprequest.status()\n        status_text = self._httprequest.status_text()\n\n        resp_headers = self._httprequest.get_all_response_headers()\n        fixed_headers = []\n        for resp_header in resp_headers.split('\\n'):\n            if (resp_header.startswith('\\t') or\\\n                resp_header.startswith(' ')) and fixed_headers:\n                \n                fixed_headers[-1] += resp_header\n            else:\n                fixed_headers.append(resp_header)\n\n        headers = []\n        for resp_header in fixed_headers:\n            if ':' in resp_header:\n                pos = resp_header.find(':')\n                headers.append(\n                    (resp_header[:pos].lower(), resp_header[pos + 1:].strip()))\n\n        body = self._httprequest.response_body()\n        length = len(body)\n\n        return _Response(status, status_text, length, headers, body)","method_summary":"Gets the response and generates the _Response object","original_method_code":"def getresponse(self):\n        ''' Gets the response and generates the _Response object'''\n        status = self._httprequest.status()\n        status_text = self._httprequest.status_text()\n\n        resp_headers = self._httprequest.get_all_response_headers()\n        fixed_headers = []\n        for resp_header in resp_headers.split('\\n'):\n            if (resp_header.startswith('\\t') or\\\n                resp_header.startswith(' ')) and fixed_headers:\n                # append to previous header\n                fixed_headers[-1] += resp_header\n            else:\n                fixed_headers.append(resp_header)\n\n        headers = []\n        for resp_header in fixed_headers:\n            if ':' in resp_header:\n                pos = resp_header.find(':')\n                headers.append(\n                    (resp_header[:pos].lower(), resp_header[pos + 1:].strip()))\n\n        body = self._httprequest.response_body()\n        length = len(body)\n\n        return _Response(status, status_text, length, headers, body)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_http\/winhttp.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_get_readable_id","method_code":"def _get_readable_id(id_name, id_prefix_to_skip):\n    \n    \n    pos = id_name.find('\/\/')\n    if pos != -1:\n        pos += 2\n        if id_prefix_to_skip:\n            pos = id_name.find(id_prefix_to_skip, pos)\n            if pos != -1:\n                pos += len(id_prefix_to_skip)\n        pos = id_name.find('\/', pos)\n        if pos != -1:\n            return id_name[pos + 1:]\n    return id_name","method_summary":"simplified an id to be more friendly for us people","original_method_code":"def _get_readable_id(id_name, id_prefix_to_skip):\n    \"\"\"simplified an id to be more friendly for us people\"\"\"\n    # id_name is in the form 'https:\/\/namespace.host.suffix\/name'\n    # where name may contain a forward slash!\n    pos = id_name.find('\/\/')\n    if pos != -1:\n        pos += 2\n        if id_prefix_to_skip:\n            pos = id_name.find(id_prefix_to_skip, pos)\n            if pos != -1:\n                pos += len(id_prefix_to_skip)\n        pos = id_name.find('\/', pos)\n        if pos != -1:\n            return id_name[pos + 1:]\n    return id_name","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_common_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_get_serialization_name","method_code":"def _get_serialization_name(element_name):\n    known = _KNOWN_SERIALIZATION_XFORMS.get(element_name)\n    if known is not None:\n        return known\n\n    if element_name.startswith('x_ms_'):\n        return element_name.replace('_', '-')\n    if element_name.endswith('_id'):\n        element_name = element_name.replace('_id', 'ID')\n    for name in ['content_', 'last_modified', 'if_', 'cache_control']:\n        if element_name.startswith(name):\n            element_name = element_name.replace('_', '-_')\n\n    return ''.join(name.capitalize() for name in element_name.split('_'))","method_summary":"converts a Python name into a serializable name","original_method_code":"def _get_serialization_name(element_name):\n    \"\"\"converts a Python name into a serializable name\"\"\"\n    known = _KNOWN_SERIALIZATION_XFORMS.get(element_name)\n    if known is not None:\n        return known\n\n    if element_name.startswith('x_ms_'):\n        return element_name.replace('_', '-')\n    if element_name.endswith('_id'):\n        element_name = element_name.replace('_id', 'ID')\n    for name in ['content_', 'last_modified', 'if_', 'cache_control']:\n        if element_name.startswith(name):\n            element_name = element_name.replace('_', '-_')\n\n    return ''.join(name.capitalize() for name in element_name.split('_'))","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_common_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"FaceOperations.verify_face_to_person","method_code":"def verify_face_to_person(\n            self, face_id, person_id, person_group_id=None, large_person_group_id=None, custom_headers=None, raw=False, **operation_config):\n        body = models.VerifyFaceToPersonRequest(face_id=face_id, person_group_id=person_group_id, large_person_group_id=large_person_group_id, person_id=person_id)\n\n        \n        url = self.verify_face_to_person.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n\n        \n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        \n        body_content = self._serialize.body(body, 'VerifyFaceToPersonRequest')\n\n        \n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.APIErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('VerifyResult', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_summary":"Verify whether two faces belong to a same person. Compares a face Id with a Person Id.","original_method_code":"def verify_face_to_person(\n            self, face_id, person_id, person_group_id=None, large_person_group_id=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Verify whether two faces belong to a same person. Compares a face Id\n        with a Person Id.\n\n        :param face_id: FaceId of the face, comes from Face - Detect\n        :type face_id: str\n        :param person_id: Specify a certain person in a person group or a\n         large person group. personId is created in PersonGroup Person - Create\n         or LargePersonGroup Person - Create.\n        :type person_id: str\n        :param person_group_id: Using existing personGroupId and personId for\n         fast loading a specified person. personGroupId is created in\n         PersonGroup - Create. Parameter personGroupId and largePersonGroupId\n         should not be provided at the same time.\n        :type person_group_id: str\n        :param large_person_group_id: Using existing largePersonGroupId and\n         personId for fast loading a specified person. largePersonGroupId is\n         created in LargePersonGroup - Create. Parameter personGroupId and\n         largePersonGroupId should not be provided at the same time.\n        :type large_person_group_id: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: VerifyResult or ClientRawResponse if raw=true\n        :rtype: ~azure.cognitiveservices.vision.face.models.VerifyResult or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`APIErrorException<azure.cognitiveservices.vision.face.models.APIErrorException>`\n        \"\"\"\n        body = models.VerifyFaceToPersonRequest(face_id=face_id, person_group_id=person_group_id, large_person_group_id=large_person_group_id, person_id=person_id)\n\n        # Construct URL\n        url = self.verify_face_to_person.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(body, 'VerifyFaceToPersonRequest')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.APIErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('VerifyResult', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-cognitiveservices-vision-face\/azure\/cognitiveservices\/vision\/face\/operations\/face_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_MinidomXmlToObject.get_entry_properties_from_node","method_code":"def get_entry_properties_from_node(entry, include_id, id_prefix_to_skip=None, use_title_as_id=False):\n        properties = {}\n\n        etag = entry.getAttributeNS(METADATA_NS, 'etag')\n        if etag:\n            properties['etag'] = etag\n        for updated in _MinidomXmlToObject.get_child_nodes(entry, 'updated'):\n            properties['updated'] = updated.firstChild.nodeValue\n        for name in _MinidomXmlToObject.get_children_from_path(entry, 'author', 'name'):\n            if name.firstChild is not None:\n                properties['author'] = name.firstChild.nodeValue\n\n        if include_id:\n            if use_title_as_id:\n                for title in _MinidomXmlToObject.get_child_nodes(entry, 'title'):\n                    properties['name'] = title.firstChild.nodeValue\n            else:\n                \n                for id in _MinidomXmlToObject.get_child_nodes(entry, 'id'):\n                    properties['name'] = _get_readable_id(\n                        id.firstChild.nodeValue, id_prefix_to_skip)\n\n        return properties","method_summary":"get properties from entry xml","original_method_code":"def get_entry_properties_from_node(entry, include_id, id_prefix_to_skip=None, use_title_as_id=False):\n        ''' get properties from entry xml '''\n        properties = {}\n\n        etag = entry.getAttributeNS(METADATA_NS, 'etag')\n        if etag:\n            properties['etag'] = etag\n        for updated in _MinidomXmlToObject.get_child_nodes(entry, 'updated'):\n            properties['updated'] = updated.firstChild.nodeValue\n        for name in _MinidomXmlToObject.get_children_from_path(entry, 'author', 'name'):\n            if name.firstChild is not None:\n                properties['author'] = name.firstChild.nodeValue\n\n        if include_id:\n            if use_title_as_id:\n                for title in _MinidomXmlToObject.get_child_nodes(entry, 'title'):\n                    properties['name'] = title.firstChild.nodeValue\n            else:\n                # TODO: check if this is used\n                for id in _MinidomXmlToObject.get_child_nodes(entry, 'id'):\n                    properties['name'] = _get_readable_id(\n                        id.firstChild.nodeValue, id_prefix_to_skip)\n\n        return properties","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_MinidomXmlToObject.get_children_from_path","method_code":"def get_children_from_path(node, *path):\n        cur = node\n        for index, child in enumerate(path):\n            if isinstance(child, _strtype):\n                next = _MinidomXmlToObject.get_child_nodes(cur, child)\n            else:\n                next = _MinidomXmlToObject._get_child_nodesNS(cur, *child)\n            if index == len(path) - 1:\n                return next\n            elif not next:\n                break\n\n            cur = next[0]\n        return []","method_summary":"descends through a hierarchy of nodes returning the list of children at the inner most level. Only returns children who share a common parent, not cousins.","original_method_code":"def get_children_from_path(node, *path):\n        '''descends through a hierarchy of nodes returning the list of children\n        at the inner most level.  Only returns children who share a common parent,\n        not cousins.'''\n        cur = node\n        for index, child in enumerate(path):\n            if isinstance(child, _strtype):\n                next = _MinidomXmlToObject.get_child_nodes(cur, child)\n            else:\n                next = _MinidomXmlToObject._get_child_nodesNS(cur, *child)\n            if index == len(path) - 1:\n                return next\n            elif not next:\n                break\n\n            cur = next[0]\n        return []","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_MinidomXmlToObject._find_namespaces_from_child","method_code":"def _find_namespaces_from_child(parent, child, namespaces):\n        for cur_child in parent.childNodes:\n            if cur_child is child:\n                return True\n            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):\n                \n                for key in cur_child.attributes.keys():\n                    if key.startswith('xmlns:') or key == 'xmlns':\n                        namespaces[key] = cur_child.attributes[key]\n                break\n        return False","method_summary":"Recursively searches from the parent to the child, gathering all the applicable namespaces along the way","original_method_code":"def _find_namespaces_from_child(parent, child, namespaces):\n        \"\"\"Recursively searches from the parent to the child,\n        gathering all the applicable namespaces along the way\"\"\"\n        for cur_child in parent.childNodes:\n            if cur_child is child:\n                return True\n            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):\n                # we are the parent node\n                for key in cur_child.attributes.keys():\n                    if key.startswith('xmlns:') or key == 'xmlns':\n                        namespaces[key] = cur_child.attributes[key]\n                break\n        return False","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_ServiceBusManagementXmlSerializer.xml_to_namespace","method_code":"def xml_to_namespace(xmlstr):\n        xmldoc = minidom.parseString(xmlstr)\n        namespace = ServiceBusNamespace()\n\n        mappings = (\n            ('Name', 'name', None),\n            ('Region', 'region', None),\n            ('DefaultKey', 'default_key', None),\n            ('Status', 'status', None),\n            ('CreatedAt', 'created_at', None),\n            ('AcsManagementEndpoint', 'acs_management_endpoint', None),\n            ('ServiceBusEndpoint', 'servicebus_endpoint', None),\n            ('ConnectionString', 'connection_string', None),\n            ('SubscriptionId', 'subscription_id', None),\n            ('Enabled', 'enabled', _parse_bool),\n        )\n\n        for desc in _MinidomXmlToObject.get_children_from_path(\n            xmldoc,\n            'entry',\n            'content',\n            'NamespaceDescription'):\n            for xml_name, field_name, conversion_func in mappings:\n                node_value = _MinidomXmlToObject.get_first_child_node_value(desc, xml_name)\n                if node_value is not None:\n                    if conversion_func is not None:\n                        node_value = conversion_func(node_value)\n                    setattr(namespace, field_name, node_value)\n\n        return namespace","method_summary":"Converts xml response to service bus namespace The xml format for","original_method_code":"def xml_to_namespace(xmlstr):\n        '''Converts xml response to service bus namespace\n\n        The xml format for namespace:\n<entry>\n<id>uuid:00000000-0000-0000-0000-000000000000;id=0000000<\/id>\n<title type=\"text\">myunittests<\/title>\n<updated>2012-08-22T16:48:10Z<\/updated>\n<content type=\"application\/xml\">\n    <NamespaceDescription\n        xmlns=\"http:\/\/schemas.microsoft.com\/netservices\/2010\/10\/servicebus\/connect\"\n        xmlns:i=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\">\n    <Name>myunittests<\/Name>\n    <Region>West US<\/Region>\n    <DefaultKey>0000000000000000000000000000000000000000000=<\/DefaultKey>\n    <Status>Active<\/Status>\n    <CreatedAt>2012-08-22T16:48:10.217Z<\/CreatedAt>\n    <AcsManagementEndpoint>https:\/\/myunittests-sb.accesscontrol.windows.net\/<\/AcsManagementEndpoint>\n    <ServiceBusEndpoint>https:\/\/myunittests.servicebus.windows.net\/<\/ServiceBusEndpoint>\n    <ConnectionString>Endpoint=sb:\/\/myunittests.servicebus.windows.net\/;SharedSecretIssuer=owner;SharedSecretValue=0000000000000000000000000000000000000000000=<\/ConnectionString>\n    <SubscriptionId>00000000000000000000000000000000<\/SubscriptionId>\n    <Enabled>true<\/Enabled>\n    <\/NamespaceDescription>\n<\/content>\n<\/entry>\n        '''\n        xmldoc = minidom.parseString(xmlstr)\n        namespace = ServiceBusNamespace()\n\n        mappings = (\n            ('Name', 'name', None),\n            ('Region', 'region', None),\n            ('DefaultKey', 'default_key', None),\n            ('Status', 'status', None),\n            ('CreatedAt', 'created_at', None),\n            ('AcsManagementEndpoint', 'acs_management_endpoint', None),\n            ('ServiceBusEndpoint', 'servicebus_endpoint', None),\n            ('ConnectionString', 'connection_string', None),\n            ('SubscriptionId', 'subscription_id', None),\n            ('Enabled', 'enabled', _parse_bool),\n        )\n\n        for desc in _MinidomXmlToObject.get_children_from_path(\n            xmldoc,\n            'entry',\n            'content',\n            'NamespaceDescription'):\n            for xml_name, field_name, conversion_func in mappings:\n                node_value = _MinidomXmlToObject.get_first_child_node_value(desc, xml_name)\n                if node_value is not None:\n                    if conversion_func is not None:\n                        node_value = conversion_func(node_value)\n                    setattr(namespace, field_name, node_value)\n\n        return namespace","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_ServiceBusManagementXmlSerializer.xml_to_region","method_code":"def xml_to_region(xmlstr):\n        xmldoc = minidom.parseString(xmlstr)\n        region = ServiceBusRegion()\n\n        for desc in _MinidomXmlToObject.get_children_from_path(xmldoc, 'entry', 'content',\n                                            'RegionCodeDescription'):\n            node_value = _MinidomXmlToObject.get_first_child_node_value(desc, 'Code')\n            if node_value is not None:\n                region.code = node_value\n            node_value = _MinidomXmlToObject.get_first_child_node_value(desc, 'FullName')\n            if node_value is not None:\n                region.fullname = node_value\n\n        return region","method_summary":"Converts xml response to service bus region The xml format for","original_method_code":"def xml_to_region(xmlstr):\n        '''Converts xml response to service bus region\n\n        The xml format for region:\n<entry>\n<id>uuid:157c311f-081f-4b4a-a0ba-a8f990ffd2a3;id=1756759<\/id>\n<title type=\"text\"><\/title>\n<updated>2013-04-10T18:25:29Z<\/updated>\n<content type=\"application\/xml\">\n    <RegionCodeDescription\n        xmlns=\"http:\/\/schemas.microsoft.com\/netservices\/2010\/10\/servicebus\/connect\"\n        xmlns:i=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\">\n    <Code>East Asia<\/Code>\n    <FullName>East Asia<\/FullName>\n    <\/RegionCodeDescription>\n<\/content>\n<\/entry>\n          '''\n        xmldoc = minidom.parseString(xmlstr)\n        region = ServiceBusRegion()\n\n        for desc in _MinidomXmlToObject.get_children_from_path(xmldoc, 'entry', 'content',\n                                            'RegionCodeDescription'):\n            node_value = _MinidomXmlToObject.get_first_child_node_value(desc, 'Code')\n            if node_value is not None:\n                region.code = node_value\n            node_value = _MinidomXmlToObject.get_first_child_node_value(desc, 'FullName')\n            if node_value is not None:\n                region.fullname = node_value\n\n        return region","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_ServiceBusManagementXmlSerializer.xml_to_namespace_availability","method_code":"def xml_to_namespace_availability(xmlstr):\n        xmldoc = minidom.parseString(xmlstr)\n        availability = AvailabilityResponse()\n\n        for desc in _MinidomXmlToObject.get_children_from_path(xmldoc, 'entry', 'content',\n                                            'NamespaceAvailability'):\n            node_value = _MinidomXmlToObject.get_first_child_node_value(desc, 'Result')\n            if node_value is not None:\n                availability.result = _parse_bool(node_value)\n\n        return availability","method_summary":"Converts xml response to service bus namespace availability The xml","original_method_code":"def xml_to_namespace_availability(xmlstr):\n        '''Converts xml response to service bus namespace availability\n\n        The xml format:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<entry xmlns=\"http:\/\/www.w3.org\/2005\/Atom\">\n    <id>uuid:9fc7c652-1856-47ab-8d74-cd31502ea8e6;id=3683292<\/id>\n    <title type=\"text\"><\/title>\n    <updated>2013-04-16T03:03:37Z<\/updated>\n    <content type=\"application\/xml\">\n        <NamespaceAvailability\n            xmlns=\"http:\/\/schemas.microsoft.com\/netservices\/2010\/10\/servicebus\/connect\"\n            xmlns:i=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\">\n            <Result>false<\/Result>\n        <\/NamespaceAvailability>\n    <\/content>\n<\/entry>\n        '''\n        xmldoc = minidom.parseString(xmlstr)\n        availability = AvailabilityResponse()\n\n        for desc in _MinidomXmlToObject.get_children_from_path(xmldoc, 'entry', 'content',\n                                            'NamespaceAvailability'):\n            node_value = _MinidomXmlToObject.get_first_child_node_value(desc, 'Result')\n            if node_value is not None:\n                availability.result = _parse_bool(node_value)\n\n        return availability","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_ServiceBusManagementXmlSerializer.xml_to_metrics","method_code":"def xml_to_metrics(xmlstr, object_type):\n        xmldoc = minidom.parseString(xmlstr)\n        return_obj = object_type()\n\n        members = dict(vars(return_obj))\n\n        \n        for xml_entry in _MinidomXmlToObject.get_children_from_path(xmldoc,\n                                                 'entry'):\n            for node in _MinidomXmlToObject.get_children_from_path(xml_entry,\n                                                'content',\n                                                'properties'):\n                for name in members:\n                    xml_name = _get_serialization_name(name)\n                    children = _MinidomXmlToObject.get_child_nodes(node, xml_name)\n                    if not children:\n                        continue\n                    child = children[0]\n                    node_type = child.getAttributeNS(\"http:\/\/schemas.microsoft.com\/ado\/2007\/08\/dataservices\/metadata\", 'type')\n                    node_value = _ServiceBusManagementXmlSerializer.odata_converter(child.firstChild.nodeValue, node_type)\n                    setattr(return_obj, name, node_value)\n            for name, value in _MinidomXmlToObject.get_entry_properties_from_node(\n                xml_entry,\n                include_id=True,\n                use_title_as_id=False).items():\n                if name in members:\n                    continue  \n                setattr(return_obj, name, value)\n        return return_obj","method_summary":"Converts xml response to service bus metrics objects The xml format for MetricProperties <entry> <id>","original_method_code":"def xml_to_metrics(xmlstr, object_type):\n        '''Converts xml response to service bus metrics objects\n\n        The xml format for MetricProperties\n<entry>\n    <id>https:\/\/sbgm.windows.net\/Metrics(\\'listeners.active\\')<\/id>\n    <title\/>\n    <updated>2014-10-09T11:56:50Z<\/updated>\n    <author>\n        <name\/>\n    <\/author>\n    <content type=\"application\/xml\">\n        <m:properties>\n            <d:Name>listeners.active<\/d:Name>\n            <d:PrimaryAggregation>Average<\/d:PrimaryAggregation>\n            <d:Unit>Count<\/d:Unit>\n            <d:DisplayName>Active listeners<\/d:DisplayName>\n        <\/m:properties>\n    <\/content>\n<\/entry>\n\n        The xml format for MetricValues\n    <entry>\n        <id>https:\/\/sbgm.windows.net\/MetricValues(datetime\\'2014-10-02T00:00:00Z\\')<\/id>\n        <title\/>\n        <updated>2014-10-09T18:38:28Z<\/updated>\n        <author>\n            <name\/>\n        <\/author>\n        <content type=\"application\/xml\">\n            <m:properties>\n                <d:Timestamp m:type=\"Edm.DateTime\">2014-10-02T00:00:00Z<\/d:Timestamp>\n                <d:Min m:type=\"Edm.Int64\">-118<\/d:Min>\n                <d:Max m:type=\"Edm.Int64\">15<\/d:Max>\n                <d:Average m:type=\"Edm.Single\">-78.44444<\/d:Average>\n                <d:Total m:type=\"Edm.Int64\">0<\/d:Total>\n            <\/m:properties>\n        <\/content>\n    <\/entry>\n        '''\n\n        xmldoc = minidom.parseString(xmlstr)\n        return_obj = object_type()\n\n        members = dict(vars(return_obj))\n\n        # Only one entry here\n        for xml_entry in _MinidomXmlToObject.get_children_from_path(xmldoc,\n                                                 'entry'):\n            for node in _MinidomXmlToObject.get_children_from_path(xml_entry,\n                                                'content',\n                                                'properties'):\n                for name in members:\n                    xml_name = _get_serialization_name(name)\n                    children = _MinidomXmlToObject.get_child_nodes(node, xml_name)\n                    if not children:\n                        continue\n                    child = children[0]\n                    node_type = child.getAttributeNS(\"http:\/\/schemas.microsoft.com\/ado\/2007\/08\/dataservices\/metadata\", 'type')\n                    node_value = _ServiceBusManagementXmlSerializer.odata_converter(child.firstChild.nodeValue, node_type)\n                    setattr(return_obj, name, node_value)\n            for name, value in _MinidomXmlToObject.get_entry_properties_from_node(\n                xml_entry,\n                include_id=True,\n                use_title_as_id=False).items():\n                if name in members:\n                    continue  # Do not override if already members\n                setattr(return_obj, name, value)\n        return return_obj","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"RunbookDraftOperations.replace_content","method_code":"def replace_content(\n            self, resource_group_name, automation_account_name, runbook_name, runbook_content, custom_headers=None, raw=False, callback=None, polling=True, **operation_config):\n        raw_result = self._replace_content_initial(\n            resource_group_name=resource_group_name,\n            automation_account_name=automation_account_name,\n            runbook_name=runbook_name,\n            runbook_content=runbook_content,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            header_dict = {\n                'location': 'str',\n            }\n            deserialized = self._deserialize('object', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                client_raw_response.add_headers(header_dict)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Replaces the runbook draft content.","original_method_code":"def replace_content(\n            self, resource_group_name, automation_account_name, runbook_name, runbook_content, custom_headers=None, raw=False, callback=None, polling=True, **operation_config):\n        \"\"\"Replaces the runbook draft content.\n\n        :param resource_group_name: Name of an Azure Resource group.\n        :type resource_group_name: str\n        :param automation_account_name: The name of the automation account.\n        :type automation_account_name: str\n        :param runbook_name: The runbook name.\n        :type runbook_name: str\n        :param runbook_content: The\u00a0runbook\u00a0draft\u00a0content.\n        :type runbook_content: Generator\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns object or\n         ClientRawResponse<object> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[Generator]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[Generator]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._replace_content_initial(\n            resource_group_name=resource_group_name,\n            automation_account_name=automation_account_name,\n            runbook_name=runbook_name,\n            runbook_content=runbook_content,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            header_dict = {\n                'location': 'str',\n            }\n            deserialized = self._deserialize('object', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                client_raw_response.add_headers(header_dict)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-automation\/azure\/mgmt\/automation\/operations\/runbook_draft_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"DomainsOperations.list_recommendations","method_code":"def list_recommendations(\n            self, keywords=None, max_domain_recommendations=None, custom_headers=None, raw=False, **operation_config):\n        parameters = models.DomainRecommendationSearchParameters(keywords=keywords, max_domain_recommendations=max_domain_recommendations)\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                \n                url = self.list_recommendations.metadata['url']\n                path_format_arguments = {\n                    'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                \n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            \n            header_parameters = {}\n            header_parameters['Accept'] = 'application\/json'\n            header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            \n            body_content = self._serialize.body(parameters, 'DomainRecommendationSearchParameters')\n\n            \n            request = self._client.post(url, query_parameters, header_parameters, body_content)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.DefaultErrorResponseException(self._deserialize, response)\n\n            return response\n\n        \n        deserialized = models.NameIdentifierPaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.NameIdentifierPaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized","method_summary":"Get domain name recommendations based on keywords. Get domain name recommendations based on keywords.","original_method_code":"def list_recommendations(\n            self, keywords=None, max_domain_recommendations=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Get domain name recommendations based on keywords.\n\n        Get domain name recommendations based on keywords.\n\n        :param keywords: Keywords to be used for generating domain\n         recommendations.\n        :type keywords: str\n        :param max_domain_recommendations: Maximum number of recommendations.\n        :type max_domain_recommendations: int\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: An iterator like instance of NameIdentifier\n        :rtype:\n         ~azure.mgmt.web.models.NameIdentifierPaged[~azure.mgmt.web.models.NameIdentifier]\n        :raises:\n         :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`\n        \"\"\"\n        parameters = models.DomainRecommendationSearchParameters(keywords=keywords, max_domain_recommendations=max_domain_recommendations)\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                # Construct URL\n                url = self.list_recommendations.metadata['url']\n                path_format_arguments = {\n                    'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                # Construct parameters\n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            # Construct headers\n            header_parameters = {}\n            header_parameters['Accept'] = 'application\/json'\n            header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            # Construct body\n            body_content = self._serialize.body(parameters, 'DomainRecommendationSearchParameters')\n\n            # Construct and send request\n            request = self._client.post(url, query_parameters, header_parameters, body_content)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.DefaultErrorResponseException(self._deserialize, response)\n\n            return response\n\n        # Deserialize response\n        deserialized = models.NameIdentifierPaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.NameIdentifierPaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-mgmt-web\/azure\/mgmt\/web\/operations\/domains_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"KnowledgebaseOperations.update","method_code":"def update(\n            self, kb_id, update_kb, custom_headers=None, raw=False, **operation_config):\n        \n        url = self.update.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True),\n            'kbId': self._serialize.url(\"kb_id\", kb_id, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n\n        \n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        \n        body_content = self._serialize.body(update_kb, 'UpdateKbOperationDTO')\n\n        \n        request = self._client.patch(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [202]:\n            raise models.ErrorResponseException(self._deserialize, response)\n\n        deserialized = None\n        header_dict = {}\n\n        if response.status_code == 202:\n            deserialized = self._deserialize('Operation', response)\n            header_dict = {\n                'Location': 'str',\n            }\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            client_raw_response.add_headers(header_dict)\n            return client_raw_response\n\n        return deserialized","method_summary":"Asynchronous operation to modify a knowledgebase.","original_method_code":"def update(\n            self, kb_id, update_kb, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Asynchronous operation to modify a knowledgebase.\n\n        :param kb_id: Knowledgebase id.\n        :type kb_id: str\n        :param update_kb: Post body of the request.\n        :type update_kb:\n         ~azure.cognitiveservices.knowledge.qnamaker.models.UpdateKbOperationDTO\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: Operation or ClientRawResponse if raw=true\n        :rtype: ~azure.cognitiveservices.knowledge.qnamaker.models.Operation\n         or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`ErrorResponseException<azure.cognitiveservices.knowledge.qnamaker.models.ErrorResponseException>`\n        \"\"\"\n        # Construct URL\n        url = self.update.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True),\n            'kbId': self._serialize.url(\"kb_id\", kb_id, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(update_kb, 'UpdateKbOperationDTO')\n\n        # Construct and send request\n        request = self._client.patch(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [202]:\n            raise models.ErrorResponseException(self._deserialize, response)\n\n        deserialized = None\n        header_dict = {}\n\n        if response.status_code == 202:\n            deserialized = self._deserialize('Operation', response)\n            header_dict = {\n                'Location': 'str',\n            }\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            client_raw_response.add_headers(header_dict)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-cognitiveservices-knowledge-qnamaker\/azure\/cognitiveservices\/knowledge\/qnamaker\/operations\/knowledgebase_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"UsersOperations.get_member_groups","method_code":"def get_member_groups(\n            self, object_id, security_enabled_only, additional_properties=None, custom_headers=None, raw=False, **operation_config):\n        parameters = models.UserGetMemberGroupsParameters(additional_properties=additional_properties, security_enabled_only=security_enabled_only)\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                \n                url = self.get_member_groups.metadata['url']\n                path_format_arguments = {\n                    'objectId': self._serialize.url(\"object_id\", object_id, 'str'),\n                    'tenantID': self._serialize.url(\"self.config.tenant_id\", self.config.tenant_id, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                \n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            \n            header_parameters = {}\n            header_parameters['Accept'] = 'application\/json'\n            header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            \n            body_content = self._serialize.body(parameters, 'UserGetMemberGroupsParameters')\n\n            \n            request = self._client.post(url, query_parameters, header_parameters, body_content)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.GraphErrorException(self._deserialize, response)\n\n            return response\n\n        \n        deserialized = models.StrPaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.StrPaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized","method_summary":"Gets a collection that contains the object IDs of the groups of which the user is a member.","original_method_code":"def get_member_groups(\n            self, object_id, security_enabled_only, additional_properties=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Gets a collection that contains the object IDs of the groups of which\n        the user is a member.\n\n        :param object_id: The object ID of the user for which to get group\n         membership.\n        :type object_id: str\n        :param security_enabled_only: If true, only membership in\n         security-enabled groups should be checked. Otherwise, membership in\n         all groups should be checked.\n        :type security_enabled_only: bool\n        :param additional_properties: Unmatched properties from the message\n         are deserialized this collection\n        :type additional_properties: dict[str, object]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: An iterator like instance of str\n        :rtype: ~azure.graphrbac.models.StrPaged[str]\n        :raises:\n         :class:`GraphErrorException<azure.graphrbac.models.GraphErrorException>`\n        \"\"\"\n        parameters = models.UserGetMemberGroupsParameters(additional_properties=additional_properties, security_enabled_only=security_enabled_only)\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                # Construct URL\n                url = self.get_member_groups.metadata['url']\n                path_format_arguments = {\n                    'objectId': self._serialize.url(\"object_id\", object_id, 'str'),\n                    'tenantID': self._serialize.url(\"self.config.tenant_id\", self.config.tenant_id, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                # Construct parameters\n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            # Construct headers\n            header_parameters = {}\n            header_parameters['Accept'] = 'application\/json'\n            header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            # Construct body\n            body_content = self._serialize.body(parameters, 'UserGetMemberGroupsParameters')\n\n            # Construct and send request\n            request = self._client.post(url, query_parameters, header_parameters, body_content)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.GraphErrorException(self._deserialize, response)\n\n            return response\n\n        # Deserialize response\n        deserialized = models.StrPaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.StrPaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-graphrbac\/azure\/graphrbac\/operations\/users_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"build_package_from_pr_number","method_code":"def build_package_from_pr_number(gh_token, sdk_id, pr_number, output_folder, *, with_comment=False):\n    con = Github(gh_token)\n    repo = con.get_repo(sdk_id)\n    sdk_pr = repo.get_pull(pr_number)\n    \n    package_names = {f.filename.split('\/')[0] for f in sdk_pr.get_files() if f.filename.startswith(\"azure\")}\n    absolute_output_folder = Path(output_folder).resolve()\n\n    with tempfile.TemporaryDirectory() as temp_dir, \\\n            manage_git_folder(gh_token, Path(temp_dir) \/ Path(\"sdk\"), sdk_id, pr_number=pr_number) as sdk_folder:\n\n        for package_name in package_names:\n            _LOGGER.debug(\"Build {}\".format(package_name))\n            execute_simple_command(\n                [\"python\", \".\/build_package.py\", \"--dest\", str(absolute_output_folder), package_name],\n                cwd=sdk_folder\n            )\n            _LOGGER.debug(\"Build finished: {}\".format(package_name))\n\n    if with_comment:\n        files = [f.name for f in absolute_output_folder.iterdir()]\n        comment_message = None\n        dashboard = DashboardCommentableObject(sdk_pr, \"(message created by the CI based on PR content)\")\n        try:\n            installation_message = build_installation_message(sdk_pr)\n            download_message = build_download_message(sdk_pr, files)\n            comment_message = installation_message + \"\\n\\n\" + download_message\n            dashboard.create_comment(comment_message)\n        except Exception:\n            _LOGGER.critical(\"Unable to do PR comment:\\n%s\", comment_message)","method_summary":"Will clone the given PR branch and vuild the package with the given name.","original_method_code":"def build_package_from_pr_number(gh_token, sdk_id, pr_number, output_folder, *, with_comment=False):\n    \"\"\"Will clone the given PR branch and vuild the package with the given name.\"\"\"\n\n    con = Github(gh_token)\n    repo = con.get_repo(sdk_id)\n    sdk_pr = repo.get_pull(pr_number)\n    # \"get_files\" of Github only download the first 300 files. Might not be enough.\n    package_names = {f.filename.split('\/')[0] for f in sdk_pr.get_files() if f.filename.startswith(\"azure\")}\n    absolute_output_folder = Path(output_folder).resolve()\n\n    with tempfile.TemporaryDirectory() as temp_dir, \\\n            manage_git_folder(gh_token, Path(temp_dir) \/ Path(\"sdk\"), sdk_id, pr_number=pr_number) as sdk_folder:\n\n        for package_name in package_names:\n            _LOGGER.debug(\"Build {}\".format(package_name))\n            execute_simple_command(\n                [\"python\", \".\/build_package.py\", \"--dest\", str(absolute_output_folder), package_name],\n                cwd=sdk_folder\n            )\n            _LOGGER.debug(\"Build finished: {}\".format(package_name))\n\n    if with_comment:\n        files = [f.name for f in absolute_output_folder.iterdir()]\n        comment_message = None\n        dashboard = DashboardCommentableObject(sdk_pr, \"(message created by the CI based on PR content)\")\n        try:\n            installation_message = build_installation_message(sdk_pr)\n            download_message = build_download_message(sdk_pr, files)\n            comment_message = installation_message + \"\\n\\n\" + download_message\n            dashboard.create_comment(comment_message)\n        except Exception:\n            _LOGGER.critical(\"Unable to do PR comment:\\n%s\", comment_message)","method_path":"azure-sdk-tools\/packaging_tools\/drop_tools.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"RedisOperations.import_data","method_code":"def import_data(\n            self, resource_group_name, name, files, format=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._import_data_initial(\n            resource_group_name=resource_group_name,\n            name=name,\n            files=files,\n            format=format,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Import data into Redis cache.","original_method_code":"def import_data(\n            self, resource_group_name, name, files, format=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Import data into Redis cache.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param name: The name of the Redis cache.\n        :type name: str\n        :param files: files to import.\n        :type files: list[str]\n        :param format: File format.\n        :type format: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._import_data_initial(\n            resource_group_name=resource_group_name,\n            name=name,\n            files=files,\n            format=format,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-redis\/azure\/mgmt\/redis\/operations\/redis_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"RunbookOperations.publish","method_code":"def publish(\n            self, resource_group_name, automation_account_name, runbook_name, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._publish_initial(\n            resource_group_name=resource_group_name,\n            automation_account_name=automation_account_name,\n            runbook_name=runbook_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                client_raw_response.add_headers({\n                    'location': 'str',\n                })\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Publish runbook draft.","original_method_code":"def publish(\n            self, resource_group_name, automation_account_name, runbook_name, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Publish runbook draft.\n\n        :param resource_group_name: Name of an Azure Resource group.\n        :type resource_group_name: str\n        :param automation_account_name: The name of the automation account.\n        :type automation_account_name: str\n        :param runbook_name: The parameters supplied to the publish runbook\n         operation.\n        :type runbook_name: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._publish_initial(\n            resource_group_name=resource_group_name,\n            automation_account_name=automation_account_name,\n            runbook_name=runbook_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                client_raw_response.add_headers({\n                    'location': 'str',\n                })\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-automation\/azure\/mgmt\/automation\/operations\/runbook_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"AlterationsOperations.replace","method_code":"def replace(\n            self, word_alterations, custom_headers=None, raw=False, **operation_config):\n        word_alterations1 = models.WordAlterationsDTO(word_alterations=word_alterations)\n\n        \n        url = self.replace.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n\n        \n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        \n        body_content = self._serialize.body(word_alterations1, 'WordAlterationsDTO')\n\n        \n        request = self._client.put(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [204]:\n            raise models.ErrorResponseException(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response","method_summary":"Replace alterations data.","original_method_code":"def replace(\n            self, word_alterations, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Replace alterations data.\n\n        :param word_alterations: Collection of word alterations.\n        :type word_alterations:\n         list[~azure.cognitiveservices.knowledge.qnamaker.models.AlterationsDTO]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`ErrorResponseException<azure.cognitiveservices.knowledge.qnamaker.models.ErrorResponseException>`\n        \"\"\"\n        word_alterations1 = models.WordAlterationsDTO(word_alterations=word_alterations)\n\n        # Construct URL\n        url = self.replace.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(word_alterations1, 'WordAlterationsDTO')\n\n        # Construct and send request\n        request = self._client.put(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [204]:\n            raise models.ErrorResponseException(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response","method_path":"azure-cognitiveservices-knowledge-qnamaker\/azure\/cognitiveservices\/knowledge\/qnamaker\/operations\/alterations_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.regenerate_storage_account_keys","method_code":"def regenerate_storage_account_keys(self, service_name, key_type):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('key_type', key_type)\n        return self._perform_post(\n            self._get_storage_service_path(\n                service_name) + '\/keys?action=regenerate',\n            _XmlSerializer.regenerate_keys_to_xml(\n                key_type),\n            StorageService)","method_summary":"Regenerates the primary or secondary access key for the specified storage account.","original_method_code":"def regenerate_storage_account_keys(self, service_name, key_type):\n        '''\n        Regenerates the primary or secondary access key for the specified\n        storage account.\n\n        service_name:\n            Name of the storage service account.\n        key_type:\n            Specifies which key to regenerate. Valid values are:\n            Primary, Secondary\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('key_type', key_type)\n        return self._perform_post(\n            self._get_storage_service_path(\n                service_name) + '\/keys?action=regenerate',\n            _XmlSerializer.regenerate_keys_to_xml(\n                key_type),\n            StorageService)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.create_storage_account","method_code":"def create_storage_account(self, service_name, description, label,\n                               affinity_group=None, location=None,\n                               geo_replication_enabled=None,\n                               extended_properties=None,\n                               account_type='Standard_GRS'):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('description', description)\n        _validate_not_none('label', label)\n        if affinity_group is None and location is None:\n            raise ValueError(\n                'location or affinity_group must be specified')\n        if affinity_group is not None and location is not None:\n            raise ValueError(\n                'Only one of location or affinity_group needs to be specified')\n        if geo_replication_enabled == False:\n            account_type = 'Standard_LRS'\n        return self._perform_post(\n            self._get_storage_service_path(),\n            _XmlSerializer.create_storage_service_input_to_xml(\n                service_name,\n                description,\n                label,\n                affinity_group,\n                location,\n                account_type,\n                extended_properties),\n            as_async=True)","method_summary":"Creates a new storage account in Windows Azure.","original_method_code":"def create_storage_account(self, service_name, description, label,\n                               affinity_group=None, location=None,\n                               geo_replication_enabled=None,\n                               extended_properties=None,\n                               account_type='Standard_GRS'):\n        '''\n        Creates a new storage account in Windows Azure.\n\n        service_name:\n            A name for the storage account that is unique within Windows Azure.\n            Storage account names must be between 3 and 24 characters in length\n            and use numbers and lower-case letters only.\n        description:\n            A description for the storage account. The description may be up\n            to 1024 characters in length.\n        label:\n            A name for the storage account. The name may be up to 100\n            characters in length. The name can be used to identify the storage\n            account for your tracking purposes.\n        affinity_group:\n            The name of an existing affinity group in the specified\n            subscription. You can specify either a location or affinity_group,\n            but not both.\n        location:\n            The location where the storage account is created. You can specify\n            either a location or affinity_group, but not both.\n        geo_replication_enabled:\n            Deprecated. Replaced by the account_type parameter.\n        extended_properties:\n            Dictionary containing name\/value pairs of storage account\n            properties. You can have a maximum of 50 extended property\n            name\/value pairs. The maximum length of the Name element is 64\n            characters, only alphanumeric characters and underscores are valid\n            in the Name, and the name must start with a letter. The value has\n            a maximum length of 255 characters.\n        account_type:\n            Specifies whether the account supports locally-redundant storage,\n            geo-redundant storage, zone-redundant storage, or read access\n            geo-redundant storage.\n            Possible values are:\n                Standard_LRS, Standard_ZRS, Standard_GRS, Standard_RAGRS\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('description', description)\n        _validate_not_none('label', label)\n        if affinity_group is None and location is None:\n            raise ValueError(\n                'location or affinity_group must be specified')\n        if affinity_group is not None and location is not None:\n            raise ValueError(\n                'Only one of location or affinity_group needs to be specified')\n        if geo_replication_enabled == False:\n            account_type = 'Standard_LRS'\n        return self._perform_post(\n            self._get_storage_service_path(),\n            _XmlSerializer.create_storage_service_input_to_xml(\n                service_name,\n                description,\n                label,\n                affinity_group,\n                location,\n                account_type,\n                extended_properties),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.update_storage_account","method_code":"def update_storage_account(self, service_name, description=None,\n                               label=None, geo_replication_enabled=None,\n                               extended_properties=None,\n                               account_type='Standard_GRS'):\n        _validate_not_none('service_name', service_name)\n        if geo_replication_enabled == False:\n            account_type = 'Standard_LRS'\n        return self._perform_put(\n            self._get_storage_service_path(service_name),\n            _XmlSerializer.update_storage_service_input_to_xml(\n                description,\n                label,\n                account_type,\n                extended_properties))","method_summary":"Updates the label, the description, and enables or disables the geo-replication status for a storage account in Windows Azure.","original_method_code":"def update_storage_account(self, service_name, description=None,\n                               label=None, geo_replication_enabled=None,\n                               extended_properties=None,\n                               account_type='Standard_GRS'):\n        '''\n        Updates the label, the description, and enables or disables the\n        geo-replication status for a storage account in Windows Azure.\n\n        service_name:\n            Name of the storage service account.\n        description:\n            A description for the storage account. The description may be up\n            to 1024 characters in length.\n        label:\n            A name for the storage account. The name may be up to 100\n            characters in length. The name can be used to identify the storage\n            account for your tracking purposes.\n        geo_replication_enabled:\n            Deprecated. Replaced by the account_type parameter.\n        extended_properties:\n            Dictionary containing name\/value pairs of storage account\n            properties. You can have a maximum of 50 extended property\n            name\/value pairs. The maximum length of the Name element is 64\n            characters, only alphanumeric characters and underscores are valid\n            in the Name, and the name must start with a letter. The value has\n            a maximum length of 255 characters.\n        account_type:\n            Specifies whether the account supports locally-redundant storage,\n            geo-redundant storage, zone-redundant storage, or read access\n            geo-redundant storage.\n            Possible values are:\n                Standard_LRS, Standard_ZRS, Standard_GRS, Standard_RAGRS\n        '''\n        _validate_not_none('service_name', service_name)\n        if geo_replication_enabled == False:\n            account_type = 'Standard_LRS'\n        return self._perform_put(\n            self._get_storage_service_path(service_name),\n            _XmlSerializer.update_storage_service_input_to_xml(\n                description,\n                label,\n                account_type,\n                extended_properties))","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_storage_account","method_code":"def delete_storage_account(self, service_name):\n        _validate_not_none('service_name', service_name)\n        return self._perform_delete(\n            self._get_storage_service_path(service_name),\n            as_async=True)","method_summary":"Deletes the specified storage account from Windows Azure.","original_method_code":"def delete_storage_account(self, service_name):\n        '''\n        Deletes the specified storage account from Windows Azure.\n\n        service_name:\n            Name of the storage service account.\n        '''\n        _validate_not_none('service_name', service_name)\n        return self._perform_delete(\n            self._get_storage_service_path(service_name),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.check_storage_account_name_availability","method_code":"def check_storage_account_name_availability(self, service_name):\n        _validate_not_none('service_name', service_name)\n        return self._perform_get(\n            self._get_storage_service_path() +\n            '\/operations\/isavailable\/' +\n            _str(service_name) + '',\n            AvailabilityResponse)","method_summary":"Checks to see if the specified storage account name is available, or if it has already been taken.","original_method_code":"def check_storage_account_name_availability(self, service_name):\n        '''\n        Checks to see if the specified storage account name is available, or\n        if it has already been taken.\n\n        service_name:\n            Name of the storage service account.\n        '''\n        _validate_not_none('service_name', service_name)\n        return self._perform_get(\n            self._get_storage_service_path() +\n            '\/operations\/isavailable\/' +\n            _str(service_name) + '',\n            AvailabilityResponse)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.create_hosted_service","method_code":"def create_hosted_service(self, service_name, label, description=None,\n                              location=None, affinity_group=None,\n                              extended_properties=None):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('label', label)\n        if affinity_group is None and location is None:\n            raise ValueError(\n                'location or affinity_group must be specified')\n        if affinity_group is not None and location is not None:\n            raise ValueError(\n                'Only one of location or affinity_group needs to be specified')\n        return self._perform_post(self._get_hosted_service_path(),\n                                  _XmlSerializer.create_hosted_service_to_xml(\n                                      service_name,\n                                      label,\n                                      description,\n                                      location,\n                                      affinity_group,\n                                      extended_properties),\n                                  as_async=True)","method_summary":"Creates a new hosted service in Windows Azure.","original_method_code":"def create_hosted_service(self, service_name, label, description=None,\n                              location=None, affinity_group=None,\n                              extended_properties=None):\n        '''\n        Creates a new hosted service in Windows Azure.\n\n        service_name:\n            A name for the hosted service that is unique within Windows Azure.\n            This name is the DNS prefix name and can be used to access the\n            hosted service.\n        label:\n            A name for the hosted service. The name can be up to 100 characters\n            in length. The name can be used to identify the storage account for\n            your tracking purposes.\n        description:\n            A description for the hosted service. The description can be up to\n            1024 characters in length.\n        location:\n            The location where the hosted service will be created. You can\n            specify either a location or affinity_group, but not both.\n        affinity_group:\n            The name of an existing affinity group associated with this\n            subscription. This name is a GUID and can be retrieved by examining\n            the name element of the response body returned by\n            list_affinity_groups. You can specify either a location or\n            affinity_group, but not both.\n        extended_properties:\n            Dictionary containing name\/value pairs of storage account\n            properties. You can have a maximum of 50 extended property\n            name\/value pairs. The maximum length of the Name element is 64\n            characters, only alphanumeric characters and underscores are valid\n            in the Name, and the name must start with a letter. The value has\n            a maximum length of 255 characters.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('label', label)\n        if affinity_group is None and location is None:\n            raise ValueError(\n                'location or affinity_group must be specified')\n        if affinity_group is not None and location is not None:\n            raise ValueError(\n                'Only one of location or affinity_group needs to be specified')\n        return self._perform_post(self._get_hosted_service_path(),\n                                  _XmlSerializer.create_hosted_service_to_xml(\n                                      service_name,\n                                      label,\n                                      description,\n                                      location,\n                                      affinity_group,\n                                      extended_properties),\n                                  as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_hosted_service","method_code":"def delete_hosted_service(self, service_name, complete=False):\n        _validate_not_none('service_name', service_name)\n\n        path = self._get_hosted_service_path(service_name)\n\n        if complete == True:\n            path = path +'?comp=media'\n\n        return self._perform_delete(path, as_async=True)","method_summary":"Deletes the specified hosted service from Windows Azure.","original_method_code":"def delete_hosted_service(self, service_name, complete=False):\n        '''\n        Deletes the specified hosted service from Windows Azure.\n\n        service_name:\n            Name of the hosted service.\n        complete:\n            True if all OS\/data disks and the source blobs for the disks should\n            also be deleted from storage.\n        '''\n\n        _validate_not_none('service_name', service_name)\n\n        path = self._get_hosted_service_path(service_name)\n\n        if complete == True:\n            path = path +'?comp=media'\n\n        return self._perform_delete(path, as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.create_deployment","method_code":"def create_deployment(self, service_name, deployment_slot, name,\n                          package_url, label, configuration,\n                          start_deployment=False,\n                          treat_warnings_as_error=False,\n                          extended_properties=None):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_slot', deployment_slot)\n        _validate_not_none('name', name)\n        _validate_not_none('package_url', package_url)\n        _validate_not_none('label', label)\n        _validate_not_none('configuration', configuration)\n        return self._perform_post(\n            self._get_deployment_path_using_slot(\n                service_name, deployment_slot),\n            _XmlSerializer.create_deployment_to_xml(\n                name,\n                package_url,\n                label,\n                configuration,\n                start_deployment,\n                treat_warnings_as_error,\n                extended_properties),\n            as_async=True)","method_summary":"Uploads a new service package and creates a new deployment on staging or production.","original_method_code":"def create_deployment(self, service_name, deployment_slot, name,\n                          package_url, label, configuration,\n                          start_deployment=False,\n                          treat_warnings_as_error=False,\n                          extended_properties=None):\n        '''\n        Uploads a new service package and creates a new deployment on staging\n        or production.\n\n        service_name:\n            Name of the hosted service.\n        deployment_slot:\n            The environment to which the hosted service is deployed. Valid\n            values are: staging, production\n        name:\n            The name for the deployment. The deployment name must be unique\n            among other deployments for the hosted service.\n        package_url:\n            A URL that refers to the location of the service package in the\n            Blob service. The service package can be located either in a\n            storage account beneath the same subscription or a Shared Access\n            Signature (SAS) URI from any storage account.\n        label:\n            A name for the hosted service. The name can be up to 100 characters\n            in length. It is recommended that the label be unique within the\n            subscription. The name can be used to identify the hosted service\n            for your tracking purposes.\n        configuration:\n            The base-64 encoded service configuration file for the deployment.\n        start_deployment:\n            Indicates whether to start the deployment immediately after it is\n            created. If false, the service model is still deployed to the\n            virtual machines but the code is not run immediately. Instead, the\n            service is Suspended until you call Update Deployment Status and\n            set the status to Running, at which time the service will be\n            started. A deployed service still incurs charges, even if it is\n            suspended.\n        treat_warnings_as_error:\n            Indicates whether to treat package validation warnings as errors.\n            If set to true, the Created Deployment operation fails if there\n            are validation warnings on the service package.\n        extended_properties:\n            Dictionary containing name\/value pairs of storage account\n            properties. You can have a maximum of 50 extended property\n            name\/value pairs. The maximum length of the Name element is 64\n            characters, only alphanumeric characters and underscores are valid\n            in the Name, and the name must start with a letter. The value has\n            a maximum length of 255 characters.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_slot', deployment_slot)\n        _validate_not_none('name', name)\n        _validate_not_none('package_url', package_url)\n        _validate_not_none('label', label)\n        _validate_not_none('configuration', configuration)\n        return self._perform_post(\n            self._get_deployment_path_using_slot(\n                service_name, deployment_slot),\n            _XmlSerializer.create_deployment_to_xml(\n                name,\n                package_url,\n                label,\n                configuration,\n                start_deployment,\n                treat_warnings_as_error,\n                extended_properties),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_deployment","method_code":"def delete_deployment(self, service_name, deployment_name,delete_vhd=False):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        path= self._get_deployment_path_using_name(service_name, deployment_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(\n                path,\n            as_async=True)","method_summary":"Deletes the specified deployment.","original_method_code":"def delete_deployment(self, service_name, deployment_name,delete_vhd=False):\n        '''\n        Deletes the specified deployment.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name of the deployment.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        path= self._get_deployment_path_using_name(service_name, deployment_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(\n                path,\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.change_deployment_configuration","method_code":"def change_deployment_configuration(self, service_name, deployment_name,\n                                        configuration,\n                                        treat_warnings_as_error=False,\n                                        mode='Auto', extended_properties=None):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('configuration', configuration)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + '\/?comp=config',\n            _XmlSerializer.change_deployment_to_xml(\n                configuration,\n                treat_warnings_as_error,\n                mode,\n                extended_properties),\n            as_async=True)","method_summary":"Initiates a change to the deployment configuration.","original_method_code":"def change_deployment_configuration(self, service_name, deployment_name,\n                                        configuration,\n                                        treat_warnings_as_error=False,\n                                        mode='Auto', extended_properties=None):\n        '''\n        Initiates a change to the deployment configuration.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name of the deployment.\n        configuration:\n            The base-64 encoded service configuration file for the deployment.\n        treat_warnings_as_error:\n            Indicates whether to treat package validation warnings as errors.\n            If set to true, the Created Deployment operation fails if there\n            are validation warnings on the service package.\n        mode:\n            If set to Manual, WalkUpgradeDomain must be called to apply the\n            update. If set to Auto, the Windows Azure platform will\n            automatically apply the update To each upgrade domain for the\n            service. Possible values are: Auto, Manual\n        extended_properties:\n            Dictionary containing name\/value pairs of storage account\n            properties. You can have a maximum of 50 extended property\n            name\/value pairs. The maximum length of the Name element is 64\n            characters, only alphanumeric characters and underscores are valid\n            in the Name, and the name must start with a letter. The value has\n            a maximum length of 255 characters.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('configuration', configuration)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + '\/?comp=config',\n            _XmlSerializer.change_deployment_to_xml(\n                configuration,\n                treat_warnings_as_error,\n                mode,\n                extended_properties),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.update_deployment_status","method_code":"def update_deployment_status(self, service_name, deployment_name, status):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('status', status)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + '\/?comp=status',\n            _XmlSerializer.update_deployment_status_to_xml(\n                status),\n            as_async=True)","method_summary":"Initiates a change in deployment status.","original_method_code":"def update_deployment_status(self, service_name, deployment_name, status):\n        '''\n        Initiates a change in deployment status.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name of the deployment.\n        status:\n            The change to initiate to the deployment status. Possible values\n            include:\n                Running, Suspended\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('status', status)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + '\/?comp=status',\n            _XmlSerializer.update_deployment_status_to_xml(\n                status),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.upgrade_deployment","method_code":"def upgrade_deployment(self, service_name, deployment_name, mode,\n                           package_url, configuration, label, force,\n                           role_to_upgrade=None, extended_properties=None):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('mode', mode)\n        _validate_not_none('package_url', package_url)\n        _validate_not_none('configuration', configuration)\n        _validate_not_none('label', label)\n        _validate_not_none('force', force)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + '\/?comp=upgrade',\n            _XmlSerializer.upgrade_deployment_to_xml(\n                mode,\n                package_url,\n                configuration,\n                label,\n                role_to_upgrade,\n                force,\n                extended_properties),\n            as_async=True)","method_summary":"Initiates an upgrade.","original_method_code":"def upgrade_deployment(self, service_name, deployment_name, mode,\n                           package_url, configuration, label, force,\n                           role_to_upgrade=None, extended_properties=None):\n        '''\n        Initiates an upgrade.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name of the deployment.\n        mode:\n            If set to Manual, WalkUpgradeDomain must be called to apply the\n            update. If set to Auto, the Windows Azure platform will\n            automatically apply the update To each upgrade domain for the\n            service. Possible values are: Auto, Manual\n        package_url:\n            A URL that refers to the location of the service package in the\n            Blob service. The service package can be located either in a\n            storage account beneath the same subscription or a Shared Access\n            Signature (SAS) URI from any storage account.\n        configuration:\n            The base-64 encoded service configuration file for the deployment.\n        label:\n            A name for the hosted service. The name can be up to 100 characters\n            in length. It is recommended that the label be unique within the\n            subscription. The name can be used to identify the hosted service\n            for your tracking purposes.\n        force:\n            Specifies whether the rollback should proceed even when it will\n            cause local data to be lost from some role instances. True if the\n            rollback should proceed; otherwise false if the rollback should\n            fail.\n        role_to_upgrade:\n            The name of the specific role to upgrade.\n        extended_properties:\n            Dictionary containing name\/value pairs of storage account\n            properties. You can have a maximum of 50 extended property\n            name\/value pairs. The maximum length of the Name element is 64\n            characters, only alphanumeric characters and underscores are valid\n            in the Name, and the name must start with a letter. The value has\n            a maximum length of 255 characters.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('mode', mode)\n        _validate_not_none('package_url', package_url)\n        _validate_not_none('configuration', configuration)\n        _validate_not_none('label', label)\n        _validate_not_none('force', force)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + '\/?comp=upgrade',\n            _XmlSerializer.upgrade_deployment_to_xml(\n                mode,\n                package_url,\n                configuration,\n                label,\n                role_to_upgrade,\n                force,\n                extended_properties),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.walk_upgrade_domain","method_code":"def walk_upgrade_domain(self, service_name, deployment_name,\n                            upgrade_domain):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('upgrade_domain', upgrade_domain)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + '\/?comp=walkupgradedomain',\n            _XmlSerializer.walk_upgrade_domain_to_xml(\n                upgrade_domain),\n            as_async=True)","method_summary":"Specifies the next upgrade domain to be walked during manual in-place upgrade or configuration change.","original_method_code":"def walk_upgrade_domain(self, service_name, deployment_name,\n                            upgrade_domain):\n        '''\n        Specifies the next upgrade domain to be walked during manual in-place\n        upgrade or configuration change.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name of the deployment.\n        upgrade_domain:\n            An integer value that identifies the upgrade domain to walk.\n            Upgrade domains are identified with a zero-based index: the first\n            upgrade domain has an ID of 0, the second has an ID of 1, and so on.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('upgrade_domain', upgrade_domain)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + '\/?comp=walkupgradedomain',\n            _XmlSerializer.walk_upgrade_domain_to_xml(\n                upgrade_domain),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.reboot_role_instance","method_code":"def reboot_role_instance(self, service_name, deployment_name,\n                             role_instance_name):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_instance_name', role_instance_name)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + \\\n                    '\/roleinstances\/' + _str(role_instance_name) + \\\n                    '?comp=reboot',\n            '',\n            as_async=True)","method_summary":"Requests a reboot of a role instance that is running in a deployment.","original_method_code":"def reboot_role_instance(self, service_name, deployment_name,\n                             role_instance_name):\n        '''\n        Requests a reboot of a role instance that is running in a deployment.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name of the deployment.\n        role_instance_name:\n            The name of the role instance.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_instance_name', role_instance_name)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + \\\n                    '\/roleinstances\/' + _str(role_instance_name) + \\\n                    '?comp=reboot',\n            '',\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.check_hosted_service_name_availability","method_code":"def check_hosted_service_name_availability(self, service_name):\n        _validate_not_none('service_name', service_name)\n        return self._perform_get(\n            '\/' + self.subscription_id +\n            '\/services\/hostedservices\/operations\/isavailable\/' +\n            _str(service_name) + '',\n            AvailabilityResponse)","method_summary":"Checks to see if the specified hosted service name is available, or if it has already been taken.","original_method_code":"def check_hosted_service_name_availability(self, service_name):\n        '''\n        Checks to see if the specified hosted service name is available, or if\n        it has already been taken.\n\n        service_name:\n            Name of the hosted service.\n        '''\n        _validate_not_none('service_name', service_name)\n        return self._perform_get(\n            '\/' + self.subscription_id +\n            '\/services\/hostedservices\/operations\/isavailable\/' +\n            _str(service_name) + '',\n            AvailabilityResponse)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.list_service_certificates","method_code":"def list_service_certificates(self, service_name):\n        _validate_not_none('service_name', service_name)\n        return self._perform_get(\n            '\/' + self.subscription_id + '\/services\/hostedservices\/' +\n            _str(service_name) + '\/certificates',\n            Certificates)","method_summary":"Lists all of the service certificates associated with the specified hosted service.","original_method_code":"def list_service_certificates(self, service_name):\n        '''\n        Lists all of the service certificates associated with the specified\n        hosted service.\n\n        service_name:\n            Name of the hosted service.\n        '''\n        _validate_not_none('service_name', service_name)\n        return self._perform_get(\n            '\/' + self.subscription_id + '\/services\/hostedservices\/' +\n            _str(service_name) + '\/certificates',\n            Certificates)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.add_service_certificate","method_code":"def add_service_certificate(self, service_name, data, certificate_format,\n                                password=None):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('data', data)\n        _validate_not_none('certificate_format', certificate_format)\n        _validate_not_none('password', password)\n\n        return self._perform_post(\n            '\/' + self.subscription_id + '\/services\/hostedservices\/' +\n            _str(service_name) + '\/certificates',\n            _XmlSerializer.certificate_file_to_xml(\n                data, certificate_format, password),\n            as_async=True)","method_summary":"Adds a certificate to a hosted service.","original_method_code":"def add_service_certificate(self, service_name, data, certificate_format,\n                                password=None):\n        '''\n        Adds a certificate to a hosted service.\n\n        service_name:\n            Name of the hosted service.\n        data:\n            The base-64 encoded form of the pfx\/cer file.\n        certificate_format:\n            The service certificate format.\n        password:\n            The certificate password. Default to None when using cer format.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('data', data)\n        _validate_not_none('certificate_format', certificate_format)\n        _validate_not_none('password', password)\n\n        return self._perform_post(\n            '\/' + self.subscription_id + '\/services\/hostedservices\/' +\n            _str(service_name) + '\/certificates',\n            _XmlSerializer.certificate_file_to_xml(\n                data, certificate_format, password),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_service_certificate","method_code":"def delete_service_certificate(self, service_name, thumbalgorithm,\n                                   thumbprint):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('thumbalgorithm', thumbalgorithm)\n        _validate_not_none('thumbprint', thumbprint)\n        return self._perform_delete(\n            '\/' + self.subscription_id + '\/services\/hostedservices\/' +\n            _str(service_name) + '\/certificates\/' +\n            _str(thumbalgorithm) + '-' + _str(thumbprint),\n            as_async=True)","method_summary":"Deletes a service certificate from the certificate store of a hosted service.","original_method_code":"def delete_service_certificate(self, service_name, thumbalgorithm,\n                                   thumbprint):\n        '''\n        Deletes a service certificate from the certificate store of a hosted\n        service.\n\n        service_name:\n            Name of the hosted service.\n        thumbalgorithm:\n            The algorithm for the certificate's thumbprint.\n        thumbprint:\n            The hexadecimal representation of the thumbprint.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('thumbalgorithm', thumbalgorithm)\n        _validate_not_none('thumbprint', thumbprint)\n        return self._perform_delete(\n            '\/' + self.subscription_id + '\/services\/hostedservices\/' +\n            _str(service_name) + '\/certificates\/' +\n            _str(thumbalgorithm) + '-' + _str(thumbprint),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.create_affinity_group","method_code":"def create_affinity_group(self, name, label, location, description=None):\n        _validate_not_none('name', name)\n        _validate_not_none('label', label)\n        _validate_not_none('location', location)\n        return self._perform_post(\n            '\/' + self.subscription_id + '\/affinitygroups',\n            _XmlSerializer.create_affinity_group_to_xml(name,\n                                                        label,\n                                                        description,\n                                                        location))","method_summary":"Creates a new affinity group for the specified subscription.","original_method_code":"def create_affinity_group(self, name, label, location, description=None):\n        '''\n        Creates a new affinity group for the specified subscription.\n\n        name:\n            A name for the affinity group that is unique to the subscription.\n        label:\n            A name for the affinity group. The name can be up to 100 characters\n            in length.\n        location:\n            The data center location where the affinity group will be created.\n            To list available locations, use the list_location function.\n        description:\n            A description for the affinity group. The description can be up to\n            1024 characters in length.\n        '''\n        _validate_not_none('name', name)\n        _validate_not_none('label', label)\n        _validate_not_none('location', location)\n        return self._perform_post(\n            '\/' + self.subscription_id + '\/affinitygroups',\n            _XmlSerializer.create_affinity_group_to_xml(name,\n                                                        label,\n                                                        description,\n                                                        location))","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_affinity_group","method_code":"def delete_affinity_group(self, affinity_group_name):\n        _validate_not_none('affinity_group_name', affinity_group_name)\n        return self._perform_delete('\/' + self.subscription_id + \\\n                                    '\/affinitygroups\/' + \\\n                                    _str(affinity_group_name))","method_summary":"Deletes an affinity group in the specified subscription.","original_method_code":"def delete_affinity_group(self, affinity_group_name):\n        '''\n        Deletes an affinity group in the specified subscription.\n\n        affinity_group_name:\n            The name of the affinity group.\n        '''\n        _validate_not_none('affinity_group_name', affinity_group_name)\n        return self._perform_delete('\/' + self.subscription_id + \\\n                                    '\/affinitygroups\/' + \\\n                                    _str(affinity_group_name))","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.list_subscription_operations","method_code":"def list_subscription_operations(self, start_time=None, end_time=None, object_id_filter=None,\n                                     operation_result_filter=None, continuation_token=None):\n        start_time = ('StartTime=' + start_time) if start_time else ''\n        end_time = ('EndTime=' + end_time) if end_time else ''\n        object_id_filter = ('ObjectIdFilter=' + object_id_filter) if object_id_filter else ''\n        operation_result_filter = ('OperationResultFilter=' + operation_result_filter) if operation_result_filter else ''\n        continuation_token = ('ContinuationToken=' + continuation_token) if continuation_token else ''\n\n        parameters = ('&'.join(v for v in (start_time, end_time, object_id_filter, operation_result_filter, continuation_token) if v))\n        parameters = '?' + parameters if parameters else ''\n\n        return self._perform_get(self._get_list_subscription_operations_path() + parameters,\n                                 SubscriptionOperationCollection)","method_summary":"List subscription operations.","original_method_code":"def list_subscription_operations(self, start_time=None, end_time=None, object_id_filter=None,\n                                     operation_result_filter=None, continuation_token=None):\n        '''\n        List subscription operations.\n\n        start_time: Required. An ISO8601 date.\n        end_time: Required. An ISO8601 date.\n        object_id_filter: Optional. Returns subscription operations only for the specified object type and object ID\n        operation_result_filter: Optional. Returns subscription operations only for the specified result status, either Succeeded, Failed, or InProgress.\n        continuation_token: Optional.\n        More information at:\n        https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/gg715318.aspx\n        '''\n        start_time = ('StartTime=' + start_time) if start_time else ''\n        end_time = ('EndTime=' + end_time) if end_time else ''\n        object_id_filter = ('ObjectIdFilter=' + object_id_filter) if object_id_filter else ''\n        operation_result_filter = ('OperationResultFilter=' + operation_result_filter) if operation_result_filter else ''\n        continuation_token = ('ContinuationToken=' + continuation_token) if continuation_token else ''\n\n        parameters = ('&'.join(v for v in (start_time, end_time, object_id_filter, operation_result_filter, continuation_token) if v))\n        parameters = '?' + parameters if parameters else ''\n\n        return self._perform_get(self._get_list_subscription_operations_path() + parameters,\n                                 SubscriptionOperationCollection)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.create_reserved_ip_address","method_code":"def create_reserved_ip_address(self, name, label=None, location=None):\n        _validate_not_none('name', name)\n        return self._perform_post(\n            self._get_reserved_ip_path(),\n            _XmlSerializer.create_reserved_ip_to_xml(name, label, location),\n            as_async=True)","method_summary":"Reserves an IPv4 address for the specified subscription.","original_method_code":"def create_reserved_ip_address(self, name, label=None, location=None):\n        '''\n        Reserves an IPv4 address for the specified subscription.\n\n        name:\n            Required. Specifies the name for the reserved IP address.\n        label:\n            Optional. Specifies a label for the reserved IP address. The label\n            can be up to 100 characters long and can be used for your tracking\n            purposes.\n        location:\n            Required. Specifies the location of the reserved IP address. This\n            should be the same location that is assigned to the cloud service\n            containing the deployment that will use the reserved IP address.\n            To see the available locations, you can use list_locations.\n        '''\n        _validate_not_none('name', name)\n        return self._perform_post(\n            self._get_reserved_ip_path(),\n            _XmlSerializer.create_reserved_ip_to_xml(name, label, location),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_reserved_ip_address","method_code":"def delete_reserved_ip_address(self, name):\n        _validate_not_none('name', name)\n        return self._perform_delete(self._get_reserved_ip_path(name),\n                                    as_async=True)","method_summary":"Deletes a reserved IP address from the specified subscription.","original_method_code":"def delete_reserved_ip_address(self, name):\n        '''\n        Deletes a reserved IP address from the specified subscription.\n\n        name:\n            Required. Name of the reserved IP address.\n        '''\n        _validate_not_none('name', name)\n        return self._perform_delete(self._get_reserved_ip_path(name),\n                                    as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.associate_reserved_ip_address","method_code":"def associate_reserved_ip_address(\n        self, name, service_name, deployment_name, virtual_ip_name=None\n    ):\n        _validate_not_none('name', name)\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        return self._perform_post(\n            self._get_reserved_ip_path_for_association(name),\n            _XmlSerializer.associate_reserved_ip_to_xml(\n                service_name, deployment_name, virtual_ip_name\n            ),\n            as_async=True,\n            x_ms_version='2015-02-01'\n        )","method_summary":"Associate an existing reservedIP to a deployment.","original_method_code":"def associate_reserved_ip_address(\n        self, name, service_name, deployment_name, virtual_ip_name=None\n    ):\n        '''\n        Associate an existing reservedIP to a deployment.\n\n        name:\n            Required. Name of the reserved IP address.\n\n        service_name:\n            Required. Name of the hosted service.\n\n        deployment_name:\n            Required. Name of the deployment.\n\n        virtual_ip_name:\n            Optional. Name of the VirtualIP in case of multi Vip tenant.\n            If this value is not specified default virtualIP is used\n            for this operation.\n        '''\n        _validate_not_none('name', name)\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        return self._perform_post(\n            self._get_reserved_ip_path_for_association(name),\n            _XmlSerializer.associate_reserved_ip_to_xml(\n                service_name, deployment_name, virtual_ip_name\n            ),\n            as_async=True,\n            x_ms_version='2015-02-01'\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.disassociate_reserved_ip_address","method_code":"def disassociate_reserved_ip_address(\n        self, name, service_name, deployment_name, virtual_ip_name=None\n    ):\n        _validate_not_none('name', name)\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        return self._perform_post(\n            self._get_reserved_ip_path_for_disassociation(name),\n            _XmlSerializer.associate_reserved_ip_to_xml(\n                service_name, deployment_name, virtual_ip_name\n            ),\n            as_async=True,\n            x_ms_version='2015-02-01'\n        )","method_summary":"Disassociate an existing reservedIP from the given deployment.","original_method_code":"def disassociate_reserved_ip_address(\n        self, name, service_name, deployment_name, virtual_ip_name=None\n    ):\n        '''\n        Disassociate an existing reservedIP from the given deployment.\n\n        name:\n            Required. Name of the reserved IP address.\n\n        service_name:\n            Required. Name of the hosted service.\n\n        deployment_name:\n            Required. Name of the deployment.\n\n        virtual_ip_name:\n            Optional. Name of the VirtualIP in case of multi Vip tenant.\n            If this value is not specified default virtualIP is used\n            for this operation.\n        '''\n        _validate_not_none('name', name)\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        return self._perform_post(\n            self._get_reserved_ip_path_for_disassociation(name),\n            _XmlSerializer.associate_reserved_ip_to_xml(\n                service_name, deployment_name, virtual_ip_name\n            ),\n            as_async=True,\n            x_ms_version='2015-02-01'\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.get_reserved_ip_address","method_code":"def get_reserved_ip_address(self, name):\n        _validate_not_none('name', name)\n        return self._perform_get(self._get_reserved_ip_path(name), ReservedIP)","method_summary":"Retrieves information about the specified reserved IP address.","original_method_code":"def get_reserved_ip_address(self, name):\n        '''\n        Retrieves information about the specified reserved IP address.\n\n        name:\n            Required. Name of the reserved IP address.\n        '''\n        _validate_not_none('name', name)\n        return self._perform_get(self._get_reserved_ip_path(name), ReservedIP)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.get_role","method_code":"def get_role(self, service_name, deployment_name, role_name):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        return self._perform_get(\n            self._get_role_path(service_name, deployment_name, role_name),\n            PersistentVMRole)","method_summary":"Retrieves the specified virtual machine.","original_method_code":"def get_role(self, service_name, deployment_name, role_name):\n        '''\n        Retrieves the specified virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        return self._perform_get(\n            self._get_role_path(service_name, deployment_name, role_name),\n            PersistentVMRole)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.create_virtual_machine_deployment","method_code":"def create_virtual_machine_deployment(self, service_name, deployment_name,\n                                          deployment_slot, label, role_name,\n                                          system_config, os_virtual_hard_disk,\n                                          network_config=None,\n                                          availability_set_name=None,\n                                          data_virtual_hard_disks=None,\n                                          role_size=None,\n                                          role_type='PersistentVMRole',\n                                          virtual_network_name=None,\n                                          resource_extension_references=None,\n                                          provision_guest_agent=None,\n                                          vm_image_name=None,\n                                          media_location=None,\n                                          dns_servers=None,\n                                          reserved_ip_name=None):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('deployment_slot', deployment_slot)\n        _validate_not_none('label', label)\n        _validate_not_none('role_name', role_name)\n        return self._perform_post(\n            self._get_deployment_path_using_name(service_name),\n            _XmlSerializer.virtual_machine_deployment_to_xml(\n                deployment_name,\n                deployment_slot,\n                label,\n                role_name,\n                system_config,\n                os_virtual_hard_disk,\n                role_type,\n                network_config,\n                availability_set_name,\n                data_virtual_hard_disks,\n                role_size,\n                virtual_network_name,\n                resource_extension_references,\n                provision_guest_agent,\n                vm_image_name,\n                media_location,\n                dns_servers,\n                reserved_ip_name),\n            as_async=True)","method_summary":"Provisions a virtual machine based on the supplied configuration.","original_method_code":"def create_virtual_machine_deployment(self, service_name, deployment_name,\n                                          deployment_slot, label, role_name,\n                                          system_config, os_virtual_hard_disk,\n                                          network_config=None,\n                                          availability_set_name=None,\n                                          data_virtual_hard_disks=None,\n                                          role_size=None,\n                                          role_type='PersistentVMRole',\n                                          virtual_network_name=None,\n                                          resource_extension_references=None,\n                                          provision_guest_agent=None,\n                                          vm_image_name=None,\n                                          media_location=None,\n                                          dns_servers=None,\n                                          reserved_ip_name=None):\n        '''\n        Provisions a virtual machine based on the supplied configuration.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name for the deployment. The deployment name must be unique\n            among other deployments for the hosted service.\n        deployment_slot:\n            The environment to which the hosted service is deployed. Valid\n            values are: staging, production\n        label:\n            Specifies an identifier for the deployment. The label can be up to\n            100 characters long. The label can be used for tracking purposes.\n        role_name:\n            The name of the role.\n        system_config:\n            Contains the metadata required to provision a virtual machine from\n            a Windows or Linux OS image.  Use an instance of\n            WindowsConfigurationSet or LinuxConfigurationSet.\n        os_virtual_hard_disk:\n            Contains the parameters Windows Azure uses to create the operating\n            system disk for the virtual machine. If you are creating a Virtual\n            Machine by using a VM Image, this parameter is not used.\n        network_config:\n            Encapsulates the metadata required to create the virtual network\n            configuration for a virtual machine. If you do not include a\n            network configuration set you will not be able to access the VM\n            through VIPs over the internet. If your virtual machine belongs to\n            a virtual network you can not specify which subnet address space\n            it resides under. Use an instance of ConfigurationSet.\n        availability_set_name:\n            Specifies the name of an availability set to which to add the\n            virtual machine. This value controls the virtual machine\n            allocation in the Windows Azure environment. Virtual machines\n            specified in the same availability set are allocated to different\n            nodes to maximize availability.\n        data_virtual_hard_disks:\n            Contains the parameters Windows Azure uses to create a data disk\n            for a virtual machine.\n        role_size:\n            The size of the virtual machine to allocate. The default value is\n            Small. Possible values are: ExtraSmall,Small,Medium,Large,\n            ExtraLarge,A5,A6,A7,A8,A9,Basic_A0,Basic_A1,Basic_A2,Basic_A3,\n            Basic_A4,Standard_D1,Standard_D2,Standard_D3,Standard_D4,\n            Standard_D11,Standard_D12,Standard_D13,Standard_D14,Standard_G1,\n            Standard_G2,Sandard_G3,Standard_G4,Standard_G5. The specified\n            value must be compatible with the disk selected in the \n            OSVirtualHardDisk values.\n        role_type:\n            The type of the role for the virtual machine. The only supported\n            value is PersistentVMRole.\n        virtual_network_name:\n            Specifies the name of an existing virtual network to which the\n            deployment will belong.\n        resource_extension_references:\n            Optional. Contains a collection of resource extensions that are to\n            be installed on the Virtual Machine. This element is used if\n            provision_guest_agent is set to True. Use an iterable of instances\n            of ResourceExtensionReference.\n        provision_guest_agent:\n            Optional. Indicates whether the VM Agent is installed on the\n            Virtual Machine. To run a resource extension in a Virtual Machine,\n            this service must be installed.\n        vm_image_name:\n            Optional. Specifies the name of the VM Image that is to be used to\n            create the Virtual Machine. If this is specified, the\n            system_config and network_config parameters are not used.\n        media_location:\n            Optional. Required if the Virtual Machine is being created from a\n            published VM Image. Specifies the location of the VHD file that is\n            created when VMImageName specifies a published VM Image.\n        dns_servers:\n            Optional. List of DNS servers (use DnsServer class) to associate\n            with the Virtual Machine.\n        reserved_ip_name:\n            Optional. Specifies the name of a reserved IP address that is to be\n            assigned to the deployment. You must run create_reserved_ip_address\n            before you can assign the address to the deployment using this\n            element.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('deployment_slot', deployment_slot)\n        _validate_not_none('label', label)\n        _validate_not_none('role_name', role_name)\n        return self._perform_post(\n            self._get_deployment_path_using_name(service_name),\n            _XmlSerializer.virtual_machine_deployment_to_xml(\n                deployment_name,\n                deployment_slot,\n                label,\n                role_name,\n                system_config,\n                os_virtual_hard_disk,\n                role_type,\n                network_config,\n                availability_set_name,\n                data_virtual_hard_disks,\n                role_size,\n                virtual_network_name,\n                resource_extension_references,\n                provision_guest_agent,\n                vm_image_name,\n                media_location,\n                dns_servers,\n                reserved_ip_name),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.add_role","method_code":"def add_role(self, service_name, deployment_name, role_name, system_config,\n                 os_virtual_hard_disk, network_config=None,\n                 availability_set_name=None, data_virtual_hard_disks=None,\n                 role_size=None, role_type='PersistentVMRole',\n                 resource_extension_references=None,\n                 provision_guest_agent=None, vm_image_name=None,\n                 media_location=None):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        return self._perform_post(\n            self._get_role_path(service_name, deployment_name),\n            _XmlSerializer.add_role_to_xml(\n                role_name,\n                system_config,\n                os_virtual_hard_disk,\n                role_type,\n                network_config,\n                availability_set_name,\n                data_virtual_hard_disks,\n                role_size,\n                resource_extension_references,\n                provision_guest_agent,\n                vm_image_name,\n                media_location),\n            as_async=True)","method_summary":"Adds a virtual machine to an existing deployment.","original_method_code":"def add_role(self, service_name, deployment_name, role_name, system_config,\n                 os_virtual_hard_disk, network_config=None,\n                 availability_set_name=None, data_virtual_hard_disks=None,\n                 role_size=None, role_type='PersistentVMRole',\n                 resource_extension_references=None,\n                 provision_guest_agent=None, vm_image_name=None,\n                 media_location=None):\n        '''\n        Adds a virtual machine to an existing deployment.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        system_config:\n            Contains the metadata required to provision a virtual machine from\n            a Windows or Linux OS image.  Use an instance of\n            WindowsConfigurationSet or LinuxConfigurationSet.\n        os_virtual_hard_disk:\n            Contains the parameters Windows Azure uses to create the operating\n            system disk for the virtual machine. If you are creating a Virtual\n            Machine by using a VM Image, this parameter is not used.\n        network_config:\n            Encapsulates the metadata required to create the virtual network\n            configuration for a virtual machine. If you do not include a\n            network configuration set you will not be able to access the VM\n            through VIPs over the internet. If your virtual machine belongs to\n            a virtual network you can not specify which subnet address space\n            it resides under.\n        availability_set_name:\n            Specifies the name of an availability set to which to add the\n            virtual machine. This value controls the virtual machine allocation\n            in the Windows Azure environment. Virtual machines specified in the\n            same availability set are allocated to different nodes to maximize\n            availability.\n        data_virtual_hard_disks:\n            Contains the parameters Windows Azure uses to create a data disk\n            for a virtual machine.\n        role_size:\n            The size of the virtual machine to allocate. The default value is\n            Small. Possible values are: ExtraSmall, Small, Medium, Large,\n            ExtraLarge. The specified value must be compatible with the disk\n            selected in the OSVirtualHardDisk values.\n        role_type:\n            The type of the role for the virtual machine. The only supported\n            value is PersistentVMRole.\n        resource_extension_references:\n            Optional. Contains a collection of resource extensions that are to\n            be installed on the Virtual Machine. This element is used if\n            provision_guest_agent is set to True.\n        provision_guest_agent:\n            Optional. Indicates whether the VM Agent is installed on the\n            Virtual Machine. To run a resource extension in a Virtual Machine,\n            this service must be installed.\n        vm_image_name:\n            Optional. Specifies the name of the VM Image that is to be used to\n            create the Virtual Machine. If this is specified, the\n            system_config and network_config parameters are not used.\n        media_location:\n            Optional. Required if the Virtual Machine is being created from a\n            published VM Image. Specifies the location of the VHD file that is\n            created when VMImageName specifies a published VM Image.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        return self._perform_post(\n            self._get_role_path(service_name, deployment_name),\n            _XmlSerializer.add_role_to_xml(\n                role_name,\n                system_config,\n                os_virtual_hard_disk,\n                role_type,\n                network_config,\n                availability_set_name,\n                data_virtual_hard_disks,\n                role_size,\n                resource_extension_references,\n                provision_guest_agent,\n                vm_image_name,\n                media_location),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.update_role","method_code":"def update_role(self, service_name, deployment_name, role_name,\n                    os_virtual_hard_disk=None, network_config=None,\n                    availability_set_name=None, data_virtual_hard_disks=None,\n                    role_size=None, role_type='PersistentVMRole',\n                    resource_extension_references=None,\n                    provision_guest_agent=None):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        return self._perform_put(\n            self._get_role_path(service_name, deployment_name, role_name),\n            _XmlSerializer.update_role_to_xml(\n                role_name,\n                os_virtual_hard_disk,\n                role_type,\n                network_config,\n                availability_set_name,\n                data_virtual_hard_disks,\n                role_size,\n                resource_extension_references,\n                provision_guest_agent),\n            as_async=True)","method_summary":"Updates the specified virtual machine.","original_method_code":"def update_role(self, service_name, deployment_name, role_name,\n                    os_virtual_hard_disk=None, network_config=None,\n                    availability_set_name=None, data_virtual_hard_disks=None,\n                    role_size=None, role_type='PersistentVMRole',\n                    resource_extension_references=None,\n                    provision_guest_agent=None):\n        '''\n        Updates the specified virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        os_virtual_hard_disk:\n            Contains the parameters Windows Azure uses to create the operating\n            system disk for the virtual machine.\n        network_config:\n            Encapsulates the metadata required to create the virtual network\n            configuration for a virtual machine. If you do not include a\n            network configuration set you will not be able to access the VM\n            through VIPs over the internet. If your virtual machine belongs to\n            a virtual network you can not specify which subnet address space\n            it resides under.\n        availability_set_name:\n            Specifies the name of an availability set to which to add the\n            virtual machine. This value controls the virtual machine allocation\n            in the Windows Azure environment. Virtual machines specified in the\n            same availability set are allocated to different nodes to maximize\n            availability.\n        data_virtual_hard_disks:\n            Contains the parameters Windows Azure uses to create a data disk\n            for a virtual machine.\n        role_size:\n            The size of the virtual machine to allocate. The default value is\n            Small. Possible values are: ExtraSmall, Small, Medium, Large,\n            ExtraLarge. The specified value must be compatible with the disk\n            selected in the OSVirtualHardDisk values.\n        role_type:\n            The type of the role for the virtual machine. The only supported\n            value is PersistentVMRole.\n        resource_extension_references:\n            Optional. Contains a collection of resource extensions that are to\n            be installed on the Virtual Machine. This element is used if\n            provision_guest_agent is set to True.\n        provision_guest_agent:\n            Optional. Indicates whether the VM Agent is installed on the\n            Virtual Machine. To run a resource extension in a Virtual Machine,\n            this service must be installed.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        return self._perform_put(\n            self._get_role_path(service_name, deployment_name, role_name),\n            _XmlSerializer.update_role_to_xml(\n                role_name,\n                os_virtual_hard_disk,\n                role_type,\n                network_config,\n                availability_set_name,\n                data_virtual_hard_disks,\n                role_size,\n                resource_extension_references,\n                provision_guest_agent),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_role","method_code":"def delete_role(self, service_name, deployment_name, role_name, complete = False):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n\n        path = self._get_role_path(service_name, deployment_name, role_name)\n        \n        if complete == True:\n            path = path +'?comp=media'\n\n        return self._perform_delete(path,\n                                    as_async=True)","method_summary":"Deletes the specified virtual machine.","original_method_code":"def delete_role(self, service_name, deployment_name, role_name, complete = False):\n        '''\n        Deletes the specified virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        complete:\n            True if all OS\/data disks and the source blobs for the disks should\n            also be deleted from storage.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n\n        path = self._get_role_path(service_name, deployment_name, role_name)\n        \n        if complete == True:\n            path = path +'?comp=media'\n\n        return self._perform_delete(path,\n                                    as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.capture_role","method_code":"def capture_role(self, service_name, deployment_name, role_name,\n                     post_capture_action, target_image_name,\n                     target_image_label, provisioning_configuration=None):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('post_capture_action', post_capture_action)\n        _validate_not_none('target_image_name', target_image_name)\n        _validate_not_none('target_image_label', target_image_label)\n        return self._perform_post(\n            self._get_role_instance_operations_path(\n                service_name, deployment_name, role_name),\n            _XmlSerializer.capture_role_to_xml(\n                post_capture_action,\n                target_image_name,\n                target_image_label,\n                provisioning_configuration),\n            as_async=True)","method_summary":"The Capture Role operation captures a virtual machine image to your image gallery. From the captured image, you can create additional customized virtual machines.","original_method_code":"def capture_role(self, service_name, deployment_name, role_name,\n                     post_capture_action, target_image_name,\n                     target_image_label, provisioning_configuration=None):\n        '''\n        The Capture Role operation captures a virtual machine image to your\n        image gallery. From the captured image, you can create additional\n        customized virtual machines.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        post_capture_action:\n            Specifies the action after capture operation completes. Possible\n            values are: Delete, Reprovision.\n        target_image_name:\n            Specifies the image name of the captured virtual machine.\n        target_image_label:\n            Specifies the friendly name of the captured virtual machine.\n        provisioning_configuration:\n            Use an instance of WindowsConfigurationSet or LinuxConfigurationSet.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('post_capture_action', post_capture_action)\n        _validate_not_none('target_image_name', target_image_name)\n        _validate_not_none('target_image_label', target_image_label)\n        return self._perform_post(\n            self._get_role_instance_operations_path(\n                service_name, deployment_name, role_name),\n            _XmlSerializer.capture_role_to_xml(\n                post_capture_action,\n                target_image_name,\n                target_image_label,\n                provisioning_configuration),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.start_role","method_code":"def start_role(self, service_name, deployment_name, role_name):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        return self._perform_post(\n            self._get_role_instance_operations_path(\n                service_name, deployment_name, role_name),\n            _XmlSerializer.start_role_operation_to_xml(),\n            as_async=True)","method_summary":"Starts the specified virtual machine.","original_method_code":"def start_role(self, service_name, deployment_name, role_name):\n        '''\n        Starts the specified virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        return self._perform_post(\n            self._get_role_instance_operations_path(\n                service_name, deployment_name, role_name),\n            _XmlSerializer.start_role_operation_to_xml(),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.start_roles","method_code":"def start_roles(self, service_name, deployment_name, role_names):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_names', role_names)\n        return self._perform_post(\n            self._get_roles_operations_path(service_name, deployment_name),\n            _XmlSerializer.start_roles_operation_to_xml(role_names),\n            as_async=True)","method_summary":"Starts the specified virtual machines.","original_method_code":"def start_roles(self, service_name, deployment_name, role_names):\n        '''\n        Starts the specified virtual machines.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_names:\n            The names of the roles, as an enumerable of strings.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_names', role_names)\n        return self._perform_post(\n            self._get_roles_operations_path(service_name, deployment_name),\n            _XmlSerializer.start_roles_operation_to_xml(role_names),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.restart_role","method_code":"def restart_role(self, service_name, deployment_name, role_name):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        return self._perform_post(\n            self._get_role_instance_operations_path(\n                service_name, deployment_name, role_name),\n            _XmlSerializer.restart_role_operation_to_xml(\n            ),\n            as_async=True)","method_summary":"Restarts the specified virtual machine.","original_method_code":"def restart_role(self, service_name, deployment_name, role_name):\n        '''\n        Restarts the specified virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        return self._perform_post(\n            self._get_role_instance_operations_path(\n                service_name, deployment_name, role_name),\n            _XmlSerializer.restart_role_operation_to_xml(\n            ),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.shutdown_role","method_code":"def shutdown_role(self, service_name, deployment_name, role_name,\n                      post_shutdown_action='Stopped'):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('post_shutdown_action', post_shutdown_action)\n        return self._perform_post(\n            self._get_role_instance_operations_path(\n                service_name, deployment_name, role_name),\n            _XmlSerializer.shutdown_role_operation_to_xml(post_shutdown_action),\n            as_async=True)","method_summary":"Shuts down the specified virtual machine.","original_method_code":"def shutdown_role(self, service_name, deployment_name, role_name,\n                      post_shutdown_action='Stopped'):\n        '''\n        Shuts down the specified virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        post_shutdown_action:\n            Specifies how the Virtual Machine should be shut down. Values are:\n                Stopped\n                    Shuts down the Virtual Machine but retains the compute\n                    resources. You will continue to be billed for the resources\n                    that the stopped machine uses.\n                StoppedDeallocated\n                    Shuts down the Virtual Machine and releases the compute\n                    resources. You are not billed for the compute resources that\n                    this Virtual Machine uses. If a static Virtual Network IP\n                    address is assigned to the Virtual Machine, it is reserved.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('post_shutdown_action', post_shutdown_action)\n        return self._perform_post(\n            self._get_role_instance_operations_path(\n                service_name, deployment_name, role_name),\n            _XmlSerializer.shutdown_role_operation_to_xml(post_shutdown_action),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.shutdown_roles","method_code":"def shutdown_roles(self, service_name, deployment_name, role_names,\n                       post_shutdown_action='Stopped'):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_names', role_names)\n        _validate_not_none('post_shutdown_action', post_shutdown_action)\n        return self._perform_post(\n            self._get_roles_operations_path(service_name, deployment_name),\n            _XmlSerializer.shutdown_roles_operation_to_xml(\n                role_names, post_shutdown_action),\n            as_async=True)","method_summary":"Shuts down the specified virtual machines.","original_method_code":"def shutdown_roles(self, service_name, deployment_name, role_names,\n                       post_shutdown_action='Stopped'):\n        '''\n        Shuts down the specified virtual machines.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_names:\n            The names of the roles, as an enumerable of strings.\n        post_shutdown_action:\n            Specifies how the Virtual Machine should be shut down. Values are:\n                Stopped\n                    Shuts down the Virtual Machine but retains the compute\n                    resources. You will continue to be billed for the resources\n                    that the stopped machine uses.\n                StoppedDeallocated\n                    Shuts down the Virtual Machine and releases the compute\n                    resources. You are not billed for the compute resources that\n                    this Virtual Machine uses. If a static Virtual Network IP\n                    address is assigned to the Virtual Machine, it is reserved.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_names', role_names)\n        _validate_not_none('post_shutdown_action', post_shutdown_action)\n        return self._perform_post(\n            self._get_roles_operations_path(service_name, deployment_name),\n            _XmlSerializer.shutdown_roles_operation_to_xml(\n                role_names, post_shutdown_action),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.add_dns_server","method_code":"def add_dns_server(self, service_name, deployment_name, dns_server_name, address):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('dns_server_name', dns_server_name)\n        _validate_not_none('address', address)\n        return self._perform_post(\n            self._get_dns_server_path(service_name, deployment_name),\n            _XmlSerializer.dns_server_to_xml(dns_server_name, address),\n            as_async=True)","method_summary":"Adds a DNS server definition to an existing deployment.","original_method_code":"def add_dns_server(self, service_name, deployment_name, dns_server_name, address):\n        '''\n        Adds a DNS server definition to an existing deployment.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        dns_server_name:\n            Specifies the name of the DNS server.\n        address:\n            Specifies the IP address of the DNS server.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('dns_server_name', dns_server_name)\n        _validate_not_none('address', address)\n        return self._perform_post(\n            self._get_dns_server_path(service_name, deployment_name),\n            _XmlSerializer.dns_server_to_xml(dns_server_name, address),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.update_dns_server","method_code":"def update_dns_server(self, service_name, deployment_name, dns_server_name, address):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('dns_server_name', dns_server_name)\n        _validate_not_none('address', address)\n        return self._perform_put(\n            self._get_dns_server_path(service_name,\n                                      deployment_name,\n                                      dns_server_name),\n            _XmlSerializer.dns_server_to_xml(dns_server_name, address),\n            as_async=True)","method_summary":"Updates the ip address of a DNS server.","original_method_code":"def update_dns_server(self, service_name, deployment_name, dns_server_name, address):\n        '''\n        Updates the ip address of a DNS server.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        dns_server_name:\n            Specifies the name of the DNS server.\n        address:\n            Specifies the IP address of the DNS server.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('dns_server_name', dns_server_name)\n        _validate_not_none('address', address)\n        return self._perform_put(\n            self._get_dns_server_path(service_name,\n                                      deployment_name,\n                                      dns_server_name),\n            _XmlSerializer.dns_server_to_xml(dns_server_name, address),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_dns_server","method_code":"def delete_dns_server(self, service_name, deployment_name, dns_server_name):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('dns_server_name', dns_server_name)\n        return self._perform_delete(\n            self._get_dns_server_path(service_name,\n                                      deployment_name,\n                                      dns_server_name),\n            as_async=True)","method_summary":"Deletes a DNS server from a deployment.","original_method_code":"def delete_dns_server(self, service_name, deployment_name, dns_server_name):\n        '''\n        Deletes a DNS server from a deployment.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        dns_server_name:\n            Name of the DNS server that you want to delete.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('dns_server_name', dns_server_name)\n        return self._perform_delete(\n            self._get_dns_server_path(service_name,\n                                      deployment_name,\n                                      dns_server_name),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.list_resource_extension_versions","method_code":"def list_resource_extension_versions(self, publisher_name, extension_name):\n        return self._perform_get(self._get_resource_extension_versions_path(\n                                    publisher_name, extension_name),\n                                 ResourceExtensions)","method_summary":"Lists the versions of a resource extension that are available to add to a Virtual Machine.","original_method_code":"def list_resource_extension_versions(self, publisher_name, extension_name):\n        '''\n        Lists the versions of a resource extension that are available to add\n        to a Virtual Machine.\n\n        publisher_name:\n            Name of the resource extension publisher.\n        extension_name:\n            Name of the resource extension.\n        '''\n        return self._perform_get(self._get_resource_extension_versions_path(\n                                    publisher_name, extension_name),\n                                 ResourceExtensions)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.replicate_vm_image","method_code":"def replicate_vm_image(self, vm_image_name, regions, offer, sku, version):\n        _validate_not_none('vm_image_name', vm_image_name)\n        _validate_not_none('regions', regions)\n        _validate_not_none('offer', offer)\n        _validate_not_none('sku', sku)\n        _validate_not_none('version', version)\n\n        return self._perform_put(\n            self._get_replication_path_using_vm_image_name(vm_image_name),\n            _XmlSerializer.replicate_image_to_xml(\n                regions,\n                offer,\n                sku,\n                version\n            ),\n            as_async=True,\n            x_ms_version='2015-04-01'\n        )","method_summary":"Replicate a VM image to multiple target locations. This operation is only for publishers. You have to be registered as image publisher with Microsoft Azure to be able to call this.","original_method_code":"def replicate_vm_image(self, vm_image_name, regions, offer, sku, version):\n        '''\n        Replicate a VM image to multiple target locations. This operation\n        is only for publishers. You have to be registered as image publisher\n        with Microsoft Azure to be able to call this.\n\n        vm_image_name:\n            Specifies the name of the VM Image that is to be used for\n            replication\n        regions:\n            Specified a list of regions to replicate the image to\n            Note: The regions in the request body are not additive. If a VM\n            Image has already been replicated to Regions A, B, and C, and\n            a request is made to replicate to Regions A and D, the VM\n            Image will remain in Region A, will be replicated in Region D,\n            and will be unreplicated from Regions B and C\n        offer:\n            Specifies the publisher defined name of the offer. The allowed\n            characters are uppercase or lowercase letters, digit,\n            hypen(-), period (.).The maximum allowed length is 64 characters.\n        sku:\n            Specifies the publisher defined name of the Sku. The allowed\n            characters are uppercase or lowercase letters, digit,\n            hypen(-), period (.). The maximum allowed length is 64 characters.\n        version:\n            Specifies the publisher defined version of the image.\n            The allowed characters are digit and period.\n            Format: <MajorVersion>.<MinorVersion>.<Patch>\n            Example: '1.0.0' or '1.1.0' The 3 version number to\n            follow standard of most of the RPs. See http:\/\/semver.org\n        '''\n        _validate_not_none('vm_image_name', vm_image_name)\n        _validate_not_none('regions', regions)\n        _validate_not_none('offer', offer)\n        _validate_not_none('sku', sku)\n        _validate_not_none('version', version)\n\n        return self._perform_put(\n            self._get_replication_path_using_vm_image_name(vm_image_name),\n            _XmlSerializer.replicate_image_to_xml(\n                regions,\n                offer,\n                sku,\n                version\n            ),\n            as_async=True,\n            x_ms_version='2015-04-01'\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.unreplicate_vm_image","method_code":"def unreplicate_vm_image(self, vm_image_name):\n        _validate_not_none('vm_image_name', vm_image_name)\n\n        return self._perform_put(\n            self._get_unreplication_path_using_vm_image_name(vm_image_name),\n            None,\n            as_async=True,\n            x_ms_version='2015-04-01'\n        )","method_summary":"Unreplicate a VM image from all regions This operation is only for publishers. You have to be registered as image publisher with Microsoft Azure to be able to call this","original_method_code":"def unreplicate_vm_image(self, vm_image_name):\n        '''\n        Unreplicate a VM image from all regions This operation\n        is only for publishers. You have to be registered as image publisher\n        with Microsoft Azure to be able to call this\n\n        vm_image_name:\n            Specifies the name of the VM Image that is to be used for\n            unreplication. The VM Image Name should be the user VM Image,\n            not the published name of the VM Image.\n\n        '''\n        _validate_not_none('vm_image_name', vm_image_name)\n\n        return self._perform_put(\n            self._get_unreplication_path_using_vm_image_name(vm_image_name),\n            None,\n            as_async=True,\n            x_ms_version='2015-04-01'\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.share_vm_image","method_code":"def share_vm_image(self, vm_image_name, permission):\n        _validate_not_none('vm_image_name', vm_image_name)\n        _validate_not_none('permission', permission)\n\n        path = self._get_sharing_path_using_vm_image_name(vm_image_name)\n        query = '&permission=' + permission\n        path = path + '?' + query.lstrip('&')\n\n        return self._perform_put(\n            path, None, as_async=True, x_ms_version='2015-04-01'\n        )","method_summary":"Share an already replicated OS image. This operation is only for publishers. You have to be registered as image publisher with Windows Azure to be able to call this.","original_method_code":"def share_vm_image(self, vm_image_name, permission):\n        '''\n        Share an already replicated OS image. This operation is only for\n        publishers. You have to be registered as image publisher with Windows\n        Azure to be able to call this.\n\n        vm_image_name:\n            The name of the virtual machine image to share\n        permission:\n            The sharing permission: public, msdn, or private\n        '''\n        _validate_not_none('vm_image_name', vm_image_name)\n        _validate_not_none('permission', permission)\n\n        path = self._get_sharing_path_using_vm_image_name(vm_image_name)\n        query = '&permission=' + permission\n        path = path + '?' + query.lstrip('&')\n\n        return self._perform_put(\n            path, None, as_async=True, x_ms_version='2015-04-01'\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.create_vm_image","method_code":"def create_vm_image(self, vm_image):\n        _validate_not_none('vm_image', vm_image)\n        _validate_not_none('vm_image.name', vm_image.name)\n        _validate_not_none('vm_image.label', vm_image.label)\n        _validate_not_none('vm_image.os_disk_configuration.os_state',\n                           vm_image.os_disk_configuration.os_state)\n        _validate_not_none('vm_image.os_disk_configuration.os',\n                           vm_image.os_disk_configuration.os)\n        _validate_not_none('vm_image.os_disk_configuration.media_link',\n                           vm_image.os_disk_configuration.media_link)\n        return self._perform_post(\n            self._get_vm_image_path(),\n            _XmlSerializer.create_vm_image_to_xml(vm_image),\n            as_async=True)","method_summary":"Creates a VM Image in the image repository that is associated with the specified subscription using a specified set of virtual hard disks.","original_method_code":"def create_vm_image(self, vm_image):\n        '''\n        Creates a VM Image in the image repository that is associated with the\n        specified subscription using a specified set of virtual hard disks.\n\n        vm_image:\n            An instance of VMImage class.\n        vm_image.name: Required. Specifies the name of the image.\n        vm_image.label: Required. Specifies an identifier for the image.\n        vm_image.description: Optional. Specifies the description of the image.\n        vm_image.os_disk_configuration:\n            Required. Specifies configuration information for the operating \n            system disk that is associated with the image.\n        vm_image.os_disk_configuration.host_caching:\n            Optional. Specifies the caching behavior of the operating system disk.\n            Possible values are: None, ReadOnly, ReadWrite \n        vm_image.os_disk_configuration.os_state:\n            Required. Specifies the state of the operating system in the image.\n            Possible values are: Generalized, Specialized\n            A Virtual Machine that is fully configured and running contains a\n            Specialized operating system. A Virtual Machine on which the\n            Sysprep command has been run with the generalize option contains a\n            Generalized operating system.\n        vm_image.os_disk_configuration.os:\n            Required. Specifies the operating system type of the image.\n        vm_image.os_disk_configuration.media_link:\n            Required. Specifies the location of the blob in Windows Azure\n            storage. The blob location belongs to a storage account in the\n            subscription specified by the <subscription-id> value in the\n            operation call.\n        vm_image.data_disk_configurations:\n            Optional. Specifies configuration information for the data disks\n            that are associated with the image. A VM Image might not have data\n            disks associated with it.\n        vm_image.data_disk_configurations[].host_caching:\n            Optional. Specifies the caching behavior of the data disk.\n            Possible values are: None, ReadOnly, ReadWrite \n        vm_image.data_disk_configurations[].lun:\n            Optional if the lun for the disk is 0. Specifies the Logical Unit\n            Number (LUN) for the data disk.\n        vm_image.data_disk_configurations[].media_link:\n            Required. Specifies the location of the blob in Windows Azure\n            storage. The blob location belongs to a storage account in the\n            subscription specified by the <subscription-id> value in the\n            operation call.\n        vm_image.data_disk_configurations[].logical_size_in_gb:\n            Required. Specifies the size, in GB, of the data disk.\n        vm_image.language: Optional. Specifies the language of the image.\n        vm_image.image_family:\n            Optional. Specifies a value that can be used to group VM Images.\n        vm_image.recommended_vm_size:\n            Optional. Specifies the size to use for the Virtual Machine that\n            is created from the VM Image.\n        vm_image.eula:\n            Optional. Specifies the End User License Agreement that is\n            associated with the image. The value for this element is a string,\n            but it is recommended that the value be a URL that points to a EULA.\n        vm_image.icon_uri:\n            Optional. Specifies the URI to the icon that is displayed for the\n            image in the Management Portal.\n        vm_image.small_icon_uri:\n            Optional. Specifies the URI to the small icon that is displayed for\n            the image in the Management Portal.\n        vm_image.privacy_uri:\n            Optional. Specifies the URI that points to a document that contains\n            the privacy policy related to the image.\n        vm_image.published_date:\n            Optional. Specifies the date when the image was added to the image\n            repository.\n        vm_image.show_in_gui:\n            Optional. Indicates whether the VM Images should be listed in the\n            portal.\n        '''\n        _validate_not_none('vm_image', vm_image)\n        _validate_not_none('vm_image.name', vm_image.name)\n        _validate_not_none('vm_image.label', vm_image.label)\n        _validate_not_none('vm_image.os_disk_configuration.os_state',\n                           vm_image.os_disk_configuration.os_state)\n        _validate_not_none('vm_image.os_disk_configuration.os',\n                           vm_image.os_disk_configuration.os)\n        _validate_not_none('vm_image.os_disk_configuration.media_link',\n                           vm_image.os_disk_configuration.media_link)\n        return self._perform_post(\n            self._get_vm_image_path(),\n            _XmlSerializer.create_vm_image_to_xml(vm_image),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_vm_image","method_code":"def delete_vm_image(self, vm_image_name, delete_vhd=False):\n        _validate_not_none('vm_image_name', vm_image_name)\n        path = self._get_vm_image_path(vm_image_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)","method_summary":"Deletes the specified VM Image from the image repository that is associated with the specified subscription.","original_method_code":"def delete_vm_image(self, vm_image_name, delete_vhd=False):\n        '''\n        Deletes the specified VM Image from the image repository that is\n        associated with the specified subscription.\n\n        vm_image_name:\n            The name of the image.\n        delete_vhd:\n            Deletes the underlying vhd blob in Azure storage.\n        '''\n        _validate_not_none('vm_image_name', vm_image_name)\n        path = self._get_vm_image_path(vm_image_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.list_vm_images","method_code":"def list_vm_images(self, location=None, publisher=None, category=None):\n        path = self._get_vm_image_path()\n        query = ''\n        if location:\n            query += '&location=' + location\n        if publisher:\n            query += '&publisher=' + publisher\n        if category:\n            query += '&category=' + category\n        if query:\n            path = path + '?' + query.lstrip('&')\n        return self._perform_get(path, VMImages)","method_summary":"Retrieves a list of the VM Images from the image repository that is associated with the specified subscription.","original_method_code":"def list_vm_images(self, location=None, publisher=None, category=None):\n        '''\n        Retrieves a list of the VM Images from the image repository that is\n        associated with the specified subscription.\n        '''\n        path = self._get_vm_image_path()\n        query = ''\n        if location:\n            query += '&location=' + location\n        if publisher:\n            query += '&publisher=' + publisher\n        if category:\n            query += '&category=' + category\n        if query:\n            path = path + '?' + query.lstrip('&')\n        return self._perform_get(path, VMImages)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.update_vm_image","method_code":"def update_vm_image(self, vm_image_name, vm_image):\n        _validate_not_none('vm_image_name', vm_image_name)\n        _validate_not_none('vm_image', vm_image)\n        return self._perform_put(self._get_vm_image_path(vm_image_name),\n                                 _XmlSerializer.update_vm_image_to_xml(vm_image),\n                                 as_async=True)","method_summary":"Updates a VM Image in the image repository that is associated with the specified subscription.","original_method_code":"def update_vm_image(self, vm_image_name, vm_image):\n        '''\n        Updates a VM Image in the image repository that is associated with the\n        specified subscription.\n\n        vm_image_name:\n            Name of image to update.\n        vm_image:\n            An instance of VMImage class.\n        vm_image.label: Optional. Specifies an identifier for the image.\n        vm_image.os_disk_configuration:\n            Required. Specifies configuration information for the operating \n            system disk that is associated with the image.\n        vm_image.os_disk_configuration.host_caching:\n            Optional. Specifies the caching behavior of the operating system disk.\n            Possible values are: None, ReadOnly, ReadWrite \n        vm_image.data_disk_configurations:\n            Optional. Specifies configuration information for the data disks\n            that are associated with the image. A VM Image might not have data\n            disks associated with it.\n        vm_image.data_disk_configurations[].name:\n            Required. Specifies the name of the data disk.\n        vm_image.data_disk_configurations[].host_caching:\n            Optional. Specifies the caching behavior of the data disk.\n            Possible values are: None, ReadOnly, ReadWrite \n        vm_image.data_disk_configurations[].lun:\n            Optional if the lun for the disk is 0. Specifies the Logical Unit\n            Number (LUN) for the data disk.\n        vm_image.description: Optional. Specifies the description of the image.\n        vm_image.language: Optional. Specifies the language of the image.\n        vm_image.image_family:\n            Optional. Specifies a value that can be used to group VM Images.\n        vm_image.recommended_vm_size:\n            Optional. Specifies the size to use for the Virtual Machine that\n            is created from the VM Image.\n        vm_image.eula:\n            Optional. Specifies the End User License Agreement that is\n            associated with the image. The value for this element is a string,\n            but it is recommended that the value be a URL that points to a EULA.\n        vm_image.icon_uri:\n            Optional. Specifies the URI to the icon that is displayed for the\n            image in the Management Portal.\n        vm_image.small_icon_uri:\n            Optional. Specifies the URI to the small icon that is displayed for\n            the image in the Management Portal.\n        vm_image.privacy_uri:\n            Optional. Specifies the URI that points to a document that contains\n            the privacy policy related to the image.\n        vm_image.published_date:\n            Optional. Specifies the date when the image was added to the image\n            repository.\n        vm_image.show_in_gui:\n            Optional. Indicates whether the VM Images should be listed in the\n            portal.\n        '''\n        _validate_not_none('vm_image_name', vm_image_name)\n        _validate_not_none('vm_image', vm_image)\n        return self._perform_put(self._get_vm_image_path(vm_image_name),\n                                 _XmlSerializer.update_vm_image_to_xml(vm_image),\n                                 as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.add_os_image","method_code":"def add_os_image(self, label, media_link, name, os):\n        _validate_not_none('label', label)\n        _validate_not_none('media_link', media_link)\n        _validate_not_none('name', name)\n        _validate_not_none('os', os)\n        return self._perform_post(self._get_image_path(),\n                                  _XmlSerializer.os_image_to_xml(\n                                      label, media_link, name, os),\n                                  as_async=True)","method_summary":"Adds an OS image that is currently stored in a storage account in your subscription to the image repository.","original_method_code":"def add_os_image(self, label, media_link, name, os):\n        '''\n        Adds an OS image that is currently stored in a storage account in your\n        subscription to the image repository.\n\n        label:\n            Specifies the friendly name of the image.\n        media_link:\n            Specifies the location of the blob in Windows Azure blob store\n            where the media for the image is located. The blob location must\n            belong to a storage account in the subscription specified by the\n            <subscription-id> value in the operation call. Example:\n            http:\/\/example.blob.core.windows.net\/disks\/mydisk.vhd\n        name:\n            Specifies a name for the OS image that Windows Azure uses to\n            identify the image when creating one or more virtual machines.\n        os:\n            The operating system type of the OS image. Possible values are:\n            Linux, Windows\n        '''\n        _validate_not_none('label', label)\n        _validate_not_none('media_link', media_link)\n        _validate_not_none('name', name)\n        _validate_not_none('os', os)\n        return self._perform_post(self._get_image_path(),\n                                  _XmlSerializer.os_image_to_xml(\n                                      label, media_link, name, os),\n                                  as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.update_os_image","method_code":"def update_os_image(self, image_name, label, media_link, name, os):\n        _validate_not_none('image_name', image_name)\n        _validate_not_none('label', label)\n        _validate_not_none('media_link', media_link)\n        _validate_not_none('name', name)\n        _validate_not_none('os', os)\n        return self._perform_put(self._get_image_path(image_name),\n                                 _XmlSerializer.os_image_to_xml(\n                                     label, media_link, name, os),\n                                 as_async=True)","method_summary":"Updates an OS image that in your image repository.","original_method_code":"def update_os_image(self, image_name, label, media_link, name, os):\n        '''\n        Updates an OS image that in your image repository.\n\n        image_name:\n            The name of the image to update.\n        label:\n            Specifies the friendly name of the image to be updated. You cannot\n            use this operation to update images provided by the Windows Azure\n            platform.\n        media_link:\n            Specifies the location of the blob in Windows Azure blob store\n            where the media for the image is located. The blob location must\n            belong to a storage account in the subscription specified by the\n            <subscription-id> value in the operation call. Example:\n            http:\/\/example.blob.core.windows.net\/disks\/mydisk.vhd\n        name:\n            Specifies a name for the OS image that Windows Azure uses to\n            identify the image when creating one or more VM Roles.\n        os:\n            The operating system type of the OS image. Possible values are:\n            Linux, Windows\n        '''\n        _validate_not_none('image_name', image_name)\n        _validate_not_none('label', label)\n        _validate_not_none('media_link', media_link)\n        _validate_not_none('name', name)\n        _validate_not_none('os', os)\n        return self._perform_put(self._get_image_path(image_name),\n                                 _XmlSerializer.os_image_to_xml(\n                                     label, media_link, name, os),\n                                 as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.update_os_image_from_image_reference","method_code":"def update_os_image_from_image_reference(self, image_name, os_image):\n        _validate_not_none('image_name', image_name)\n        _validate_not_none('os_image', os_image)\n        return self._perform_put(self._get_image_path(image_name),\n            _XmlSerializer.update_os_image_to_xml(os_image), as_async=True\n        )","method_summary":"Updates metadata elements from a given OS image reference.","original_method_code":"def update_os_image_from_image_reference(self, image_name, os_image):\n        '''\n        Updates metadata elements from a given OS image reference.\n\n        image_name:\n            The name of the image to update.\n        os_image:\n            An instance of OSImage class.\n        os_image.label: Optional. Specifies an identifier for the image.\n        os_image.description: Optional. Specifies the description of the image.\n        os_image.language: Optional. Specifies the language of the image.\n        os_image.image_family:\n            Optional. Specifies a value that can be used to group VM Images.\n        os_image.recommended_vm_size:\n            Optional. Specifies the size to use for the Virtual Machine that\n            is created from the VM Image.\n        os_image.eula:\n            Optional. Specifies the End User License Agreement that is\n            associated with the image. The value for this element is a string,\n            but it is recommended that the value be a URL that points to a EULA.\n        os_image.icon_uri:\n            Optional. Specifies the URI to the icon that is displayed for the\n            image in the Management Portal.\n        os_image.small_icon_uri:\n            Optional. Specifies the URI to the small icon that is displayed for\n            the image in the Management Portal.\n        os_image.privacy_uri:\n            Optional. Specifies the URI that points to a document that contains\n            the privacy policy related to the image.\n        os_image.published_date:\n            Optional. Specifies the date when the image was added to the image\n            repository.\n        os.image.media_link:\n            Required: Specifies the location of the blob in Windows Azure\n            blob store where the media for the image is located. The blob\n            location must belong to a storage account in the subscription\n            specified by the <subscription-id> value in the operation call.\n            Example:\n            http:\/\/example.blob.core.windows.net\/disks\/mydisk.vhd\n        os_image.name:\n            Specifies a name for the OS image that Windows Azure uses to\n            identify the image when creating one or more VM Roles.\n        os_image.os:\n            The operating system type of the OS image. Possible values are:\n            Linux, Windows\n        '''\n        _validate_not_none('image_name', image_name)\n        _validate_not_none('os_image', os_image)\n        return self._perform_put(self._get_image_path(image_name),\n            _XmlSerializer.update_os_image_to_xml(os_image), as_async=True\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_os_image","method_code":"def delete_os_image(self, image_name, delete_vhd=False):\n        _validate_not_none('image_name', image_name)\n        path = self._get_image_path(image_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)","method_summary":"Deletes the specified OS image from your image repository.","original_method_code":"def delete_os_image(self, image_name, delete_vhd=False):\n        '''\n        Deletes the specified OS image from your image repository.\n\n        image_name:\n            The name of the image.\n        delete_vhd:\n            Deletes the underlying vhd blob in Azure storage.\n        '''\n        _validate_not_none('image_name', image_name)\n        path = self._get_image_path(image_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.get_data_disk","method_code":"def get_data_disk(self, service_name, deployment_name, role_name, lun):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        return self._perform_get(\n            self._get_data_disk_path(\n                service_name, deployment_name, role_name, lun),\n            DataVirtualHardDisk)","method_summary":"Retrieves the specified data disk from a virtual machine.","original_method_code":"def get_data_disk(self, service_name, deployment_name, role_name, lun):\n        '''\n        Retrieves the specified data disk from a virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        lun:\n            The Logical Unit Number (LUN) for the disk.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        return self._perform_get(\n            self._get_data_disk_path(\n                service_name, deployment_name, role_name, lun),\n            DataVirtualHardDisk)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.add_data_disk","method_code":"def add_data_disk(self, service_name, deployment_name, role_name, lun,\n                      host_caching=None, media_link=None, disk_label=None,\n                      disk_name=None, logical_disk_size_in_gb=None,\n                      source_media_link=None):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        return self._perform_post(\n            self._get_data_disk_path(service_name, deployment_name, role_name),\n            _XmlSerializer.data_virtual_hard_disk_to_xml(\n                host_caching,\n                disk_label,\n                disk_name,\n                lun,\n                logical_disk_size_in_gb,\n                media_link,\n                source_media_link),\n            as_async=True)","method_summary":"Adds a data disk to a virtual machine.","original_method_code":"def add_data_disk(self, service_name, deployment_name, role_name, lun,\n                      host_caching=None, media_link=None, disk_label=None,\n                      disk_name=None, logical_disk_size_in_gb=None,\n                      source_media_link=None):\n        '''\n        Adds a data disk to a virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        lun:\n            Specifies the Logical Unit Number (LUN) for the disk. The LUN\n            specifies the slot in which the data drive appears when mounted\n            for usage by the virtual machine. Valid LUN values are 0 through 15.\n        host_caching:\n            Specifies the platform caching behavior of data disk blob for\n            read\/write efficiency. The default vault is ReadOnly. Possible\n            values are: None, ReadOnly, ReadWrite\n        media_link:\n            Specifies the location of the blob in Windows Azure blob store\n            where the media for the disk is located. The blob location must\n            belong to the storage account in the subscription specified by the\n            <subscription-id> value in the operation call. Example:\n            http:\/\/example.blob.core.windows.net\/disks\/mydisk.vhd\n        disk_label:\n            Specifies the description of the data disk. When you attach a disk,\n            either by directly referencing a media using the MediaLink element\n            or specifying the target disk size, you can use the DiskLabel\n            element to customize the name property of the target data disk.\n        disk_name:\n            Specifies the name of the disk. Windows Azure uses the specified\n            disk to create the data disk for the machine and populates this\n            field with the disk name.\n        logical_disk_size_in_gb:\n            Specifies the size, in GB, of an empty disk to be attached to the\n            role. The disk can be created as part of disk attach or create VM\n            role call by specifying the value for this property. Windows Azure\n            creates the empty disk based on size preference and attaches the\n            newly created disk to the Role.\n        source_media_link:\n            Specifies the location of a blob in account storage which is\n            mounted as a data disk when the virtual machine is created.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        return self._perform_post(\n            self._get_data_disk_path(service_name, deployment_name, role_name),\n            _XmlSerializer.data_virtual_hard_disk_to_xml(\n                host_caching,\n                disk_label,\n                disk_name,\n                lun,\n                logical_disk_size_in_gb,\n                media_link,\n                source_media_link),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.update_data_disk","method_code":"def update_data_disk(self, service_name, deployment_name, role_name, lun,\n                         host_caching=None, media_link=None, updated_lun=None,\n                         disk_label=None, disk_name=None,\n                         logical_disk_size_in_gb=None):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        return self._perform_put(\n            self._get_data_disk_path(\n                service_name, deployment_name, role_name, lun),\n            _XmlSerializer.data_virtual_hard_disk_to_xml(\n                host_caching,\n                disk_label,\n                disk_name,\n                updated_lun,\n                logical_disk_size_in_gb,\n                media_link,\n                None),\n            as_async=True)","method_summary":"Updates the specified data disk attached to the specified virtual machine.","original_method_code":"def update_data_disk(self, service_name, deployment_name, role_name, lun,\n                         host_caching=None, media_link=None, updated_lun=None,\n                         disk_label=None, disk_name=None,\n                         logical_disk_size_in_gb=None):\n        '''\n        Updates the specified data disk attached to the specified virtual\n        machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        lun:\n            Specifies the Logical Unit Number (LUN) for the disk. The LUN\n            specifies the slot in which the data drive appears when mounted\n            for usage by the virtual machine. Valid LUN values are 0 through\n            15.\n        host_caching:\n            Specifies the platform caching behavior of data disk blob for\n            read\/write efficiency. The default vault is ReadOnly. Possible\n            values are: None, ReadOnly, ReadWrite\n        media_link:\n            Specifies the location of the blob in Windows Azure blob store\n            where the media for the disk is located. The blob location must\n            belong to the storage account in the subscription specified by\n            the <subscription-id> value in the operation call. Example:\n            http:\/\/example.blob.core.windows.net\/disks\/mydisk.vhd\n        updated_lun:\n            Specifies the Logical Unit Number (LUN) for the disk. The LUN\n            specifies the slot in which the data drive appears when mounted\n            for usage by the virtual machine. Valid LUN values are 0 through 15.\n        disk_label:\n            Specifies the description of the data disk. When you attach a disk,\n            either by directly referencing a media using the MediaLink element\n            or specifying the target disk size, you can use the DiskLabel\n            element to customize the name property of the target data disk.\n        disk_name:\n            Specifies the name of the disk. Windows Azure uses the specified\n            disk to create the data disk for the machine and populates this\n            field with the disk name.\n        logical_disk_size_in_gb:\n            Specifies the size, in GB, of an empty disk to be attached to the\n            role. The disk can be created as part of disk attach or create VM\n            role call by specifying the value for this property. Windows Azure\n            creates the empty disk based on size preference and attaches the\n            newly created disk to the Role.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        return self._perform_put(\n            self._get_data_disk_path(\n                service_name, deployment_name, role_name, lun),\n            _XmlSerializer.data_virtual_hard_disk_to_xml(\n                host_caching,\n                disk_label,\n                disk_name,\n                updated_lun,\n                logical_disk_size_in_gb,\n                media_link,\n                None),\n            as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_data_disk","method_code":"def delete_data_disk(self, service_name, deployment_name, role_name, lun, delete_vhd=False):\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        path = self._get_data_disk_path(service_name, deployment_name, role_name, lun)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)","method_summary":"Removes the specified data disk from a virtual machine.","original_method_code":"def delete_data_disk(self, service_name, deployment_name, role_name, lun, delete_vhd=False):\n        '''\n        Removes the specified data disk from a virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        lun:\n            The Logical Unit Number (LUN) for the disk.\n        delete_vhd:\n            Deletes the underlying vhd blob in Azure storage.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        path = self._get_data_disk_path(service_name, deployment_name, role_name, lun)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.add_disk","method_code":"def add_disk(self, has_operating_system, label, media_link, name, os):\n        _validate_not_none('label', label)\n        _validate_not_none('media_link', media_link)\n        _validate_not_none('name', name)\n        _validate_not_none('os', os)\n        return self._perform_post(self._get_disk_path(),\n                                  _XmlSerializer.disk_to_xml(\n                                      label,\n                                      media_link,\n                                      name,\n                                      os))","method_summary":"Adds a disk to the user image repository. The disk can be an OS disk or a data disk.","original_method_code":"def add_disk(self, has_operating_system, label, media_link, name, os):\n        '''\n        Adds a disk to the user image repository. The disk can be an OS disk\n        or a data disk.\n\n        has_operating_system:\n            Deprecated.\n        label:\n            Specifies the description of the disk.\n        media_link:\n            Specifies the location of the blob in Windows Azure blob store\n            where the media for the disk is located. The blob location must\n            belong to the storage account in the current subscription specified\n            by the <subscription-id> value in the operation call. Example:\n            http:\/\/example.blob.core.windows.net\/disks\/mydisk.vhd\n        name:\n            Specifies a name for the disk. Windows Azure uses the name to\n            identify the disk when creating virtual machines from the disk.\n        os:\n            The OS type of the disk. Possible values are: Linux, Windows\n        '''\n        _validate_not_none('label', label)\n        _validate_not_none('media_link', media_link)\n        _validate_not_none('name', name)\n        _validate_not_none('os', os)\n        return self._perform_post(self._get_disk_path(),\n                                  _XmlSerializer.disk_to_xml(\n                                      label,\n                                      media_link,\n                                      name,\n                                      os))","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.update_disk","method_code":"def update_disk(self, disk_name, has_operating_system=None, label=None, media_link=None,\n                    name=None, os=None):\n        _validate_not_none('disk_name', disk_name)\n        _validate_not_none('label', label)\n        return self._perform_put(self._get_disk_path(disk_name),\n                                 _XmlSerializer.disk_to_xml(\n                                     label,\n                                     None,\n                                     None,\n                                     None))","method_summary":"Updates an existing disk in your image repository.","original_method_code":"def update_disk(self, disk_name, has_operating_system=None, label=None, media_link=None,\n                    name=None, os=None):\n        '''\n        Updates an existing disk in your image repository.\n\n        disk_name:\n            The name of the disk to update.\n        has_operating_system:\n            Deprecated.\n        label:\n            Specifies the description of the disk.\n        media_link:\n            Deprecated.\n        name:\n            Deprecated.\n        os:\n            Deprecated.\n        '''\n        _validate_not_none('disk_name', disk_name)\n        _validate_not_none('label', label)\n        return self._perform_put(self._get_disk_path(disk_name),\n                                 _XmlSerializer.disk_to_xml(\n                                     label,\n                                     None,\n                                     None,\n                                     None))","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceManagementService.delete_disk","method_code":"def delete_disk(self, disk_name, delete_vhd=False):\n        _validate_not_none('disk_name', disk_name)\n        path = self._get_disk_path(disk_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path)","method_summary":"Deletes the specified data or operating system disk from your image repository.","original_method_code":"def delete_disk(self, disk_name, delete_vhd=False):\n        '''\n        Deletes the specified data or operating system disk from your image\n        repository.\n\n        disk_name:\n            The name of the disk to delete.\n        delete_vhd:\n            Deletes the underlying vhd blob in Azure storage.\n        '''\n        _validate_not_none('disk_name', disk_name)\n        path = self._get_disk_path(disk_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"PolicyStatesOperations.summarize_for_management_group","method_code":"def summarize_for_management_group(\n            self, management_group_name, query_options=None, custom_headers=None, raw=False, **operation_config):\n        top = None\n        if query_options is not None:\n            top = query_options.top\n        from_parameter = None\n        if query_options is not None:\n            from_parameter = query_options.from_property\n        to = None\n        if query_options is not None:\n            to = query_options.to\n        filter = None\n        if query_options is not None:\n            filter = query_options.filter\n\n        \n        url = self.summarize_for_management_group.metadata['url']\n        path_format_arguments = {\n            'policyStatesSummaryResource': self._serialize.url(\"self.policy_states_summary_resource\", self.policy_states_summary_resource, 'str'),\n            'managementGroupsNamespace': self._serialize.url(\"self.management_groups_namespace\", self.management_groups_namespace, 'str'),\n            'managementGroupName': self._serialize.url(\"management_group_name\", management_group_name, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query(\"top\", top, 'int', minimum=0)\n        if from_parameter is not None:\n            query_parameters['$from'] = self._serialize.query(\"from_parameter\", from_parameter, 'iso-8601')\n        if to is not None:\n            query_parameters['$to'] = self._serialize.query(\"to\", to, 'iso-8601')\n        if filter is not None:\n            query_parameters['$filter'] = self._serialize.query(\"filter\", filter, 'str')\n\n        \n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        \n        request = self._client.post(url, query_parameters, header_parameters)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.QueryFailureException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('SummarizeResults', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_summary":"Summarizes policy states for the resources under the management group.","original_method_code":"def summarize_for_management_group(\n            self, management_group_name, query_options=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Summarizes policy states for the resources under the management group.\n\n        :param management_group_name: Management group name.\n        :type management_group_name: str\n        :param query_options: Additional parameters for the operation\n        :type query_options: ~azure.mgmt.policyinsights.models.QueryOptions\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: SummarizeResults or ClientRawResponse if raw=true\n        :rtype: ~azure.mgmt.policyinsights.models.SummarizeResults or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`QueryFailureException<azure.mgmt.policyinsights.models.QueryFailureException>`\n        \"\"\"\n        top = None\n        if query_options is not None:\n            top = query_options.top\n        from_parameter = None\n        if query_options is not None:\n            from_parameter = query_options.from_property\n        to = None\n        if query_options is not None:\n            to = query_options.to\n        filter = None\n        if query_options is not None:\n            filter = query_options.filter\n\n        # Construct URL\n        url = self.summarize_for_management_group.metadata['url']\n        path_format_arguments = {\n            'policyStatesSummaryResource': self._serialize.url(\"self.policy_states_summary_resource\", self.policy_states_summary_resource, 'str'),\n            'managementGroupsNamespace': self._serialize.url(\"self.management_groups_namespace\", self.management_groups_namespace, 'str'),\n            'managementGroupName': self._serialize.url(\"management_group_name\", management_group_name, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query(\"top\", top, 'int', minimum=0)\n        if from_parameter is not None:\n            query_parameters['$from'] = self._serialize.query(\"from_parameter\", from_parameter, 'iso-8601')\n        if to is not None:\n            query_parameters['$to'] = self._serialize.query(\"to\", to, 'iso-8601')\n        if filter is not None:\n            query_parameters['$filter'] = self._serialize.query(\"filter\", filter, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters, header_parameters)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.QueryFailureException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('SummarizeResults', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-mgmt-policyinsights\/azure\/mgmt\/policyinsights\/operations\/policy_states_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"Receiver._build_receiver","method_code":"def _build_receiver(self):\n        \n        self._handler.message_handler = self._handler.receiver_type(\n            self._handler._session,\n            self._handler._remote_address,\n            self._handler._name,\n            on_message_received=self._handler._message_received,\n            name='receiver-link-{}'.format(uuid.uuid4()),\n            debug=self._handler._debug_trace,\n            prefetch=self._handler._prefetch,\n            max_message_size=self._handler._max_message_size,\n            properties=self._handler._link_properties,\n            error_policy=self._handler._error_policy,\n            encoding=self._handler._encoding)\n        if self.mode != ReceiveSettleMode.PeekLock:\n            self._handler.message_handler.send_settle_mode = constants.SenderSettleMode.Settled\n            self._handler.message_handler.receive_settle_mode = constants.ReceiverSettleMode.ReceiveAndDelete\n            self._handler.message_handler._settle_mode = constants.ReceiverSettleMode.ReceiveAndDelete\n        self._handler.message_handler.open()","method_summary":"This is a temporary patch pending a fix in uAMQP.","original_method_code":"def _build_receiver(self):\n        \"\"\"This is a temporary patch pending a fix in uAMQP.\"\"\"\n        # pylint: disable=protected-access\n        self._handler.message_handler = self._handler.receiver_type(\n            self._handler._session,\n            self._handler._remote_address,\n            self._handler._name,\n            on_message_received=self._handler._message_received,\n            name='receiver-link-{}'.format(uuid.uuid4()),\n            debug=self._handler._debug_trace,\n            prefetch=self._handler._prefetch,\n            max_message_size=self._handler._max_message_size,\n            properties=self._handler._link_properties,\n            error_policy=self._handler._error_policy,\n            encoding=self._handler._encoding)\n        if self.mode != ReceiveSettleMode.PeekLock:\n            self._handler.message_handler.send_settle_mode = constants.SenderSettleMode.Settled\n            self._handler.message_handler.receive_settle_mode = constants.ReceiverSettleMode.ReceiveAndDelete\n            self._handler.message_handler._settle_mode = constants.ReceiverSettleMode.ReceiveAndDelete\n        self._handler.message_handler.open()","method_path":"azure-servicebus\/azure\/servicebus\/receive_handler.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"VirtualMachineScaleSetsOperations.create_or_update","method_code":"def create_or_update(\n            self, resource_group_name, vm_scale_set_name, parameters, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._create_or_update_initial(\n            resource_group_name=resource_group_name,\n            vm_scale_set_name=vm_scale_set_name,\n            parameters=parameters,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('VirtualMachineScaleSet', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Create or update a VM scale set.","original_method_code":"def create_or_update(\n            self, resource_group_name, vm_scale_set_name, parameters, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Create or update a VM scale set.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param vm_scale_set_name: The name of the VM scale set to create or\n         update.\n        :type vm_scale_set_name: str\n        :param parameters: The scale set object.\n        :type parameters:\n         ~azure.mgmt.compute.v2019_03_01.models.VirtualMachineScaleSet\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns VirtualMachineScaleSet\n         or ClientRawResponse<VirtualMachineScaleSet> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.compute.v2019_03_01.models.VirtualMachineScaleSet]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.compute.v2019_03_01.models.VirtualMachineScaleSet]]\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._create_or_update_initial(\n            resource_group_name=resource_group_name,\n            vm_scale_set_name=vm_scale_set_name,\n            parameters=parameters,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('VirtualMachineScaleSet', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-compute\/azure\/mgmt\/compute\/v2019_03_01\/operations\/virtual_machine_scale_sets_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"VirtualMachineScaleSetsOperations.convert_to_single_placement_group","method_code":"def convert_to_single_placement_group(\n            self, resource_group_name, vm_scale_set_name, active_placement_group_id=None, custom_headers=None, raw=False, **operation_config):\n        parameters = models.VMScaleSetConvertToSinglePlacementGroupInput(active_placement_group_id=active_placement_group_id)\n\n        \n        url = self.convert_to_single_placement_group.metadata['url']\n        path_format_arguments = {\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'vmScaleSetName': self._serialize.url(\"vm_scale_set_name\", vm_scale_set_name, 'str'),\n            'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n\n        \n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        \n        body_content = self._serialize.body(parameters, 'VMScaleSetConvertToSinglePlacementGroupInput')\n\n        \n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            exp = CloudError(response)\n            exp.request_id = response.headers.get('x-ms-request-id')\n            raise exp\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response","method_summary":"Converts SinglePlacementGroup property to false for a existing virtual machine scale set.","original_method_code":"def convert_to_single_placement_group(\n            self, resource_group_name, vm_scale_set_name, active_placement_group_id=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Converts SinglePlacementGroup property to false for a existing virtual\n        machine scale set.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param vm_scale_set_name: The name of the virtual machine scale set to\n         create or update.\n        :type vm_scale_set_name: str\n        :param active_placement_group_id: Id of the placement group in which\n         you want future virtual machine instances to be placed. To query\n         placement group Id, please use Virtual Machine Scale Set VMs - Get\n         API. If not provided, the platform will choose one with maximum number\n         of virtual machine instances.\n        :type active_placement_group_id: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        parameters = models.VMScaleSetConvertToSinglePlacementGroupInput(active_placement_group_id=active_placement_group_id)\n\n        # Construct URL\n        url = self.convert_to_single_placement_group.metadata['url']\n        path_format_arguments = {\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'vmScaleSetName': self._serialize.url(\"vm_scale_set_name\", vm_scale_set_name, 'str'),\n            'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        # Construct body\n        body_content = self._serialize.body(parameters, 'VMScaleSetConvertToSinglePlacementGroupInput')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            exp = CloudError(response)\n            exp.request_id = response.headers.get('x-ms-request-id')\n            raise exp\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response","method_path":"azure-mgmt-compute\/azure\/mgmt\/compute\/v2019_03_01\/operations\/virtual_machine_scale_sets_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"TextModerationOperations.screen_text","method_code":"def screen_text(\n            self, text_content_type, text_content, language=None, autocorrect=False, pii=False, list_id=None, classify=False, custom_headers=None, raw=False, callback=None, **operation_config):\n        \n        url = self.screen_text.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n        if language is not None:\n            query_parameters['language'] = self._serialize.query(\"language\", language, 'str')\n        if autocorrect is not None:\n            query_parameters['autocorrect'] = self._serialize.query(\"autocorrect\", autocorrect, 'bool')\n        if pii is not None:\n            query_parameters['PII'] = self._serialize.query(\"pii\", pii, 'bool')\n        if list_id is not None:\n            query_parameters['listId'] = self._serialize.query(\"list_id\", list_id, 'str')\n        if classify is not None:\n            query_parameters['classify'] = self._serialize.query(\"classify\", classify, 'bool')\n\n        \n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'text\/plain'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        header_parameters['Content-Type'] = self._serialize.header(\"text_content_type\", text_content_type, 'str')\n\n        \n        body_content = self._client.stream_upload(text_content, callback)\n\n        \n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.APIErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('Screen', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_summary":"Detect profanity and match against custom and shared blacklists. Detects profanity in more than 100 languages and match against custom and shared blacklists.","original_method_code":"def screen_text(\n            self, text_content_type, text_content, language=None, autocorrect=False, pii=False, list_id=None, classify=False, custom_headers=None, raw=False, callback=None, **operation_config):\n        \"\"\"Detect profanity and match against custom and shared blacklists.\n\n        Detects profanity in more than 100 languages and match against custom\n        and shared blacklists.\n\n        :param text_content_type: The content type. Possible values include:\n         'text\/plain', 'text\/html', 'text\/xml', 'text\/markdown'\n        :type text_content_type: str\n        :param text_content: Content to screen.\n        :type text_content: Generator\n        :param language: Language of the text.\n        :type language: str\n        :param autocorrect: Autocorrect text.\n        :type autocorrect: bool\n        :param pii: Detect personal identifiable information.\n        :type pii: bool\n        :param list_id: The list Id.\n        :type list_id: str\n        :param classify: Classify input.\n        :type classify: bool\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param callback: When specified, will be called with each chunk of\n         data that is streamed. The callback should take two arguments, the\n         bytes of the current chunk of data and the response object. If the\n         data is uploading, response will be None.\n        :type callback: Callable[Bytes, response=None]\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: Screen or ClientRawResponse if raw=true\n        :rtype: ~azure.cognitiveservices.vision.contentmoderator.models.Screen\n         or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`APIErrorException<azure.cognitiveservices.vision.contentmoderator.models.APIErrorException>`\n        \"\"\"\n        # Construct URL\n        url = self.screen_text.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        if language is not None:\n            query_parameters['language'] = self._serialize.query(\"language\", language, 'str')\n        if autocorrect is not None:\n            query_parameters['autocorrect'] = self._serialize.query(\"autocorrect\", autocorrect, 'bool')\n        if pii is not None:\n            query_parameters['PII'] = self._serialize.query(\"pii\", pii, 'bool')\n        if list_id is not None:\n            query_parameters['listId'] = self._serialize.query(\"list_id\", list_id, 'str')\n        if classify is not None:\n            query_parameters['classify'] = self._serialize.query(\"classify\", classify, 'bool')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'text\/plain'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        header_parameters['Content-Type'] = self._serialize.header(\"text_content_type\", text_content_type, 'str')\n\n        # Construct body\n        body_content = self._client.stream_upload(text_content, callback)\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.APIErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('Screen', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-cognitiveservices-vision-contentmoderator\/azure\/cognitiveservices\/vision\/contentmoderator\/operations\/text_moderation_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"KeyVaultClient.set_certificate_issuer","method_code":"def set_certificate_issuer(\n            self, vault_base_url, issuer_name, provider, credentials=None, organization_details=None, attributes=None, custom_headers=None, raw=False, **operation_config):\n        parameter = models.CertificateIssuerSetParameters(provider=provider, credentials=credentials, organization_details=organization_details, attributes=attributes)\n\n        \n        url = self.set_certificate_issuer.metadata['url']\n        path_format_arguments = {\n            'vaultBaseUrl': self._serialize.url(\"vault_base_url\", vault_base_url, 'str', skip_quote=True),\n            'issuer-name': self._serialize.url(\"issuer_name\", issuer_name, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        \n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        \n        body_content = self._serialize.body(parameter, 'CertificateIssuerSetParameters')\n\n        \n        request = self._client.put(url, query_parameters)\n        response = self._client.send(\n            request, header_parameters, body_content, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.KeyVaultErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('IssuerBundle', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_summary":"Sets the specified certificate issuer. The SetCertificateIssuer operation adds or updates the specified certificate issuer. This operation requires the certificates\/setissuers permission.","original_method_code":"def set_certificate_issuer(\n            self, vault_base_url, issuer_name, provider, credentials=None, organization_details=None, attributes=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Sets the specified certificate issuer.\n\n        The SetCertificateIssuer operation adds or updates the specified\n        certificate issuer. This operation requires the certificates\/setissuers\n        permission.\n\n        :param vault_base_url: The vault name, for example\n         https:\/\/myvault.vault.azure.net.\n        :type vault_base_url: str\n        :param issuer_name: The name of the issuer.\n        :type issuer_name: str\n        :param provider: The issuer provider.\n        :type provider: str\n        :param credentials: The credentials to be used for the issuer.\n        :type credentials:\n         ~azure.keyvault.v2016_10_01.models.IssuerCredentials\n        :param organization_details: Details of the organization as provided\n         to the issuer.\n        :type organization_details:\n         ~azure.keyvault.v2016_10_01.models.OrganizationDetails\n        :param attributes: Attributes of the issuer object.\n        :type attributes: ~azure.keyvault.v2016_10_01.models.IssuerAttributes\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: IssuerBundle or ClientRawResponse if raw=true\n        :rtype: ~azure.keyvault.v2016_10_01.models.IssuerBundle or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`KeyVaultErrorException<azure.keyvault.v2016_10_01.models.KeyVaultErrorException>`\n        \"\"\"\n        parameter = models.CertificateIssuerSetParameters(provider=provider, credentials=credentials, organization_details=organization_details, attributes=attributes)\n\n        # Construct URL\n        url = self.set_certificate_issuer.metadata['url']\n        path_format_arguments = {\n            'vaultBaseUrl': self._serialize.url(\"vault_base_url\", vault_base_url, 'str', skip_quote=True),\n            'issuer-name': self._serialize.url(\"issuer_name\", issuer_name, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        # Construct body\n        body_content = self._serialize.body(parameter, 'CertificateIssuerSetParameters')\n\n        # Construct and send request\n        request = self._client.put(url, query_parameters)\n        response = self._client.send(\n            request, header_parameters, body_content, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.KeyVaultErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('IssuerBundle', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-keyvault\/azure\/keyvault\/v2016_10_01\/key_vault_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusClient.from_connection_string","method_code":"def from_connection_string(cls, conn_str, *, loop=None, **kwargs):\n        address, policy, key, _ = parse_conn_str(conn_str)\n        parsed_namespace = urlparse(address)\n        namespace, _, base = parsed_namespace.hostname.partition('.')\n        return cls(\n            service_namespace=namespace,\n            shared_access_key_name=policy,\n            shared_access_key_value=key,\n            host_base='.' + base,\n            loop=loop,\n            **kwargs)","method_summary":"Create a Service Bus client from a connection string.","original_method_code":"def from_connection_string(cls, conn_str, *, loop=None, **kwargs):\n        \"\"\"Create a Service Bus client from a connection string.\n\n        :param conn_str: The connection string.\n        :type conn_str: str\n\n        Example:\n            .. literalinclude:: ..\/examples\/async_examples\/test_examples_async.py\n                :start-after: [START create_async_servicebus_client_connstr]\n                :end-before: [END create_async_servicebus_client_connstr]\n                :language: python\n                :dedent: 4\n                :caption: Create a ServiceBusClient via a connection string.\n\n        \"\"\"\n        address, policy, key, _ = parse_conn_str(conn_str)\n        parsed_namespace = urlparse(address)\n        namespace, _, base = parsed_namespace.hostname.partition('.')\n        return cls(\n            service_namespace=namespace,\n            shared_access_key_name=policy,\n            shared_access_key_value=key,\n            host_base='.' + base,\n            loop=loop,\n            **kwargs)","method_path":"azure-servicebus\/azure\/servicebus\/aio\/async_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusClient.get_subscription","method_code":"def get_subscription(self, topic_name, subscription_name):\n        try:\n            subscription = self.mgmt_client.get_subscription(topic_name, subscription_name)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except AzureServiceBusResourceNotFound:\n            raise ServiceBusResourceNotFound(\"Specificed subscription does not exist.\")\n        return SubscriptionClient.from_entity(\n            self._get_host(), topic_name, subscription,\n            shared_access_key_name=self.shared_access_key_name,\n            shared_access_key_value=self.shared_access_key_value,\n            loop=self.loop,\n            debug=self.debug)","method_summary":"Get an async client for a subscription entity.","original_method_code":"def get_subscription(self, topic_name, subscription_name):\n        \"\"\"Get an async client for a subscription entity.\n\n        :param topic_name: The name of the topic.\n        :type topic_name: str\n        :param subscription_name: The name of the subscription.\n        :type subscription_name: str\n        :rtype: ~azure.servicebus.aio.async_client.SubscriptionClient\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the subscription is not found.\n\n        Example:\n            .. literalinclude:: ..\/examples\/async_examples\/test_examples_async.py\n                :start-after: [START get_async_subscription_client]\n                :end-before: [END get_async_subscription_client]\n                :language: python\n                :dedent: 4\n                :caption: Get a TopicClient for the specified topic.\n\n        \"\"\"\n        try:\n            subscription = self.mgmt_client.get_subscription(topic_name, subscription_name)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except AzureServiceBusResourceNotFound:\n            raise ServiceBusResourceNotFound(\"Specificed subscription does not exist.\")\n        return SubscriptionClient.from_entity(\n            self._get_host(), topic_name, subscription,\n            shared_access_key_name=self.shared_access_key_name,\n            shared_access_key_value=self.shared_access_key_value,\n            loop=self.loop,\n            debug=self.debug)","method_path":"azure-servicebus\/azure\/servicebus\/aio\/async_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusClient.list_subscriptions","method_code":"def list_subscriptions(self, topic_name):\n        try:\n            subs = self.mgmt_client.list_subscriptions(topic_name)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except AzureServiceBusResourceNotFound:\n            raise ServiceBusResourceNotFound(\"Specificed topic does not exist.\")\n        sub_clients = []\n        for sub in subs:\n            sub_clients.append(SubscriptionClient.from_entity(\n                self._get_host(), topic_name, sub,\n                shared_access_key_name=self.shared_access_key_name,\n                shared_access_key_value=self.shared_access_key_value,\n                loop=self.loop,\n                debug=self.debug))\n        return sub_clients","method_summary":"Get an async client for all subscription entities in the topic.","original_method_code":"def list_subscriptions(self, topic_name):\n        \"\"\"Get an async client for all subscription entities in the topic.\n\n        :param topic_name: The topic to list subscriptions for.\n        :type topic_name: str\n        :rtype: list[~azure.servicebus.aio.async_client.SubscriptionClient]\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the topic is not found.\n        \"\"\"\n        try:\n            subs = self.mgmt_client.list_subscriptions(topic_name)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except AzureServiceBusResourceNotFound:\n            raise ServiceBusResourceNotFound(\"Specificed topic does not exist.\")\n        sub_clients = []\n        for sub in subs:\n            sub_clients.append(SubscriptionClient.from_entity(\n                self._get_host(), topic_name, sub,\n                shared_access_key_name=self.shared_access_key_name,\n                shared_access_key_value=self.shared_access_key_value,\n                loop=self.loop,\n                debug=self.debug))\n        return sub_clients","method_path":"azure-servicebus\/azure\/servicebus\/aio\/async_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SendClientMixin.get_sender","method_code":"def get_sender(self, message_timeout=0, session=None, **kwargs):\n        handler_id = str(uuid.uuid4())\n        if self.entity and self.requires_session:\n            return SessionSender(\n                handler_id,\n                self.entity_uri,\n                self.auth_config,\n                session=session,\n                loop=self.loop,\n                debug=self.debug,\n                msg_timeout=message_timeout,\n                **kwargs)\n        return Sender(\n            handler_id,\n            self.entity_uri,\n            self.auth_config,\n            session=session,\n            loop=self.loop,\n            debug=self.debug,\n            msg_timeout=message_timeout,\n            **kwargs)","method_summary":"Get a Sender for the Service Bus endpoint. A Sender represents a single open connection within which multiple send operations can be made.","original_method_code":"def get_sender(self, message_timeout=0, session=None, **kwargs):\n        \"\"\"Get a Sender for the Service Bus endpoint.\n\n        A Sender represents a single open connection within which multiple send operations can be made.\n\n        :param message_timeout: The period in seconds during which messages sent with\n         this Sender must be sent. If the send is not completed in this time it will fail.\n        :type message_timeout: int\n        :param session: An optional session ID. If supplied this session ID will be\n         applied to every outgoing message sent with this Sender.\n         If an individual message already has a session ID, that will be\n         used instead. If no session ID is supplied here, nor set on an outgoing\n         message, a ValueError will be raised if the entity is sessionful.\n        :type session: str or ~uuid.Guid\n        :returns: A Sender instance with an unopened connection.\n        :rtype: ~azure.servicebus.aio.async_send_handler.Sender\n\n        Example:\n            .. literalinclude:: ..\/examples\/async_examples\/test_examples_async.py\n                :start-after: [START open_close_sender_context]\n                :end-before: [END open_close_sender_context]\n                :language: python\n                :dedent: 4\n                :caption: Send multiple messages with a Sender.\n\n        \"\"\"\n        handler_id = str(uuid.uuid4())\n        if self.entity and self.requires_session:\n            return SessionSender(\n                handler_id,\n                self.entity_uri,\n                self.auth_config,\n                session=session,\n                loop=self.loop,\n                debug=self.debug,\n                msg_timeout=message_timeout,\n                **kwargs)\n        return Sender(\n            handler_id,\n            self.entity_uri,\n            self.auth_config,\n            session=session,\n            loop=self.loop,\n            debug=self.debug,\n            msg_timeout=message_timeout,\n            **kwargs)","method_path":"azure-servicebus\/azure\/servicebus\/aio\/async_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ReceiveClientMixin.get_receiver","method_code":"def get_receiver(self, session=None, prefetch=0, mode=ReceiveSettleMode.PeekLock, idle_timeout=0, **kwargs):\n        if self.entity and not self.requires_session and session:\n            raise ValueError(\"A session cannot be used with a non-sessionful entitiy.\")\n        if self.entity and self.requires_session and not session:\n            raise ValueError(\"This entity requires a session.\")\n        if int(prefetch) < 0 or int(prefetch) > 50000:\n            raise ValueError(\"Prefetch must be an integer between 0 and 50000 inclusive.\")\n\n        prefetch += 1\n        handler_id = str(uuid.uuid4())\n        if session:\n            return SessionReceiver(\n                handler_id,\n                self.entity_uri,\n                self.auth_config,\n                session=session,\n                loop=self.loop,\n                debug=self.debug,\n                timeout=int(idle_timeout * 1000),\n                prefetch=prefetch,\n                mode=mode,\n                **kwargs)\n        return Receiver(\n            handler_id,\n            self.entity_uri,\n            self.auth_config,\n            loop=self.loop,\n            debug=self.debug,\n            timeout=int(idle_timeout * 1000),\n            prefetch=prefetch,\n            mode=mode,\n            **kwargs)","method_summary":"Get a Receiver for the Service Bus endpoint. A Receiver represents a single open connection with which multiple receive operations can be made.","original_method_code":"def get_receiver(self, session=None, prefetch=0, mode=ReceiveSettleMode.PeekLock, idle_timeout=0, **kwargs):\n        \"\"\"Get a Receiver for the Service Bus endpoint.\n\n        A Receiver represents a single open connection with which multiple receive operations can be made.\n\n        :param session: A specific session from which to receive. This must be specified for a\n         sessionful entity, otherwise it must be None. In order to receive the next available\n         session, set this to NEXT_AVAILABLE.\n        :type session: str or ~azure.servicebus.common.constants.NEXT_AVAILABLE\n        :param prefetch: The maximum number of messages to cache with each request to the service.\n         The default value is 0, meaning messages will be received from the service and processed\n         one at a time. Increasing this value will improve message throughput performance but increase\n         the chance that messages will expire while they are cached if they're not processed fast enough.\n        :type prefetch: int\n        :param mode: The mode with which messages will be retrieved from the entity. The two options\n         are PeekLock and ReceiveAndDelete. Messages received with PeekLock must be settled within a given\n         lock period before they will be removed from the queue. Messages received with ReceiveAndDelete\n         will be immediately removed from the queue, and cannot be subsequently rejected or re-received if\n         the client fails to process the message. The default mode is PeekLock.\n        :type mode: ~azure.servicebus.common.constants.ReceiveSettleMode\n        :param idle_timeout: The timeout in seconds between received messages after which the receiver will\n         automatically shutdown. The default value is 0, meaning no timeout.\n        :type idle_timeout: int\n        :returns: A Receiver instance with an unopened connection.\n        :rtype: ~azure.servicebus.aio.async_receive_handler.Receiver\n\n        Example:\n            .. literalinclude:: ..\/examples\/async_examples\/test_examples_async.py\n                :start-after: [START open_close_receiver_context]\n                :end-before: [END open_close_receiver_context]\n                :language: python\n                :dedent: 4\n                :caption: Receive messages with a Receiver.\n\n        \"\"\"\n        if self.entity and not self.requires_session and session:\n            raise ValueError(\"A session cannot be used with a non-sessionful entitiy.\")\n        if self.entity and self.requires_session and not session:\n            raise ValueError(\"This entity requires a session.\")\n        if int(prefetch) < 0 or int(prefetch) > 50000:\n            raise ValueError(\"Prefetch must be an integer between 0 and 50000 inclusive.\")\n\n        prefetch += 1\n        handler_id = str(uuid.uuid4())\n        if session:\n            return SessionReceiver(\n                handler_id,\n                self.entity_uri,\n                self.auth_config,\n                session=session,\n                loop=self.loop,\n                debug=self.debug,\n                timeout=int(idle_timeout * 1000),\n                prefetch=prefetch,\n                mode=mode,\n                **kwargs)\n        return Receiver(\n            handler_id,\n            self.entity_uri,\n            self.auth_config,\n            loop=self.loop,\n            debug=self.debug,\n            timeout=int(idle_timeout * 1000),\n            prefetch=prefetch,\n            mode=mode,\n            **kwargs)","method_path":"azure-servicebus\/azure\/servicebus\/aio\/async_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ReceiveClientMixin.get_deadletter_receiver","method_code":"def get_deadletter_receiver(\n            self, transfer_deadletter=False, prefetch=0,\n            mode=ReceiveSettleMode.PeekLock, idle_timeout=0, **kwargs):\n        if int(prefetch) < 0 or int(prefetch) > 50000:\n            raise ValueError(\"Prefetch must be an integer between 0 and 50000 inclusive.\")\n\n        prefetch += 1\n        handler_id = str(uuid.uuid4())\n        if transfer_deadletter:\n            entity_uri = self.mgmt_client.format_transfer_dead_letter_queue_name(self.entity_uri)\n        else:\n            entity_uri = self.mgmt_client.format_dead_letter_queue_name(self.entity_uri)\n        return Receiver(\n            handler_id,\n            entity_uri,\n            self.auth_config,\n            loop=self.loop,\n            debug=self.debug,\n            timeout=int(idle_timeout * 1000),\n            prefetch=prefetch,\n            mode=mode,\n            **kwargs)","method_summary":"Get a Receiver for the deadletter endpoint of the entity. A Receiver represents a single open connection with which multiple receive operations can be made.","original_method_code":"def get_deadletter_receiver(\n            self, transfer_deadletter=False, prefetch=0,\n            mode=ReceiveSettleMode.PeekLock, idle_timeout=0, **kwargs):\n        \"\"\"Get a Receiver for the deadletter endpoint of the entity.\n\n        A Receiver represents a single open connection with which multiple receive operations can be made.\n\n        :param transfer_deadletter: Whether to connect to the transfer deadletter queue, or the standard\n         deadletter queue. Default is False, using the standard deadletter endpoint.\n        :type transfer_deadletter: bool\n        :param prefetch: The maximum number of messages to cache with each request to the service.\n         The default value is 0, meaning messages will be received from the service and processed\n         one at a time. Increasing this value will improve message throughput performance but increase\n         the change that messages will expire while they are cached if they're not processed fast enough.\n        :type prefetch: int\n        :param mode: The mode with which messages will be retrieved from the entity. The two options\n         are PeekLock and ReceiveAndDelete. Messages received with PeekLock must be settled within a given\n         lock period before they will be removed from the queue. Messages received with ReceiveAndDelete\n         will be immediately removed from the queue, and cannot be subsequently rejected or re-received if\n         the client fails to process the message. The default mode is PeekLock.\n        :type mode: ~azure.servicebus.common.constants.ReceiveSettleMode\n        :param idle_timeout: The timeout in seconds between received messages after which the receiver will\n         automatically shutdown. The default value is 0, meaning no timeout.\n        :type idle_timeout: int\n        :returns: A Receiver instance with an unopened Connection.\n        :rtype: ~azure.servicebus.aio.async_receive_handler.Receiver\n\n        Example:\n            .. literalinclude:: ..\/examples\/async_examples\/test_examples_async.py\n                :start-after: [START receiver_deadletter_messages]\n                :end-before: [END receiver_deadletter_messages]\n                :language: python\n                :dedent: 4\n                :caption: Receive dead-lettered messages.\n\n        \"\"\"\n        if int(prefetch) < 0 or int(prefetch) > 50000:\n            raise ValueError(\"Prefetch must be an integer between 0 and 50000 inclusive.\")\n\n        prefetch += 1\n        handler_id = str(uuid.uuid4())\n        if transfer_deadletter:\n            entity_uri = self.mgmt_client.format_transfer_dead_letter_queue_name(self.entity_uri)\n        else:\n            entity_uri = self.mgmt_client.format_dead_letter_queue_name(self.entity_uri)\n        return Receiver(\n            handler_id,\n            entity_uri,\n            self.auth_config,\n            loop=self.loop,\n            debug=self.debug,\n            timeout=int(idle_timeout * 1000),\n            prefetch=prefetch,\n            mode=mode,\n            **kwargs)","method_path":"azure-servicebus\/azure\/servicebus\/aio\/async_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"parse_response_for_async_op","method_code":"def parse_response_for_async_op(response):\n    if response is None:\n        return None\n\n    result = AsynchronousOperationResult()\n    if response.headers:\n        for name, value in response.headers:\n            if name.lower() == 'x-ms-request-id':\n                result.request_id = value\n\n    return result","method_summary":"Extracts request id from response header.","original_method_code":"def parse_response_for_async_op(response):\n    ''' Extracts request id from response header. '''\n\n    if response is None:\n        return None\n\n    result = AsynchronousOperationResult()\n    if response.headers:\n        for name, value in response.headers:\n            if name.lower() == 'x-ms-request-id':\n                result.request_id = value\n\n    return result","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementclient.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_ServiceManagementClient.perform_get","method_code":"def perform_get(self, path, x_ms_version=None):\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self.host\n        request.path = path\n        request.path, request.query = self._httpclient._update_request_uri_query(request)\n        request.headers = self._update_management_header(request, x_ms_version)\n        response = self._perform_request(request)\n\n        return response","method_summary":"Performs a GET request and returns the response.","original_method_code":"def perform_get(self, path, x_ms_version=None):\n        '''\n        Performs a GET request and returns the response.\n\n        path:\n            Path to the resource.\n            Ex: '\/<subscription-id>\/services\/hostedservices\/<service-name>'\n        x_ms_version:\n            If specified, this is used for the x-ms-version header.\n            Otherwise, self.x_ms_version is used.\n        '''\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self.host\n        request.path = path\n        request.path, request.query = self._httpclient._update_request_uri_query(request)\n        request.headers = self._update_management_header(request, x_ms_version)\n        response = self._perform_request(request)\n\n        return response","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementclient.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_ServiceManagementClient.perform_put","method_code":"def perform_put(self, path, body, x_ms_version=None):\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self.host\n        request.path = path\n        request.body = _get_request_body(body)\n        request.path, request.query = self._httpclient._update_request_uri_query(request)\n        request.headers = self._update_management_header(request, x_ms_version)\n        response = self._perform_request(request)\n\n        return response","method_summary":"Performs a PUT request and returns the response.","original_method_code":"def perform_put(self, path, body, x_ms_version=None):\n        '''\n        Performs a PUT request and returns the response.\n\n        path:\n            Path to the resource.\n            Ex: '\/<subscription-id>\/services\/hostedservices\/<service-name>'\n        body:\n            Body for the PUT request.\n        x_ms_version:\n            If specified, this is used for the x-ms-version header.\n            Otherwise, self.x_ms_version is used.\n        '''\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self.host\n        request.path = path\n        request.body = _get_request_body(body)\n        request.path, request.query = self._httpclient._update_request_uri_query(request)\n        request.headers = self._update_management_header(request, x_ms_version)\n        response = self._perform_request(request)\n\n        return response","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementclient.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_ServiceManagementClient._update_management_header","method_code":"def _update_management_header(self, request, x_ms_version):\n        if request.method in ['PUT', 'POST', 'MERGE', 'DELETE']:\n            request.headers.append(('Content-Length', str(len(request.body))))\n\n        \n        request.headers.append(('x-ms-version', x_ms_version or self.x_ms_version))\n\n        \n        if not request.method in ['GET', 'HEAD']:\n            for name, _ in request.headers:\n                if 'content-type' == name.lower():\n                    break\n            else:\n                request.headers.append(\n                    ('Content-Type',\n                     self.content_type))\n\n        return request.headers","method_summary":"Add additional headers for management.","original_method_code":"def _update_management_header(self, request, x_ms_version):\n        ''' Add additional headers for management. '''\n\n        if request.method in ['PUT', 'POST', 'MERGE', 'DELETE']:\n            request.headers.append(('Content-Length', str(len(request.body))))\n\n        # append additional headers base on the service\n        request.headers.append(('x-ms-version', x_ms_version or self.x_ms_version))\n\n        # if it is not GET or HEAD request, must set content-type.\n        if not request.method in ['GET', 'HEAD']:\n            for name, _ in request.headers:\n                if 'content-type' == name.lower():\n                    break\n            else:\n                request.headers.append(\n                    ('Content-Type',\n                     self.content_type))\n\n        return request.headers","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicemanagementclient.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"travis_build_package","method_code":"def travis_build_package():\n    travis_tag = os.environ.get('TRAVIS_TAG')\n    if not travis_tag:\n        print(\"TRAVIS_TAG environment variable is not present\")\n        return \"TRAVIS_TAG environment variable is not present\"\n\n    try:\n        name, version = travis_tag.split(\"_\")\n    except ValueError:\n        print(\"TRAVIS_TAG is not '<package_name>_<version>' (tag is: {})\".format(travis_tag))\n        return \"TRAVIS_TAG is not '<package_name>_<version>' (tag is: {})\".format(travis_tag)\n\n    try:\n        version = Version(version)\n    except InvalidVersion:\n        print(\"Version must be a valid PEP440 version (version is: {})\".format(version))\n        return \"Version must be a valid PEP440 version (version is: {})\".format(version)\n\n    if name.lower() in OMITTED_RELEASE_PACKAGES:\n        print(\"The input package {} has been disabled for release from Travis.CI.\".format(name))\n        return\n\n    abs_dist_path = Path(os.environ['TRAVIS_BUILD_DIR'], 'dist')\n    create_package(name, str(abs_dist_path))\n\n    print(\"Produced:\\n{}\".format(list(abs_dist_path.glob('*'))))\n\n    pattern = \"*{}*\".format(version)\n    packages = list(abs_dist_path.glob(pattern))\n    if not packages:\n        return \"Package version does not match tag {}, abort\".format(version)\n    pypi_server = os.environ.get(\"PYPI_SERVER\", \"default PyPI server\")\n    print(\"Package created as expected and will be pushed to {}\".format(pypi_server))","method_summary":"Assumed called on Travis, to prepare a package to be deployed This method prints on stdout for Travis. Return is obj to pass to sys.exit() directly","original_method_code":"def travis_build_package():\n    \"\"\"Assumed called on Travis, to prepare a package to be deployed\n\n    This method prints on stdout for Travis.\n    Return is obj to pass to sys.exit() directly\n    \"\"\"\n\n    travis_tag = os.environ.get('TRAVIS_TAG')\n    if not travis_tag:\n        print(\"TRAVIS_TAG environment variable is not present\")\n        return \"TRAVIS_TAG environment variable is not present\"\n\n    try:\n        name, version = travis_tag.split(\"_\")\n    except ValueError:\n        print(\"TRAVIS_TAG is not '<package_name>_<version>' (tag is: {})\".format(travis_tag))\n        return \"TRAVIS_TAG is not '<package_name>_<version>' (tag is: {})\".format(travis_tag)\n\n    try:\n        version = Version(version)\n    except InvalidVersion:\n        print(\"Version must be a valid PEP440 version (version is: {})\".format(version))\n        return \"Version must be a valid PEP440 version (version is: {})\".format(version)\n\n    if name.lower() in OMITTED_RELEASE_PACKAGES:\n        print(\"The input package {} has been disabled for release from Travis.CI.\".format(name))\n        return\n\n    abs_dist_path = Path(os.environ['TRAVIS_BUILD_DIR'], 'dist')\n    create_package(name, str(abs_dist_path))\n\n    print(\"Produced:\\n{}\".format(list(abs_dist_path.glob('*'))))\n\n    pattern = \"*{}*\".format(version)\n    packages = list(abs_dist_path.glob(pattern))\n    if not packages:\n        return \"Package version does not match tag {}, abort\".format(version)\n    pypi_server = os.environ.get(\"PYPI_SERVER\", \"default PyPI server\")\n    print(\"Package created as expected and will be pushed to {}\".format(pypi_server))","method_path":"build_package.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.get_regions","method_code":"def get_regions(self):\n        response = self._perform_get(\n            self._get_path('services\/serviceBus\/Regions\/', None),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            _ServiceBusManagementXmlSerializer.xml_to_region)","method_summary":"Get list of available service bus regions.","original_method_code":"def get_regions(self):\n        '''\n        Get list of available service bus regions.\n        '''\n        response = self._perform_get(\n            self._get_path('services\/serviceBus\/Regions\/', None),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            _ServiceBusManagementXmlSerializer.xml_to_region)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.list_namespaces","method_code":"def list_namespaces(self):\n        response = self._perform_get(\n            self._get_path('services\/serviceBus\/Namespaces\/', None),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            _ServiceBusManagementXmlSerializer.xml_to_namespace)","method_summary":"List the service bus namespaces defined on the account.","original_method_code":"def list_namespaces(self):\n        '''\n        List the service bus namespaces defined on the account.\n        '''\n        response = self._perform_get(\n            self._get_path('services\/serviceBus\/Namespaces\/', None),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            _ServiceBusManagementXmlSerializer.xml_to_namespace)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.get_namespace","method_code":"def get_namespace(self, name):\n        response = self._perform_get(\n            self._get_path('services\/serviceBus\/Namespaces', name),\n            None)\n\n        return _ServiceBusManagementXmlSerializer.xml_to_namespace(\n            response.body)","method_summary":"Get details about a specific namespace.","original_method_code":"def get_namespace(self, name):\n        '''\n        Get details about a specific namespace.\n\n        name:\n            Name of the service bus namespace.\n        '''\n        response = self._perform_get(\n            self._get_path('services\/serviceBus\/Namespaces', name),\n            None)\n\n        return _ServiceBusManagementXmlSerializer.xml_to_namespace(\n            response.body)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.create_namespace","method_code":"def create_namespace(self, name, region):\n        _validate_not_none('name', name)\n\n        return self._perform_put(\n            self._get_path('services\/serviceBus\/Namespaces', name),\n            _ServiceBusManagementXmlSerializer.namespace_to_xml(region))","method_summary":"Create a new service bus namespace.","original_method_code":"def create_namespace(self, name, region):\n        '''\n        Create a new service bus namespace.\n\n        name:\n            Name of the service bus namespace to create.\n        region:\n            Region to create the namespace in.\n        '''\n        _validate_not_none('name', name)\n\n        return self._perform_put(\n            self._get_path('services\/serviceBus\/Namespaces', name),\n            _ServiceBusManagementXmlSerializer.namespace_to_xml(region))","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.delete_namespace","method_code":"def delete_namespace(self, name):\n        _validate_not_none('name', name)\n\n        return self._perform_delete(\n            self._get_path('services\/serviceBus\/Namespaces', name),\n            None)","method_summary":"Delete a service bus namespace.","original_method_code":"def delete_namespace(self, name):\n        '''\n        Delete a service bus namespace.\n\n        name:\n            Name of the service bus namespace to delete.\n        '''\n        _validate_not_none('name', name)\n\n        return self._perform_delete(\n            self._get_path('services\/serviceBus\/Namespaces', name),\n            None)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.check_namespace_availability","method_code":"def check_namespace_availability(self, name):\n        _validate_not_none('name', name)\n\n        response = self._perform_get(\n            self._get_path('services\/serviceBus\/CheckNamespaceAvailability',\n                           None) + '\/?namespace=' + _str(name), None)\n\n        return _ServiceBusManagementXmlSerializer.xml_to_namespace_availability(\n            response.body)","method_summary":"Checks to see if the specified service bus namespace is available, or if it has already been taken.","original_method_code":"def check_namespace_availability(self, name):\n        '''\n        Checks to see if the specified service bus namespace is available, or\n        if it has already been taken.\n\n        name:\n            Name of the service bus namespace to validate.\n        '''\n        _validate_not_none('name', name)\n\n        response = self._perform_get(\n            self._get_path('services\/serviceBus\/CheckNamespaceAvailability',\n                           None) + '\/?namespace=' + _str(name), None)\n\n        return _ServiceBusManagementXmlSerializer.xml_to_namespace_availability(\n            response.body)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.list_topics","method_code":"def list_topics(self, name):\n        response = self._perform_get(\n            self._get_list_topics_path(name),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _MinidomXmlToObject.convert_xml_to_azure_object,\n                azure_type=TopicDescription\n            )\n        )","method_summary":"Retrieves the topics in the service namespace.","original_method_code":"def list_topics(self, name):\n        '''\n        Retrieves the topics in the service namespace.\n\n        name:\n            Name of the service bus namespace.\n        '''\n        response = self._perform_get(\n            self._get_list_topics_path(name),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _MinidomXmlToObject.convert_xml_to_azure_object,\n                azure_type=TopicDescription\n            )\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.list_notification_hubs","method_code":"def list_notification_hubs(self, name):\n        response = self._perform_get(\n            self._get_list_notification_hubs_path(name),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _MinidomXmlToObject.convert_xml_to_azure_object,\n                azure_type=NotificationHubDescription\n            )\n        )","method_summary":"Retrieves the notification hubs in the service namespace.","original_method_code":"def list_notification_hubs(self, name):\n        '''\n        Retrieves the notification hubs in the service namespace.\n\n        name:\n            Name of the service bus namespace.\n        '''\n        response = self._perform_get(\n            self._get_list_notification_hubs_path(name),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _MinidomXmlToObject.convert_xml_to_azure_object,\n                azure_type=NotificationHubDescription\n            )\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.list_relays","method_code":"def list_relays(self, name):\n        response = self._perform_get(\n            self._get_list_relays_path(name),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _MinidomXmlToObject.convert_xml_to_azure_object,\n                azure_type=RelayDescription\n            )\n        )","method_summary":"Retrieves the relays in the service namespace.","original_method_code":"def list_relays(self, name):\n        '''\n        Retrieves the relays in the service namespace.\n\n        name:\n            Name of the service bus namespace.\n        '''\n        response = self._perform_get(\n            self._get_list_relays_path(name),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _MinidomXmlToObject.convert_xml_to_azure_object,\n                azure_type=RelayDescription\n            )\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.get_metrics_rollups_queue","method_code":"def get_metrics_rollups_queue(self, name, queue_name, metric):\n        response = self._perform_get(\n            self._get_get_metrics_rollup_queue_path(name, queue_name, metric),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _ServiceBusManagementXmlSerializer.xml_to_metrics,\n                object_type=MetricRollups\n            )\n        )","method_summary":"This operation gets rollup data for Service Bus metrics queue. Rollup data includes the time granularity for the telemetry aggregation as well as the retention settings for each time granularity.","original_method_code":"def get_metrics_rollups_queue(self, name, queue_name, metric):\n        '''\n        This operation gets rollup data for Service Bus metrics queue.\n        Rollup data includes the time granularity for the telemetry aggregation as well as\n        the retention settings for each time granularity.\n\n        name:\n            Name of the service bus namespace.\n        queue_name:\n            Name of the service bus queue in this namespace.\n        metric:\n            name of a supported metric\n        '''\n        response = self._perform_get(\n            self._get_get_metrics_rollup_queue_path(name, queue_name, metric),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _ServiceBusManagementXmlSerializer.xml_to_metrics,\n                object_type=MetricRollups\n            )\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.get_metrics_rollups_topic","method_code":"def get_metrics_rollups_topic(self, name, topic_name, metric):\n        response = self._perform_get(\n            self._get_get_metrics_rollup_topic_path(name, topic_name, metric),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _ServiceBusManagementXmlSerializer.xml_to_metrics,\n                object_type=MetricRollups\n            )\n        )","method_summary":"This operation gets rollup data for Service Bus metrics topic. Rollup data includes the time granularity for the telemetry aggregation as well as the retention settings for each time granularity.","original_method_code":"def get_metrics_rollups_topic(self, name, topic_name, metric):\n        '''\n        This operation gets rollup data for Service Bus metrics topic.\n        Rollup data includes the time granularity for the telemetry aggregation as well as\n        the retention settings for each time granularity.\n\n        name:\n            Name of the service bus namespace.\n        topic_name:\n            Name of the service bus queue in this namespace.\n        metric:\n            name of a supported metric\n        '''\n        response = self._perform_get(\n            self._get_get_metrics_rollup_topic_path(name, topic_name, metric),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _ServiceBusManagementXmlSerializer.xml_to_metrics,\n                object_type=MetricRollups\n            )\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusManagementService.get_metrics_rollups_relay","method_code":"def get_metrics_rollups_relay(self, name, relay_name, metric):\n        response = self._perform_get(\n            self._get_get_metrics_rollup_relay_path(name, relay_name, metric),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _ServiceBusManagementXmlSerializer.xml_to_metrics,\n                object_type=MetricRollups\n            )\n        )","method_summary":"This operation gets rollup data for Service Bus metrics relay. Rollup data includes the time granularity for the telemetry aggregation as well as the retention settings for each time granularity.","original_method_code":"def get_metrics_rollups_relay(self, name, relay_name, metric):\n        '''\n        This operation gets rollup data for Service Bus metrics relay.\n        Rollup data includes the time granularity for the telemetry aggregation as well as\n        the retention settings for each time granularity.\n\n        name:\n            Name of the service bus namespace.\n        relay_name:\n            Name of the service bus relay in this namespace.\n        metric:\n            name of a supported metric\n        '''\n        response = self._perform_get(\n            self._get_get_metrics_rollup_relay_path(name, relay_name, metric),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _ServiceBusManagementXmlSerializer.xml_to_metrics,\n                object_type=MetricRollups\n            )\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/servicebusmanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"create","method_code":"def create(env_dir, system_site_packages=False, clear=False,\n                    symlinks=False, with_pip=False, prompt=None):\n    builder = ExtendedEnvBuilder(system_site_packages=system_site_packages,\n                                 clear=clear, symlinks=symlinks, with_pip=with_pip,\n                                 prompt=prompt)\n    builder.create(env_dir)\n    return builder.context","method_summary":"Create a virtual environment in a directory.","original_method_code":"def create(env_dir, system_site_packages=False, clear=False,\n                    symlinks=False, with_pip=False, prompt=None):\n    \"\"\"Create a virtual environment in a directory.\"\"\"\n    builder = ExtendedEnvBuilder(system_site_packages=system_site_packages,\n                                 clear=clear, symlinks=symlinks, with_pip=with_pip,\n                                 prompt=prompt)\n    builder.create(env_dir)\n    return builder.context","method_path":"azure-sdk-tools\/packaging_tools\/venvtools.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"create_venv_with_package","method_code":"def create_venv_with_package(packages):\n    with tempfile.TemporaryDirectory() as tempdir:\n        myenv = create(tempdir, with_pip=True)\n        pip_call = [\n            myenv.env_exe,\n            \"-m\",\n            \"pip\",\n            \"install\",\n        ]\n        subprocess.check_call(pip_call + ['-U', 'pip'])\n        if packages:\n            subprocess.check_call(pip_call + packages)\n        yield myenv","method_summary":"Create a venv with these packages in a temp dir and yielf the env. packages should be an iterable of pip version instructio (e.g. package~=1.2.3)","original_method_code":"def create_venv_with_package(packages):\n    \"\"\"Create a venv with these packages in a temp dir and yielf the env.\n\n    packages should be an iterable of pip version instructio (e.g. package~=1.2.3)\n    \"\"\"\n    with tempfile.TemporaryDirectory() as tempdir:\n        myenv = create(tempdir, with_pip=True)\n        pip_call = [\n            myenv.env_exe,\n            \"-m\",\n            \"pip\",\n            \"install\",\n        ]\n        subprocess.check_call(pip_call + ['-U', 'pip'])\n        if packages:\n            subprocess.check_call(pip_call + packages)\n        yield myenv","method_path":"azure-sdk-tools\/packaging_tools\/venvtools.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.create_server","method_code":"def create_server(self, admin_login, admin_password, location):\n        _validate_not_none('admin_login', admin_login)\n        _validate_not_none('admin_password', admin_password)\n        _validate_not_none('location', location)\n        response = self.perform_post(\n            self._get_servers_path(),\n            _SqlManagementXmlSerializer.create_server_to_xml(\n                admin_login,\n                admin_password,\n                location\n            )\n        )\n\n        return _SqlManagementXmlSerializer.xml_to_create_server_response(\n            response.body)","method_summary":"Create a new Azure SQL Database server.","original_method_code":"def create_server(self, admin_login, admin_password, location):\n        '''\n        Create a new Azure SQL Database server.\n\n        admin_login:\n            The administrator login name for the new server.\n        admin_password:\n            The administrator login password for the new server.\n        location:\n            The region to deploy the new server.\n        '''\n        _validate_not_none('admin_login', admin_login)\n        _validate_not_none('admin_password', admin_password)\n        _validate_not_none('location', location)\n        response = self.perform_post(\n            self._get_servers_path(),\n            _SqlManagementXmlSerializer.create_server_to_xml(\n                admin_login,\n                admin_password,\n                location\n            )\n        )\n\n        return _SqlManagementXmlSerializer.xml_to_create_server_response(\n            response.body)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.set_server_admin_password","method_code":"def set_server_admin_password(self, server_name, admin_password):\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('admin_password', admin_password)\n        return self._perform_post(\n            self._get_servers_path(server_name) + '?op=ResetPassword',\n            _SqlManagementXmlSerializer.set_server_admin_password_to_xml(\n                admin_password\n            )\n        )","method_summary":"Reset the administrator password for a server.","original_method_code":"def set_server_admin_password(self, server_name, admin_password):\n        '''\n        Reset the administrator password for a server.\n\n        server_name:\n            Name of the server to change the password.\n        admin_password:\n            The new administrator password for the server.\n        '''\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('admin_password', admin_password)\n        return self._perform_post(\n            self._get_servers_path(server_name) + '?op=ResetPassword',\n            _SqlManagementXmlSerializer.set_server_admin_password_to_xml(\n                admin_password\n            )\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.list_quotas","method_code":"def list_quotas(self, server_name):\n        _validate_not_none('server_name', server_name)\n        response = self._perform_get(self._get_quotas_path(server_name),\n                                     None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, ServerQuota)","method_summary":"Gets quotas for an Azure SQL Database Server.","original_method_code":"def list_quotas(self, server_name):\n        '''\n        Gets quotas for an Azure SQL Database Server.\n\n        server_name:\n            Name of the server.\n        '''\n        _validate_not_none('server_name', server_name)\n        response = self._perform_get(self._get_quotas_path(server_name),\n                                     None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, ServerQuota)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.get_server_event_logs","method_code":"def get_server_event_logs(self, server_name, start_date,\n                              interval_size_in_minutes, event_types=''):\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('start_date', start_date)\n        _validate_not_none('interval_size_in_minutes', interval_size_in_minutes)\n        _validate_not_none('event_types', event_types)\n        path = self._get_server_event_logs_path(server_name) + \\\n               '?startDate={0}&intervalSizeInMinutes={1}&eventTypes={2}'.format(\n            start_date, interval_size_in_minutes, event_types)\n        response = self._perform_get(path, None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, EventLog)","method_summary":"Gets the event logs for an Azure SQL Database Server.","original_method_code":"def get_server_event_logs(self, server_name, start_date,\n                              interval_size_in_minutes, event_types=''):\n        '''\n        Gets the event logs for an Azure SQL Database Server.\n\n        server_name:\n            Name of the server to retrieve the event logs from.\n        start_date:\n            The starting date and time of the events to retrieve in UTC format,\n            for example '2011-09-28 16:05:00'.\n        interval_size_in_minutes:\n            Size of the event logs to retrieve (in minutes).\n            Valid values are: 5, 60, or 1440.\n        event_types:\n            The event type of the log entries you want to retrieve.\n            Valid values are: \n                - connection_successful\n                - connection_failed\n                - connection_terminated\n                - deadlock\n                - throttling\n                - throttling_long_transaction\n            To return all event types pass in an empty string.\n        '''\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('start_date', start_date)\n        _validate_not_none('interval_size_in_minutes', interval_size_in_minutes)\n        _validate_not_none('event_types', event_types)\n        path = self._get_server_event_logs_path(server_name) + \\\n               '?startDate={0}&intervalSizeInMinutes={1}&eventTypes={2}'.format(\n            start_date, interval_size_in_minutes, event_types)\n        response = self._perform_get(path, None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, EventLog)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.create_firewall_rule","method_code":"def create_firewall_rule(self, server_name, name, start_ip_address,\n                             end_ip_address):\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('name', name)\n        _validate_not_none('start_ip_address', start_ip_address)\n        _validate_not_none('end_ip_address', end_ip_address)\n        return self._perform_post(\n            self._get_firewall_rules_path(server_name),\n            _SqlManagementXmlSerializer.create_firewall_rule_to_xml(\n                name, start_ip_address, end_ip_address\n            )\n        )","method_summary":"Creates an Azure SQL Database server firewall rule.","original_method_code":"def create_firewall_rule(self, server_name, name, start_ip_address,\n                             end_ip_address):\n        '''\n        Creates an Azure SQL Database server firewall rule.\n\n        server_name:\n            Name of the server to set the firewall rule on. \n        name:\n            The name of the new firewall rule.\n        start_ip_address:\n            The lowest IP address in the range of the server-level firewall\n            setting. IP addresses equal to or greater than this can attempt to\n            connect to the server. The lowest possible IP address is 0.0.0.0.\n        end_ip_address:\n            The highest IP address in the range of the server-level firewall\n            setting. IP addresses equal to or less than this can attempt to\n            connect to the server. The highest possible IP address is\n            255.255.255.255.\n        '''\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('name', name)\n        _validate_not_none('start_ip_address', start_ip_address)\n        _validate_not_none('end_ip_address', end_ip_address)\n        return self._perform_post(\n            self._get_firewall_rules_path(server_name),\n            _SqlManagementXmlSerializer.create_firewall_rule_to_xml(\n                name, start_ip_address, end_ip_address\n            )\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.update_firewall_rule","method_code":"def update_firewall_rule(self, server_name, name, start_ip_address,\n                             end_ip_address):\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('name', name)\n        _validate_not_none('start_ip_address', start_ip_address)\n        _validate_not_none('end_ip_address', end_ip_address)\n        return self._perform_put(\n            self._get_firewall_rules_path(server_name, name),\n            _SqlManagementXmlSerializer.update_firewall_rule_to_xml(\n                name, start_ip_address, end_ip_address\n            )\n        )","method_summary":"Update a firewall rule for an Azure SQL Database server.","original_method_code":"def update_firewall_rule(self, server_name, name, start_ip_address,\n                             end_ip_address):\n        '''\n        Update a firewall rule for an Azure SQL Database server.\n\n        server_name:\n            Name of the server to set the firewall rule on. \n        name:\n            The name of the firewall rule to update.\n        start_ip_address:\n            The lowest IP address in the range of the server-level firewall\n            setting. IP addresses equal to or greater than this can attempt to\n            connect to the server. The lowest possible IP address is 0.0.0.0.\n        end_ip_address:\n            The highest IP address in the range of the server-level firewall\n            setting. IP addresses equal to or less than this can attempt to\n            connect to the server. The highest possible IP address is\n            255.255.255.255.\n        '''\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('name', name)\n        _validate_not_none('start_ip_address', start_ip_address)\n        _validate_not_none('end_ip_address', end_ip_address)\n        return self._perform_put(\n            self._get_firewall_rules_path(server_name, name),\n            _SqlManagementXmlSerializer.update_firewall_rule_to_xml(\n                name, start_ip_address, end_ip_address\n            )\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.delete_firewall_rule","method_code":"def delete_firewall_rule(self, server_name, name):\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('name', name)\n        return self._perform_delete(\n            self._get_firewall_rules_path(server_name, name))","method_summary":"Deletes an Azure SQL Database server firewall rule.","original_method_code":"def delete_firewall_rule(self, server_name, name):\n        '''\n        Deletes an Azure SQL Database server firewall rule.\n\n        server_name:\n            Name of the server with the firewall rule you want to delete.\n        name:\n            Name of the firewall rule you want to delete.\n        '''\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('name', name)\n        return self._perform_delete(\n            self._get_firewall_rules_path(server_name, name))","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.list_firewall_rules","method_code":"def list_firewall_rules(self, server_name):\n        _validate_not_none('server_name', server_name)\n        response = self._perform_get(self._get_firewall_rules_path(server_name),\n                                     None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, FirewallRule)","method_summary":"Retrieves the set of firewall rules for an Azure SQL Database Server.","original_method_code":"def list_firewall_rules(self, server_name):\n        '''\n        Retrieves the set of firewall rules for an Azure SQL Database Server.\n\n        server_name:\n            Name of the server.\n        '''\n        _validate_not_none('server_name', server_name)\n        response = self._perform_get(self._get_firewall_rules_path(server_name),\n                                     None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, FirewallRule)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.list_service_level_objectives","method_code":"def list_service_level_objectives(self, server_name):\n        _validate_not_none('server_name', server_name)\n        response = self._perform_get(\n            self._get_service_objectives_path(server_name), None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, ServiceObjective)","method_summary":"Gets the service level objectives for an Azure SQL Database server.","original_method_code":"def list_service_level_objectives(self, server_name):\n        '''\n        Gets the service level objectives for an Azure SQL Database server.\n\n        server_name:\n            Name of the server.\n        '''\n        _validate_not_none('server_name', server_name)\n        response = self._perform_get(\n            self._get_service_objectives_path(server_name), None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, ServiceObjective)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.create_database","method_code":"def create_database(self, server_name, name, service_objective_id,\n                        edition=None, collation_name=None,\n                        max_size_bytes=None):\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('name', name)\n        _validate_not_none('service_objective_id', service_objective_id)\n        return self._perform_post(\n            self._get_databases_path(server_name),\n            _SqlManagementXmlSerializer.create_database_to_xml(\n                name, service_objective_id, edition, collation_name,\n                max_size_bytes\n            )\n        )","method_summary":"Creates a new Azure SQL Database.","original_method_code":"def create_database(self, server_name, name, service_objective_id,\n                        edition=None, collation_name=None,\n                        max_size_bytes=None):\n        '''\n        Creates a new Azure SQL Database.\n\n        server_name:\n            Name of the server to contain the new database.\n        name:\n            Required. The name for the new database. See Naming Requirements\n            in Azure SQL Database General Guidelines and Limitations and\n            Database Identifiers for more information.\n        service_objective_id:\n            Required. The GUID corresponding to the performance level for\n            Edition. See List Service Level Objectives for current values.\n        edition:\n            Optional. The Service Tier (Edition) for the new database. If\n            omitted, the default is Web. Valid values are Web, Business,\n            Basic, Standard, and Premium. See Azure SQL Database Service Tiers\n            (Editions) and Web and Business Edition Sunset FAQ for more\n            information.\n        collation_name:\n            Optional. The database collation. This can be any collation\n            supported by SQL. If omitted, the default collation is used. See\n            SQL Server Collation Support in Azure SQL Database General\n            Guidelines and Limitations for more information.\n        max_size_bytes:\n            Optional. Sets the maximum size, in bytes, for the database. This\n            value must be within the range of allowed values for Edition. If\n            omitted, the default value for the edition is used. See Azure SQL\n            Database Service Tiers (Editions) for current maximum databases\n            sizes. Convert MB or GB values to bytes.\n            1 MB = 1048576 bytes. 1 GB = 1073741824 bytes.\n        '''\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('name', name)\n        _validate_not_none('service_objective_id', service_objective_id)\n        return self._perform_post(\n            self._get_databases_path(server_name),\n            _SqlManagementXmlSerializer.create_database_to_xml(\n                name, service_objective_id, edition, collation_name,\n                max_size_bytes\n            )\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.update_database","method_code":"def update_database(self, server_name, name, new_database_name=None,\n                        service_objective_id=None, edition=None,\n                        max_size_bytes=None):\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('name', name)\n        return self._perform_put(\n            self._get_databases_path(server_name, name),\n            _SqlManagementXmlSerializer.update_database_to_xml(\n                new_database_name, service_objective_id, edition,\n                max_size_bytes\n            )\n        )","method_summary":"Updates existing database details.","original_method_code":"def update_database(self, server_name, name, new_database_name=None,\n                        service_objective_id=None, edition=None,\n                        max_size_bytes=None):\n        '''\n        Updates existing database details.\n\n        server_name:\n            Name of the server to contain the new database.\n        name:\n            Required. The name for the new database. See Naming Requirements\n            in Azure SQL Database General Guidelines and Limitations and\n            Database Identifiers for more information.\n        new_database_name:\n            Optional. The new name for the new database.\n        service_objective_id:\n            Optional. The new service level to apply to the database. For more\n            information about service levels, see Azure SQL Database Service\n            Tiers and Performance Levels. Use List Service Level Objectives to\n            get the correct ID for the desired service objective.\n        edition:\n            Optional. The new edition for the new database.\n        max_size_bytes:\n            Optional. The new size of the database in bytes. For information on\n            available sizes for each edition, see Azure SQL Database Service\n            Tiers (Editions).\n        '''\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('name', name)\n        return self._perform_put(\n            self._get_databases_path(server_name, name),\n            _SqlManagementXmlSerializer.update_database_to_xml(\n                new_database_name, service_objective_id, edition,\n                max_size_bytes\n            )\n        )","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.delete_database","method_code":"def delete_database(self, server_name, name):\n        return self._perform_delete(self._get_databases_path(server_name, name))","method_summary":"Deletes an Azure SQL Database.","original_method_code":"def delete_database(self, server_name, name):\n        '''\n        Deletes an Azure SQL Database.\n\n        server_name:\n            Name of the server where the database is located.\n        name:\n            Name of the database to delete.\n        '''\n        return self._perform_delete(self._get_databases_path(server_name, name))","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SqlDatabaseManagementService.list_databases","method_code":"def list_databases(self, name):\n        response = self._perform_get(self._get_list_databases_path(name),\n                                     None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, Database)","method_summary":"List the SQL databases defined on the specified server name","original_method_code":"def list_databases(self, name):\n        '''\n        List the SQL databases defined on the specified server name\n        '''\n        response = self._perform_get(self._get_list_databases_path(name),\n                                     None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, Database)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/sqldatabasemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"TopLevelDomainsOperations.list_agreements","method_code":"def list_agreements(\n            self, name, include_privacy=None, for_transfer=None, custom_headers=None, raw=False, **operation_config):\n        agreement_option = models.TopLevelDomainAgreementOption(include_privacy=include_privacy, for_transfer=for_transfer)\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                \n                url = self.list_agreements.metadata['url']\n                path_format_arguments = {\n                    'name': self._serialize.url(\"name\", name, 'str'),\n                    'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                \n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            \n            header_parameters = {}\n            header_parameters['Accept'] = 'application\/json'\n            header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            \n            body_content = self._serialize.body(agreement_option, 'TopLevelDomainAgreementOption')\n\n            \n            request = self._client.post(url, query_parameters, header_parameters, body_content)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.DefaultErrorResponseException(self._deserialize, response)\n\n            return response\n\n        \n        deserialized = models.TldLegalAgreementPaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.TldLegalAgreementPaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized","method_summary":"Gets all legal agreements that user needs to accept before purchasing a domain. Gets all legal agreements that user needs to accept before purchasing a domain.","original_method_code":"def list_agreements(\n            self, name, include_privacy=None, for_transfer=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Gets all legal agreements that user needs to accept before purchasing a\n        domain.\n\n        Gets all legal agreements that user needs to accept before purchasing a\n        domain.\n\n        :param name: Name of the top-level domain.\n        :type name: str\n        :param include_privacy: If <code>true<\/code>, then the list of\n         agreements will include agreements for domain privacy as well;\n         otherwise, <code>false<\/code>.\n        :type include_privacy: bool\n        :param for_transfer: If <code>true<\/code>, then the list of agreements\n         will include agreements for domain transfer as well; otherwise,\n         <code>false<\/code>.\n        :type for_transfer: bool\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: An iterator like instance of TldLegalAgreement\n        :rtype:\n         ~azure.mgmt.web.models.TldLegalAgreementPaged[~azure.mgmt.web.models.TldLegalAgreement]\n        :raises:\n         :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`\n        \"\"\"\n        agreement_option = models.TopLevelDomainAgreementOption(include_privacy=include_privacy, for_transfer=for_transfer)\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                # Construct URL\n                url = self.list_agreements.metadata['url']\n                path_format_arguments = {\n                    'name': self._serialize.url(\"name\", name, 'str'),\n                    'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                # Construct parameters\n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            # Construct headers\n            header_parameters = {}\n            header_parameters['Accept'] = 'application\/json'\n            header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            # Construct body\n            body_content = self._serialize.body(agreement_option, 'TopLevelDomainAgreementOption')\n\n            # Construct and send request\n            request = self._client.post(url, query_parameters, header_parameters, body_content)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.DefaultErrorResponseException(self._deserialize, response)\n\n            return response\n\n        # Deserialize response\n        deserialized = models.TldLegalAgreementPaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.TldLegalAgreementPaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-mgmt-web\/azure\/mgmt\/web\/operations\/top_level_domains_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SessionReceiver.get_session_state","method_code":"async def get_session_state(self):\n        await self._can_run()\n        response = await self._mgmt_request_response(\n            REQUEST_RESPONSE_GET_SESSION_STATE_OPERATION,\n            {'session-id': self.session_id},\n            mgmt_handlers.default)\n        session_state = response.get(b'session-state')\n        if isinstance(session_state, six.binary_type):\n            session_state = session_state.decode('UTF-8')\n        return session_state","method_summary":"Get the session state.","original_method_code":"async def get_session_state(self):\n        \"\"\"Get the session state.\n\n        Returns None if no state has been set.\n\n        :rtype: str\n\n        Example:\n            .. literalinclude:: ..\/examples\/async_examples\/test_examples_async.py\n                :start-after: [START set_session_state]\n                :end-before: [END set_session_state]\n                :language: python\n                :dedent: 4\n                :caption: Getting and setting the state of a session.\n\n        \"\"\"\n        await self._can_run()\n        response = await self._mgmt_request_response(\n            REQUEST_RESPONSE_GET_SESSION_STATE_OPERATION,\n            {'session-id': self.session_id},\n            mgmt_handlers.default)\n        session_state = response.get(b'session-state')\n        if isinstance(session_state, six.binary_type):\n            session_state = session_state.decode('UTF-8')\n        return session_state","method_path":"azure-servicebus\/azure\/servicebus\/aio\/async_receive_handler.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SessionReceiver.set_session_state","method_code":"async def set_session_state(self, state):\n        await self._can_run()\n        state = state.encode(self.encoding) if isinstance(state, six.text_type) else state\n        return await self._mgmt_request_response(\n            REQUEST_RESPONSE_SET_SESSION_STATE_OPERATION,\n            {'session-id': self.session_id, 'session-state': bytearray(state)},\n            mgmt_handlers.default)","method_summary":"Set the session state.","original_method_code":"async def set_session_state(self, state):\n        \"\"\"Set the session state.\n\n        :param state: The state value.\n        :type state: str or bytes or bytearray\n\n        Example:\n            .. literalinclude:: ..\/examples\/async_examples\/test_examples_async.py\n                :start-after: [START set_session_state]\n                :end-before: [END set_session_state]\n                :language: python\n                :dedent: 4\n                :caption: Getting and setting the state of a session.\n\n        \"\"\"\n        await self._can_run()\n        state = state.encode(self.encoding) if isinstance(state, six.text_type) else state\n        return await self._mgmt_request_response(\n            REQUEST_RESPONSE_SET_SESSION_STATE_OPERATION,\n            {'session-id': self.session_id, 'session-state': bytearray(state)},\n            mgmt_handlers.default)","method_path":"azure-servicebus\/azure\/servicebus\/aio\/async_receive_handler.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ReservationOperations.merge","method_code":"def merge(\n            self, reservation_order_id, sources=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._merge_initial(\n            reservation_order_id=reservation_order_id,\n            sources=sources,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('[ReservationResponse]', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Merges two `Reservation`s. Merge the specified `Reservation`s into a new `Reservation`. The two `Reservation`s being merged must have same properties.","original_method_code":"def merge(\n            self, reservation_order_id, sources=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Merges two `Reservation`s.\n\n        Merge the specified `Reservation`s into a new `Reservation`. The two\n        `Reservation`s being merged must have same properties.\n\n        :param reservation_order_id: Order Id of the reservation\n        :type reservation_order_id: str\n        :param sources: Format of the resource id should be\n         \/providers\/Microsoft.Capacity\/reservationOrders\/{reservationOrderId}\/reservations\/{reservationId}\n        :type sources: list[str]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns list or\n         ClientRawResponse<list> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[list[~azure.mgmt.reservations.models.ReservationResponse]]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[list[~azure.mgmt.reservations.models.ReservationResponse]]]\n        :raises:\n         :class:`ErrorException<azure.mgmt.reservations.models.ErrorException>`\n        \"\"\"\n        raw_result = self._merge_initial(\n            reservation_order_id=reservation_order_id,\n            sources=sources,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('[ReservationResponse]', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-reservations\/azure\/mgmt\/reservations\/operations\/reservation_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"HttpBearerChallenge._validate_challenge","method_code":"def _validate_challenge(self, challenge):\n        bearer_string = 'Bearer '\n        if not challenge:\n            raise ValueError('Challenge cannot be empty')\n\n        challenge = challenge.strip()\n        if not challenge.startswith(bearer_string):\n            raise ValueError('Challenge is not Bearer')\n\n        return challenge[len(bearer_string):]","method_summary":"Verifies that the challenge is a Bearer challenge and returns the key=value pairs.","original_method_code":"def _validate_challenge(self, challenge):\n        \"\"\" Verifies that the challenge is a Bearer challenge and returns the key=value pairs. \"\"\"\n        bearer_string = 'Bearer '\n        if not challenge:\n            raise ValueError('Challenge cannot be empty')\n\n        challenge = challenge.strip()\n        if not challenge.startswith(bearer_string):\n            raise ValueError('Challenge is not Bearer')\n\n        return challenge[len(bearer_string):]","method_path":"azure-keyvault\/azure\/keyvault\/http_bearer_challenge.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WorkspacesOperations.purge","method_code":"def purge(\n            self, resource_group_name, workspace_name, table, filters, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._purge_initial(\n            resource_group_name=resource_group_name,\n            workspace_name=workspace_name,\n            table=table,\n            filters=filters,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('object', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Purges data in an Log Analytics workspace by a set of user-defined filters.","original_method_code":"def purge(\n            self, resource_group_name, workspace_name, table, filters, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Purges data in an Log Analytics workspace by a set of user-defined\n        filters.\n\n        :param resource_group_name: The name of the resource group to get. The\n         name is case insensitive.\n        :type resource_group_name: str\n        :param workspace_name: Log Analytics workspace name\n        :type workspace_name: str\n        :param table: Table from which to purge data.\n        :type table: str\n        :param filters: The set of columns and filters (queries) to run over\n         them to purge the resulting data.\n        :type filters:\n         list[~azure.mgmt.loganalytics.models.WorkspacePurgeBodyFilters]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns object or\n         ClientRawResponse<object> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[object] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[object]]\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._purge_initial(\n            resource_group_name=resource_group_name,\n            workspace_name=workspace_name,\n            table=table,\n            filters=filters,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('object', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-loganalytics\/azure\/mgmt\/loganalytics\/operations\/workspaces_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_error_handler","method_code":"def _error_handler(error):\n    if error.condition == b'com.microsoft:server-busy':\n        return errors.ErrorAction(retry=True, backoff=4)\n    if error.condition == b'com.microsoft:timeout':\n        return errors.ErrorAction(retry=True, backoff=2)\n    if error.condition == b'com.microsoft:operation-cancelled':\n        return errors.ErrorAction(retry=True)\n    if error.condition == b\"com.microsoft:container-close\":\n        return errors.ErrorAction(retry=True, backoff=4)\n    if error.condition in _NO_RETRY_ERRORS:\n        return errors.ErrorAction(retry=False)\n    return errors.ErrorAction(retry=True)","method_summary":"Handle connection and service errors. Called internally when an event has failed to send so we can parse the error to determine whether we should attempt to retry sending the event again.","original_method_code":"def _error_handler(error):\n    \"\"\"Handle connection and service errors.\n\n    Called internally when an event has failed to send so we\n    can parse the error to determine whether we should attempt\n    to retry sending the event again.\n    Returns the action to take according to error type.\n\n    :param error: The error received in the send attempt.\n    :type error: Exception\n    :rtype: ~uamqp.errors.ErrorAction\n    \"\"\"\n    if error.condition == b'com.microsoft:server-busy':\n        return errors.ErrorAction(retry=True, backoff=4)\n    if error.condition == b'com.microsoft:timeout':\n        return errors.ErrorAction(retry=True, backoff=2)\n    if error.condition == b'com.microsoft:operation-cancelled':\n        return errors.ErrorAction(retry=True)\n    if error.condition == b\"com.microsoft:container-close\":\n        return errors.ErrorAction(retry=True, backoff=4)\n    if error.condition in _NO_RETRY_ERRORS:\n        return errors.ErrorAction(retry=False)\n    return errors.ErrorAction(retry=True)","method_path":"azure-servicebus\/azure\/servicebus\/common\/errors.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.create_queue","method_code":"def create_queue(self, queue_name, queue=None, fail_on_exist=False):\n        _validate_not_none('queue_name', queue_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + _str(queue_name) + ''\n        request.body = _get_request_body(_convert_queue_to_xml(queue))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        if not fail_on_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_on_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_summary":"Creates a new queue. Once created, this queue's resource manifest is immutable.","original_method_code":"def create_queue(self, queue_name, queue=None, fail_on_exist=False):\n        '''\n        Creates a new queue. Once created, this queue's resource manifest is\n        immutable.\n\n        queue_name:\n            Name of the queue to create.\n        queue:\n            Queue object to create.\n        fail_on_exist:\n            Specify whether to throw an exception when the queue exists.\n        '''\n        _validate_not_none('queue_name', queue_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + _str(queue_name) + ''\n        request.body = _get_request_body(_convert_queue_to_xml(queue))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        if not fail_on_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_on_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.delete_queue","method_code":"def delete_queue(self, queue_name, fail_not_exist=False):\n        _validate_not_none('queue_name', queue_name)\n        request = HTTPRequest()\n        request.method = 'DELETE'\n        request.host = self._get_host()\n        request.path = '\/' + _str(queue_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        if not fail_not_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_not_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_summary":"Deletes an existing queue. This operation will also remove all associated state including messages in the queue.","original_method_code":"def delete_queue(self, queue_name, fail_not_exist=False):\n        '''\n        Deletes an existing queue. This operation will also remove all\n        associated state including messages in the queue.\n\n        queue_name:\n            Name of the queue to delete.\n        fail_not_exist:\n            Specify whether to throw an exception if the queue doesn't exist.\n        '''\n        _validate_not_none('queue_name', queue_name)\n        request = HTTPRequest()\n        request.method = 'DELETE'\n        request.host = self._get_host()\n        request.path = '\/' + _str(queue_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        if not fail_not_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_not_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.get_queue","method_code":"def get_queue(self, queue_name):\n        _validate_not_none('queue_name', queue_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + _str(queue_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_queue(response)","method_summary":"Retrieves an existing queue.","original_method_code":"def get_queue(self, queue_name):\n        '''\n        Retrieves an existing queue.\n\n        queue_name:\n            Name of the queue.\n        '''\n        _validate_not_none('queue_name', queue_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + _str(queue_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_queue(response)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.create_topic","method_code":"def create_topic(self, topic_name, topic=None, fail_on_exist=False):\n        _validate_not_none('topic_name', topic_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + _str(topic_name) + ''\n        request.body = _get_request_body(_convert_topic_to_xml(topic))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        if not fail_on_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_on_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_summary":"Creates a new topic. Once created, this topic resource manifest is immutable.","original_method_code":"def create_topic(self, topic_name, topic=None, fail_on_exist=False):\n        '''\n        Creates a new topic. Once created, this topic resource manifest is\n        immutable.\n\n        topic_name:\n            Name of the topic to create.\n        topic:\n            Topic object to create.\n        fail_on_exist:\n            Specify whether to throw an exception when the topic exists.\n        '''\n        _validate_not_none('topic_name', topic_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + _str(topic_name) + ''\n        request.body = _get_request_body(_convert_topic_to_xml(topic))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        if not fail_on_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_on_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.get_topic","method_code":"def get_topic(self, topic_name):\n        _validate_not_none('topic_name', topic_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + _str(topic_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_topic(response)","method_summary":"Retrieves the description for the specified topic.","original_method_code":"def get_topic(self, topic_name):\n        '''\n        Retrieves the description for the specified topic.\n\n        topic_name:\n            Name of the topic.\n        '''\n        _validate_not_none('topic_name', topic_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + _str(topic_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_topic(response)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.create_rule","method_code":"def create_rule(self, topic_name, subscription_name, rule_name, rule=None,\n                    fail_on_exist=False):\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        _validate_not_none('rule_name', rule_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + _str(topic_name) + '\/subscriptions\/' + \\\n            _str(subscription_name) + \\\n            '\/rules\/' + _str(rule_name) + ''\n        request.body = _get_request_body(_convert_rule_to_xml(rule))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        if not fail_on_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_on_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_summary":"Creates a new rule. Once created, this rule's resource manifest is immutable.","original_method_code":"def create_rule(self, topic_name, subscription_name, rule_name, rule=None,\n                    fail_on_exist=False):\n        '''\n        Creates a new rule. Once created, this rule's resource manifest is\n        immutable.\n\n        topic_name:\n            Name of the topic.\n        subscription_name:\n            Name of the subscription.\n        rule_name:\n            Name of the rule.\n        fail_on_exist:\n            Specify whether to throw an exception when the rule exists.\n        '''\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        _validate_not_none('rule_name', rule_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + _str(topic_name) + '\/subscriptions\/' + \\\n            _str(subscription_name) + \\\n            '\/rules\/' + _str(rule_name) + ''\n        request.body = _get_request_body(_convert_rule_to_xml(rule))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        if not fail_on_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_on_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.get_rule","method_code":"def get_rule(self, topic_name, subscription_name, rule_name):\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        _validate_not_none('rule_name', rule_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + _str(topic_name) + '\/subscriptions\/' + \\\n            _str(subscription_name) + \\\n            '\/rules\/' + _str(rule_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_rule(response)","method_summary":"Retrieves the description for the specified rule.","original_method_code":"def get_rule(self, topic_name, subscription_name, rule_name):\n        '''\n        Retrieves the description for the specified rule.\n\n        topic_name:\n            Name of the topic.\n        subscription_name:\n            Name of the subscription.\n        rule_name:\n            Name of the rule.\n        '''\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        _validate_not_none('rule_name', rule_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + _str(topic_name) + '\/subscriptions\/' + \\\n            _str(subscription_name) + \\\n            '\/rules\/' + _str(rule_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_rule(response)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.list_rules","method_code":"def list_rules(self, topic_name, subscription_name):\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + \\\n            _str(topic_name) + '\/subscriptions\/' + \\\n            _str(subscription_name) + '\/rules\/'\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _ETreeXmlToObject.convert_response_to_feeds(\n            response, _convert_etree_element_to_rule)","method_summary":"Retrieves the rules that exist under the specified subscription.","original_method_code":"def list_rules(self, topic_name, subscription_name):\n        '''\n        Retrieves the rules that exist under the specified subscription.\n\n        topic_name:\n            Name of the topic.\n        subscription_name:\n            Name of the subscription.\n        '''\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + \\\n            _str(topic_name) + '\/subscriptions\/' + \\\n            _str(subscription_name) + '\/rules\/'\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _ETreeXmlToObject.convert_response_to_feeds(\n            response, _convert_etree_element_to_rule)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.create_subscription","method_code":"def create_subscription(self, topic_name, subscription_name,\n                            subscription=None, fail_on_exist=False):\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + \\\n            _str(topic_name) + '\/subscriptions\/' + _str(subscription_name) + ''\n        request.body = _get_request_body(\n            _convert_subscription_to_xml(subscription))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        if not fail_on_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_on_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_summary":"Creates a new subscription. Once created, this subscription resource manifest is immutable.","original_method_code":"def create_subscription(self, topic_name, subscription_name,\n                            subscription=None, fail_on_exist=False):\n        '''\n        Creates a new subscription. Once created, this subscription resource\n        manifest is immutable.\n\n        topic_name:\n            Name of the topic.\n        subscription_name:\n            Name of the subscription.\n        fail_on_exist:\n            Specify whether throw exception when subscription exists.\n        '''\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + \\\n            _str(topic_name) + '\/subscriptions\/' + _str(subscription_name) + ''\n        request.body = _get_request_body(\n            _convert_subscription_to_xml(subscription))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        if not fail_on_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_on_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.get_subscription","method_code":"def get_subscription(self, topic_name, subscription_name):\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + \\\n            _str(topic_name) + '\/subscriptions\/' + _str(subscription_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_subscription(response)","method_summary":"Gets an existing subscription.","original_method_code":"def get_subscription(self, topic_name, subscription_name):\n        '''\n        Gets an existing subscription.\n\n        topic_name:\n            Name of the topic.\n        subscription_name:\n            Name of the subscription.\n        '''\n        _validate_not_none('topic_name', topic_name)\n        _validate_not_none('subscription_name', subscription_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + \\\n            _str(topic_name) + '\/subscriptions\/' + _str(subscription_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_subscription(response)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.list_subscriptions","method_code":"def list_subscriptions(self, topic_name):\n        _validate_not_none('topic_name', topic_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + _str(topic_name) + '\/subscriptions\/'\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _ETreeXmlToObject.convert_response_to_feeds(\n            response, _convert_etree_element_to_subscription)","method_summary":"Retrieves the subscriptions in the specified topic.","original_method_code":"def list_subscriptions(self, topic_name):\n        '''\n        Retrieves the subscriptions in the specified topic.\n\n        topic_name:\n            Name of the topic.\n        '''\n        _validate_not_none('topic_name', topic_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + _str(topic_name) + '\/subscriptions\/'\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _ETreeXmlToObject.convert_response_to_feeds(\n            response, _convert_etree_element_to_subscription)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.receive_queue_message","method_code":"def receive_queue_message(self, queue_name, peek_lock=True, timeout=60):\n        if peek_lock:\n            return self.peek_lock_queue_message(queue_name, timeout)\n        return self.read_delete_queue_message(queue_name, timeout)","method_summary":"Receive a message from a queue for processing.","original_method_code":"def receive_queue_message(self, queue_name, peek_lock=True, timeout=60):\n        '''\n        Receive a message from a queue for processing.\n\n        queue_name:\n            Name of the queue.\n        peek_lock:\n            Optional. True to retrieve and lock the message. False to read and\n            delete the message. Default is True (lock).\n        timeout:\n            Optional. The timeout parameter is expressed in seconds.\n        '''\n        if peek_lock:\n            return self.peek_lock_queue_message(queue_name, timeout)\n        return self.read_delete_queue_message(queue_name, timeout)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.receive_subscription_message","method_code":"def receive_subscription_message(self, topic_name, subscription_name,\n                                     peek_lock=True, timeout=60):\n        if peek_lock:\n            return self.peek_lock_subscription_message(topic_name,\n                                                       subscription_name,\n                                                       timeout)\n        return self.read_delete_subscription_message(topic_name,\n                                                     subscription_name,\n                                                     timeout)","method_summary":"Receive a message from a subscription for processing.","original_method_code":"def receive_subscription_message(self, topic_name, subscription_name,\n                                     peek_lock=True, timeout=60):\n        '''\n        Receive a message from a subscription for processing.\n\n        topic_name:\n            Name of the topic.\n        subscription_name:\n            Name of the subscription.\n        peek_lock:\n            Optional. True to retrieve and lock the message. False to read and\n            delete the message. Default is True (lock).\n        timeout:\n            Optional. The timeout parameter is expressed in seconds.\n        '''\n        if peek_lock:\n            return self.peek_lock_subscription_message(topic_name,\n                                                       subscription_name,\n                                                       timeout)\n        return self.read_delete_subscription_message(topic_name,\n                                                     subscription_name,\n                                                     timeout)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.create_event_hub","method_code":"def create_event_hub(self, hub_name, hub=None, fail_on_exist=False):\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + _str(hub_name) + '?api-version=2014-01'\n        request.body = _get_request_body(_convert_event_hub_to_xml(hub))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        if not fail_on_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_on_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_summary":"Creates a new Event Hub.","original_method_code":"def create_event_hub(self, hub_name, hub=None, fail_on_exist=False):\n        '''\n        Creates a new Event Hub.\n\n        hub_name:\n            Name of event hub.\n        hub:\n            Optional. Event hub properties. Instance of EventHub class.\n        hub.message_retention_in_days:\n            Number of days to retain the events for this Event Hub.\n        hub.status:\n            Status of the Event Hub (enabled or disabled).\n        hub.user_metadata:\n            User metadata.\n        hub.partition_count:\n            Number of shards on the Event Hub.\n        fail_on_exist:\n            Specify whether to throw an exception when the event hub exists.\n        '''\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + _str(hub_name) + '?api-version=2014-01'\n        request.body = _get_request_body(_convert_event_hub_to_xml(hub))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        if not fail_on_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_on_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.update_event_hub","method_code":"def update_event_hub(self, hub_name, hub=None):\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + _str(hub_name) + '?api-version=2014-01'\n        request.body = _get_request_body(_convert_event_hub_to_xml(hub))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers.append(('If-Match', '*'))\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_event_hub(response)","method_summary":"Updates an Event Hub.","original_method_code":"def update_event_hub(self, hub_name, hub=None):\n        '''\n        Updates an Event Hub.\n\n        hub_name:\n            Name of event hub.\n        hub:\n            Optional. Event hub properties. Instance of EventHub class.\n        hub.message_retention_in_days:\n            Number of days to retain the events for this Event Hub.\n        '''\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host = self._get_host()\n        request.path = '\/' + _str(hub_name) + '?api-version=2014-01'\n        request.body = _get_request_body(_convert_event_hub_to_xml(hub))\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers.append(('If-Match', '*'))\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_event_hub(response)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.get_event_hub","method_code":"def get_event_hub(self, hub_name):\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + _str(hub_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_event_hub(response)","method_summary":"Retrieves an existing event hub.","original_method_code":"def get_event_hub(self, hub_name):\n        '''\n        Retrieves an existing event hub.\n\n        hub_name:\n            Name of the event hub.\n        '''\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '\/' + _str(hub_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_event_hub(response)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService.send_event","method_code":"def send_event(self, hub_name, message, device_id=None,\n                   broker_properties=None):\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'POST'\n        request.host = self._get_host()\n        if device_id:\n            request.path = '\/{0}\/publishers\/{1}\/messages?api-version=2014-01'.format(hub_name, device_id)\n        else:\n            request.path = '\/{0}\/messages?api-version=2014-01'.format(hub_name)\n        if broker_properties:\n            request.headers.append(\n                ('BrokerProperties', str(broker_properties)))\n        request.body = _get_request_body(message)\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  \n        request.headers = self._update_service_bus_header(request)\n        self._perform_request(request)","method_summary":"Sends a new message event to an Event Hub.","original_method_code":"def send_event(self, hub_name, message, device_id=None,\n                   broker_properties=None):\n        '''\n        Sends a new message event to an Event Hub.\n        '''\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'POST'\n        request.host = self._get_host()\n        if device_id:\n            request.path = '\/{0}\/publishers\/{1}\/messages?api-version=2014-01'.format(hub_name, device_id)\n        else:\n            request.path = '\/{0}\/messages?api-version=2014-01'.format(hub_name)\n        if broker_properties:\n            request.headers.append(\n                ('BrokerProperties', str(broker_properties)))\n        request.body = _get_request_body(message)\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        self._perform_request(request)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusService._update_service_bus_header","method_code":"def _update_service_bus_header(self, request):\n        if request.method in ['PUT', 'POST', 'MERGE', 'DELETE']:\n            request.headers.append(('Content-Length', str(len(request.body))))\n\n        \n        if not request.method in ['GET', 'HEAD']:\n            for name, _ in request.headers:\n                if name.lower() == 'content-type':\n                    break\n            else:\n                request.headers.append(\n                    ('Content-Type',\n                     'application\/atom+xml;type=entry;charset=utf-8'))\n\n        \n        self.authentication.sign_request(request, self._httpclient)\n\n        return request.headers","method_summary":"Add additional headers for Service Bus.","original_method_code":"def _update_service_bus_header(self, request):\n        ''' Add additional headers for Service Bus. '''\n\n        if request.method in ['PUT', 'POST', 'MERGE', 'DELETE']:\n            request.headers.append(('Content-Length', str(len(request.body))))\n\n        # if it is not GET or HEAD request, must set content-type.\n        if not request.method in ['GET', 'HEAD']:\n            for name, _ in request.headers:\n                if name.lower() == 'content-type':\n                    break\n            else:\n                request.headers.append(\n                    ('Content-Type',\n                     'application\/atom+xml;type=entry;charset=utf-8'))\n\n        # Adds authorization header for authentication.\n        self.authentication.sign_request(request, self._httpclient)\n\n        return request.headers","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusWrapTokenAuthentication._get_authorization","method_code":"def _get_authorization(self, request, httpclient):\n        return 'WRAP access_token=\"' + \\\n                self._get_token(request.host, request.path, httpclient) + '\"'","method_summary":"return the signed string with token.","original_method_code":"def _get_authorization(self, request, httpclient):\n        ''' return the signed string with token. '''\n        return 'WRAP access_token=\"' + \\\n                self._get_token(request.host, request.path, httpclient) + '\"'","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusWrapTokenAuthentication._token_is_expired","method_code":"def _token_is_expired(self, token):  \n        time_pos_begin = token.find('ExpiresOn=') + len('ExpiresOn=')\n        time_pos_end = token.find('&', time_pos_begin)\n        token_expire_time = int(token[time_pos_begin:time_pos_end])\n        time_now = time.mktime(time.localtime())\n\n        \n        \n        return (token_expire_time - time_now) < 30","method_summary":"Check if token expires or not.","original_method_code":"def _token_is_expired(self, token):  # pylint: disable=no-self-use\n        ''' Check if token expires or not. '''\n        time_pos_begin = token.find('ExpiresOn=') + len('ExpiresOn=')\n        time_pos_end = token.find('&', time_pos_begin)\n        token_expire_time = int(token[time_pos_begin:time_pos_end])\n        time_now = time.mktime(time.localtime())\n\n        # Adding 30 seconds so the token wouldn't be expired when we send the\n        # token to server.\n        return (token_expire_time - time_now) < 30","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/servicebusservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ManagedClustersOperations.reset_service_principal_profile","method_code":"def reset_service_principal_profile(\n            self, resource_group_name, resource_name, client_id, secret=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._reset_service_principal_profile_initial(\n            resource_group_name=resource_group_name,\n            resource_name=resource_name,\n            client_id=client_id,\n            secret=secret,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Reset Service Principal Profile of a managed cluster. Update the service principal Profile for a managed cluster.","original_method_code":"def reset_service_principal_profile(\n            self, resource_group_name, resource_name, client_id, secret=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Reset Service Principal Profile of a managed cluster.\n\n        Update the service principal Profile for a managed cluster.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param resource_name: The name of the managed cluster resource.\n        :type resource_name: str\n        :param client_id: The ID for the service principal.\n        :type client_id: str\n        :param secret: The secret password associated with the service\n         principal in plain text.\n        :type secret: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._reset_service_principal_profile_initial(\n            resource_group_name=resource_group_name,\n            resource_name=resource_name,\n            client_id=client_id,\n            secret=secret,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-containerservice\/azure\/mgmt\/containerservice\/v2018_08_01_preview\/operations\/managed_clusters_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"Message.delete","method_code":"def delete(self):\n        if self._queue_name:\n            self.service_bus_service.delete_queue_message(\n                self._queue_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        elif self._topic_name and self._subscription_name:\n            self.service_bus_service.delete_subscription_message(\n                self._topic_name,\n                self._subscription_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        else:\n            raise AzureServiceBusPeekLockError(_ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_DELETE)","method_summary":"Deletes itself if find queue name or topic name and subscription name.","original_method_code":"def delete(self):\n        ''' Deletes itself if find queue name or topic name and subscription\n        name. '''\n        if self._queue_name:\n            self.service_bus_service.delete_queue_message(\n                self._queue_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        elif self._topic_name and self._subscription_name:\n            self.service_bus_service.delete_subscription_message(\n                self._topic_name,\n                self._subscription_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        else:\n            raise AzureServiceBusPeekLockError(_ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_DELETE)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/models.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"Message.unlock","method_code":"def unlock(self):\n        if self._queue_name:\n            self.service_bus_service.unlock_queue_message(\n                self._queue_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        elif self._topic_name and self._subscription_name:\n            self.service_bus_service.unlock_subscription_message(\n                self._topic_name,\n                self._subscription_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        else:\n            raise AzureServiceBusPeekLockError(_ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_UNLOCK)","method_summary":"Unlocks itself if find queue name or topic name and subscription name.","original_method_code":"def unlock(self):\n        ''' Unlocks itself if find queue name or topic name and subscription\n        name. '''\n        if self._queue_name:\n            self.service_bus_service.unlock_queue_message(\n                self._queue_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        elif self._topic_name and self._subscription_name:\n            self.service_bus_service.unlock_subscription_message(\n                self._topic_name,\n                self._subscription_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        else:\n            raise AzureServiceBusPeekLockError(_ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_UNLOCK)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/models.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"Message.renew_lock","method_code":"def renew_lock(self):\n        if self._queue_name:\n            self.service_bus_service.renew_lock_queue_message(\n                self._queue_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        elif self._topic_name and self._subscription_name:\n            self.service_bus_service.renew_lock_subscription_message(\n                self._topic_name,\n                self._subscription_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        else:\n            raise AzureServiceBusPeekLockError(_ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_RENEW_LOCK)","method_summary":"Renew lock on itself if find queue name or topic name and subscription name.","original_method_code":"def renew_lock(self):\n        ''' Renew lock on itself if find queue name or topic name and subscription\n        name. '''\n        if self._queue_name:\n            self.service_bus_service.renew_lock_queue_message(\n                self._queue_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        elif self._topic_name and self._subscription_name:\n            self.service_bus_service.renew_lock_subscription_message(\n                self._topic_name,\n                self._subscription_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        else:\n            raise AzureServiceBusPeekLockError(_ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_RENEW_LOCK)","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/models.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"Message.add_headers","method_code":"def add_headers(self, request):\n        \n        if self.custom_properties:\n            for name, value in self.custom_properties.items():\n                request.headers.append((name, self._serialize_escaped_properties_value(value)))\n\n        \n        request.headers.append(('Content-Type', self.type))\n\n        \n        if self.broker_properties:\n            if hasattr(self.broker_properties, 'items'):\n                broker_properties = {name: self._serialize_basic_properties_value(value)\n                                     for name, value\n                                     in self.broker_properties.items()}\n                broker_properties = json.dumps(broker_properties)\n            else:\n                broker_properties = self.broker_properties\n            request.headers.append(\n                ('BrokerProperties', str(broker_properties)))\n\n        return request.headers","method_summary":"add addtional headers to request for message request.","original_method_code":"def add_headers(self, request):\n        ''' add addtional headers to request for message request.'''\n\n        # Adds custom properties\n        if self.custom_properties:\n            for name, value in self.custom_properties.items():\n                request.headers.append((name, self._serialize_escaped_properties_value(value)))\n\n        # Adds content-type\n        request.headers.append(('Content-Type', self.type))\n\n        # Adds BrokerProperties\n        if self.broker_properties:\n            if hasattr(self.broker_properties, 'items'):\n                broker_properties = {name: self._serialize_basic_properties_value(value)\n                                     for name, value\n                                     in self.broker_properties.items()}\n                broker_properties = json.dumps(broker_properties)\n            else:\n                broker_properties = self.broker_properties\n            request.headers.append(\n                ('BrokerProperties', str(broker_properties)))\n\n        return request.headers","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/models.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"Message.as_batch_body","method_code":"def as_batch_body(self):\n        if sys.version_info >= (3,) and isinstance(self.body, bytes):\n            \n            body = self.body.decode('utf-8')\n        else:\n            \n            body = self.body\n        result = {'Body': body}\n\n        \n        if self.custom_properties:\n            result['UserProperties'] = {name: self._serialize_basic_properties_value(value)\n                                        for name, value\n                                        in self.custom_properties.items()}\n\n        \n        if self.broker_properties:\n            result['BrokerProperties'] = {name: self._serialize_basic_properties_value(value)\n                                          for name, value\n                                          in self.broker_properties.items()}\n\n        return result","method_summary":"return the current message as expected by batch body format","original_method_code":"def as_batch_body(self):\n        ''' return the current message as expected by batch body format'''\n        if sys.version_info >= (3,) and isinstance(self.body, bytes):\n            # It HAS to be string to be serialized in JSON\n            body = self.body.decode('utf-8')\n        else:\n            # Python 2.7 people handle this themself\n            body = self.body\n        result = {'Body': body}\n\n        # Adds custom properties\n        if self.custom_properties:\n            result['UserProperties'] = {name: self._serialize_basic_properties_value(value)\n                                        for name, value\n                                        in self.custom_properties.items()}\n\n        # Adds BrokerProperties\n        if self.broker_properties:\n            result['BrokerProperties'] = {name: self._serialize_basic_properties_value(value)\n                                          for name, value\n                                          in self.broker_properties.items()}\n\n        return result","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/models.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceFabricClientAPIs.get_repair_task_list","method_code":"def get_repair_task_list(\n            self, task_id_filter=None, state_filter=None, executor_filter=None, custom_headers=None, raw=False, **operation_config):\n        api_version = \"6.0\"\n\n        \n        url = self.get_repair_task_list.metadata['url']\n\n        \n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n        if task_id_filter is not None:\n            query_parameters['TaskIdFilter'] = self._serialize.query(\"task_id_filter\", task_id_filter, 'str')\n        if state_filter is not None:\n            query_parameters['StateFilter'] = self._serialize.query(\"state_filter\", state_filter, 'int')\n        if executor_filter is not None:\n            query_parameters['ExecutorFilter'] = self._serialize.query(\"executor_filter\", executor_filter, 'str')\n\n        \n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        \n        request = self._client.get(url, query_parameters, header_parameters)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.FabricErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('[RepairTask]', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_summary":"Gets a list of repair tasks matching the given filters. This API supports the Service Fabric platform; it is not meant to be used directly from your code.","original_method_code":"def get_repair_task_list(\n            self, task_id_filter=None, state_filter=None, executor_filter=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Gets a list of repair tasks matching the given filters.\n\n        This API supports the Service Fabric platform; it is not meant to be\n        used directly from your code.\n\n        :param task_id_filter: The repair task ID prefix to be matched.\n        :type task_id_filter: str\n        :param state_filter: A bitwise-OR of the following values, specifying\n         which task states should be included in the result list.\n         - 1 - Created\n         - 2 - Claimed\n         - 4 - Preparing\n         - 8 - Approved\n         - 16 - Executing\n         - 32 - Restoring\n         - 64 - Completed\n        :type state_filter: int\n        :param executor_filter: The name of the repair executor whose claimed\n         tasks should be included in the list.\n        :type executor_filter: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: list or ClientRawResponse if raw=true\n        :rtype: list[~azure.servicefabric.models.RepairTask] or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`\n        \"\"\"\n        api_version = \"6.0\"\n\n        # Construct URL\n        url = self.get_repair_task_list.metadata['url']\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n        if task_id_filter is not None:\n            query_parameters['TaskIdFilter'] = self._serialize.query(\"task_id_filter\", task_id_filter, 'str')\n        if state_filter is not None:\n            query_parameters['StateFilter'] = self._serialize.query(\"state_filter\", state_filter, 'int')\n        if executor_filter is not None:\n            query_parameters['ExecutorFilter'] = self._serialize.query(\"executor_filter\", executor_filter, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct and send request\n        request = self._client.get(url, query_parameters, header_parameters)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.FabricErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('[RepairTask]', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-servicefabric\/azure\/servicefabric\/service_fabric_client_ap_is.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceFabricClientAPIs.submit_property_batch","method_code":"def submit_property_batch(\n            self, name_id, timeout=60, operations=None, custom_headers=None, raw=False, **operation_config):\n        property_batch_description_list = models.PropertyBatchDescriptionList(operations=operations)\n\n        api_version = \"6.0\"\n\n        \n        url = self.submit_property_batch.metadata['url']\n        path_format_arguments = {\n            'nameId': self._serialize.url(\"name_id\", name_id, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n        if timeout is not None:\n            query_parameters['timeout'] = self._serialize.query(\"timeout\", timeout, 'long', maximum=4294967295, minimum=1)\n\n        \n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        \n        body_content = self._serialize.body(property_batch_description_list, 'PropertyBatchDescriptionList')\n\n        \n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200, 409]:\n            raise models.FabricErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('SuccessfulPropertyBatchInfo', response)\n        if response.status_code == 409:\n            deserialized = self._deserialize('FailedPropertyBatchInfo', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_summary":"Submits a property batch. Submits a batch of property operations. Either all or none of the operations will be committed.","original_method_code":"def submit_property_batch(\n            self, name_id, timeout=60, operations=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Submits a property batch.\n\n        Submits a batch of property operations. Either all or none of the\n        operations will be committed.\n\n        :param name_id: The Service Fabric name, without the 'fabric:' URI\n         scheme.\n        :type name_id: str\n        :param timeout: The server timeout for performing the operation in\n         seconds. This timeout specifies the time duration that the client is\n         willing to wait for the requested operation to complete. The default\n         value for this parameter is 60 seconds.\n        :type timeout: long\n        :param operations: A list of the property batch operations to be\n         executed.\n        :type operations:\n         list[~azure.servicefabric.models.PropertyBatchOperation]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: PropertyBatchInfo or ClientRawResponse if raw=true\n        :rtype: ~azure.servicefabric.models.PropertyBatchInfo or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`\n        \"\"\"\n        property_batch_description_list = models.PropertyBatchDescriptionList(operations=operations)\n\n        api_version = \"6.0\"\n\n        # Construct URL\n        url = self.submit_property_batch.metadata['url']\n        path_format_arguments = {\n            'nameId': self._serialize.url(\"name_id\", name_id, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n        if timeout is not None:\n            query_parameters['timeout'] = self._serialize.query(\"timeout\", timeout, 'long', maximum=4294967295, minimum=1)\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(property_batch_description_list, 'PropertyBatchDescriptionList')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200, 409]:\n            raise models.FabricErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('SuccessfulPropertyBatchInfo', response)\n        if response.status_code == 409:\n            deserialized = self._deserialize('FailedPropertyBatchInfo', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-servicefabric\/azure\/servicefabric\/service_fabric_client_ap_is.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_general_error_handler","method_code":"def _general_error_handler(http_error):\n    message = str(http_error)\n    if http_error.respbody is not None:\n        message += '\\n' + http_error.respbody.decode('utf-8-sig')\n    raise AzureHttpError(message, http_error.status)","method_summary":"Simple error handler for azure.","original_method_code":"def _general_error_handler(http_error):\n    ''' Simple error handler for azure.'''\n    message = str(http_error)\n    if http_error.respbody is not None:\n        message += '\\n' + http_error.respbody.decode('utf-8-sig')\n    raise AzureHttpError(message, http_error.status)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/_common_error.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebAppsOperations.start_web_site_network_trace_operation","method_code":"def start_web_site_network_trace_operation(\n            self, resource_group_name, name, duration_in_seconds=None, max_frame_length=None, sas_url=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._start_web_site_network_trace_operation_initial(\n            resource_group_name=resource_group_name,\n            name=name,\n            duration_in_seconds=duration_in_seconds,\n            max_frame_length=max_frame_length,\n            sas_url=sas_url,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('[NetworkTrace]', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Start capturing network packets for the site. Start capturing network packets for the site.","original_method_code":"def start_web_site_network_trace_operation(\n            self, resource_group_name, name, duration_in_seconds=None, max_frame_length=None, sas_url=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Start capturing network packets for the site.\n\n        Start capturing network packets for the site.\n\n        :param resource_group_name: Name of the resource group to which the\n         resource belongs.\n        :type resource_group_name: str\n        :param name: The name of the web app.\n        :type name: str\n        :param duration_in_seconds: The duration to keep capturing in seconds.\n        :type duration_in_seconds: int\n        :param max_frame_length: The maximum frame length in bytes (Optional).\n        :type max_frame_length: int\n        :param sas_url: The Blob URL to store capture file.\n        :type sas_url: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns list or\n         ClientRawResponse<list> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[list[~azure.mgmt.web.models.NetworkTrace]]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[list[~azure.mgmt.web.models.NetworkTrace]]]\n        :raises:\n         :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`\n        \"\"\"\n        raw_result = self._start_web_site_network_trace_operation_initial(\n            resource_group_name=resource_group_name,\n            name=name,\n            duration_in_seconds=duration_in_seconds,\n            max_frame_length=max_frame_length,\n            sas_url=sas_url,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('[NetworkTrace]', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-web\/azure\/mgmt\/web\/operations\/web_apps_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebAppsOperations.list_slot_differences_slot","method_code":"def list_slot_differences_slot(\n            self, resource_group_name, name, slot, target_slot, preserve_vnet, custom_headers=None, raw=False, **operation_config):\n        slot_swap_entity = models.CsmSlotEntity(target_slot=target_slot, preserve_vnet=preserve_vnet)\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                \n                url = self.list_slot_differences_slot.metadata['url']\n                path_format_arguments = {\n                    'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\\w\\._\\(\\)]+[^\\.]$'),\n                    'name': self._serialize.url(\"name\", name, 'str'),\n                    'slot': self._serialize.url(\"slot\", slot, 'str'),\n                    'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                \n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            \n            header_parameters = {}\n            header_parameters['Accept'] = 'application\/json'\n            header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            \n            body_content = self._serialize.body(slot_swap_entity, 'CsmSlotEntity')\n\n            \n            request = self._client.post(url, query_parameters, header_parameters, body_content)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.DefaultErrorResponseException(self._deserialize, response)\n\n            return response\n\n        \n        deserialized = models.SlotDifferencePaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.SlotDifferencePaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized","method_summary":"Get the difference in configuration settings between two web app slots. Get the difference in configuration settings between two web app slots.","original_method_code":"def list_slot_differences_slot(\n            self, resource_group_name, name, slot, target_slot, preserve_vnet, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Get the difference in configuration settings between two web app slots.\n\n        Get the difference in configuration settings between two web app slots.\n\n        :param resource_group_name: Name of the resource group to which the\n         resource belongs.\n        :type resource_group_name: str\n        :param name: Name of the app.\n        :type name: str\n        :param slot: Name of the source slot. If a slot is not specified, the\n         production slot is used as the source slot.\n        :type slot: str\n        :param target_slot: Destination deployment slot during swap operation.\n        :type target_slot: str\n        :param preserve_vnet: <code>true<\/code> to preserve Virtual Network to\n         the slot during swap; otherwise, <code>false<\/code>.\n        :type preserve_vnet: bool\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: An iterator like instance of SlotDifference\n        :rtype:\n         ~azure.mgmt.web.models.SlotDifferencePaged[~azure.mgmt.web.models.SlotDifference]\n        :raises:\n         :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`\n        \"\"\"\n        slot_swap_entity = models.CsmSlotEntity(target_slot=target_slot, preserve_vnet=preserve_vnet)\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                # Construct URL\n                url = self.list_slot_differences_slot.metadata['url']\n                path_format_arguments = {\n                    'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\\w\\._\\(\\)]+[^\\.]$'),\n                    'name': self._serialize.url(\"name\", name, 'str'),\n                    'slot': self._serialize.url(\"slot\", slot, 'str'),\n                    'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                # Construct parameters\n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            # Construct headers\n            header_parameters = {}\n            header_parameters['Accept'] = 'application\/json'\n            header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            # Construct body\n            body_content = self._serialize.body(slot_swap_entity, 'CsmSlotEntity')\n\n            # Construct and send request\n            request = self._client.post(url, query_parameters, header_parameters, body_content)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.DefaultErrorResponseException(self._deserialize, response)\n\n            return response\n\n        # Deserialize response\n        deserialized = models.SlotDifferencePaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.SlotDifferencePaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-mgmt-web\/azure\/mgmt\/web\/operations\/web_apps_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebAppsOperations.swap_slot_slot","method_code":"def swap_slot_slot(\n            self, resource_group_name, name, slot, target_slot, preserve_vnet, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._swap_slot_slot_initial(\n            resource_group_name=resource_group_name,\n            name=name,\n            slot=slot,\n            target_slot=target_slot,\n            preserve_vnet=preserve_vnet,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Swaps two deployment slots of an app. Swaps two deployment slots of an app.","original_method_code":"def swap_slot_slot(\n            self, resource_group_name, name, slot, target_slot, preserve_vnet, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Swaps two deployment slots of an app.\n\n        Swaps two deployment slots of an app.\n\n        :param resource_group_name: Name of the resource group to which the\n         resource belongs.\n        :type resource_group_name: str\n        :param name: Name of the app.\n        :type name: str\n        :param slot: Name of the source slot. If a slot is not specified, the\n         production slot is used as the source slot.\n        :type slot: str\n        :param target_slot: Destination deployment slot during swap operation.\n        :type target_slot: str\n        :param preserve_vnet: <code>true<\/code> to preserve Virtual Network to\n         the slot during swap; otherwise, <code>false<\/code>.\n        :type preserve_vnet: bool\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._swap_slot_slot_initial(\n            resource_group_name=resource_group_name,\n            name=name,\n            slot=slot,\n            target_slot=target_slot,\n            preserve_vnet=preserve_vnet,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-web\/azure\/mgmt\/web\/operations\/web_apps_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"EventsOperations.get_by_type","method_code":"def get_by_type(\n            self, app_id, event_type, timespan=None, filter=None, search=None, orderby=None, select=None, skip=None, top=None, format=None, count=None, apply=None, custom_headers=None, raw=False, **operation_config):\n        \n        url = self.get_by_type.metadata['url']\n        path_format_arguments = {\n            'appId': self._serialize.url(\"app_id\", app_id, 'str'),\n            'eventType': self._serialize.url(\"event_type\", event_type, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n        if timespan is not None:\n            query_parameters['timespan'] = self._serialize.query(\"timespan\", timespan, 'str')\n        if filter is not None:\n            query_parameters['$filter'] = self._serialize.query(\"filter\", filter, 'str')\n        if search is not None:\n            query_parameters['$search'] = self._serialize.query(\"search\", search, 'str')\n        if orderby is not None:\n            query_parameters['$orderby'] = self._serialize.query(\"orderby\", orderby, 'str')\n        if select is not None:\n            query_parameters['$select'] = self._serialize.query(\"select\", select, 'str')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query(\"skip\", skip, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query(\"top\", top, 'int')\n        if format is not None:\n            query_parameters['$format'] = self._serialize.query(\"format\", format, 'str')\n        if count is not None:\n            query_parameters['$count'] = self._serialize.query(\"count\", count, 'bool')\n        if apply is not None:\n            query_parameters['$apply'] = self._serialize.query(\"apply\", apply, 'str')\n\n        \n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        \n        request = self._client.get(url, query_parameters, header_parameters)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorResponseException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('EventsResults', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_summary":"Execute OData query. Executes an OData query for events.","original_method_code":"def get_by_type(\n            self, app_id, event_type, timespan=None, filter=None, search=None, orderby=None, select=None, skip=None, top=None, format=None, count=None, apply=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Execute OData query.\n\n        Executes an OData query for events.\n\n        :param app_id: ID of the application. This is Application ID from the\n         API Access settings blade in the Azure portal.\n        :type app_id: str\n        :param event_type: The type of events to query; either a standard\n         event type (`traces`, `customEvents`, `pageViews`, `requests`,\n         `dependencies`, `exceptions`, `availabilityResults`) or `$all` to\n         query across all event types. Possible values include: '$all',\n         'traces', 'customEvents', 'pageViews', 'browserTimings', 'requests',\n         'dependencies', 'exceptions', 'availabilityResults',\n         'performanceCounters', 'customMetrics'\n        :type event_type: str or ~azure.applicationinsights.models.EventType\n        :param timespan: Optional. The timespan over which to retrieve events.\n         This is an ISO8601 time period value.  This timespan is applied in\n         addition to any that are specified in the Odata expression.\n        :type timespan: str\n        :param filter: An expression used to filter the returned events\n        :type filter: str\n        :param search: A free-text search expression to match for whether a\n         particular event should be returned\n        :type search: str\n        :param orderby: A comma-separated list of properties with \\\\\"asc\\\\\"\n         (the default) or \\\\\"desc\\\\\" to control the order of returned events\n        :type orderby: str\n        :param select: Limits the properties to just those requested on each\n         returned event\n        :type select: str\n        :param skip: The number of items to skip over before returning events\n        :type skip: int\n        :param top: The number of events to return\n        :type top: int\n        :param format: Format for the returned events\n        :type format: str\n        :param count: Request a count of matching items included with the\n         returned events\n        :type count: bool\n        :param apply: An expression used for aggregation over returned events\n        :type apply: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: EventsResults or ClientRawResponse if raw=true\n        :rtype: ~azure.applicationinsights.models.EventsResults or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`ErrorResponseException<azure.applicationinsights.models.ErrorResponseException>`\n        \"\"\"\n        # Construct URL\n        url = self.get_by_type.metadata['url']\n        path_format_arguments = {\n            'appId': self._serialize.url(\"app_id\", app_id, 'str'),\n            'eventType': self._serialize.url(\"event_type\", event_type, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        if timespan is not None:\n            query_parameters['timespan'] = self._serialize.query(\"timespan\", timespan, 'str')\n        if filter is not None:\n            query_parameters['$filter'] = self._serialize.query(\"filter\", filter, 'str')\n        if search is not None:\n            query_parameters['$search'] = self._serialize.query(\"search\", search, 'str')\n        if orderby is not None:\n            query_parameters['$orderby'] = self._serialize.query(\"orderby\", orderby, 'str')\n        if select is not None:\n            query_parameters['$select'] = self._serialize.query(\"select\", select, 'str')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query(\"skip\", skip, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query(\"top\", top, 'int')\n        if format is not None:\n            query_parameters['$format'] = self._serialize.query(\"format\", format, 'str')\n        if count is not None:\n            query_parameters['$count'] = self._serialize.query(\"count\", count, 'bool')\n        if apply is not None:\n            query_parameters['$apply'] = self._serialize.query(\"apply\", apply, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct and send request\n        request = self._client.get(url, query_parameters, header_parameters)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorResponseException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('EventsResults', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-applicationinsights\/azure\/applicationinsights\/operations\/events_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"LargeFaceListOperations.add_face_from_stream","method_code":"def add_face_from_stream(\n            self, large_face_list_id, image, user_data=None, target_face=None, custom_headers=None, raw=False, callback=None, **operation_config):\n        \n        url = self.add_face_from_stream.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True),\n            'largeFaceListId': self._serialize.url(\"large_face_list_id\", large_face_list_id, 'str', max_length=64, pattern=r'^[a-z0-9-_]+$')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n        if user_data is not None:\n            query_parameters['userData'] = self._serialize.query(\"user_data\", user_data, 'str', max_length=1024)\n        if target_face is not None:\n            query_parameters['targetFace'] = self._serialize.query(\"target_face\", target_face, '[int]', div=',')\n\n        \n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/octet-stream'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        \n        body_content = self._client.stream_upload(image, callback)\n\n        \n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.APIErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('PersistedFace', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_summary":"Add a face to a large face list. The input face is specified as an image with a targetFace rectangle. It returns a persistedFaceId representing the added face, and persistedFaceId will not expire.","original_method_code":"def add_face_from_stream(\n            self, large_face_list_id, image, user_data=None, target_face=None, custom_headers=None, raw=False, callback=None, **operation_config):\n        \"\"\"Add a face to a large face list. The input face is specified as an\n        image with a targetFace rectangle. It returns a persistedFaceId\n        representing the added face, and persistedFaceId will not expire.\n\n        :param large_face_list_id: Id referencing a particular large face\n         list.\n        :type large_face_list_id: str\n        :param image: An image stream.\n        :type image: Generator\n        :param user_data: User-specified data about the face for any purpose.\n         The maximum length is 1KB.\n        :type user_data: str\n        :param target_face: A face rectangle to specify the target face to be\n         added to a person in the format of \"targetFace=left,top,width,height\".\n         E.g. \"targetFace=10,10,100,100\". If there is more than one face in the\n         image, targetFace is required to specify which face to add. No\n         targetFace means there is only one face detected in the entire image.\n        :type target_face: list[int]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param callback: When specified, will be called with each chunk of\n         data that is streamed. The callback should take two arguments, the\n         bytes of the current chunk of data and the response object. If the\n         data is uploading, response will be None.\n        :type callback: Callable[Bytes, response=None]\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: PersistedFace or ClientRawResponse if raw=true\n        :rtype: ~azure.cognitiveservices.vision.face.models.PersistedFace or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`APIErrorException<azure.cognitiveservices.vision.face.models.APIErrorException>`\n        \"\"\"\n        # Construct URL\n        url = self.add_face_from_stream.metadata['url']\n        path_format_arguments = {\n            'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True),\n            'largeFaceListId': self._serialize.url(\"large_face_list_id\", large_face_list_id, 'str', max_length=64, pattern=r'^[a-z0-9-_]+$')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        if user_data is not None:\n            query_parameters['userData'] = self._serialize.query(\"user_data\", user_data, 'str', max_length=1024)\n        if target_face is not None:\n            query_parameters['targetFace'] = self._serialize.query(\"target_face\", target_face, '[int]', div=',')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application\/json'\n        header_parameters['Content-Type'] = 'application\/octet-stream'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._client.stream_upload(image, callback)\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.APIErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('PersistedFace', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-cognitiveservices-vision-face\/azure\/cognitiveservices\/vision\/face\/operations\/large_face_list_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"KeyVaultAuthBase._handle_redirect","method_code":"def _handle_redirect(self, r, **kwargs):\n        if r.is_redirect:\n            self._thread_local.auth_attempted = False","method_summary":"Reset auth_attempted on redirects.","original_method_code":"def _handle_redirect(self, r, **kwargs):\n        \"\"\"Reset auth_attempted on redirects.\"\"\"\n        if r.is_redirect:\n            self._thread_local.auth_attempted = False","method_path":"azure-keyvault\/azure\/keyvault\/key_vault_authentication.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"MigrationConfigsOperations.create_and_start_migration","method_code":"def create_and_start_migration(\n            self, resource_group_name, namespace_name, target_namespace, post_migration_name, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._create_and_start_migration_initial(\n            resource_group_name=resource_group_name,\n            namespace_name=namespace_name,\n            target_namespace=target_namespace,\n            post_migration_name=post_migration_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('MigrationConfigProperties', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Creates Migration configuration and starts migration of entities from Standard to Premium namespace.","original_method_code":"def create_and_start_migration(\n            self, resource_group_name, namespace_name, target_namespace, post_migration_name, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Creates Migration configuration and starts migration of entities from\n        Standard to Premium namespace.\n\n        :param resource_group_name: Name of the Resource group within the\n         Azure subscription.\n        :type resource_group_name: str\n        :param namespace_name: The namespace name\n        :type namespace_name: str\n        :param target_namespace: Existing premium Namespace ARM Id name which\n         has no entities, will be used for migration\n        :type target_namespace: str\n        :param post_migration_name: Name to access Standard Namespace after\n         migration\n        :type post_migration_name: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns\n         MigrationConfigProperties or\n         ClientRawResponse<MigrationConfigProperties> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.servicebus.models.MigrationConfigProperties]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.servicebus.models.MigrationConfigProperties]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.servicebus.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._create_and_start_migration_initial(\n            resource_group_name=resource_group_name,\n            namespace_name=namespace_name,\n            target_namespace=target_namespace,\n            post_migration_name=post_migration_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('MigrationConfigProperties', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-servicebus\/azure\/mgmt\/servicebus\/operations\/migration_configs_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"EventGridClient.publish_events","method_code":"def publish_events(\n            self, topic_hostname, events, custom_headers=None, raw=False, **operation_config):\n        \n        url = self.publish_events.metadata['url']\n        path_format_arguments = {\n            'topicHostname': self._serialize.url(\"topic_hostname\", topic_hostname, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        \n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        \n        body_content = self._serialize.body(events, '[EventGridEvent]')\n\n        \n        request = self._client.post(url, query_parameters)\n        response = self._client.send(\n            request, header_parameters, body_content, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise HttpOperationError(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response","method_summary":"Publishes a batch of events to an Azure Event Grid topic.","original_method_code":"def publish_events(\n            self, topic_hostname, events, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Publishes a batch of events to an Azure Event Grid topic.\n\n        :param topic_hostname: The host name of the topic, e.g.\n         topic1.westus2-1.eventgrid.azure.net\n        :type topic_hostname: str\n        :param events: An array of events to be published to Event Grid.\n        :type events: list[~azure.eventgrid.models.EventGridEvent]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`HttpOperationError<msrest.exceptions.HttpOperationError>`\n        \"\"\"\n        # Construct URL\n        url = self.publish_events.metadata['url']\n        path_format_arguments = {\n            'topicHostname': self._serialize.url(\"topic_hostname\", topic_hostname, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(events, '[EventGridEvent]')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters)\n        response = self._client.send(\n            request, header_parameters, body_content, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise HttpOperationError(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response","method_path":"azure-eventgrid\/azure\/eventgrid\/event_grid_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"DefaultProfile.use","method_code":"def use(self, profile):\n        if not isinstance(profile, (KnownProfiles, ProfileDefinition)):\n            raise ValueError(\"Can only set as default a ProfileDefinition or a KnownProfiles\")\n        type(self).profile = profile","method_summary":"Define a new default profile.","original_method_code":"def use(self, profile):\n        \"\"\"Define a new default profile.\"\"\"\n        if not isinstance(profile, (KnownProfiles, ProfileDefinition)):\n            raise ValueError(\"Can only set as default a ProfileDefinition or a KnownProfiles\")\n        type(self).profile = profile","method_path":"azure-common\/azure\/profiles\/__init__.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"PolicyTrackedResourcesOperations.list_query_results_for_management_group","method_code":"def list_query_results_for_management_group(\n            self, management_group_name, query_options=None, custom_headers=None, raw=False, **operation_config):\n        top = None\n        if query_options is not None:\n            top = query_options.top\n        filter = None\n        if query_options is not None:\n            filter = query_options.filter\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                \n                url = self.list_query_results_for_management_group.metadata['url']\n                path_format_arguments = {\n                    'managementGroupsNamespace': self._serialize.url(\"self.management_groups_namespace\", self.management_groups_namespace, 'str'),\n                    'managementGroupName': self._serialize.url(\"management_group_name\", management_group_name, 'str'),\n                    'policyTrackedResourcesResource': self._serialize.url(\"self.policy_tracked_resources_resource\", self.policy_tracked_resources_resource, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                \n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n                if top is not None:\n                    query_parameters['$top'] = self._serialize.query(\"top\", top, 'int', minimum=0)\n                if filter is not None:\n                    query_parameters['$filter'] = self._serialize.query(\"filter\", filter, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            \n            header_parameters = {}\n            header_parameters['Accept'] = 'application\/json'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            \n            request = self._client.post(url, query_parameters, header_parameters)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.QueryFailureException(self._deserialize, response)\n\n            return response\n\n        \n        deserialized = models.PolicyTrackedResourcePaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.PolicyTrackedResourcePaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized","method_summary":"Queries policy tracked resources under the management group.","original_method_code":"def list_query_results_for_management_group(\n            self, management_group_name, query_options=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Queries policy tracked resources under the management group.\n\n        :param management_group_name: Management group name.\n        :type management_group_name: str\n        :param query_options: Additional parameters for the operation\n        :type query_options: ~azure.mgmt.policyinsights.models.QueryOptions\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: An iterator like instance of PolicyTrackedResource\n        :rtype:\n         ~azure.mgmt.policyinsights.models.PolicyTrackedResourcePaged[~azure.mgmt.policyinsights.models.PolicyTrackedResource]\n        :raises:\n         :class:`QueryFailureException<azure.mgmt.policyinsights.models.QueryFailureException>`\n        \"\"\"\n        top = None\n        if query_options is not None:\n            top = query_options.top\n        filter = None\n        if query_options is not None:\n            filter = query_options.filter\n\n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                # Construct URL\n                url = self.list_query_results_for_management_group.metadata['url']\n                path_format_arguments = {\n                    'managementGroupsNamespace': self._serialize.url(\"self.management_groups_namespace\", self.management_groups_namespace, 'str'),\n                    'managementGroupName': self._serialize.url(\"management_group_name\", management_group_name, 'str'),\n                    'policyTrackedResourcesResource': self._serialize.url(\"self.policy_tracked_resources_resource\", self.policy_tracked_resources_resource, 'str')\n                }\n                url = self._client.format_url(url, **path_format_arguments)\n\n                # Construct parameters\n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n                if top is not None:\n                    query_parameters['$top'] = self._serialize.query(\"top\", top, 'int', minimum=0)\n                if filter is not None:\n                    query_parameters['$filter'] = self._serialize.query(\"filter\", filter, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            # Construct headers\n            header_parameters = {}\n            header_parameters['Accept'] = 'application\/json'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            # Construct and send request\n            request = self._client.post(url, query_parameters, header_parameters)\n            response = self._client.send(request, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.QueryFailureException(self._deserialize, response)\n\n            return response\n\n        # Deserialize response\n        deserialized = models.PolicyTrackedResourcePaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.PolicyTrackedResourcePaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized","method_path":"azure-mgmt-policyinsights\/azure\/mgmt\/policyinsights\/operations\/policy_tracked_resources_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusMixin.create_queue","method_code":"def create_queue(\n            self, queue_name,\n            lock_duration=30, max_size_in_megabytes=None,\n            requires_duplicate_detection=False,\n            requires_session=False,\n            default_message_time_to_live=None,\n            dead_lettering_on_message_expiration=False,\n            duplicate_detection_history_time_window=None,\n            max_delivery_count=None, enable_batched_operations=None):\n        queue_properties = Queue(\n            lock_duration=\"PT{}S\".format(int(lock_duration)),\n            max_size_in_megabytes=max_size_in_megabytes,\n            requires_duplicate_detection=requires_duplicate_detection,\n            requires_session=requires_session,\n            default_message_time_to_live=default_message_time_to_live,\n            dead_lettering_on_message_expiration=dead_lettering_on_message_expiration,\n            duplicate_detection_history_time_window=duplicate_detection_history_time_window,\n            max_delivery_count=max_delivery_count,\n            enable_batched_operations=enable_batched_operations)\n        try:\n            return self.mgmt_client.create_queue(queue_name, queue=queue_properties, fail_on_exist=True)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)","method_summary":"Create a queue entity.","original_method_code":"def create_queue(\n            self, queue_name,\n            lock_duration=30, max_size_in_megabytes=None,\n            requires_duplicate_detection=False,\n            requires_session=False,\n            default_message_time_to_live=None,\n            dead_lettering_on_message_expiration=False,\n            duplicate_detection_history_time_window=None,\n            max_delivery_count=None, enable_batched_operations=None):\n        \"\"\"Create a queue entity.\n\n        :param queue_name: The name of the new queue.\n        :type queue_name: str\n        :param lock_duration: The lock durection in seconds for each message in the queue.\n        :type lock_duration: int\n        :param max_size_in_megabytes: The max size to allow the queue to grow to.\n        :type max_size_in_megabytes: int\n        :param requires_duplicate_detection: Whether the queue will require every message with\n         a specified time frame to have a unique ID. Non-unique messages will be discarded.\n         Default value is False.\n        :type requires_duplicate_detection: bool\n        :param requires_session: Whether the queue will be sessionful, and therefore require all\n         message to have a Session ID and be received by a sessionful receiver.\n         Default value is False.\n        :type requires_session: bool\n        :param default_message_time_to_live: The length of time a message will remain in the queue\n         before it is either discarded or moved to the dead letter queue.\n        :type default_message_time_to_live: ~datetime.timedelta\n        :param dead_lettering_on_message_expiration: Whether to move expired messages to the\n         dead letter queue. Default value is False.\n        :type dead_lettering_on_message_expiration: bool\n        :param duplicate_detection_history_time_window: The period within which all incoming messages\n         must have a unique message ID.\n        :type duplicate_detection_history_time_window: ~datetime.timedelta\n        :param max_delivery_count: The maximum number of times a message will attempt to be delivered\n         before it is moved to the dead letter queue.\n        :type max_delivery_count: int\n        :param enable_batched_operations:\n        :type: enable_batched_operations: bool\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n        :raises: ~azure.common.AzureConflictHttpError if a queue of the same name already exists.\n        \"\"\"\n        queue_properties = Queue(\n            lock_duration=\"PT{}S\".format(int(lock_duration)),\n            max_size_in_megabytes=max_size_in_megabytes,\n            requires_duplicate_detection=requires_duplicate_detection,\n            requires_session=requires_session,\n            default_message_time_to_live=default_message_time_to_live,\n            dead_lettering_on_message_expiration=dead_lettering_on_message_expiration,\n            duplicate_detection_history_time_window=duplicate_detection_history_time_window,\n            max_delivery_count=max_delivery_count,\n            enable_batched_operations=enable_batched_operations)\n        try:\n            return self.mgmt_client.create_queue(queue_name, queue=queue_properties, fail_on_exist=True)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)","method_path":"azure-servicebus\/azure\/servicebus\/common\/mixins.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusMixin.delete_queue","method_code":"def delete_queue(self, queue_name, fail_not_exist=False):\n        try:\n            return self.mgmt_client.delete_queue(queue_name, fail_not_exist=fail_not_exist)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except azure.common.AzureMissingResourceHttpError as e:\n            raise ServiceBusResourceNotFound(\"Specificed queue '{}' does not exist.\".format(queue_name), e)","method_summary":"Delete a queue entity.","original_method_code":"def delete_queue(self, queue_name, fail_not_exist=False):\n        \"\"\"Delete a queue entity.\n\n        :param queue_name: The name of the queue to delete.\n        :type queue_name: str\n        :param fail_not_exist: Whether to raise an exception if the named queue is not\n         found. If set to True, a ServiceBusResourceNotFound will be raised.\n         Default value is False.\n        :type fail_not_exist: bool\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namesapce is not found.\n        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the queue is not found\n         and `fail_not_exist` is set to True.\n        \"\"\"\n        try:\n            return self.mgmt_client.delete_queue(queue_name, fail_not_exist=fail_not_exist)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except azure.common.AzureMissingResourceHttpError as e:\n            raise ServiceBusResourceNotFound(\"Specificed queue '{}' does not exist.\".format(queue_name), e)","method_path":"azure-servicebus\/azure\/servicebus\/common\/mixins.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusMixin.create_topic","method_code":"def create_topic(\n            self, topic_name,\n            default_message_time_to_live=None,\n            max_size_in_megabytes=None, requires_duplicate_detection=None,\n            duplicate_detection_history_time_window=None,\n            enable_batched_operations=None):\n        topic_properties = Topic(\n            max_size_in_megabytes=max_size_in_megabytes,\n            requires_duplicate_detection=requires_duplicate_detection,\n            default_message_time_to_live=default_message_time_to_live,\n            duplicate_detection_history_time_window=duplicate_detection_history_time_window,\n            enable_batched_operations=enable_batched_operations)\n        try:\n            return self.mgmt_client.create_topic(topic_name, topic=topic_properties, fail_on_exist=True)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)","method_summary":"Create a topic entity.","original_method_code":"def create_topic(\n            self, topic_name,\n            default_message_time_to_live=None,\n            max_size_in_megabytes=None, requires_duplicate_detection=None,\n            duplicate_detection_history_time_window=None,\n            enable_batched_operations=None):\n        \"\"\"Create a topic entity.\n\n        :param topic_name: The name of the new topic.\n        :type topic_name: str\n        :param max_size_in_megabytes: The max size to allow the topic to grow to.\n        :type max_size_in_megabytes: int\n        :param requires_duplicate_detection: Whether the topic will require every message with\n         a specified time frame to have a unique ID. Non-unique messages will be discarded.\n         Default value is False.\n        :type requires_duplicate_detection: bool\n        :param default_message_time_to_live: The length of time a message will remain in the topic\n         before it is either discarded or moved to the dead letter queue.\n        :type default_message_time_to_live: ~datetime.timedelta\n        :param duplicate_detection_history_time_window: The period within which all incoming messages\n         must have a unique message ID.\n        :type duplicate_detection_history_time_window: ~datetime.timedelta\n        :param enable_batched_operations:\n        :type: enable_batched_operations: bool\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n        :raises: ~azure.common.AzureConflictHttpError if a topic of the same name already exists.\n        \"\"\"\n        topic_properties = Topic(\n            max_size_in_megabytes=max_size_in_megabytes,\n            requires_duplicate_detection=requires_duplicate_detection,\n            default_message_time_to_live=default_message_time_to_live,\n            duplicate_detection_history_time_window=duplicate_detection_history_time_window,\n            enable_batched_operations=enable_batched_operations)\n        try:\n            return self.mgmt_client.create_topic(topic_name, topic=topic_properties, fail_on_exist=True)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)","method_path":"azure-servicebus\/azure\/servicebus\/common\/mixins.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusMixin.delete_topic","method_code":"def delete_topic(self, topic_name, fail_not_exist=False):\n        try:\n            return self.mgmt_client.delete_topic(topic_name, fail_not_exist=fail_not_exist)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except azure.common.AzureMissingResourceHttpError as e:\n            raise ServiceBusResourceNotFound(\"Specificed queue does not exist.\", e)","method_summary":"Delete a topic entity.","original_method_code":"def delete_topic(self, topic_name, fail_not_exist=False):\n        \"\"\"Delete a topic entity.\n\n        :param topic_name: The name of the topic to delete.\n        :type topic_name: str\n        :param fail_not_exist: Whether to raise an exception if the named topic is not\n         found. If set to True, a ServiceBusResourceNotFound will be raised.\n         Default value is False.\n        :type fail_not_exist: bool\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namesapce is not found.\n        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the topic is not found\n         and `fail_not_exist` is set to True.\n        \"\"\"\n        try:\n            return self.mgmt_client.delete_topic(topic_name, fail_not_exist=fail_not_exist)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except azure.common.AzureMissingResourceHttpError as e:\n            raise ServiceBusResourceNotFound(\"Specificed queue does not exist.\", e)","method_path":"azure-servicebus\/azure\/servicebus\/common\/mixins.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusMixin.create_subscription","method_code":"def create_subscription(\n            self, topic_name, subscription_name,\n            lock_duration=30, requires_session=None,\n            default_message_time_to_live=None,\n            dead_lettering_on_message_expiration=None,\n            dead_lettering_on_filter_evaluation_exceptions=None,\n            enable_batched_operations=None, max_delivery_count=None):\n        sub_properties = Subscription(\n            lock_duration=\"PT{}S\".format(int(lock_duration)),\n            requires_session=requires_session,\n            default_message_time_to_live=default_message_time_to_live,\n            dead_lettering_on_message_expiration=dead_lettering_on_message_expiration,\n            dead_lettering_on_filter_evaluation_exceptions=dead_lettering_on_filter_evaluation_exceptions,\n            max_delivery_count=max_delivery_count,\n            enable_batched_operations=enable_batched_operations)\n        try:\n            return self.mgmt_client.create_subscription(\n                topic_name, subscription_name,\n                subscription=sub_properties, fail_on_exist=True)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)","method_summary":"Create a subscription entity.","original_method_code":"def create_subscription(\n            self, topic_name, subscription_name,\n            lock_duration=30, requires_session=None,\n            default_message_time_to_live=None,\n            dead_lettering_on_message_expiration=None,\n            dead_lettering_on_filter_evaluation_exceptions=None,\n            enable_batched_operations=None, max_delivery_count=None):\n        \"\"\"Create a subscription entity.\n\n        :param topic_name: The name of the topic under which to create the subscription.\n        :param subscription_name: The name of the new subscription.\n        :type subscription_name: str\n        :param lock_duration: The lock durection in seconds for each message in the subscription.\n        :type lock_duration: int\n        :param requires_session: Whether the subscription will be sessionful, and therefore require all\n         message to have a Session ID and be received by a sessionful receiver.\n         Default value is False.\n        :type requires_session: bool\n        :param default_message_time_to_live: The length of time a message will remain in the subscription\n         before it is either discarded or moved to the dead letter queue.\n        :type default_message_time_to_live: ~datetime.timedelta\n        :param dead_lettering_on_message_expiration: Whether to move expired messages to the\n         dead letter queue. Default value is False.\n        :type dead_lettering_on_message_expiration: bool\n        :param dead_lettering_on_filter_evaluation_exceptions: Whether to move messages that error on\n         filtering into the dead letter queue. Default is False, and the messages will be discarded.\n        :type dead_lettering_on_filter_evaluation_exceptions: bool\n        :param max_delivery_count: The maximum number of times a message will attempt to be delivered\n         before it is moved to the dead letter queue.\n        :type max_delivery_count: int\n        :param enable_batched_operations:\n        :type: enable_batched_operations: bool\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n        :raises: ~azure.common.AzureConflictHttpError if a queue of the same name already exists.\n        \"\"\"\n        sub_properties = Subscription(\n            lock_duration=\"PT{}S\".format(int(lock_duration)),\n            requires_session=requires_session,\n            default_message_time_to_live=default_message_time_to_live,\n            dead_lettering_on_message_expiration=dead_lettering_on_message_expiration,\n            dead_lettering_on_filter_evaluation_exceptions=dead_lettering_on_filter_evaluation_exceptions,\n            max_delivery_count=max_delivery_count,\n            enable_batched_operations=enable_batched_operations)\n        try:\n            return self.mgmt_client.create_subscription(\n                topic_name, subscription_name,\n                subscription=sub_properties, fail_on_exist=True)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)","method_path":"azure-servicebus\/azure\/servicebus\/common\/mixins.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"BaseClient.from_connection_string","method_code":"def from_connection_string(cls, conn_str, name=None, **kwargs):\n        address, policy, key, entity = parse_conn_str(conn_str)\n        entity = name or entity\n        address = build_uri(address, entity)\n        name = address.split('\/')[-1]\n        return cls(address, name, shared_access_key_name=policy, shared_access_key_value=key, **kwargs)","method_summary":"Create a Client from a Service Bus connection string.","original_method_code":"def from_connection_string(cls, conn_str, name=None, **kwargs):\n        \"\"\"Create a Client from a Service Bus connection string.\n\n        :param conn_str: The connection string.\n        :type conn_str: str\n        :param name: The name of the entity, if the 'EntityName' property is\n         not included in the connection string.\n        \"\"\"\n        address, policy, key, entity = parse_conn_str(conn_str)\n        entity = name or entity\n        address = build_uri(address, entity)\n        name = address.split('\/')[-1]\n        return cls(address, name, shared_access_key_name=policy, shared_access_key_value=key, **kwargs)","method_path":"azure-servicebus\/azure\/servicebus\/common\/mixins.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"BaseClient.get_properties","method_code":"def get_properties(self):\n        try:\n            self.entity = self._get_entity()\n            self.properties = dict(self.entity)\n            if hasattr(self.entity, 'requires_session'):\n                self.requires_session = self.entity.requires_session\n            return self.properties\n        except AzureServiceBusResourceNotFound:\n            raise ServiceBusResourceNotFound(\"Specificed queue does not exist.\")\n        except azure.common.AzureHttpError:\n            self.entity = None\n            self.properties = {}\n            self.requires_session = False\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace not found\", e)","method_summary":"Perform an operation to update the properties of the entity.","original_method_code":"def get_properties(self):\n        \"\"\"Perform an operation to update the properties of the entity.\n\n        :returns: The properties of the entity as a dictionary.\n        :rtype: dict[str, Any]\n        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the entity does not exist.\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the endpoint cannot be reached.\n        :raises: ~azure.common.AzureHTTPError if the credentials are invalid.\n        \"\"\"\n        try:\n            self.entity = self._get_entity()\n            self.properties = dict(self.entity)\n            if hasattr(self.entity, 'requires_session'):\n                self.requires_session = self.entity.requires_session\n            return self.properties\n        except AzureServiceBusResourceNotFound:\n            raise ServiceBusResourceNotFound(\"Specificed queue does not exist.\")\n        except azure.common.AzureHttpError:\n            self.entity = None\n            self.properties = {}\n            self.requires_session = False\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace not found\", e)","method_path":"azure-servicebus\/azure\/servicebus\/common\/mixins.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SessionMixin.expired","method_code":"def expired(self):\n        if self.locked_until and self.locked_until <= datetime.datetime.now():\n            return True\n        return False","method_summary":"Whether the receivers lock on a particular session has expired.","original_method_code":"def expired(self):\n        \"\"\"Whether the receivers lock on a particular session has expired.\n\n        :rtype: bool\n        \"\"\"\n        if self.locked_until and self.locked_until <= datetime.datetime.now():\n            return True\n        return False","method_path":"azure-servicebus\/azure\/servicebus\/common\/mixins.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SessionOperations.create","method_code":"def create(\n            self, resource_group_name, node_name, session, user_name=None, password=None, retention_period=None, credential_data_format=None, encryption_certificate_thumbprint=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._create_initial(\n            resource_group_name=resource_group_name,\n            node_name=node_name,\n            session=session,\n            user_name=user_name,\n            password=password,\n            retention_period=retention_period,\n            credential_data_format=credential_data_format,\n            encryption_certificate_thumbprint=encryption_certificate_thumbprint,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('SessionResource', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Creates a session for a node.","original_method_code":"def create(\n            self, resource_group_name, node_name, session, user_name=None, password=None, retention_period=None, credential_data_format=None, encryption_certificate_thumbprint=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Creates a session for a node.\n\n        :param resource_group_name: The resource group name uniquely\n         identifies the resource group within the user subscriptionId.\n        :type resource_group_name: str\n        :param node_name: The node name (256 characters maximum).\n        :type node_name: str\n        :param session: The sessionId from the user.\n        :type session: str\n        :param user_name: Encrypted User name to be used to connect to node.\n        :type user_name: str\n        :param password: Encrypted Password associated with user name.\n        :type password: str\n        :param retention_period: Session retention period. Possible values\n         include: 'Session', 'Persistent'\n        :type retention_period: str or\n         ~azure.mgmt.servermanager.models.RetentionPeriod\n        :param credential_data_format: Credential data format. Possible values\n         include: 'RsaEncrypted'\n        :type credential_data_format: str or\n         ~azure.mgmt.servermanager.models.CredentialDataFormat\n        :param encryption_certificate_thumbprint: Encryption certificate\n         thumbprint.\n        :type encryption_certificate_thumbprint: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns SessionResource or\n         ClientRawResponse<SessionResource> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.servermanager.models.SessionResource]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.servermanager.models.SessionResource]]\n        :raises:\n         :class:`ErrorException<azure.mgmt.servermanager.models.ErrorException>`\n        \"\"\"\n        raw_result = self._create_initial(\n            resource_group_name=resource_group_name,\n            node_name=node_name,\n            session=session,\n            user_name=user_name,\n            password=password,\n            retention_period=retention_period,\n            credential_data_format=credential_data_format,\n            encryption_certificate_thumbprint=encryption_certificate_thumbprint,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('SessionResource', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-servermanager\/azure\/mgmt\/servermanager\/operations\/session_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SubscriptionFactoryOperations.create_subscription","method_code":"def create_subscription(\n            self, billing_account_name, invoice_section_name, body, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._create_subscription_initial(\n            billing_account_name=billing_account_name,\n            invoice_section_name=invoice_section_name,\n            body=body,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            header_dict = {\n                'Location': 'str',\n                'Retry-After': 'int',\n            }\n            deserialized = self._deserialize('SubscriptionCreationResult', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                client_raw_response.add_headers(header_dict)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Creates an Azure subscription.","original_method_code":"def create_subscription(\n            self, billing_account_name, invoice_section_name, body, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Creates an Azure subscription.\n\n        :param billing_account_name: The name of the commerce root billing\n         account.\n        :type billing_account_name: str\n        :param invoice_section_name: The name of the invoice section.\n        :type invoice_section_name: str\n        :param body: The subscription creation parameters.\n        :type body:\n         ~azure.mgmt.subscription.models.SubscriptionCreationParameters\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns\n         SubscriptionCreationResult or\n         ClientRawResponse<SubscriptionCreationResult> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.subscription.models.SubscriptionCreationResult]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.subscription.models.SubscriptionCreationResult]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.subscription.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._create_subscription_initial(\n            billing_account_name=billing_account_name,\n            invoice_section_name=invoice_section_name,\n            body=body,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            header_dict = {\n                'Location': 'str',\n                'Retry-After': 'int',\n            }\n            deserialized = self._deserialize('SubscriptionCreationResult', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                client_raw_response.add_headers(header_dict)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-subscription\/azure\/mgmt\/subscription\/operations\/subscription_factory_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"LogAnalyticsOperations.export_request_rate_by_interval","method_code":"def export_request_rate_by_interval(\n            self, parameters, location, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._export_request_rate_by_interval_initial(\n            parameters=parameters,\n            location=location,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('LogAnalyticsOperationResult', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, lro_options={'final-state-via': 'azure-async-operation'}, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Export logs that show Api requests made by this subscription in the given time window to show throttling activities.","original_method_code":"def export_request_rate_by_interval(\n            self, parameters, location, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Export logs that show Api requests made by this subscription in the\n        given time window to show throttling activities.\n\n        :param parameters: Parameters supplied to the LogAnalytics\n         getRequestRateByInterval Api.\n        :type parameters:\n         ~azure.mgmt.compute.v2018_04_01.models.RequestRateByIntervalInput\n        :param location: The location upon which virtual-machine-sizes is\n         queried.\n        :type location: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns\n         LogAnalyticsOperationResult or\n         ClientRawResponse<LogAnalyticsOperationResult> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.compute.v2018_04_01.models.LogAnalyticsOperationResult]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.compute.v2018_04_01.models.LogAnalyticsOperationResult]]\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._export_request_rate_by_interval_initial(\n            parameters=parameters,\n            location=location,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('LogAnalyticsOperationResult', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, lro_options={'final-state-via': 'azure-async-operation'}, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-compute\/azure\/mgmt\/compute\/v2018_04_01\/operations\/log_analytics_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_handle_output","method_code":"def _handle_output(results_queue):\n    results = []\n    while results_queue:\n        queue_item = results_queue.pop()\n        results.append(queue_item)\n    return results","method_summary":"Scan output for exceptions If there is an output from an add task collection call add it to the results.","original_method_code":"def _handle_output(results_queue):\n    \"\"\"Scan output for exceptions\n\n    If there is an output from an add task collection call add it to the results.\n\n    :param results_queue: Queue containing results of attempted add_collection's\n    :type results_queue: collections.deque\n    :return: list of TaskAddResults\n    :rtype: list[~TaskAddResult]\n    \"\"\"\n    results = []\n    while results_queue:\n        queue_item = results_queue.pop()\n        results.append(queue_item)\n    return results","method_path":"azure-batch\/azure\/batch\/custom\/patch.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_TaskWorkflowManager._bulk_add_tasks","method_code":"def _bulk_add_tasks(self, results_queue, chunk_tasks_to_add):\n        try:\n            add_collection_response = self._original_add_collection(\n                self._client,\n                self._job_id,\n                chunk_tasks_to_add,\n                self._task_add_collection_options,\n                self._custom_headers,\n                self._raw)\n        except BatchErrorException as e:\n            \n            \n            \n            if e.error.code == \"RequestBodyTooLarge\":  \n                \n                \n                \n                \n                if len(chunk_tasks_to_add) == 1:\n                    failed_task = chunk_tasks_to_add.pop()\n                    self.errors.appendleft(e)\n                    _LOGGER.error(\"Failed to add task with ID %s due to the body\"\n                                  \" exceeding the maximum request size\", failed_task.id)\n                else:\n                    \n                    \n                    \n                    midpoint = int(len(chunk_tasks_to_add) \/ 2)\n                    \n                    \n                    with self._max_tasks_lock:\n                        if midpoint < self._max_tasks_per_request:\n                            self._max_tasks_per_request = midpoint\n                            _LOGGER.info(\"Amount of tasks per request reduced from %s to %s due to the\"\n                                         \" request body being too large\", str(self._max_tasks_per_request),\n                                         str(midpoint))\n\n                    \n                    \n                    \n                    \n                    self.tasks_to_add.extendleft(chunk_tasks_to_add[midpoint:])\n                    self._bulk_add_tasks(results_queue, chunk_tasks_to_add[:midpoint])\n            \n            elif 500 <= e.response.status_code <= 599:\n                self.tasks_to_add.extendleft(chunk_tasks_to_add)\n            else:\n                \n                self.tasks_to_add.extendleft(chunk_tasks_to_add)\n                \n                self.errors.appendleft(e)\n        except Exception as e:  \n            \n            self.tasks_to_add.extendleft(chunk_tasks_to_add)\n            \n            self.errors.appendleft(e)\n        else:\n            try:\n                add_collection_response = add_collection_response.output\n            except AttributeError:\n                pass\n\n            for task_result in add_collection_response.value:  \n                if task_result.status == TaskAddStatus.server_error:\n                    \n                    with self._pending_queue_lock:\n                        for task in chunk_tasks_to_add:\n                            if task.id == task_result.task_id:\n                                self.tasks_to_add.appendleft(task)\n                elif (task_result.status == TaskAddStatus.client_error\n                        and not task_result.error.code == \"TaskExists\"):\n                    \n                    self.failure_tasks.appendleft(task_result)\n                else:\n                    results_queue.appendleft(task_result)","method_summary":"Adds a chunk of tasks to the job Retry chunk if body exceeds the maximum request size and retry tasks if failed due to server errors.","original_method_code":"def _bulk_add_tasks(self, results_queue, chunk_tasks_to_add):\n        \"\"\"Adds a chunk of tasks to the job\n\n        Retry chunk if body exceeds the maximum request size and retry tasks\n        if failed due to server errors.\n\n        :param results_queue: Queue to place the return value of the request\n        :type results_queue: collections.deque\n        :param chunk_tasks_to_add: Chunk of at most 100 tasks with retry details\n        :type chunk_tasks_to_add: list[~TrackedCloudTask]\n        \"\"\"\n\n        try:\n            add_collection_response = self._original_add_collection(\n                self._client,\n                self._job_id,\n                chunk_tasks_to_add,\n                self._task_add_collection_options,\n                self._custom_headers,\n                self._raw)\n        except BatchErrorException as e:\n            # In case of a chunk exceeding the MaxMessageSize split chunk in half\n            # and resubmit smaller chunk requests\n            # TODO: Replace string with constant variable once available in SDK\n            if e.error.code == \"RequestBodyTooLarge\":  # pylint: disable=no-member\n                # In this case the task is misbehaved and will not be able to be added due to:\n                #   1) The task exceeding the max message size\n                #   2) A single cell of the task exceeds the per-cell limit, or\n                #   3) Sum of all cells exceeds max row limit\n                if len(chunk_tasks_to_add) == 1:\n                    failed_task = chunk_tasks_to_add.pop()\n                    self.errors.appendleft(e)\n                    _LOGGER.error(\"Failed to add task with ID %s due to the body\"\n                                  \" exceeding the maximum request size\", failed_task.id)\n                else:\n                    # Assumption: Tasks are relatively close in size therefore if one batch exceeds size limit\n                    # we should decrease the initial task collection size to avoid repeating the error\n                    # Midpoint is lower bounded by 1 due to above base case\n                    midpoint = int(len(chunk_tasks_to_add) \/ 2)\n                    # Restrict one thread at a time to do this compare and set,\n                    # therefore forcing max_tasks_per_request to be strictly decreasing\n                    with self._max_tasks_lock:\n                        if midpoint < self._max_tasks_per_request:\n                            self._max_tasks_per_request = midpoint\n                            _LOGGER.info(\"Amount of tasks per request reduced from %s to %s due to the\"\n                                         \" request body being too large\", str(self._max_tasks_per_request),\n                                         str(midpoint))\n\n                    # Not the most efficient solution for all cases, but the goal of this is to handle this\n                    # exception and have it work in all cases where tasks are well behaved\n                    # Behavior retries as a smaller chunk and\n                    # appends extra tasks to queue to be picked up by another thread .\n                    self.tasks_to_add.extendleft(chunk_tasks_to_add[midpoint:])\n                    self._bulk_add_tasks(results_queue, chunk_tasks_to_add[:midpoint])\n            # Retry server side errors\n            elif 500 <= e.response.status_code <= 599:\n                self.tasks_to_add.extendleft(chunk_tasks_to_add)\n            else:\n                # Re-add to pending queue as unknown status \/ don't have result\n                self.tasks_to_add.extendleft(chunk_tasks_to_add)\n                # Unknown State - don't know if tasks failed to add or were successful\n                self.errors.appendleft(e)\n        except Exception as e:  # pylint: disable=broad-except\n            # Re-add to pending queue as unknown status \/ don't have result\n            self.tasks_to_add.extendleft(chunk_tasks_to_add)\n            # Unknown State - don't know if tasks failed to add or were successful\n            self.errors.appendleft(e)\n        else:\n            try:\n                add_collection_response = add_collection_response.output\n            except AttributeError:\n                pass\n\n            for task_result in add_collection_response.value:  # pylint: disable=no-member\n                if task_result.status == TaskAddStatus.server_error:\n                    # Server error will be retried\n                    with self._pending_queue_lock:\n                        for task in chunk_tasks_to_add:\n                            if task.id == task_result.task_id:\n                                self.tasks_to_add.appendleft(task)\n                elif (task_result.status == TaskAddStatus.client_error\n                        and not task_result.error.code == \"TaskExists\"):\n                    # Client error will be recorded unless Task already exists\n                    self.failure_tasks.appendleft(task_result)\n                else:\n                    results_queue.appendleft(task_result)","method_path":"azure-batch\/azure\/batch\/custom\/patch.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_TaskWorkflowManager.task_collection_thread_handler","method_code":"def task_collection_thread_handler(self, results_queue):\n        \n        while self.tasks_to_add and not self.errors:\n            max_tasks = self._max_tasks_per_request  \n            chunk_tasks_to_add = []\n            with self._pending_queue_lock:\n                while len(chunk_tasks_to_add) < max_tasks and self.tasks_to_add:\n                    chunk_tasks_to_add.append(self.tasks_to_add.pop())\n\n            if chunk_tasks_to_add:\n                self._bulk_add_tasks(results_queue, chunk_tasks_to_add)","method_summary":"Main method for worker to run Pops a chunk of tasks off the collection of pending tasks to be added and submits them to be added.","original_method_code":"def task_collection_thread_handler(self, results_queue):\n        \"\"\"Main method for worker to run\n\n        Pops a chunk of tasks off the collection of pending tasks to be added and submits them to be added.\n\n        :param collections.deque results_queue: Queue for worker to output results to\n        \"\"\"\n        # Add tasks until either we run out or we run into an unexpected error\n        while self.tasks_to_add and not self.errors:\n            max_tasks = self._max_tasks_per_request  # local copy\n            chunk_tasks_to_add = []\n            with self._pending_queue_lock:\n                while len(chunk_tasks_to_add) < max_tasks and self.tasks_to_add:\n                    chunk_tasks_to_add.append(self.tasks_to_add.pop())\n\n            if chunk_tasks_to_add:\n                self._bulk_add_tasks(results_queue, chunk_tasks_to_add)","method_path":"azure-batch\/azure\/batch\/custom\/patch.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"build_config","method_code":"def build_config(config : Dict[str, Any]) -> Dict[str, str]:\n    result = config.copy()\n    \n    is_stable = result.pop(\"is_stable\", False)\n    if is_stable:\n        result[\"classifier\"] = \"Development Status :: 5 - Production\/Stable\"\n    else:\n        result[\"classifier\"] = \"Development Status :: 4 - Beta\"\n    \n    package_name = result[\"package_name\"]\n    result[\"package_nspkg\"] = result.pop(\n        \"package_nspkg\",\n        package_name[:package_name.rindex('-')]+\"-nspkg\"\n    )\n    \n    result['is_arm'] = result.pop(\"is_arm\", True)\n\n    \n    result['need_msrestazure'] = result.pop(\"need_msrestazure\", True)\n\n    \n    package_parts = result[\"package_nspkg\"][:-len('-nspkg')].split('-')\n    result['nspkg_names'] = [\n        \".\".join(package_parts[:i+1])\n        for i in range(len(package_parts))\n    ]\n    result['init_names'] = [\n        \"\/\".join(package_parts[:i+1])+\"\/__init__.py\"\n        for i in range(len(package_parts))\n    ]\n\n    \n    return result","method_summary":"Will build the actual config for Jinja2, based on SDK config.","original_method_code":"def build_config(config : Dict[str, Any]) -> Dict[str, str]:\n    \"\"\"Will build the actual config for Jinja2, based on SDK config.\n    \"\"\"\n    result = config.copy()\n    # Manage the classifier stable\/beta\n    is_stable = result.pop(\"is_stable\", False)\n    if is_stable:\n        result[\"classifier\"] = \"Development Status :: 5 - Production\/Stable\"\n    else:\n        result[\"classifier\"] = \"Development Status :: 4 - Beta\"\n    # Manage the nspkg\n    package_name = result[\"package_name\"]\n    result[\"package_nspkg\"] = result.pop(\n        \"package_nspkg\",\n        package_name[:package_name.rindex('-')]+\"-nspkg\"\n    )\n    # ARM?\n    result['is_arm'] = result.pop(\"is_arm\", True)\n\n    # Do I need msrestazure for this package?\n    result['need_msrestazure'] = result.pop(\"need_msrestazure\", True)\n\n    # Pre-compute some Jinja variable that are complicated to do inside the templates\n    package_parts = result[\"package_nspkg\"][:-len('-nspkg')].split('-')\n    result['nspkg_names'] = [\n        \".\".join(package_parts[:i+1])\n        for i in range(len(package_parts))\n    ]\n    result['init_names'] = [\n        \"\/\".join(package_parts[:i+1])+\"\/__init__.py\"\n        for i in range(len(package_parts))\n    ]\n\n    # Return result\n    return result","method_path":"azure-sdk-tools\/packaging_tools\/__init__.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"GlobalUsersOperations.reset_password","method_code":"def reset_password(\n            self, user_name, reset_password_payload, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._reset_password_initial(\n            user_name=user_name,\n            reset_password_payload=reset_password_payload,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Resets the user password on an environment This operation can take a while to complete.","original_method_code":"def reset_password(\n            self, user_name, reset_password_payload, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Resets the user password on an environment This operation can take a\n        while to complete.\n\n        :param user_name: The name of the user.\n        :type user_name: str\n        :param reset_password_payload: Represents the payload for resetting\n         passwords.\n        :type reset_password_payload:\n         ~azure.mgmt.labservices.models.ResetPasswordPayload\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._reset_password_initial(\n            user_name=user_name,\n            reset_password_payload=reset_password_payload,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-labservices\/azure\/mgmt\/labservices\/operations\/global_users_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"GlobalUsersOperations.start_environment","method_code":"def start_environment(\n            self, user_name, environment_id, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._start_environment_initial(\n            user_name=user_name,\n            environment_id=environment_id,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Starts an environment by starting all resources inside the environment. This operation can take a while to complete.","original_method_code":"def start_environment(\n            self, user_name, environment_id, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Starts an environment by starting all resources inside the environment.\n        This operation can take a while to complete.\n\n        :param user_name: The name of the user.\n        :type user_name: str\n        :param environment_id: The resourceId of the environment\n        :type environment_id: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._start_environment_initial(\n            user_name=user_name,\n            environment_id=environment_id,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-labservices\/azure\/mgmt\/labservices\/operations\/global_users_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_create_message","method_code":"def _create_message(response, service_instance):\n    respbody = response.body\n    custom_properties = {}\n    broker_properties = None\n    message_type = None\n    message_location = None\n\n    \n    for name, value in response.headers:\n        if name.lower() == 'brokerproperties':\n            broker_properties = json.loads(value)\n        elif name.lower() == 'content-type':\n            message_type = value\n        elif name.lower() == 'location':\n            message_location = value\n        \n        \n        \n        elif name.lower() not in ['transfer-encoding',\n                                  'server',\n                                  'date',\n                                  'strict-transport-security']:\n            \n            \n            if '\"' in value:\n                value = value[1:-1].replace('\\\\\"', '\"')\n                try:\n                    custom_properties[name] = datetime.strptime(\n                        value, '%a, %d %b %Y %H:%M:%S GMT')\n                except ValueError:\n                    custom_properties[name] = value\n            elif value.lower() == 'true':\n                custom_properties[name] = True\n            elif value.lower() == 'false':\n                custom_properties[name] = False\n            else:  \n                try:\n                    \n                    float_value = float(value)\n                    if str(int(float_value)) == value:\n                        custom_properties[name] = int(value)\n                    else:\n                        custom_properties[name] = float_value\n                except ValueError:\n                    \n                    \n                    \n                    pass\n\n    if message_type is None:\n        message = Message(\n            respbody, service_instance, message_location, custom_properties,\n            'application\/atom+xml;type=entry;charset=utf-8', broker_properties)\n    else:\n        message = Message(respbody, service_instance, message_location,\n                          custom_properties, message_type, broker_properties)\n    return message","method_summary":"Create message from response.","original_method_code":"def _create_message(response, service_instance):\n    ''' Create message from response.\n\n    response:\n        response from Service Bus cloud server.\n    service_instance:\n        the Service Bus client.\n    '''\n    respbody = response.body\n    custom_properties = {}\n    broker_properties = None\n    message_type = None\n    message_location = None\n\n    # gets all information from respheaders.\n    for name, value in response.headers:\n        if name.lower() == 'brokerproperties':\n            broker_properties = json.loads(value)\n        elif name.lower() == 'content-type':\n            message_type = value\n        elif name.lower() == 'location':\n            message_location = value\n        # Exclude common HTTP headers to avoid noise. List\n        # is not exhaustive. At worst, custom properties will contains\n        # an unexpected content generated by the webserver and not the customer.\n        elif name.lower() not in ['transfer-encoding',\n                                  'server',\n                                  'date',\n                                  'strict-transport-security']:\n            # Follow the spec:\n            # https:\/\/docs.microsoft.com\/rest\/api\/servicebus\/message-headers-and-properties\n            if '\"' in value:\n                value = value[1:-1].replace('\\\\\"', '\"')\n                try:\n                    custom_properties[name] = datetime.strptime(\n                        value, '%a, %d %b %Y %H:%M:%S GMT')\n                except ValueError:\n                    custom_properties[name] = value\n            elif value.lower() == 'true':\n                custom_properties[name] = True\n            elif value.lower() == 'false':\n                custom_properties[name] = False\n            else:  # in theory, only int or float\n                try:\n                    # int('3.1') doesn't work so need to get float('3.14') first\n                    float_value = float(value)\n                    if str(int(float_value)) == value:\n                        custom_properties[name] = int(value)\n                    else:\n                        custom_properties[name] = float_value\n                except ValueError:\n                    # If we are here, this header does not respect the spec.\n                    # Could be an unexpected HTTP header or an invalid\n                    # header value. In both case we ignore without failing.\n                    pass\n\n    if message_type is None:\n        message = Message(\n            respbody, service_instance, message_location, custom_properties,\n            'application\/atom+xml;type=entry;charset=utf-8', broker_properties)\n    else:\n        message = Message(respbody, service_instance, message_location,\n                          custom_properties, message_type, broker_properties)\n    return message","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_convert_etree_element_to_rule","method_code":"def _convert_etree_element_to_rule(entry_element):\n    rule = Rule()\n\n    rule_element = entry_element.find('.\/atom:content\/sb:RuleDescription', _etree_sb_feed_namespaces)\n    if rule_element is not None:\n        filter_element = rule_element.find('.\/sb:Filter', _etree_sb_feed_namespaces)\n        if filter_element is not None:\n            rule.filter_type = filter_element.attrib.get(\n                _make_etree_ns_attr_name(_etree_sb_feed_namespaces['i'], 'type'), None)\n            sql_exp_element = filter_element.find('.\/sb:SqlExpression', _etree_sb_feed_namespaces)\n            if sql_exp_element is not None:\n                rule.filter_expression = sql_exp_element.text\n\n        action_element = rule_element.find('.\/sb:Action', _etree_sb_feed_namespaces)\n        if action_element is not None:\n            rule.action_type = action_element.attrib.get(\n                _make_etree_ns_attr_name(_etree_sb_feed_namespaces['i'], 'type'), None)\n            sql_exp_element = action_element.find('.\/sb:SqlExpression', _etree_sb_feed_namespaces)\n            if sql_exp_element is not None:\n                rule.action_expression = sql_exp_element.text\n\n\n    \n    for name, value in _ETreeXmlToObject.get_entry_properties_from_element(\n            entry_element, True, '\/rules').items():\n        setattr(rule, name, value)\n\n    return rule","method_summary":"Converts entry element to rule object. The format of xml for","original_method_code":"def _convert_etree_element_to_rule(entry_element):\n    ''' Converts entry element to rule object.\n\n    The format of xml for rule:\n<entry xmlns='http:\/\/www.w3.org\/2005\/Atom'>\n<content type='application\/xml'>\n<RuleDescription\n    xmlns:i=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\"\n    xmlns=\"http:\/\/schemas.microsoft.com\/netservices\/2010\/10\/servicebus\/connect\">\n    <Filter i:type=\"SqlFilterExpression\">\n        <SqlExpression>MyProperty='XYZ'<\/SqlExpression>\n    <\/Filter>\n    <Action i:type=\"SqlFilterAction\">\n        <SqlExpression>set MyProperty2 = 'ABC'<\/SqlExpression>\n    <\/Action>\n<\/RuleDescription>\n<\/content>\n<\/entry>\n    '''\n    rule = Rule()\n\n    rule_element = entry_element.find('.\/atom:content\/sb:RuleDescription', _etree_sb_feed_namespaces)\n    if rule_element is not None:\n        filter_element = rule_element.find('.\/sb:Filter', _etree_sb_feed_namespaces)\n        if filter_element is not None:\n            rule.filter_type = filter_element.attrib.get(\n                _make_etree_ns_attr_name(_etree_sb_feed_namespaces['i'], 'type'), None)\n            sql_exp_element = filter_element.find('.\/sb:SqlExpression', _etree_sb_feed_namespaces)\n            if sql_exp_element is not None:\n                rule.filter_expression = sql_exp_element.text\n\n        action_element = rule_element.find('.\/sb:Action', _etree_sb_feed_namespaces)\n        if action_element is not None:\n            rule.action_type = action_element.attrib.get(\n                _make_etree_ns_attr_name(_etree_sb_feed_namespaces['i'], 'type'), None)\n            sql_exp_element = action_element.find('.\/sb:SqlExpression', _etree_sb_feed_namespaces)\n            if sql_exp_element is not None:\n                rule.action_expression = sql_exp_element.text\n\n\n    # extract id, updated and name value from feed entry and set them of rule.\n    for name, value in _ETreeXmlToObject.get_entry_properties_from_element(\n            entry_element, True, '\/rules').items():\n        setattr(rule, name, value)\n\n    return rule","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_convert_etree_element_to_queue","method_code":"def _convert_etree_element_to_queue(entry_element):\n    queue = Queue()\n\n    \n    \n    invalid_queue = True\n\n    queue_element = entry_element.find('.\/atom:content\/sb:QueueDescription', _etree_sb_feed_namespaces)\n    if queue_element is not None:\n        mappings = [\n            ('LockDuration', 'lock_duration', None),\n            ('MaxSizeInMegabytes', 'max_size_in_megabytes', int),\n            ('RequiresDuplicateDetection', 'requires_duplicate_detection', _parse_bool),\n            ('RequiresSession', 'requires_session', _parse_bool),\n            ('DefaultMessageTimeToLive', 'default_message_time_to_live', None),\n            ('DeadLetteringOnMessageExpiration', 'dead_lettering_on_message_expiration', _parse_bool),\n            ('DuplicateDetectionHistoryTimeWindow', 'duplicate_detection_history_time_window', None),\n            ('EnableBatchedOperations', 'enable_batched_operations', _parse_bool),\n            ('MaxDeliveryCount', 'max_delivery_count', int),\n            ('MessageCount', 'message_count', int),\n            ('SizeInBytes', 'size_in_bytes', int),\n        ]\n\n        for mapping in mappings:\n            if _read_etree_element(queue_element, mapping[0], queue, mapping[1], mapping[2]):\n                invalid_queue = False\n\n    if invalid_queue:\n        raise AzureServiceBusResourceNotFound(_ERROR_QUEUE_NOT_FOUND)\n\n    \n    for name, value in _ETreeXmlToObject.get_entry_properties_from_element(\n            entry_element, True).items():\n        setattr(queue, name, value)\n\n    return queue","method_summary":"Converts entry element to queue object. The format of xml response for","original_method_code":"def _convert_etree_element_to_queue(entry_element):\n    ''' Converts entry element to queue object.\n\n    The format of xml response for queue:\n<QueueDescription\n    xmlns=\\\"http:\/\/schemas.microsoft.com\/netservices\/2010\/10\/servicebus\/connect\\\">\n    <MaxSizeInBytes>10000<\/MaxSizeInBytes>\n    <DefaultMessageTimeToLive>PT5M<\/DefaultMessageTimeToLive>\n    <LockDuration>PT2M<\/LockDuration>\n    <RequiresGroupedReceives>False<\/RequiresGroupedReceives>\n    <SupportsDuplicateDetection>False<\/SupportsDuplicateDetection>\n    ...\n<\/QueueDescription>\n\n    '''\n    queue = Queue()\n\n    # get node for each attribute in Queue class, if nothing found then the\n    # response is not valid xml for Queue.\n    invalid_queue = True\n\n    queue_element = entry_element.find('.\/atom:content\/sb:QueueDescription', _etree_sb_feed_namespaces)\n    if queue_element is not None:\n        mappings = [\n            ('LockDuration', 'lock_duration', None),\n            ('MaxSizeInMegabytes', 'max_size_in_megabytes', int),\n            ('RequiresDuplicateDetection', 'requires_duplicate_detection', _parse_bool),\n            ('RequiresSession', 'requires_session', _parse_bool),\n            ('DefaultMessageTimeToLive', 'default_message_time_to_live', None),\n            ('DeadLetteringOnMessageExpiration', 'dead_lettering_on_message_expiration', _parse_bool),\n            ('DuplicateDetectionHistoryTimeWindow', 'duplicate_detection_history_time_window', None),\n            ('EnableBatchedOperations', 'enable_batched_operations', _parse_bool),\n            ('MaxDeliveryCount', 'max_delivery_count', int),\n            ('MessageCount', 'message_count', int),\n            ('SizeInBytes', 'size_in_bytes', int),\n        ]\n\n        for mapping in mappings:\n            if _read_etree_element(queue_element, mapping[0], queue, mapping[1], mapping[2]):\n                invalid_queue = False\n\n    if invalid_queue:\n        raise AzureServiceBusResourceNotFound(_ERROR_QUEUE_NOT_FOUND)\n\n    # extract id, updated and name value from feed entry and set them of queue.\n    for name, value in _ETreeXmlToObject.get_entry_properties_from_element(\n            entry_element, True).items():\n        setattr(queue, name, value)\n\n    return queue","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_convert_etree_element_to_topic","method_code":"def _convert_etree_element_to_topic(entry_element):\n    topic = Topic()\n\n    invalid_topic = True\n\n    topic_element = entry_element.find('.\/atom:content\/sb:TopicDescription', _etree_sb_feed_namespaces)\n    if topic_element is not None:\n        mappings = [\n            ('DefaultMessageTimeToLive', 'default_message_time_to_live', None),\n            ('MaxSizeInMegabytes', 'max_size_in_megabytes', int),\n            ('RequiresDuplicateDetection', 'requires_duplicate_detection', _parse_bool),\n            ('DuplicateDetectionHistoryTimeWindow', 'duplicate_detection_history_time_window', None),\n            ('EnableBatchedOperations', 'enable_batched_operations', _parse_bool),\n            ('SizeInBytes', 'size_in_bytes', int),\n        ]\n\n        for mapping in mappings:\n            if _read_etree_element(topic_element, mapping[0], topic, mapping[1], mapping[2]):\n                invalid_topic = False\n\n    if invalid_topic:\n        raise AzureServiceBusResourceNotFound(_ERROR_TOPIC_NOT_FOUND)\n\n    \n    for name, value in _ETreeXmlToObject.get_entry_properties_from_element(\n            entry_element, True).items():\n        setattr(topic, name, value)\n\n    return topic","method_summary":"Converts entry element to topic The xml format for","original_method_code":"def _convert_etree_element_to_topic(entry_element):\n    '''Converts entry element to topic\n\n    The xml format for topic:\n<entry xmlns='http:\/\/www.w3.org\/2005\/Atom'>\n    <content type='application\/xml'>\n    <TopicDescription\n        xmlns:i=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\"\n        xmlns=\"http:\/\/schemas.microsoft.com\/netservices\/2010\/10\/servicebus\/connect\">\n        <DefaultMessageTimeToLive>P10675199DT2H48M5.4775807S<\/DefaultMessageTimeToLive>\n        <MaxSizeInMegabytes>1024<\/MaxSizeInMegabytes>\n        <RequiresDuplicateDetection>false<\/RequiresDuplicateDetection>\n        <DuplicateDetectionHistoryTimeWindow>P7D<\/DuplicateDetectionHistoryTimeWindow>\n        <DeadLetteringOnFilterEvaluationExceptions>true<\/DeadLetteringOnFilterEvaluationExceptions>\n    <\/TopicDescription>\n    <\/content>\n<\/entry>\n    '''\n    topic = Topic()\n\n    invalid_topic = True\n\n    topic_element = entry_element.find('.\/atom:content\/sb:TopicDescription', _etree_sb_feed_namespaces)\n    if topic_element is not None:\n        mappings = [\n            ('DefaultMessageTimeToLive', 'default_message_time_to_live', None),\n            ('MaxSizeInMegabytes', 'max_size_in_megabytes', int),\n            ('RequiresDuplicateDetection', 'requires_duplicate_detection', _parse_bool),\n            ('DuplicateDetectionHistoryTimeWindow', 'duplicate_detection_history_time_window', None),\n            ('EnableBatchedOperations', 'enable_batched_operations', _parse_bool),\n            ('SizeInBytes', 'size_in_bytes', int),\n        ]\n\n        for mapping in mappings:\n            if _read_etree_element(topic_element, mapping[0], topic, mapping[1], mapping[2]):\n                invalid_topic = False\n\n    if invalid_topic:\n        raise AzureServiceBusResourceNotFound(_ERROR_TOPIC_NOT_FOUND)\n\n    # extract id, updated and name value from feed entry and set them of topic.\n    for name, value in _ETreeXmlToObject.get_entry_properties_from_element(\n            entry_element, True).items():\n        setattr(topic, name, value)\n\n    return topic","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_convert_etree_element_to_subscription","method_code":"def _convert_etree_element_to_subscription(entry_element):\n    subscription = Subscription()\n\n    subscription_element = entry_element.find('.\/atom:content\/sb:SubscriptionDescription', _etree_sb_feed_namespaces)\n    if subscription_element is not None:\n        mappings = [\n            ('LockDuration', 'lock_duration', None),\n            ('RequiresSession', 'requires_session', _parse_bool),\n            ('DefaultMessageTimeToLive', 'default_message_time_to_live', None),\n            ('DeadLetteringOnFilterEvaluationExceptions', 'dead_lettering_on_filter_evaluation_exceptions', _parse_bool),  \n            ('DeadLetteringOnMessageExpiration', 'dead_lettering_on_message_expiration', _parse_bool),\n            ('EnableBatchedOperations', 'enable_batched_operations', _parse_bool),\n            ('MaxDeliveryCount', 'max_delivery_count', int),\n            ('MessageCount', 'message_count', int),\n        ]\n\n        for mapping in mappings:\n            _read_etree_element(subscription_element, mapping[0], subscription, mapping[1], mapping[2])\n\n    for name, value in _ETreeXmlToObject.get_entry_properties_from_element(\n            entry_element, True, '\/subscriptions').items():\n        setattr(subscription, name, value)\n\n    return subscription","method_summary":"Converts entry element to subscription The xml format for","original_method_code":"def _convert_etree_element_to_subscription(entry_element):\n    '''Converts entry element to subscription\n\n    The xml format for subscription:\n<entry xmlns='http:\/\/www.w3.org\/2005\/Atom'>\n    <content type='application\/xml'>\n    <SubscriptionDescription\n        xmlns:i=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\"\n        xmlns=\"http:\/\/schemas.microsoft.com\/netservices\/2010\/10\/servicebus\/connect\">\n        <LockDuration>PT5M<\/LockDuration>\n        <RequiresSession>false<\/RequiresSession>\n        <DefaultMessageTimeToLive>P10675199DT2H48M5.4775807S<\/DefaultMessageTimeToLive>\n        <DeadLetteringOnMessageExpiration>false<\/DeadLetteringOnMessageExpiration>\n        <DeadLetteringOnFilterEvaluationExceptions>true<\/DeadLetteringOnFilterEvaluationExceptions>\n    <\/SubscriptionDescription>\n    <\/content>\n<\/entry>\n    '''\n    subscription = Subscription()\n\n    subscription_element = entry_element.find('.\/atom:content\/sb:SubscriptionDescription', _etree_sb_feed_namespaces)\n    if subscription_element is not None:\n        mappings = [\n            ('LockDuration', 'lock_duration', None),\n            ('RequiresSession', 'requires_session', _parse_bool),\n            ('DefaultMessageTimeToLive', 'default_message_time_to_live', None),\n            ('DeadLetteringOnFilterEvaluationExceptions', 'dead_lettering_on_filter_evaluation_exceptions', _parse_bool),  # pylint: disable=line-too-long\n            ('DeadLetteringOnMessageExpiration', 'dead_lettering_on_message_expiration', _parse_bool),\n            ('EnableBatchedOperations', 'enable_batched_operations', _parse_bool),\n            ('MaxDeliveryCount', 'max_delivery_count', int),\n            ('MessageCount', 'message_count', int),\n        ]\n\n        for mapping in mappings:\n            _read_etree_element(subscription_element, mapping[0], subscription, mapping[1], mapping[2])\n\n    for name, value in _ETreeXmlToObject.get_entry_properties_from_element(\n            entry_element, True, '\/subscriptions').items():\n        setattr(subscription, name, value)\n\n    return subscription","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"CertificateOperations.create","method_code":"def create(\n            self, resource_group_name, account_name, certificate_name, parameters, if_match=None, if_none_match=None, custom_headers=None, raw=False, **operation_config):\n        raw_result = self._create_initial(\n            resource_group_name=resource_group_name,\n            account_name=account_name,\n            certificate_name=certificate_name,\n            parameters=parameters,\n            if_match=if_match,\n            if_none_match=if_none_match,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n        if raw:\n            return raw_result\n\n        \n        def long_running_send():\n            return raw_result.response\n\n        def get_long_running_status(status_link, headers=None):\n\n            request = self._client.get(status_link)\n            if headers:\n                request.headers.update(headers)\n            header_parameters = {}\n            header_parameters['x-ms-client-request-id'] = raw_result.response.request.headers['x-ms-client-request-id']\n            return self._client.send(\n                request, header_parameters, stream=False, **operation_config)\n\n        def get_long_running_output(response):\n\n            if response.status_code not in [200]:\n                exp = CloudError(response)\n                exp.request_id = response.headers.get('x-ms-request-id')\n                raise exp\n\n            header_dict = {\n                'ETag': 'str',\n            }\n            deserialized = self._deserialize('Certificate', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                client_raw_response.add_headers(header_dict)\n                return client_raw_response\n\n            return deserialized\n\n        long_running_operation_timeout = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        return AzureOperationPoller(\n            long_running_send, get_long_running_output,\n            get_long_running_status, long_running_operation_timeout)","method_summary":"Creates a new certificate inside the specified account.","original_method_code":"def create(\n            self, resource_group_name, account_name, certificate_name, parameters, if_match=None, if_none_match=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Creates a new certificate inside the specified account.\n\n        :param resource_group_name: The name of the resource group that\n         contains the Batch account.\n        :type resource_group_name: str\n        :param account_name: The name of the Batch account.\n        :type account_name: str\n        :param certificate_name: The identifier for the certificate. This must\n         be made up of algorithm and thumbprint separated by a dash, and must\n         match the certificate data in the request. For example SHA1-a3d1c5.\n        :type certificate_name: str\n        :param parameters: Additional parameters for certificate creation.\n        :type parameters:\n         ~azure.mgmt.batch.models.CertificateCreateOrUpdateParameters\n        :param if_match: The entity state (ETag) version of the certificate to\n         update. A value of \"*\" can be used to apply the operation only if the\n         certificate already exists. If omitted, this operation will always be\n         applied.\n        :type if_match: str\n        :param if_none_match: Set to '*' to allow a new certificate to be\n         created, but to prevent updating an existing certificate. Other values\n         will be ignored.\n        :type if_none_match: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :return: An instance of AzureOperationPoller that returns Certificate\n         or ClientRawResponse if raw=true\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.batch.models.Certificate]\n         or ~msrest.pipeline.ClientRawResponse\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._create_initial(\n            resource_group_name=resource_group_name,\n            account_name=account_name,\n            certificate_name=certificate_name,\n            parameters=parameters,\n            if_match=if_match,\n            if_none_match=if_none_match,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n        if raw:\n            return raw_result\n\n        # Construct and send request\n        def long_running_send():\n            return raw_result.response\n\n        def get_long_running_status(status_link, headers=None):\n\n            request = self._client.get(status_link)\n            if headers:\n                request.headers.update(headers)\n            header_parameters = {}\n            header_parameters['x-ms-client-request-id'] = raw_result.response.request.headers['x-ms-client-request-id']\n            return self._client.send(\n                request, header_parameters, stream=False, **operation_config)\n\n        def get_long_running_output(response):\n\n            if response.status_code not in [200]:\n                exp = CloudError(response)\n                exp.request_id = response.headers.get('x-ms-request-id')\n                raise exp\n\n            header_dict = {\n                'ETag': 'str',\n            }\n            deserialized = self._deserialize('Certificate', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                client_raw_response.add_headers(header_dict)\n                return client_raw_response\n\n            return deserialized\n\n        long_running_operation_timeout = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        return AzureOperationPoller(\n            long_running_send, get_long_running_output,\n            get_long_running_status, long_running_operation_timeout)","method_path":"azure-mgmt-batch\/azure\/mgmt\/batch\/operations\/certificate_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"CertificateOperations.delete","method_code":"def delete(\n            self, resource_group_name, account_name, certificate_name, custom_headers=None, raw=False, **operation_config):\n        raw_result = self._delete_initial(\n            resource_group_name=resource_group_name,\n            account_name=account_name,\n            certificate_name=certificate_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n        if raw:\n            return raw_result\n\n        \n        def long_running_send():\n            return raw_result.response\n\n        def get_long_running_status(status_link, headers=None):\n\n            request = self._client.get(status_link)\n            if headers:\n                request.headers.update(headers)\n            header_parameters = {}\n            header_parameters['x-ms-client-request-id'] = raw_result.response.request.headers['x-ms-client-request-id']\n            return self._client.send(\n                request, header_parameters, stream=False, **operation_config)\n\n        def get_long_running_output(response):\n\n            if response.status_code not in [200, 202, 204]:\n                exp = CloudError(response)\n                exp.request_id = response.headers.get('x-ms-request-id')\n                raise exp\n\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                client_raw_response.add_headers({\n                    'Location': 'str',\n                    'Retry-After': 'int',\n                })\n                return client_raw_response\n\n        long_running_operation_timeout = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        return AzureOperationPoller(\n            long_running_send, get_long_running_output,\n            get_long_running_status, long_running_operation_timeout)","method_summary":"Deletes the specified certificate.","original_method_code":"def delete(\n            self, resource_group_name, account_name, certificate_name, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Deletes the specified certificate.\n\n        :param resource_group_name: The name of the resource group that\n         contains the Batch account.\n        :type resource_group_name: str\n        :param account_name: The name of the Batch account.\n        :type account_name: str\n        :param certificate_name: The identifier for the certificate. This must\n         be made up of algorithm and thumbprint separated by a dash, and must\n         match the certificate data in the request. For example SHA1-a3d1c5.\n        :type certificate_name: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :return: An instance of AzureOperationPoller that returns None or\n         ClientRawResponse if raw=true\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrest.pipeline.ClientRawResponse\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._delete_initial(\n            resource_group_name=resource_group_name,\n            account_name=account_name,\n            certificate_name=certificate_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n        if raw:\n            return raw_result\n\n        # Construct and send request\n        def long_running_send():\n            return raw_result.response\n\n        def get_long_running_status(status_link, headers=None):\n\n            request = self._client.get(status_link)\n            if headers:\n                request.headers.update(headers)\n            header_parameters = {}\n            header_parameters['x-ms-client-request-id'] = raw_result.response.request.headers['x-ms-client-request-id']\n            return self._client.send(\n                request, header_parameters, stream=False, **operation_config)\n\n        def get_long_running_output(response):\n\n            if response.status_code not in [200, 202, 204]:\n                exp = CloudError(response)\n                exp.request_id = response.headers.get('x-ms-request-id')\n                raise exp\n\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                client_raw_response.add_headers({\n                    'Location': 'str',\n                    'Retry-After': 'int',\n                })\n                return client_raw_response\n\n        long_running_operation_timeout = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        return AzureOperationPoller(\n            long_running_send, get_long_running_output,\n            get_long_running_status, long_running_operation_timeout)","method_path":"azure-mgmt-batch\/azure\/mgmt\/batch\/operations\/certificate_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"get_client_from_cli_profile","method_code":"def get_client_from_cli_profile(client_class, **kwargs):\n    cloud = get_cli_active_cloud()\n    parameters = {}\n    if 'credentials' not in kwargs or 'subscription_id' not in kwargs:\n        resource, _ = _client_resource(client_class, cloud)\n        credentials, subscription_id, tenant_id = get_azure_cli_credentials(resource=resource,\n                                                                            with_tenant=True)\n        parameters.update({\n            'credentials': kwargs.get('credentials', credentials),\n            'subscription_id': kwargs.get('subscription_id', subscription_id)\n        })\n\n    args = get_arg_spec(client_class.__init__).args\n    if 'adla_job_dns_suffix' in args and 'adla_job_dns_suffix' not in kwargs:  \n        \n        \n        parameters['adla_job_dns_suffix'] = cloud.suffixes.azure_datalake_analytics_catalog_and_job_endpoint\n    elif 'base_url' in args and 'base_url' not in kwargs:\n        _, base_url = _client_resource(client_class, cloud)\n        if base_url:\n            parameters['base_url'] = base_url\n        else:\n            parameters['base_url'] = cloud.endpoints.resource_manager\n    if 'tenant_id' in args and 'tenant_id' not in kwargs:\n        parameters['tenant_id'] = tenant_id\n    parameters.update(kwargs)\n    return _instantiate_client(client_class, **parameters)","method_summary":"Return a SDK client initialized with current CLI credentials, CLI default subscription and CLI default cloud. This method will fill automatically the following client","original_method_code":"def get_client_from_cli_profile(client_class, **kwargs):\n    \"\"\"Return a SDK client initialized with current CLI credentials, CLI default subscription and CLI default cloud.\n\n    This method will fill automatically the following client parameters:\n    - credentials\n    - subscription_id\n    - base_url\n\n    Parameters provided in kwargs will override CLI parameters and be passed directly to the client.\n\n    :Example:\n\n    .. code:: python\n\n        from azure.common.client_factory import get_client_from_cli_profile\n        from azure.mgmt.compute import ComputeManagementClient\n        client = get_client_from_cli_profile(ComputeManagementClient)\n\n    .. versionadded:: 1.1.6\n\n    :param client_class: A SDK client class\n    :return: An instantiated client\n    :raises: ImportError if azure-cli-core package is not available\n    \"\"\"\n    cloud = get_cli_active_cloud()\n    parameters = {}\n    if 'credentials' not in kwargs or 'subscription_id' not in kwargs:\n        resource, _ = _client_resource(client_class, cloud)\n        credentials, subscription_id, tenant_id = get_azure_cli_credentials(resource=resource,\n                                                                            with_tenant=True)\n        parameters.update({\n            'credentials': kwargs.get('credentials', credentials),\n            'subscription_id': kwargs.get('subscription_id', subscription_id)\n        })\n\n    args = get_arg_spec(client_class.__init__).args\n    if 'adla_job_dns_suffix' in args and 'adla_job_dns_suffix' not in kwargs:  # Datalake\n        # Let it raise here with AttributeError at worst, this would mean this cloud does not define\n        # ADL endpoint and no manual suffix was given\n        parameters['adla_job_dns_suffix'] = cloud.suffixes.azure_datalake_analytics_catalog_and_job_endpoint\n    elif 'base_url' in args and 'base_url' not in kwargs:\n        _, base_url = _client_resource(client_class, cloud)\n        if base_url:\n            parameters['base_url'] = base_url\n        else:\n            parameters['base_url'] = cloud.endpoints.resource_manager\n    if 'tenant_id' in args and 'tenant_id' not in kwargs:\n        parameters['tenant_id'] = tenant_id\n    parameters.update(kwargs)\n    return _instantiate_client(client_class, **parameters)","method_path":"azure-common\/azure\/common\/client_factory.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"get_client_from_json_dict","method_code":"def get_client_from_json_dict(client_class, config_dict, **kwargs):\n    is_graphrbac = client_class.__name__ == 'GraphRbacManagementClient'\n    parameters = {\n        'subscription_id': config_dict.get('subscriptionId'),\n        'base_url': config_dict.get('resourceManagerEndpointUrl'),\n        'tenant_id': config_dict.get('tenantId')  \n    }\n    if is_graphrbac:\n        parameters['base_url'] = config_dict['activeDirectoryGraphResourceId']\n\n    if 'credentials' not in kwargs:\n        \n        if is_graphrbac:\n            resource = config_dict['activeDirectoryGraphResourceId']\n        else:\n            if \"activeDirectoryResourceId\" not in config_dict and 'resourceManagerEndpointUrl' not in config_dict:\n                raise ValueError(\"Need activeDirectoryResourceId or resourceManagerEndpointUrl key\")\n            resource = config_dict.get('activeDirectoryResourceId', config_dict['resourceManagerEndpointUrl'])\n\n        authority_url = config_dict['activeDirectoryEndpointUrl']\n        is_adfs = bool(re.match('.+(\/adfs|\/adfs\/)$', authority_url, re.I))\n        if is_adfs:\n            authority_url = authority_url.rstrip('\/')  \n        else:\n            authority_url = authority_url + '\/' + config_dict['tenantId']\n\n        context = adal.AuthenticationContext(\n            authority_url,\n            api_version=None,\n            validate_authority=not is_adfs\n        )\n        parameters['credentials'] = AdalAuthentication(\n            context.acquire_token_with_client_credentials,\n            resource,\n            config_dict['clientId'],\n            config_dict['clientSecret']\n        )\n\n    parameters.update(kwargs)\n    return _instantiate_client(client_class, **parameters)","method_summary":"Return a SDK client initialized with a JSON auth dict. The easiest way to obtain this content is to call the following CLI","original_method_code":"def get_client_from_json_dict(client_class, config_dict, **kwargs):\n    \"\"\"Return a SDK client initialized with a JSON auth dict.\n\n    The easiest way to obtain this content is to call the following CLI commands:\n\n    .. code:: bash\n\n        az ad sp create-for-rbac --sdk-auth\n\n    This method will fill automatically the following client parameters:\n    - credentials\n    - subscription_id\n    - base_url\n    - tenant_id\n\n    Parameters provided in kwargs will override parameters and be passed directly to the client.\n\n    :Example:\n\n    .. code:: python\n\n        from azure.common.client_factory import get_client_from_auth_file\n        from azure.mgmt.compute import ComputeManagementClient\n        config_dict = {\n            \"clientId\": \"ad735158-65ca-11e7-ba4d-ecb1d756380e\",\n            \"clientSecret\": \"b70bb224-65ca-11e7-810c-ecb1d756380e\",\n            \"subscriptionId\": \"bfc42d3a-65ca-11e7-95cf-ecb1d756380e\",\n            \"tenantId\": \"c81da1d8-65ca-11e7-b1d1-ecb1d756380e\",\n            \"activeDirectoryEndpointUrl\": \"https:\/\/login.microsoftonline.com\",\n            \"resourceManagerEndpointUrl\": \"https:\/\/management.azure.com\/\",\n            \"activeDirectoryGraphResourceId\": \"https:\/\/graph.windows.net\/\",\n            \"sqlManagementEndpointUrl\": \"https:\/\/management.core.windows.net:8443\/\",\n            \"galleryEndpointUrl\": \"https:\/\/gallery.azure.com\/\",\n            \"managementEndpointUrl\": \"https:\/\/management.core.windows.net\/\"\n        }\n        client = get_client_from_json_dict(ComputeManagementClient, config_dict)\n\n    .. versionadded:: 1.1.7\n\n    :param client_class: A SDK client class\n    :param dict config_dict: A config dict.\n    :return: An instantiated client\n    \"\"\"\n    is_graphrbac = client_class.__name__ == 'GraphRbacManagementClient'\n    parameters = {\n        'subscription_id': config_dict.get('subscriptionId'),\n        'base_url': config_dict.get('resourceManagerEndpointUrl'),\n        'tenant_id': config_dict.get('tenantId')  # GraphRbac\n    }\n    if is_graphrbac:\n        parameters['base_url'] = config_dict['activeDirectoryGraphResourceId']\n\n    if 'credentials' not in kwargs:\n        # Get the right resource for Credentials\n        if is_graphrbac:\n            resource = config_dict['activeDirectoryGraphResourceId']\n        else:\n            if \"activeDirectoryResourceId\" not in config_dict and 'resourceManagerEndpointUrl' not in config_dict:\n                raise ValueError(\"Need activeDirectoryResourceId or resourceManagerEndpointUrl key\")\n            resource = config_dict.get('activeDirectoryResourceId', config_dict['resourceManagerEndpointUrl'])\n\n        authority_url = config_dict['activeDirectoryEndpointUrl']\n        is_adfs = bool(re.match('.+(\/adfs|\/adfs\/)$', authority_url, re.I))\n        if is_adfs:\n            authority_url = authority_url.rstrip('\/')  # workaround: ADAL is known to reject auth urls with trailing \/\n        else:\n            authority_url = authority_url + '\/' + config_dict['tenantId']\n\n        context = adal.AuthenticationContext(\n            authority_url,\n            api_version=None,\n            validate_authority=not is_adfs\n        )\n        parameters['credentials'] = AdalAuthentication(\n            context.acquire_token_with_client_credentials,\n            resource,\n            config_dict['clientId'],\n            config_dict['clientSecret']\n        )\n\n    parameters.update(kwargs)\n    return _instantiate_client(client_class, **parameters)","method_path":"azure-common\/azure\/common\/client_factory.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"get_client_from_auth_file","method_code":"def get_client_from_auth_file(client_class, auth_path=None, **kwargs):\n    auth_path = auth_path or os.environ['AZURE_AUTH_LOCATION']\n\n    with io.open(auth_path, 'r', encoding='utf-8-sig') as auth_fd:\n        config_dict = json.load(auth_fd)\n    return get_client_from_json_dict(client_class, config_dict, **kwargs)","method_summary":"Return a SDK client initialized with auth file. The easiest way to obtain this file is to call the following CLI","original_method_code":"def get_client_from_auth_file(client_class, auth_path=None, **kwargs):\n    \"\"\"Return a SDK client initialized with auth file.\n\n    The easiest way to obtain this file is to call the following CLI commands:\n\n    .. code:: bash\n\n        az ad sp create-for-rbac --sdk-auth\n\n    You can specific the file path directly, or fill the environment variable AZURE_AUTH_LOCATION.\n    File must be UTF-8.\n\n    This method will fill automatically the following client parameters:\n    - credentials\n    - subscription_id\n    - base_url\n\n    Parameters provided in kwargs will override parameters and be passed directly to the client.\n\n    :Example:\n\n    .. code:: python\n\n        from azure.common.client_factory import get_client_from_auth_file\n        from azure.mgmt.compute import ComputeManagementClient\n        client = get_client_from_auth_file(ComputeManagementClient)\n\n    Example of file:\n\n    .. code:: json\n\n        {\n            \"clientId\": \"ad735158-65ca-11e7-ba4d-ecb1d756380e\",\n            \"clientSecret\": \"b70bb224-65ca-11e7-810c-ecb1d756380e\",\n            \"subscriptionId\": \"bfc42d3a-65ca-11e7-95cf-ecb1d756380e\",\n            \"tenantId\": \"c81da1d8-65ca-11e7-b1d1-ecb1d756380e\",\n            \"activeDirectoryEndpointUrl\": \"https:\/\/login.microsoftonline.com\",\n            \"resourceManagerEndpointUrl\": \"https:\/\/management.azure.com\/\",\n            \"activeDirectoryGraphResourceId\": \"https:\/\/graph.windows.net\/\",\n            \"sqlManagementEndpointUrl\": \"https:\/\/management.core.windows.net:8443\/\",\n            \"galleryEndpointUrl\": \"https:\/\/gallery.azure.com\/\",\n            \"managementEndpointUrl\": \"https:\/\/management.core.windows.net\/\"\n        }\n\n    .. versionadded:: 1.1.7\n\n    :param client_class: A SDK client class\n    :param str auth_path: Path to the file.\n    :return: An instantiated client\n    :raises: KeyError if AZURE_AUTH_LOCATION is not an environment variable and no path is provided\n    :raises: FileNotFoundError if provided file path does not exists\n    :raises: json.JSONDecodeError if provided file is not JSON valid\n    :raises: UnicodeDecodeError if file is not UTF8 compliant\n    \"\"\"\n    auth_path = auth_path or os.environ['AZURE_AUTH_LOCATION']\n\n    with io.open(auth_path, 'r', encoding='utf-8-sig') as auth_fd:\n        config_dict = json.load(auth_fd)\n    return get_client_from_json_dict(client_class, config_dict, **kwargs)","method_path":"azure-common\/azure\/common\/client_factory.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_ETreeXmlToObject.get_entry_properties_from_element","method_code":"def get_entry_properties_from_element(element, include_id, id_prefix_to_skip=None, use_title_as_id=False):\n        properties = {}\n\n        etag = element.attrib.get(_make_etree_ns_attr_name(_etree_entity_feed_namespaces['m'], 'etag'), None)\n        if etag is not None:\n            properties['etag'] = etag\n\n        updated = element.findtext('.\/atom:updated', '', _etree_entity_feed_namespaces)\n        if updated:\n            properties['updated'] = updated\n\n        author_name = element.findtext('.\/atom:author\/atom:name', '', _etree_entity_feed_namespaces)\n        if author_name:\n            properties['author'] = author_name\n\n        if include_id:\n            if use_title_as_id:\n                title = element.findtext('.\/atom:title', '', _etree_entity_feed_namespaces)\n                if title:\n                    properties['name'] = title\n            else:\n                element_id = element.findtext('.\/atom:id', '', _etree_entity_feed_namespaces)\n                if element_id:\n                    properties['name'] = _get_readable_id(element_id, id_prefix_to_skip)\n\n        return properties","method_summary":"get properties from element tree element","original_method_code":"def get_entry_properties_from_element(element, include_id, id_prefix_to_skip=None, use_title_as_id=False):\n        ''' get properties from element tree element '''\n        properties = {}\n\n        etag = element.attrib.get(_make_etree_ns_attr_name(_etree_entity_feed_namespaces['m'], 'etag'), None)\n        if etag is not None:\n            properties['etag'] = etag\n\n        updated = element.findtext('.\/atom:updated', '', _etree_entity_feed_namespaces)\n        if updated:\n            properties['updated'] = updated\n\n        author_name = element.findtext('.\/atom:author\/atom:name', '', _etree_entity_feed_namespaces)\n        if author_name:\n            properties['author'] = author_name\n\n        if include_id:\n            if use_title_as_id:\n                title = element.findtext('.\/atom:title', '', _etree_entity_feed_namespaces)\n                if title:\n                    properties['name'] = title\n            else:\n                element_id = element.findtext('.\/atom:id', '', _etree_entity_feed_namespaces)\n                if element_id:\n                    properties['name'] = _get_readable_id(element_id, id_prefix_to_skip)\n\n        return properties","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_common_serialization.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"DpsCertificateOperations.delete","method_code":"def delete(\n            self, resource_group_name, if_match, provisioning_service_name, certificate_name, certificatename=None, certificateraw_bytes=None, certificateis_verified=None, certificatepurpose=None, certificatecreated=None, certificatelast_updated=None, certificatehas_private_key=None, certificatenonce=None, custom_headers=None, raw=False, **operation_config):\n        \n        url = self.delete.metadata['url']\n        path_format_arguments = {\n            'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str'),\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'provisioningServiceName': self._serialize.url(\"provisioning_service_name\", provisioning_service_name, 'str'),\n            'certificateName': self._serialize.url(\"certificate_name\", certificate_name, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        \n        query_parameters = {}\n        if certificatename is not None:\n            query_parameters['certificate.name'] = self._serialize.query(\"certificatename\", certificatename, 'str')\n        if certificateraw_bytes is not None:\n            query_parameters['certificate.rawBytes'] = self._serialize.query(\"certificateraw_bytes\", certificateraw_bytes, 'bytearray')\n        if certificateis_verified is not None:\n            query_parameters['certificate.isVerified'] = self._serialize.query(\"certificateis_verified\", certificateis_verified, 'bool')\n        if certificatepurpose is not None:\n            query_parameters['certificate.purpose'] = self._serialize.query(\"certificatepurpose\", certificatepurpose, 'str')\n        if certificatecreated is not None:\n            query_parameters['certificate.created'] = self._serialize.query(\"certificatecreated\", certificatecreated, 'iso-8601')\n        if certificatelast_updated is not None:\n            query_parameters['certificate.lastUpdated'] = self._serialize.query(\"certificatelast_updated\", certificatelast_updated, 'iso-8601')\n        if certificatehas_private_key is not None:\n            query_parameters['certificate.hasPrivateKey'] = self._serialize.query(\"certificatehas_private_key\", certificatehas_private_key, 'bool')\n        if certificatenonce is not None:\n            query_parameters['certificate.nonce'] = self._serialize.query(\"certificatenonce\", certificatenonce, 'str')\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        \n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        header_parameters['If-Match'] = self._serialize.header(\"if_match\", if_match, 'str')\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        \n        request = self._client.delete(url, query_parameters)\n        response = self._client.send(request, header_parameters, stream=False, **operation_config)\n\n        if response.status_code not in [200, 204]:\n            raise models.ErrorDetailsException(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response","method_summary":"Delete the Provisioning Service Certificate. Deletes the specified certificate assosciated with the Provisioning Service.","original_method_code":"def delete(\n            self, resource_group_name, if_match, provisioning_service_name, certificate_name, certificatename=None, certificateraw_bytes=None, certificateis_verified=None, certificatepurpose=None, certificatecreated=None, certificatelast_updated=None, certificatehas_private_key=None, certificatenonce=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Delete the Provisioning Service Certificate.\n\n        Deletes the specified certificate assosciated with the Provisioning\n        Service.\n\n        :param resource_group_name: Resource group identifier.\n        :type resource_group_name: str\n        :param if_match: ETag of the certificate\n        :type if_match: str\n        :param provisioning_service_name: The name of the provisioning\n         service.\n        :type provisioning_service_name: str\n        :param certificate_name: This is a mandatory field, and is the logical\n         name of the certificate that the provisioning service will access by.\n        :type certificate_name: str\n        :param certificatename: This is optional, and it is the Common Name of\n         the certificate.\n        :type certificatename: str\n        :param certificateraw_bytes: Raw data within the certificate.\n        :type certificateraw_bytes: bytearray\n        :param certificateis_verified: Indicates if certificate has been\n         verified by owner of the private key.\n        :type certificateis_verified: bool\n        :param certificatepurpose: A description that mentions the purpose of\n         the certificate. Possible values include: 'clientAuthentication',\n         'serverAuthentication'\n        :type certificatepurpose: str or\n         ~azure.mgmt.iothubprovisioningservices.models.CertificatePurpose\n        :param certificatecreated: Time the certificate is created.\n        :type certificatecreated: datetime\n        :param certificatelast_updated: Time the certificate is last updated.\n        :type certificatelast_updated: datetime\n        :param certificatehas_private_key: Indicates if the certificate\n         contains a private key.\n        :type certificatehas_private_key: bool\n        :param certificatenonce: Random number generated to indicate Proof of\n         Possession.\n        :type certificatenonce: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`ErrorDetailsException<azure.mgmt.iothubprovisioningservices.models.ErrorDetailsException>`\n        \"\"\"\n        # Construct URL\n        url = self.delete.metadata['url']\n        path_format_arguments = {\n            'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str'),\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'provisioningServiceName': self._serialize.url(\"provisioning_service_name\", provisioning_service_name, 'str'),\n            'certificateName': self._serialize.url(\"certificate_name\", certificate_name, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        if certificatename is not None:\n            query_parameters['certificate.name'] = self._serialize.query(\"certificatename\", certificatename, 'str')\n        if certificateraw_bytes is not None:\n            query_parameters['certificate.rawBytes'] = self._serialize.query(\"certificateraw_bytes\", certificateraw_bytes, 'bytearray')\n        if certificateis_verified is not None:\n            query_parameters['certificate.isVerified'] = self._serialize.query(\"certificateis_verified\", certificateis_verified, 'bool')\n        if certificatepurpose is not None:\n            query_parameters['certificate.purpose'] = self._serialize.query(\"certificatepurpose\", certificatepurpose, 'str')\n        if certificatecreated is not None:\n            query_parameters['certificate.created'] = self._serialize.query(\"certificatecreated\", certificatecreated, 'iso-8601')\n        if certificatelast_updated is not None:\n            query_parameters['certificate.lastUpdated'] = self._serialize.query(\"certificatelast_updated\", certificatelast_updated, 'iso-8601')\n        if certificatehas_private_key is not None:\n            query_parameters['certificate.hasPrivateKey'] = self._serialize.query(\"certificatehas_private_key\", certificatehas_private_key, 'bool')\n        if certificatenonce is not None:\n            query_parameters['certificate.nonce'] = self._serialize.query(\"certificatenonce\", certificatenonce, 'str')\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application\/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        header_parameters['If-Match'] = self._serialize.header(\"if_match\", if_match, 'str')\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        # Construct and send request\n        request = self._client.delete(url, query_parameters)\n        response = self._client.send(request, header_parameters, stream=False, **operation_config)\n\n        if response.status_code not in [200, 204]:\n            raise models.ErrorDetailsException(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response","method_path":"azure-mgmt-iothubprovisioningservices\/azure\/mgmt\/iothubprovisioningservices\/operations\/dps_certificate_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusClient.get_queue","method_code":"def get_queue(self, queue_name):\n        try:\n            queue = self.mgmt_client.get_queue(queue_name)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except AzureServiceBusResourceNotFound:\n            raise ServiceBusResourceNotFound(\"Specificed queue does not exist.\")\n        return QueueClient.from_entity(\n            self._get_host(), queue,\n            shared_access_key_name=self.shared_access_key_name,\n            shared_access_key_value=self.shared_access_key_value,\n            mgmt_client=self.mgmt_client,\n            debug=self.debug)","method_summary":"Get a client for a queue entity.","original_method_code":"def get_queue(self, queue_name):\n        \"\"\"Get a client for a queue entity.\n\n        :param queue_name: The name of the queue.\n        :type queue_name: str\n        :rtype: ~azure.servicebus.servicebus_client.QueueClient\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the queue is not found.\n\n        Example:\n            .. literalinclude:: ..\/examples\/test_examples.py\n                :start-after: [START get_queue_client]\n                :end-before: [END get_queue_client]\n                :language: python\n                :dedent: 8\n                :caption: Get the specific queue client from Service Bus client\n\n        \"\"\"\n        try:\n            queue = self.mgmt_client.get_queue(queue_name)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except AzureServiceBusResourceNotFound:\n            raise ServiceBusResourceNotFound(\"Specificed queue does not exist.\")\n        return QueueClient.from_entity(\n            self._get_host(), queue,\n            shared_access_key_name=self.shared_access_key_name,\n            shared_access_key_value=self.shared_access_key_value,\n            mgmt_client=self.mgmt_client,\n            debug=self.debug)","method_path":"azure-servicebus\/azure\/servicebus\/servicebus_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusClient.list_queues","method_code":"def list_queues(self):\n        try:\n            queues = self.mgmt_client.list_queues()\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        queue_clients = []\n        for queue in queues:\n            queue_clients.append(QueueClient.from_entity(\n                self._get_host(), queue,\n                shared_access_key_name=self.shared_access_key_name,\n                shared_access_key_value=self.shared_access_key_value,\n                mgmt_client=self.mgmt_client,\n                debug=self.debug))\n        return queue_clients","method_summary":"Get clients for all queue entities in the namespace.","original_method_code":"def list_queues(self):\n        \"\"\"Get clients for all queue entities in the namespace.\n\n        :rtype: list[~azure.servicebus.servicebus_client.QueueClient]\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n\n        Example:\n            .. literalinclude:: ..\/examples\/test_examples.py\n                :start-after: [START list_queues]\n                :end-before: [END list_queues]\n                :language: python\n                :dedent: 4\n                :caption: List the queues from Service Bus client\n\n        \"\"\"\n        try:\n            queues = self.mgmt_client.list_queues()\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        queue_clients = []\n        for queue in queues:\n            queue_clients.append(QueueClient.from_entity(\n                self._get_host(), queue,\n                shared_access_key_name=self.shared_access_key_name,\n                shared_access_key_value=self.shared_access_key_value,\n                mgmt_client=self.mgmt_client,\n                debug=self.debug))\n        return queue_clients","method_path":"azure-servicebus\/azure\/servicebus\/servicebus_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusClient.get_topic","method_code":"def get_topic(self, topic_name):\n        try:\n            topic = self.mgmt_client.get_topic(topic_name)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except AzureServiceBusResourceNotFound:\n            raise ServiceBusResourceNotFound(\"Specificed topic does not exist.\")\n        return TopicClient.from_entity(\n            self._get_host(), topic,\n            shared_access_key_name=self.shared_access_key_name,\n            shared_access_key_value=self.shared_access_key_value,\n            debug=self.debug)","method_summary":"Get a client for a topic entity.","original_method_code":"def get_topic(self, topic_name):\n        \"\"\"Get a client for a topic entity.\n\n        :param topic_name: The name of the topic.\n        :type topic_name: str\n        :rtype: ~azure.servicebus.servicebus_client.TopicClient\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n        :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the topic is not found.\n\n        Example:\n            .. literalinclude:: ..\/examples\/test_examples.py\n                :start-after: [START get_topic_client]\n                :end-before: [END get_topic_client]\n                :language: python\n                :dedent: 8\n                :caption: Get the specific topic client from Service Bus client\n\n        \"\"\"\n        try:\n            topic = self.mgmt_client.get_topic(topic_name)\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        except AzureServiceBusResourceNotFound:\n            raise ServiceBusResourceNotFound(\"Specificed topic does not exist.\")\n        return TopicClient.from_entity(\n            self._get_host(), topic,\n            shared_access_key_name=self.shared_access_key_name,\n            shared_access_key_value=self.shared_access_key_value,\n            debug=self.debug)","method_path":"azure-servicebus\/azure\/servicebus\/servicebus_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ServiceBusClient.list_topics","method_code":"def list_topics(self):\n        try:\n            topics = self.mgmt_client.list_topics()\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        topic_clients = []\n        for topic in topics:\n            topic_clients.append(TopicClient.from_entity(\n                self._get_host(), topic,\n                shared_access_key_name=self.shared_access_key_name,\n                shared_access_key_value=self.shared_access_key_value,\n                debug=self.debug))\n        return topic_clients","method_summary":"Get a client for all topic entities in the namespace.","original_method_code":"def list_topics(self):\n        \"\"\"Get a client for all topic entities in the namespace.\n\n        :rtype: list[~azure.servicebus.servicebus_client.TopicClient]\n        :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.\n\n        Example:\n            .. literalinclude:: ..\/examples\/test_examples.py\n                :start-after: [START list_topics]\n                :end-before: [END list_topics]\n                :language: python\n                :dedent: 4\n                :caption: List the topics from Service Bus client\n\n        \"\"\"\n        try:\n            topics = self.mgmt_client.list_topics()\n        except requests.exceptions.ConnectionError as e:\n            raise ServiceBusConnectionError(\"Namespace: {} not found\".format(self.service_namespace), e)\n        topic_clients = []\n        for topic in topics:\n            topic_clients.append(TopicClient.from_entity(\n                self._get_host(), topic,\n                shared_access_key_name=self.shared_access_key_name,\n                shared_access_key_value=self.shared_access_key_value,\n                debug=self.debug))\n        return topic_clients","method_path":"azure-servicebus\/azure\/servicebus\/servicebus_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ReceiveClientMixin.settle_deferred_messages","method_code":"def settle_deferred_messages(self, settlement, messages, **kwargs):\n        if (self.entity and self.requires_session) or kwargs.get('session'):\n            raise ValueError(\"Sessionful deferred messages can only be settled within a locked receive session.\")\n        if settlement.lower() not in ['completed', 'suspended', 'abandoned']:\n            raise ValueError(\"Settlement must be one of: 'completed', 'suspended', 'abandoned'\")\n        if not messages:\n            raise ValueError(\"At least one message must be specified.\")\n        message = {\n            'disposition-status': settlement.lower(),\n            'lock-tokens': types.AMQPArray([m.lock_token for m in messages])}\n\n        with BaseHandler(self.entity_uri, self.auth_config, debug=self.debug, **kwargs) as handler:\n            return handler._mgmt_request_response(  \n                REQUEST_RESPONSE_UPDATE_DISPOSTION_OPERATION,\n                message,\n                mgmt_handlers.default)","method_summary":"Settle messages that have been previously deferred.","original_method_code":"def settle_deferred_messages(self, settlement, messages, **kwargs):\n        \"\"\"Settle messages that have been previously deferred.\n\n        :param settlement: How the messages are to be settled. This must be a string\n         of one of the following values: 'completed', 'suspended', 'abandoned'.\n        :type settlement: str\n        :param messages: A list of deferred messages to be settled.\n        :type messages: list[~azure.servicebus.common.message.DeferredMessage]\n\n        Example:\n            .. literalinclude:: ..\/examples\/test_examples.py\n                :start-after: [START settle_deferred_messages_service_bus]\n                :end-before: [END settle_deferred_messages_service_bus]\n                :language: python\n                :dedent: 8\n                :caption: Settle deferred messages.\n\n        \"\"\"\n        if (self.entity and self.requires_session) or kwargs.get('session'):\n            raise ValueError(\"Sessionful deferred messages can only be settled within a locked receive session.\")\n        if settlement.lower() not in ['completed', 'suspended', 'abandoned']:\n            raise ValueError(\"Settlement must be one of: 'completed', 'suspended', 'abandoned'\")\n        if not messages:\n            raise ValueError(\"At least one message must be specified.\")\n        message = {\n            'disposition-status': settlement.lower(),\n            'lock-tokens': types.AMQPArray([m.lock_token for m in messages])}\n\n        with BaseHandler(self.entity_uri, self.auth_config, debug=self.debug, **kwargs) as handler:\n            return handler._mgmt_request_response(  # pylint: disable=protected-access\n                REQUEST_RESPONSE_UPDATE_DISPOSTION_OPERATION,\n                message,\n                mgmt_handlers.default)","method_path":"azure-servicebus\/azure\/servicebus\/servicebus_client.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebsiteManagementService.get_site","method_code":"def get_site(self, webspace_name, website_name):\n        return self._perform_get(self._get_sites_details_path(webspace_name,\n                                                              website_name),\n                                 Site)","method_summary":"List the web sites defined on this webspace.","original_method_code":"def get_site(self, webspace_name, website_name):\n        '''\n        List the web sites defined on this webspace.\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        '''\n        return self._perform_get(self._get_sites_details_path(webspace_name,\n                                                              website_name),\n                                 Site)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/websitemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebsiteManagementService.create_site","method_code":"def create_site(self, webspace_name, website_name, geo_region, host_names,\n                    plan='VirtualDedicatedPlan', compute_mode='Shared',\n                    server_farm=None, site_mode=None):\n        xml = _XmlSerializer.create_website_to_xml(webspace_name, website_name, geo_region, plan, host_names, compute_mode, server_farm, site_mode)\n        return self._perform_post(\n            self._get_sites_path(webspace_name),\n            xml,\n            Site)","method_summary":"Create a website.","original_method_code":"def create_site(self, webspace_name, website_name, geo_region, host_names,\n                    plan='VirtualDedicatedPlan', compute_mode='Shared',\n                    server_farm=None, site_mode=None):\n        '''\n        Create a website.\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        geo_region:\n            The geographical region of the webspace that will be created.\n        host_names:\n            An array of fully qualified domain names for website. Only one\n            hostname can be specified in the azurewebsites.net domain.\n            The hostname should match the name of the website. Custom domains\n            can only be specified for Shared or Standard websites.\n        plan:\n            This value must be 'VirtualDedicatedPlan'.\n        compute_mode:\n            This value should be 'Shared' for the Free or Paid Shared\n            offerings, or 'Dedicated' for the Standard offering. The default\n            value is 'Shared'. If you set it to 'Dedicated', you must specify\n            a value for the server_farm parameter.\n        server_farm:\n            The name of the Server Farm associated with this website. This is\n            a required value for Standard mode.\n        site_mode:\n            Can be None, 'Limited' or 'Basic'. This value is 'Limited' for the\n            Free offering, and 'Basic' for the Paid Shared offering. Standard\n            mode does not use the site_mode parameter; it uses the compute_mode\n            parameter.\n        '''\n        xml = _XmlSerializer.create_website_to_xml(webspace_name, website_name, geo_region, plan, host_names, compute_mode, server_farm, site_mode)\n        return self._perform_post(\n            self._get_sites_path(webspace_name),\n            xml,\n            Site)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/websitemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebsiteManagementService.delete_site","method_code":"def delete_site(self, webspace_name, website_name,\n                    delete_empty_server_farm=False, delete_metrics=False):\n        path = self._get_sites_details_path(webspace_name, website_name)\n        query = ''\n        if delete_empty_server_farm:\n            query += '&deleteEmptyServerFarm=true'\n        if delete_metrics:\n            query += '&deleteMetrics=true'\n        if query:\n            path = path + '?' + query.lstrip('&')\n        return self._perform_delete(path)","method_summary":"Delete a website.","original_method_code":"def delete_site(self, webspace_name, website_name,\n                    delete_empty_server_farm=False, delete_metrics=False):\n        '''\n        Delete a website.\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        delete_empty_server_farm:\n            If the site being deleted is the last web site in a server farm,\n            you can delete the server farm by setting this to True.\n        delete_metrics:\n            To also delete the metrics for the site that you are deleting, you\n            can set this to True.\n        '''\n        path = self._get_sites_details_path(webspace_name, website_name)\n        query = ''\n        if delete_empty_server_farm:\n            query += '&deleteEmptyServerFarm=true'\n        if delete_metrics:\n            query += '&deleteMetrics=true'\n        if query:\n            path = path + '?' + query.lstrip('&')\n        return self._perform_delete(path)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/websitemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebsiteManagementService.update_site","method_code":"def update_site(self, webspace_name, website_name, state=None):\n        xml = _XmlSerializer.update_website_to_xml(state)\n        return self._perform_put(\n            self._get_sites_details_path(webspace_name, website_name),\n            xml, as_async=True)","method_summary":"Update a web site.","original_method_code":"def update_site(self, webspace_name, website_name, state=None):\n        '''\n        Update a web site.\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        state:\n            The wanted state ('Running' or 'Stopped' accepted)\n        '''\n        xml = _XmlSerializer.update_website_to_xml(state)\n        return self._perform_put(\n            self._get_sites_details_path(webspace_name, website_name),\n            xml, as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/websitemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebsiteManagementService.restart_site","method_code":"def restart_site(self, webspace_name, website_name):\n        return self._perform_post(\n            self._get_restart_path(webspace_name, website_name),\n            None, as_async=True)","method_summary":"Restart a web site.","original_method_code":"def restart_site(self, webspace_name, website_name):\n        '''\n        Restart a web site.\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        '''\n        return self._perform_post(\n            self._get_restart_path(webspace_name, website_name),\n            None, as_async=True)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/websitemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebsiteManagementService.get_historical_usage_metrics","method_code":"def get_historical_usage_metrics(self, webspace_name, website_name,\n                                     metrics = None, start_time=None, end_time=None, time_grain=None):\n        metrics = ('names='+','.join(metrics)) if metrics else ''\n        start_time = ('StartTime='+start_time) if start_time else ''\n        end_time = ('EndTime='+end_time) if end_time else ''\n        time_grain = ('TimeGrain='+time_grain) if time_grain else ''\n        parameters = ('&'.join(v for v in (metrics, start_time, end_time, time_grain) if v))\n        parameters = '?'+parameters if parameters else ''\n        return self._perform_get(self._get_historical_usage_metrics_path(webspace_name, website_name) + parameters,\n                                 MetricResponses)","method_summary":"Get historical usage metrics.","original_method_code":"def get_historical_usage_metrics(self, webspace_name, website_name,\n                                     metrics = None, start_time=None, end_time=None, time_grain=None):\n        '''\n        Get historical usage metrics.\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        metrics:\n            Optional. List of metrics name. Otherwise, all metrics returned.\n        start_time:\n            Optional. An ISO8601 date. Otherwise, current hour is used.\n        end_time:\n            Optional. An ISO8601 date. Otherwise, current time is used.\n        time_grain:\n            Optional. A rollup name, as P1D. OTherwise, default rollup for the metrics is used.\n        More information and metrics name at:\n        http:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn166964.aspx\n        '''        \n        metrics = ('names='+','.join(metrics)) if metrics else ''\n        start_time = ('StartTime='+start_time) if start_time else ''\n        end_time = ('EndTime='+end_time) if end_time else ''\n        time_grain = ('TimeGrain='+time_grain) if time_grain else ''\n        parameters = ('&'.join(v for v in (metrics, start_time, end_time, time_grain) if v))\n        parameters = '?'+parameters if parameters else ''\n        return self._perform_get(self._get_historical_usage_metrics_path(webspace_name, website_name) + parameters,\n                                 MetricResponses)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/websitemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebsiteManagementService.get_metric_definitions","method_code":"def get_metric_definitions(self, webspace_name, website_name):\n        return self._perform_get(self._get_metric_definitions_path(webspace_name, website_name),\n                                 MetricDefinitions)","method_summary":"Get metric definitions of metrics available of this web site.","original_method_code":"def get_metric_definitions(self, webspace_name, website_name):\n        '''\n        Get metric definitions of metrics available of this web site.\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        '''\n        return self._perform_get(self._get_metric_definitions_path(webspace_name, website_name),\n                                 MetricDefinitions)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/websitemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebsiteManagementService.get_publish_profile_xml","method_code":"def get_publish_profile_xml(self, webspace_name, website_name):\n        return self._perform_get(self._get_publishxml_path(webspace_name, website_name),\n                                 None).body.decode(\"utf-8\")","method_summary":"Get a site's publish profile as a string","original_method_code":"def get_publish_profile_xml(self, webspace_name, website_name):\n        '''\n        Get a site's publish profile as a string\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        '''\n        return self._perform_get(self._get_publishxml_path(webspace_name, website_name),\n                                 None).body.decode(\"utf-8\")","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/websitemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"WebsiteManagementService.get_publish_profile","method_code":"def get_publish_profile(self, webspace_name, website_name):\n        return self._perform_get(self._get_publishxml_path(webspace_name, website_name),\n                                 PublishData)","method_summary":"Get a site's publish profile as an object","original_method_code":"def get_publish_profile(self, webspace_name, website_name):\n        '''\n        Get a site's publish profile as an object\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        '''\n        return self._perform_get(self._get_publishxml_path(webspace_name, website_name),\n                                 PublishData)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/websitemanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"RegistriesOperations.update_policies","method_code":"def update_policies(\n            self, resource_group_name, registry_name, quarantine_policy=None, trust_policy=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._update_policies_initial(\n            resource_group_name=resource_group_name,\n            registry_name=registry_name,\n            quarantine_policy=quarantine_policy,\n            trust_policy=trust_policy,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('RegistryPolicies', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Updates the policies for the specified container registry.","original_method_code":"def update_policies(\n            self, resource_group_name, registry_name, quarantine_policy=None, trust_policy=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Updates the policies for the specified container registry.\n\n        :param resource_group_name: The name of the resource group to which\n         the container registry belongs.\n        :type resource_group_name: str\n        :param registry_name: The name of the container registry.\n        :type registry_name: str\n        :param quarantine_policy: An object that represents quarantine policy\n         for a container registry.\n        :type quarantine_policy:\n         ~azure.mgmt.containerregistry.v2018_02_01_preview.models.QuarantinePolicy\n        :param trust_policy: An object that represents content trust policy\n         for a container registry.\n        :type trust_policy:\n         ~azure.mgmt.containerregistry.v2018_02_01_preview.models.TrustPolicy\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns RegistryPolicies or\n         ClientRawResponse<RegistryPolicies> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.containerregistry.v2018_02_01_preview.models.RegistryPolicies]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.containerregistry.v2018_02_01_preview.models.RegistryPolicies]]\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._update_policies_initial(\n            resource_group_name=resource_group_name,\n            registry_name=registry_name,\n            quarantine_policy=quarantine_policy,\n            trust_policy=trust_policy,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('RegistryPolicies', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-containerregistry\/azure\/mgmt\/containerregistry\/v2018_02_01_preview\/operations\/registries_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SchedulerManagementService.check_job_collection_name","method_code":"def check_job_collection_name(self, cloud_service_id, job_collection_id):\n        _validate_not_none('cloud_service_id', cloud_service_id)\n        _validate_not_none('job_collection_id', job_collection_id)\n\n        path = self._get_cloud_services_path(\n            cloud_service_id, \"scheduler\", \"jobCollections\")\n        path += \"?op=checknameavailability&resourceName=\" + job_collection_id\n        return self._perform_post(path, None, AvailabilityResponse)","method_summary":"The Check Name Availability operation checks if a new job collection with the given name may be created, or if it is unavailable. The result of the operation is a Boolean true or false.","original_method_code":"def check_job_collection_name(self, cloud_service_id, job_collection_id):\n        '''\n        The Check Name Availability operation checks if a new job collection with\n        the given name may be created, or if it is unavailable. The result of the\n        operation is a Boolean true or false.\n\n        cloud_service_id:\n            The cloud service id\n        job_collection_id:\n            The name of the job_collection_id.\n        '''\n        _validate_not_none('cloud_service_id', cloud_service_id)\n        _validate_not_none('job_collection_id', job_collection_id)\n\n        path = self._get_cloud_services_path(\n            cloud_service_id, \"scheduler\", \"jobCollections\")\n        path += \"?op=checknameavailability&resourceName=\" + job_collection_id\n        return self._perform_post(path, None, AvailabilityResponse)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/schedulermanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"SchedulerManagementService.get_job_collection","method_code":"def get_job_collection(self, cloud_service_id, job_collection_id):\n        _validate_not_none('cloud_service_id', cloud_service_id)\n        _validate_not_none('job_collection_id', job_collection_id)\n\n        path = self._get_job_collection_path(\n            cloud_service_id, job_collection_id)\n\n        return self._perform_get(path, Resource)","method_summary":"The Get Job Collection operation gets the details of a job collection","original_method_code":"def get_job_collection(self, cloud_service_id, job_collection_id):\n        '''\n        The Get Job Collection operation gets the details of a job collection\n\n        cloud_service_id:\n            The cloud service id\n        job_collection_id:\n            Name of the hosted service.\n        '''\n        _validate_not_none('cloud_service_id', cloud_service_id)\n        _validate_not_none('job_collection_id', job_collection_id)\n\n        path = self._get_job_collection_path(\n            cloud_service_id, job_collection_id)\n\n        return self._perform_get(path, Resource)","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/schedulermanagementservice.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ManagedDatabasesOperations.complete_restore","method_code":"def complete_restore(\n            self, location_name, operation_id, last_backup_name, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._complete_restore_initial(\n            location_name=location_name,\n            operation_id=operation_id,\n            last_backup_name=last_backup_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Completes the restore operation on a managed database.","original_method_code":"def complete_restore(\n            self, location_name, operation_id, last_backup_name, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Completes the restore operation on a managed database.\n\n        :param location_name: The name of the region where the resource is\n         located.\n        :type location_name: str\n        :param operation_id: Management operation id that this request tries\n         to complete.\n        :type operation_id: str\n        :param last_backup_name: The last backup name to apply\n        :type last_backup_name: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        raw_result = self._complete_restore_initial(\n            location_name=location_name,\n            operation_id=operation_id,\n            last_backup_name=last_backup_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-sql\/azure\/mgmt\/sql\/operations\/managed_databases_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"Sender.cancel_scheduled_messages","method_code":"async def cancel_scheduled_messages(self, *sequence_numbers):\n        if not self.running:\n            await self.open()\n        numbers = [types.AMQPLong(s) for s in sequence_numbers]\n        request_body = {'sequence-numbers': types.AMQPArray(numbers)}\n        return await self._mgmt_request_response(\n            REQUEST_RESPONSE_CANCEL_SCHEDULED_MESSAGE_OPERATION,\n            request_body,\n            mgmt_handlers.default)","method_summary":"Cancel one or more messages that have previsouly been scheduled and are still pending.","original_method_code":"async def cancel_scheduled_messages(self, *sequence_numbers):\n        \"\"\"Cancel one or more messages that have previsouly been scheduled and are still pending.\n\n        :param sequence_numbers: The seqeuence numbers of the scheduled messages.\n        :type sequence_numbers: int\n\n        Example:\n            .. literalinclude:: ..\/examples\/async_examples\/test_examples_async.py\n                :start-after: [START cancel_schedule_messages]\n                :end-before: [END cancel_schedule_messages]\n                :language: python\n                :dedent: 4\n                :caption: Schedule messages.\n\n        \"\"\"\n        if not self.running:\n            await self.open()\n        numbers = [types.AMQPLong(s) for s in sequence_numbers]\n        request_body = {'sequence-numbers': types.AMQPArray(numbers)}\n        return await self._mgmt_request_response(\n            REQUEST_RESPONSE_CANCEL_SCHEDULED_MESSAGE_OPERATION,\n            request_body,\n            mgmt_handlers.default)","method_path":"azure-servicebus\/azure\/servicebus\/aio\/async_send_handler.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"Sender.send_pending_messages","method_code":"async def send_pending_messages(self):\n        if not self.running:\n            await self.open()\n        try:\n            pending = self._handler._pending_messages[:]  \n            await self._handler.wait_async()\n            results = []\n            for m in pending:\n                if m.state == constants.MessageState.SendFailed:\n                    results.append((False, MessageSendFailed(m._response)))  \n                else:\n                    results.append((True, None))\n            return results\n        except Exception as e:  \n            raise MessageSendFailed(e)","method_summary":"Wait until all pending messages have been sent.","original_method_code":"async def send_pending_messages(self):\n        \"\"\"Wait until all pending messages have been sent.\n\n        :returns: A list of the send results of all the pending messages. Each\n         send result is a tuple with two values. The first is a boolean, indicating `True`\n         if the message sent, or `False` if it failed. The second is an error if the message\n         failed, otherwise it will be `None`.\n        :rtype: list[tuple[bool, ~azure.servicebus.common.errors.MessageSendFailed]]\n\n        Example:\n            .. literalinclude:: ..\/examples\/async_examples\/test_examples_async.py\n                :start-after: [START queue_sender_messages]\n                :end-before: [END queue_sender_messages]\n                :language: python\n                :dedent: 4\n                :caption: Schedule messages.\n\n        \"\"\"\n        if not self.running:\n            await self.open()\n        try:\n            pending = self._handler._pending_messages[:]  # pylint: disable=protected-access\n            await self._handler.wait_async()\n            results = []\n            for m in pending:\n                if m.state == constants.MessageState.SendFailed:\n                    results.append((False, MessageSendFailed(m._response)))  # pylint: disable=protected-access\n                else:\n                    results.append((True, None))\n            return results\n        except Exception as e:  # pylint: disable=broad-except\n            raise MessageSendFailed(e)","method_path":"azure-servicebus\/azure\/servicebus\/aio\/async_send_handler.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"get_certificate_from_publish_settings","method_code":"def get_certificate_from_publish_settings(publish_settings_path, path_to_write_certificate, subscription_id=None):\n    import base64\n\n    try:\n        from xml.etree import cElementTree as ET\n    except ImportError:\n        from xml.etree import ElementTree as ET\n\n    try:\n        import OpenSSL.crypto as crypto\n    except:\n        raise Exception(\"pyopenssl is required to use get_certificate_from_publish_settings\")\n\n    _validate_not_none('publish_settings_path', publish_settings_path)\n    _validate_not_none('path_to_write_certificate', path_to_write_certificate)\n\n    \n    tree = ET.parse(publish_settings_path)\n    subscriptions = tree.getroot().findall(\".\/PublishProfile\/Subscription\")\n    \n    \n    \n    if subscription_id:\n        subscription = next((s for s in subscriptions if s.get('Id').lower() == subscription_id.lower()), None)\n    else:\n        subscription = subscriptions[0]\n\n    \n    if subscription is None:\n        raise ValueError(\"The provided subscription_id '{}' was not found in the publish settings file provided at '{}'\".format(subscription_id, publish_settings_path))\n\n    cert_string = _decode_base64_to_bytes(subscription.get('ManagementCertificate'))\n\n    \n    cert = crypto.load_pkcs12(cert_string, b'') \n\n    \n    with open(path_to_write_certificate, 'wb') as f:\n        f.write(crypto.dump_certificate(crypto.FILETYPE_PEM, cert.get_certificate()))\n        f.write(crypto.dump_privatekey(crypto.FILETYPE_PEM, cert.get_privatekey()))\n\n    return subscription.get('Id')","method_summary":"Writes a certificate file to the specified location. This can then be used to instantiate ServiceManagementService.","original_method_code":"def get_certificate_from_publish_settings(publish_settings_path, path_to_write_certificate, subscription_id=None):\n    '''\n    Writes a certificate file to the specified location.  This can then be used \n    to instantiate ServiceManagementService.  Returns the subscription ID.\n\n    publish_settings_path: \n        Path to subscription file downloaded from \n        http:\/\/go.microsoft.com\/fwlink\/?LinkID=301775\n    path_to_write_certificate:\n        Path to write the certificate file.\n    subscription_id:\n        (optional)  Provide a subscription id here if you wish to use a \n        specific subscription under the publish settings file.\n    '''\n    import base64\n\n    try:\n        from xml.etree import cElementTree as ET\n    except ImportError:\n        from xml.etree import ElementTree as ET\n\n    try:\n        import OpenSSL.crypto as crypto\n    except:\n        raise Exception(\"pyopenssl is required to use get_certificate_from_publish_settings\")\n\n    _validate_not_none('publish_settings_path', publish_settings_path)\n    _validate_not_none('path_to_write_certificate', path_to_write_certificate)\n\n    # parse the publishsettings file and find the ManagementCertificate Entry\n    tree = ET.parse(publish_settings_path)\n    subscriptions = tree.getroot().findall(\".\/PublishProfile\/Subscription\")\n    \n    # Default to the first subscription in the file if they don't specify\n    # or get the matching subscription or return none.\n    if subscription_id:\n        subscription = next((s for s in subscriptions if s.get('Id').lower() == subscription_id.lower()), None)\n    else:\n        subscription = subscriptions[0]\n\n    # validate that subscription was found\n    if subscription is None:\n        raise ValueError(\"The provided subscription_id '{}' was not found in the publish settings file provided at '{}'\".format(subscription_id, publish_settings_path))\n\n    cert_string = _decode_base64_to_bytes(subscription.get('ManagementCertificate'))\n\n    # Load the string in pkcs12 format.  Don't provide a password as it isn't encrypted.\n    cert = crypto.load_pkcs12(cert_string, b'') \n\n    # Write the data out as a PEM format to a random location in temp for use under this run.\n    with open(path_to_write_certificate, 'wb') as f:\n        f.write(crypto.dump_certificate(crypto.FILETYPE_PEM, cert.get_certificate()))\n        f.write(crypto.dump_privatekey(crypto.FILETYPE_PEM, cert.get_privatekey()))\n\n    return subscription.get('Id')","method_path":"azure-servicemanagement-legacy\/azure\/servicemanagement\/publishsettings.py"}
{"repo_name":"streamlink\/streamlink","method_name":"Plugin.load_cookies","method_code":"def load_cookies(self):\n        if not self.session or not self.cache:\n            raise RuntimeError(\"Cannot loaded cached cookies in unbound plugin\")\n\n        restored = []\n\n        for key, value in self.cache.get_all().items():\n            if key.startswith(\"__cookie\"):\n                cookie = requests.cookies.create_cookie(**value)\n                self.session.http.cookies.set_cookie(cookie)\n                restored.append(cookie.name)\n\n        if restored:\n            self.logger.debug(\"Restored cookies: {0}\".format(\", \".join(restored)))\n        return restored","method_summary":"Load any stored cookies for the plugin that have not expired.","original_method_code":"def load_cookies(self):\n        \"\"\"\n        Load any stored cookies for the plugin that have not expired.\n\n        :return: list of the restored cookie names\n        \"\"\"\n        if not self.session or not self.cache:\n            raise RuntimeError(\"Cannot loaded cached cookies in unbound plugin\")\n\n        restored = []\n\n        for key, value in self.cache.get_all().items():\n            if key.startswith(\"__cookie\"):\n                cookie = requests.cookies.create_cookie(**value)\n                self.session.http.cookies.set_cookie(cookie)\n                restored.append(cookie.name)\n\n        if restored:\n            self.logger.debug(\"Restored cookies: {0}\".format(\", \".join(restored)))\n        return restored","method_path":"src\/streamlink\/plugin\/plugin.py"}
{"repo_name":"streamlink\/streamlink","method_name":"get_cut_prefix","method_code":"def get_cut_prefix(value, max_len):\n    should_convert = isinstance(value, bytes)\n    if should_convert:\n        value = value.decode(\"utf8\", \"ignore\")\n    for i in range(len(value)):\n        if terminal_width(value[i:]) <= max_len:\n            break\n    return value[i:].encode(\"utf8\", \"ignore\") if should_convert else value[i:]","method_summary":"Drops Characters by unicode not by bytes.","original_method_code":"def get_cut_prefix(value, max_len):\n    \"\"\"Drops Characters by unicode not by bytes.\"\"\"\n    should_convert = isinstance(value, bytes)\n    if should_convert:\n        value = value.decode(\"utf8\", \"ignore\")\n    for i in range(len(value)):\n        if terminal_width(value[i:]) <= max_len:\n            break\n    return value[i:].encode(\"utf8\", \"ignore\") if should_convert else value[i:]","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"print_inplace","method_code":"def print_inplace(msg):\n    term_width = get_terminal_size().columns\n    spacing = term_width - terminal_width(msg)\n\n    \n    if is_win32:\n        spacing -= 1\n\n    sys.stderr.write(\"\\r{0}\".format(msg))\n    sys.stderr.write(\" \" * max(0, spacing))\n    sys.stderr.flush()","method_summary":"Clears out the previous line and prints a new one.","original_method_code":"def print_inplace(msg):\n    \"\"\"Clears out the previous line and prints a new one.\"\"\"\n    term_width = get_terminal_size().columns\n    spacing = term_width - terminal_width(msg)\n\n    # On windows we need one less space or we overflow the line for some reason.\n    if is_win32:\n        spacing -= 1\n\n    sys.stderr.write(\"\\r{0}\".format(msg))\n    sys.stderr.write(\" \" * max(0, spacing))\n    sys.stderr.flush()","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"format_filesize","method_code":"def format_filesize(size):\n    for suffix in (\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if size < 1024.0:\n            if suffix in (\"GB\", \"TB\"):\n                return \"{0:3.2f} {1}\".format(size, suffix)\n            else:\n                return \"{0:3.1f} {1}\".format(size, suffix)\n\n        size \/= 1024.0","method_summary":"Formats the file size into a human readable format.","original_method_code":"def format_filesize(size):\n    \"\"\"Formats the file size into a human readable format.\"\"\"\n    for suffix in (\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if size < 1024.0:\n            if suffix in (\"GB\", \"TB\"):\n                return \"{0:3.2f} {1}\".format(size, suffix)\n            else:\n                return \"{0:3.1f} {1}\".format(size, suffix)\n\n        size \/= 1024.0","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"format_time","method_code":"def format_time(elapsed):\n    hours = int(elapsed \/ (60 * 60))\n    minutes = int((elapsed % (60 * 60)) \/ 60)\n    seconds = int(elapsed % 60)\n\n    rval = \"\"\n    if hours:\n        rval += \"{0}h\".format(hours)\n\n    if elapsed > 60:\n        rval += \"{0}m\".format(minutes)\n\n    rval += \"{0}s\".format(seconds)\n    return rval","method_summary":"Formats elapsed seconds into a human readable format.","original_method_code":"def format_time(elapsed):\n    \"\"\"Formats elapsed seconds into a human readable format.\"\"\"\n    hours = int(elapsed \/ (60 * 60))\n    minutes = int((elapsed % (60 * 60)) \/ 60)\n    seconds = int(elapsed % 60)\n\n    rval = \"\"\n    if hours:\n        rval += \"{0}h\".format(hours)\n\n    if elapsed > 60:\n        rval += \"{0}m\".format(minutes)\n\n    rval += \"{0}s\".format(seconds)\n    return rval","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"create_status_line","method_code":"def create_status_line(**params):\n    max_size = get_terminal_size().columns - 1\n\n    for fmt in PROGRESS_FORMATS:\n        status = fmt.format(**params)\n\n        if len(status) <= max_size:\n            break\n\n    return status","method_summary":"Creates a status line with appropriate size.","original_method_code":"def create_status_line(**params):\n    \"\"\"Creates a status line with appropriate size.\"\"\"\n    max_size = get_terminal_size().columns - 1\n\n    for fmt in PROGRESS_FORMATS:\n        status = fmt.format(**params)\n\n        if len(status) <= max_size:\n            break\n\n    return status","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"progress","method_code":"def progress(iterator, prefix):\n    if terminal_width(prefix) > 25:\n        prefix = (\"..\" + get_cut_prefix(prefix, 23))\n    speed_updated = start = time()\n    speed_written = written = 0\n    speed_history = deque(maxlen=5)\n\n    for data in iterator:\n        yield data\n\n        now = time()\n        elapsed = now - start\n        written += len(data)\n\n        speed_elapsed = now - speed_updated\n        if speed_elapsed >= 0.5:\n            speed_history.appendleft((\n                written - speed_written,\n                speed_updated,\n            ))\n            speed_updated = now\n            speed_written = written\n\n            speed_history_written = sum(h[0] for h in speed_history)\n            speed_history_elapsed = now - speed_history[-1][1]\n            speed = speed_history_written \/ speed_history_elapsed\n\n            status = create_status_line(\n                prefix=prefix,\n                written=format_filesize(written),\n                elapsed=format_time(elapsed),\n                speed=format_filesize(speed)\n            )\n            print_inplace(status)\n    sys.stderr.write(\"\\n\")\n    sys.stderr.flush()","method_summary":"Progress an iterator and updates a pretty status line to the terminal. The status line","original_method_code":"def progress(iterator, prefix):\n    \"\"\"Progress an iterator and updates a pretty status line to the terminal.\n\n    The status line contains:\n     - Amount of data read from the iterator\n     - Time elapsed\n     - Average speed, based on the last few seconds.\n    \"\"\"\n    if terminal_width(prefix) > 25:\n        prefix = (\"..\" + get_cut_prefix(prefix, 23))\n    speed_updated = start = time()\n    speed_written = written = 0\n    speed_history = deque(maxlen=5)\n\n    for data in iterator:\n        yield data\n\n        now = time()\n        elapsed = now - start\n        written += len(data)\n\n        speed_elapsed = now - speed_updated\n        if speed_elapsed >= 0.5:\n            speed_history.appendleft((\n                written - speed_written,\n                speed_updated,\n            ))\n            speed_updated = now\n            speed_written = written\n\n            speed_history_written = sum(h[0] for h in speed_history)\n            speed_history_elapsed = now - speed_history[-1][1]\n            speed = speed_history_written \/ speed_history_elapsed\n\n            status = create_status_line(\n                prefix=prefix,\n                written=format_filesize(written),\n                elapsed=format_time(elapsed),\n                speed=format_filesize(speed)\n            )\n            print_inplace(status)\n    sys.stderr.write(\"\\n\")\n    sys.stderr.flush()","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"SegmentedStreamWorker.wait","method_code":"def wait(self, time):\n        self._wait = Event()\n        return not self._wait.wait(time)","method_summary":"Pauses the thread for a specified time.","original_method_code":"def wait(self, time):\n        \"\"\"Pauses the thread for a specified time.\n\n        Returns False if interrupted by another thread and True if the\n        time runs out normally.\n        \"\"\"\n        self._wait = Event()\n        return not self._wait.wait(time)","method_path":"src\/streamlink\/stream\/segmented.py"}
{"repo_name":"streamlink\/streamlink","method_name":"SegmentedStreamWriter.put","method_code":"def put(self, segment):\n        if self.closed:\n            return\n\n        if segment is not None:\n            future = self.executor.submit(self.fetch, segment,\n                                          retries=self.retries)\n        else:\n            future = None\n\n        self.queue(self.futures, (segment, future))","method_summary":"Adds a segment to the download pool and write queue.","original_method_code":"def put(self, segment):\n        \"\"\"Adds a segment to the download pool and write queue.\"\"\"\n        if self.closed:\n            return\n\n        if segment is not None:\n            future = self.executor.submit(self.fetch, segment,\n                                          retries=self.retries)\n        else:\n            future = None\n\n        self.queue(self.futures, (segment, future))","method_path":"src\/streamlink\/stream\/segmented.py"}
{"repo_name":"streamlink\/streamlink","method_name":"SegmentedStreamWriter.queue","method_code":"def queue(self, queue_, value):\n        while not self.closed:\n            try:\n                queue_.put(value, block=True, timeout=1)\n                return\n            except queue.Full:\n                continue","method_summary":"Puts a value into a queue but aborts if this thread is closed.","original_method_code":"def queue(self, queue_, value):\n        \"\"\"Puts a value into a queue but aborts if this thread is closed.\"\"\"\n        while not self.closed:\n            try:\n                queue_.put(value, block=True, timeout=1)\n                return\n            except queue.Full:\n                continue","method_path":"src\/streamlink\/stream\/segmented.py"}
{"repo_name":"streamlink\/streamlink","method_name":"BBCiPlayer.find_vpid","method_code":"def find_vpid(self, url, res=None):\n        log.debug(\"Looking for vpid on {0}\", url)\n        \n        res = res or self.session.http.get(url)\n        m = self.mediator_re.search(res.text)\n        vpid = m and parse_json(m.group(1), schema=self.mediator_schema)\n        return vpid","method_summary":"Find the Video Packet ID in the HTML for the provided URL","original_method_code":"def find_vpid(self, url, res=None):\n        \"\"\"\n        Find the Video Packet ID in the HTML for the provided URL\n\n        :param url: URL to download, if res is not provided.\n        :param res: Provide a cached version of the HTTP response to search\n        :type url: string\n        :type res: requests.Response\n        :return: Video Packet ID for a Programme in iPlayer\n        :rtype: string\n        \"\"\"\n        log.debug(\"Looking for vpid on {0}\", url)\n        # Use pre-fetched page if available\n        res = res or self.session.http.get(url)\n        m = self.mediator_re.search(res.text)\n        vpid = m and parse_json(m.group(1), schema=self.mediator_schema)\n        return vpid","method_path":"src\/streamlink\/plugins\/bbciplayer.py"}
{"repo_name":"streamlink\/streamlink","method_name":"parse_json","method_code":"def parse_json(data, name=\"JSON\", exception=PluginError, schema=None):\n    try:\n        json_data = json.loads(data)\n    except ValueError as err:\n        snippet = repr(data)\n        if len(snippet) > 35:\n            snippet = snippet[:35] + \" ...\"\n        else:\n            snippet = data\n\n        raise exception(\"Unable to parse {0}: {1} ({2})\".format(name, err, snippet))\n\n    if schema:\n        json_data = schema.validate(json_data, name=name, exception=exception)\n\n    return json_data","method_summary":"Wrapper around json.loads. Wraps errors in custom exception with a snippet of the data in the message.","original_method_code":"def parse_json(data, name=\"JSON\", exception=PluginError, schema=None):\n    \"\"\"Wrapper around json.loads.\n\n    Wraps errors in custom exception with a snippet of the data in the message.\n    \"\"\"\n    try:\n        json_data = json.loads(data)\n    except ValueError as err:\n        snippet = repr(data)\n        if len(snippet) > 35:\n            snippet = snippet[:35] + \" ...\"\n        else:\n            snippet = data\n\n        raise exception(\"Unable to parse {0}: {1} ({2})\".format(name, err, snippet))\n\n    if schema:\n        json_data = schema.validate(json_data, name=name, exception=exception)\n\n    return json_data","method_path":"src\/streamlink\/utils\/__init__.py"}
{"repo_name":"streamlink\/streamlink","method_name":"parse_xml","method_code":"def parse_xml(data, name=\"XML\", ignore_ns=False, exception=PluginError, schema=None, invalid_char_entities=False):\n    if is_py2 and isinstance(data, unicode):\n        data = data.encode(\"utf8\")\n    elif is_py3 and isinstance(data, str):\n        data = bytearray(data, \"utf8\")\n\n    if ignore_ns:\n        data = re.sub(br\"[\\t ]xmlns=\\\"(.+?)\\\"\", b\"\", data)\n\n    if invalid_char_entities:\n        data = re.sub(br'&(?!(?:#(?:[0-9]+|[Xx][0-9A-Fa-f]+)|[A-Za-z0-9]+);)', b'&amp;', data)\n\n    try:\n        tree = ET.fromstring(data)\n    except Exception as err:\n        snippet = repr(data)\n        if len(snippet) > 35:\n            snippet = snippet[:35] + \" ...\"\n\n        raise exception(\"Unable to parse {0}: {1} ({2})\".format(name, err, snippet))\n\n    if schema:\n        tree = schema.validate(tree, name=name, exception=exception)\n\n    return tree","method_summary":"Wrapper around ElementTree.fromstring with some extras. Provides these extra","original_method_code":"def parse_xml(data, name=\"XML\", ignore_ns=False, exception=PluginError, schema=None, invalid_char_entities=False):\n    \"\"\"Wrapper around ElementTree.fromstring with some extras.\n\n    Provides these extra features:\n     - Handles incorrectly encoded XML\n     - Allows stripping namespace information\n     - Wraps errors in custom exception with a snippet of the data in the message\n    \"\"\"\n    if is_py2 and isinstance(data, unicode):\n        data = data.encode(\"utf8\")\n    elif is_py3 and isinstance(data, str):\n        data = bytearray(data, \"utf8\")\n\n    if ignore_ns:\n        data = re.sub(br\"[\\t ]xmlns=\\\"(.+?)\\\"\", b\"\", data)\n\n    if invalid_char_entities:\n        data = re.sub(br'&(?!(?:#(?:[0-9]+|[Xx][0-9A-Fa-f]+)|[A-Za-z0-9]+);)', b'&amp;', data)\n\n    try:\n        tree = ET.fromstring(data)\n    except Exception as err:\n        snippet = repr(data)\n        if len(snippet) > 35:\n            snippet = snippet[:35] + \" ...\"\n\n        raise exception(\"Unable to parse {0}: {1} ({2})\".format(name, err, snippet))\n\n    if schema:\n        tree = schema.validate(tree, name=name, exception=exception)\n\n    return tree","method_path":"src\/streamlink\/utils\/__init__.py"}
{"repo_name":"streamlink\/streamlink","method_name":"parse_qsd","method_code":"def parse_qsd(data, name=\"query string\", exception=PluginError, schema=None, **params):\n    value = dict(parse_qsl(data, **params))\n    if schema:\n        value = schema.validate(value, name=name, exception=exception)\n\n    return value","method_summary":"Parses a query string into a dict. Unlike parse_qs and parse_qsl, duplicate keys are not preserved in favor of a simpler return value.","original_method_code":"def parse_qsd(data, name=\"query string\", exception=PluginError, schema=None, **params):\n    \"\"\"Parses a query string into a dict.\n\n    Unlike parse_qs and parse_qsl, duplicate keys are not preserved in\n    favor of a simpler return value.\n    \"\"\"\n\n    value = dict(parse_qsl(data, **params))\n    if schema:\n        value = schema.validate(value, name=name, exception=exception)\n\n    return value","method_path":"src\/streamlink\/utils\/__init__.py"}
{"repo_name":"streamlink\/streamlink","method_name":"search_dict","method_code":"def search_dict(data, key):\n    if isinstance(data, dict):\n        for dkey, value in data.items():\n            if dkey == key:\n                yield value\n            for result in search_dict(value, key):\n                yield result\n    elif isinstance(data, list):\n        for value in data:\n            for result in search_dict(value, key):\n                yield result","method_summary":"Search for a key in a nested dict, or list of nested dicts, and return the values.","original_method_code":"def search_dict(data, key):\n    \"\"\"\n    Search for a key in a nested dict, or list of nested dicts, and return the values.\n\n    :param data: dict\/list to search\n    :param key: key to find\n    :return: matches for key\n    \"\"\"\n    if isinstance(data, dict):\n        for dkey, value in data.items():\n            if dkey == key:\n                yield value\n            for result in search_dict(value, key):\n                yield result\n    elif isinstance(data, list):\n        for value in data:\n            for result in search_dict(value, key):\n                yield result","method_path":"src\/streamlink\/utils\/__init__.py"}
{"repo_name":"streamlink\/streamlink","method_name":"DASHStream.parse_manifest","method_code":"def parse_manifest(cls, session, url_or_manifest, **args):\n        ret = {}\n\n        if url_or_manifest.startswith('<?xml'):\n            mpd = MPD(parse_xml(url_or_manifest, ignore_ns=True))\n        else:\n            res = session.http.get(url_or_manifest, **args)\n            url = res.url\n\n            urlp = list(urlparse(url))\n            urlp[2], _ = urlp[2].rsplit(\"\/\", 1)\n\n            mpd = MPD(session.http.xml(res, ignore_ns=True), base_url=urlunparse(urlp), url=url)\n\n        video, audio = [], []\n\n        \n        for aset in mpd.periods[0].adaptationSets:\n            if aset.contentProtection:\n                raise PluginError(\"{} is protected by DRM\".format(url))\n            for rep in aset.representations:\n                if rep.mimeType.startswith(\"video\"):\n                    video.append(rep)\n                elif rep.mimeType.startswith(\"audio\"):\n                    audio.append(rep)\n\n        if not video:\n            video = [None]\n\n        if not audio:\n            audio = [None]\n\n        locale = session.localization\n        locale_lang = locale.language\n        lang = None\n        available_languages = set()\n\n        \n        for aud in audio:\n            if aud and aud.lang:\n                available_languages.add(aud.lang)\n                try:\n                    if locale.explicit and aud.lang and Language.get(aud.lang) == locale_lang:\n                        lang = aud.lang\n                except LookupError:\n                    continue\n\n        if not lang:\n            \n            lang = audio[0] and audio[0].lang\n\n        log.debug(\"Available languages for DASH audio streams: {0} (using: {1})\".format(\", \".join(available_languages) or \"NONE\", lang or \"n\/a\"))\n\n        \n        if len(available_languages) > 1:\n            audio = list(filter(lambda a: a.lang is None or a.lang == lang, audio))\n\n        for vid, aud in itertools.product(video, audio):\n            stream = DASHStream(session, mpd, vid, aud, **args)\n            stream_name = []\n\n            if vid:\n                stream_name.append(\"{:0.0f}{}\".format(vid.height or vid.bandwidth_rounded, \"p\" if vid.height else \"k\"))\n            if audio and len(audio) > 1:\n                stream_name.append(\"a{:0.0f}k\".format(aud.bandwidth))\n            ret['+'.join(stream_name)] = stream\n        return ret","method_summary":"Attempt to parse a DASH manifest file and return its streams","original_method_code":"def parse_manifest(cls, session, url_or_manifest, **args):\n        \"\"\"\n        Attempt to parse a DASH manifest file and return its streams\n\n        :param session: Streamlink session instance\n        :param url_or_manifest: URL of the manifest file or an XML manifest string\n        :return: a dict of name -> DASHStream instances\n        \"\"\"\n        ret = {}\n\n        if url_or_manifest.startswith('<?xml'):\n            mpd = MPD(parse_xml(url_or_manifest, ignore_ns=True))\n        else:\n            res = session.http.get(url_or_manifest, **args)\n            url = res.url\n\n            urlp = list(urlparse(url))\n            urlp[2], _ = urlp[2].rsplit(\"\/\", 1)\n\n            mpd = MPD(session.http.xml(res, ignore_ns=True), base_url=urlunparse(urlp), url=url)\n\n        video, audio = [], []\n\n        # Search for suitable video and audio representations\n        for aset in mpd.periods[0].adaptationSets:\n            if aset.contentProtection:\n                raise PluginError(\"{} is protected by DRM\".format(url))\n            for rep in aset.representations:\n                if rep.mimeType.startswith(\"video\"):\n                    video.append(rep)\n                elif rep.mimeType.startswith(\"audio\"):\n                    audio.append(rep)\n\n        if not video:\n            video = [None]\n\n        if not audio:\n            audio = [None]\n\n        locale = session.localization\n        locale_lang = locale.language\n        lang = None\n        available_languages = set()\n\n        # if the locale is explicitly set, prefer that language over others\n        for aud in audio:\n            if aud and aud.lang:\n                available_languages.add(aud.lang)\n                try:\n                    if locale.explicit and aud.lang and Language.get(aud.lang) == locale_lang:\n                        lang = aud.lang\n                except LookupError:\n                    continue\n\n        if not lang:\n            # filter by the first language that appears\n            lang = audio[0] and audio[0].lang\n\n        log.debug(\"Available languages for DASH audio streams: {0} (using: {1})\".format(\", \".join(available_languages) or \"NONE\", lang or \"n\/a\"))\n\n        # if the language is given by the stream, filter out other languages that do not match\n        if len(available_languages) > 1:\n            audio = list(filter(lambda a: a.lang is None or a.lang == lang, audio))\n\n        for vid, aud in itertools.product(video, audio):\n            stream = DASHStream(session, mpd, vid, aud, **args)\n            stream_name = []\n\n            if vid:\n                stream_name.append(\"{:0.0f}{}\".format(vid.height or vid.bandwidth_rounded, \"p\" if vid.height else \"k\"))\n            if audio and len(audio) > 1:\n                stream_name.append(\"a{:0.0f}k\".format(aud.bandwidth))\n            ret['+'.join(stream_name)] = stream\n        return ret","method_path":"src\/streamlink\/stream\/dash.py"}
{"repo_name":"streamlink\/streamlink","method_name":"HTTPSession.determine_json_encoding","method_code":"def determine_json_encoding(cls, sample):\n        nulls_at = [i for i, j in enumerate(bytearray(sample[:4])) if j == 0]\n        if nulls_at == [0, 1, 2]:\n            return \"UTF-32BE\"\n        elif nulls_at == [0, 2]:\n            return \"UTF-16BE\"\n        elif nulls_at == [1, 2, 3]:\n            return \"UTF-32LE\"\n        elif nulls_at == [1, 3]:\n            return \"UTF-16LE\"\n        else:\n            return \"UTF-8\"","method_summary":"Determine which Unicode encoding the JSON text sample is encoded with RFC4627 (","original_method_code":"def determine_json_encoding(cls, sample):\n        \"\"\"\n        Determine which Unicode encoding the JSON text sample is encoded with\n\n        RFC4627 (http:\/\/www.ietf.org\/rfc\/rfc4627.txt) suggests that the encoding of JSON text can be determined\n        by checking the pattern of NULL bytes in first 4 octets of the text.\n        :param sample: a sample of at least 4 bytes of the JSON text\n        :return: the most likely encoding of the JSON text\n        \"\"\"\n        nulls_at = [i for i, j in enumerate(bytearray(sample[:4])) if j == 0]\n        if nulls_at == [0, 1, 2]:\n            return \"UTF-32BE\"\n        elif nulls_at == [0, 2]:\n            return \"UTF-16BE\"\n        elif nulls_at == [1, 2, 3]:\n            return \"UTF-32LE\"\n        elif nulls_at == [1, 3]:\n            return \"UTF-16LE\"\n        else:\n            return \"UTF-8\"","method_path":"src\/streamlink\/plugin\/api\/http_session.py"}
{"repo_name":"streamlink\/streamlink","method_name":"HTTPSession.json","method_code":"def json(cls, res, *args, **kwargs):\n        \n        if res.encoding is None:\n            res.encoding = cls.determine_json_encoding(res.content[:4])\n        return parse_json(res.text, *args, **kwargs)","method_summary":"Parses JSON from a response.","original_method_code":"def json(cls, res, *args, **kwargs):\n        \"\"\"Parses JSON from a response.\"\"\"\n        # if an encoding is already set then use the provided encoding\n        if res.encoding is None:\n            res.encoding = cls.determine_json_encoding(res.content[:4])\n        return parse_json(res.text, *args, **kwargs)","method_path":"src\/streamlink\/plugin\/api\/http_session.py"}
{"repo_name":"streamlink\/streamlink","method_name":"HTTPSession.xml","method_code":"def xml(cls, res, *args, **kwargs):\n        return parse_xml(res.text, *args, **kwargs)","method_summary":"Parses XML from a response.","original_method_code":"def xml(cls, res, *args, **kwargs):\n        \"\"\"Parses XML from a response.\"\"\"\n        return parse_xml(res.text, *args, **kwargs)","method_path":"src\/streamlink\/plugin\/api\/http_session.py"}
{"repo_name":"streamlink\/streamlink","method_name":"HTTPSession.parse_cookies","method_code":"def parse_cookies(self, cookies, **kwargs):\n        for name, value in _parse_keyvalue_list(cookies):\n            self.cookies.set(name, value, **kwargs)","method_summary":"Parses a semi-colon delimited list of cookies.","original_method_code":"def parse_cookies(self, cookies, **kwargs):\n        \"\"\"Parses a semi-colon delimited list of cookies.\n\n        Example: foo=bar;baz=qux\n        \"\"\"\n        for name, value in _parse_keyvalue_list(cookies):\n            self.cookies.set(name, value, **kwargs)","method_path":"src\/streamlink\/plugin\/api\/http_session.py"}
{"repo_name":"streamlink\/streamlink","method_name":"HTTPSession.parse_headers","method_code":"def parse_headers(self, headers):\n        for name, value in _parse_keyvalue_list(headers):\n            self.headers[name] = value","method_summary":"Parses a semi-colon delimited list of headers.","original_method_code":"def parse_headers(self, headers):\n        \"\"\"Parses a semi-colon delimited list of headers.\n\n        Example: foo=bar;baz=qux\n        \"\"\"\n        for name, value in _parse_keyvalue_list(headers):\n            self.headers[name] = value","method_path":"src\/streamlink\/plugin\/api\/http_session.py"}
{"repo_name":"streamlink\/streamlink","method_name":"HTTPSession.parse_query_params","method_code":"def parse_query_params(self, cookies, **kwargs):\n        for name, value in _parse_keyvalue_list(cookies):\n            self.params[name] = value","method_summary":"Parses a semi-colon delimited list of query parameters.","original_method_code":"def parse_query_params(self, cookies, **kwargs):\n        \"\"\"Parses a semi-colon delimited list of query parameters.\n\n        Example: foo=bar;baz=qux\n        \"\"\"\n        for name, value in _parse_keyvalue_list(cookies):\n            self.params[name] = value","method_path":"src\/streamlink\/plugin\/api\/http_session.py"}
{"repo_name":"streamlink\/streamlink","method_name":"_LogRecord.getMessage","method_code":"def getMessage(self):\n        msg = self.msg\n        if self.args:\n            msg = msg.format(*self.args)\n        return maybe_encode(msg)","method_summary":"Return the message for this LogRecord. Return the message for this LogRecord after merging any user-supplied arguments with the message.","original_method_code":"def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        msg = self.msg\n        if self.args:\n            msg = msg.format(*self.args)\n        return maybe_encode(msg)","method_path":"src\/streamlink\/logger.py"}
{"repo_name":"streamlink\/streamlink","method_name":"StreamlinkLogger.makeRecord","method_code":"def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n                   func=None, extra=None, sinfo=None):\n        if name.startswith(\"streamlink\"):\n            rv = _LogRecord(name, level, fn, lno, msg, args, exc_info, func, sinfo)\n        else:\n            rv = _CompatLogRecord(name, level, fn, lno, msg, args, exc_info, func, sinfo)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv","method_summary":"A factory method which can be overridden in subclasses to create specialized LogRecords.","original_method_code":"def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n                   func=None, extra=None, sinfo=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        if name.startswith(\"streamlink\"):\n            rv = _LogRecord(name, level, fn, lno, msg, args, exc_info, func, sinfo)\n        else:\n            rv = _CompatLogRecord(name, level, fn, lno, msg, args, exc_info, func, sinfo)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv","method_path":"src\/streamlink\/logger.py"}
{"repo_name":"streamlink\/streamlink","method_name":"LiveEdu.login","method_code":"def login(self):\n        email = self.get_option(\"email\")\n        password = self.get_option(\"password\")\n\n        if email and password:\n            res = self.session.http.get(self.login_url)\n            csrf_match = self.csrf_re.search(res.text)\n            token = csrf_match and csrf_match.group(1)\n            self.logger.debug(\"Attempting login as {0} (token={1})\", email, token)\n\n            res = self.session.http.post(self.login_url,\n                            data=dict(login=email, password=password, csrfmiddlewaretoken=token),\n                            allow_redirects=False,\n                            raise_for_status=False,\n                            headers={\"Referer\": self.login_url})\n\n            if res.status_code != 302:\n                self.logger.error(\"Failed to login to LiveEdu account: {0}\", email)","method_summary":"Attempt a login to LiveEdu.tv","original_method_code":"def login(self):\n        \"\"\"\n        Attempt a login to LiveEdu.tv\n        \"\"\"\n        email = self.get_option(\"email\")\n        password = self.get_option(\"password\")\n\n        if email and password:\n            res = self.session.http.get(self.login_url)\n            csrf_match = self.csrf_re.search(res.text)\n            token = csrf_match and csrf_match.group(1)\n            self.logger.debug(\"Attempting login as {0} (token={1})\", email, token)\n\n            res = self.session.http.post(self.login_url,\n                            data=dict(login=email, password=password, csrfmiddlewaretoken=token),\n                            allow_redirects=False,\n                            raise_for_status=False,\n                            headers={\"Referer\": self.login_url})\n\n            if res.status_code != 302:\n                self.logger.error(\"Failed to login to LiveEdu account: {0}\", email)","method_path":"src\/streamlink\/plugins\/liveedu.py"}
{"repo_name":"streamlink\/streamlink","method_name":"update_qsd","method_code":"def update_qsd(url, qsd=None, remove=None):\n    qsd = qsd or {}\n    remove = remove or []\n\n    \n    parsed = urlparse(url)\n    current_qsd = OrderedDict(parse_qsl(parsed.query))\n\n    \n    if remove == \"*\":\n        remove = list(current_qsd.keys())\n\n    \n    for key in remove:\n        if key not in qsd:\n            del current_qsd[key]\n\n    \n    for key, value in qsd.items():\n        if value:\n            current_qsd[key] = value\n\n    return parsed._replace(query=urlencode(current_qsd)).geturl()","method_summary":"Update or remove keys from a query string in a URL","original_method_code":"def update_qsd(url, qsd=None, remove=None):\n    \"\"\"\n    Update or remove keys from a query string in a URL\n\n    :param url: URL to update\n    :param qsd: dict of keys to update, a None value leaves it unchanged\n    :param remove: list of keys to remove, or \"*\" to remove all\n                   note: updated keys are never removed, even if unchanged\n    :return: updated URL\n    \"\"\"\n    qsd = qsd or {}\n    remove = remove or []\n\n    # parse current query string\n    parsed = urlparse(url)\n    current_qsd = OrderedDict(parse_qsl(parsed.query))\n\n    # * removes all possible keys\n    if remove == \"*\":\n        remove = list(current_qsd.keys())\n\n    # remove keys before updating, but leave updated keys untouched\n    for key in remove:\n        if key not in qsd:\n            del current_qsd[key]\n\n    # and update the query string\n    for key, value in qsd.items():\n        if value:\n            current_qsd[key] = value\n\n    return parsed._replace(query=urlencode(current_qsd)).geturl()","method_path":"src\/streamlink\/utils\/url.py"}
{"repo_name":"streamlink\/streamlink","method_name":"FLVTagConcat.iter_chunks","method_code":"def iter_chunks(self, fd=None, buf=None, skip_header=None):\n        timestamps = dict(self.timestamps_add)\n        tag_iterator = self.iter_tags(fd=fd, buf=buf, skip_header=skip_header)\n\n        if not self.flv_header_written:\n            analyzed_tags = self.analyze_tags(tag_iterator)\n        else:\n            analyzed_tags = []\n\n        for tag in chain(analyzed_tags, tag_iterator):\n            if not self.flv_header_written:\n                flv_header = Header(has_video=self.has_video,\n                                    has_audio=self.has_audio)\n                yield flv_header.serialize()\n                self.flv_header_written = True\n\n            if self.verify_tag(tag):\n                self.adjust_tag_gap(tag)\n                self.adjust_tag_timestamp(tag)\n\n                if self.duration:\n                    norm_timestamp = tag.timestamp \/ 1000\n                    if norm_timestamp > self.duration:\n                        break\n                yield tag.serialize()\n                timestamps[tag.type] = tag.timestamp\n\n        if not self.flatten_timestamps:\n            self.timestamps_add = timestamps\n\n        self.tags = []","method_summary":"Reads FLV tags from fd or buf and returns them with adjusted timestamps.","original_method_code":"def iter_chunks(self, fd=None, buf=None, skip_header=None):\n        \"\"\"Reads FLV tags from fd or buf and returns them with adjusted\n           timestamps.\"\"\"\n        timestamps = dict(self.timestamps_add)\n        tag_iterator = self.iter_tags(fd=fd, buf=buf, skip_header=skip_header)\n\n        if not self.flv_header_written:\n            analyzed_tags = self.analyze_tags(tag_iterator)\n        else:\n            analyzed_tags = []\n\n        for tag in chain(analyzed_tags, tag_iterator):\n            if not self.flv_header_written:\n                flv_header = Header(has_video=self.has_video,\n                                    has_audio=self.has_audio)\n                yield flv_header.serialize()\n                self.flv_header_written = True\n\n            if self.verify_tag(tag):\n                self.adjust_tag_gap(tag)\n                self.adjust_tag_timestamp(tag)\n\n                if self.duration:\n                    norm_timestamp = tag.timestamp \/ 1000\n                    if norm_timestamp > self.duration:\n                        break\n                yield tag.serialize()\n                timestamps[tag.type] = tag.timestamp\n\n        if not self.flatten_timestamps:\n            self.timestamps_add = timestamps\n\n        self.tags = []","method_path":"src\/streamlink\/stream\/flvconcat.py"}
{"repo_name":"streamlink\/streamlink","method_name":"Arguments.requires","method_code":"def requires(self, name):\n        results = set([name])\n        argument = self.get(name)\n        for reqname in argument.requires:\n            required = self.get(reqname)\n            if not required:\n                raise KeyError(\"{0} is not a valid argument for this plugin\".format(reqname))\n\n            if required.name in results:\n                raise RuntimeError(\"cycle detected in plugin argument config\")\n            results.add(required.name)\n            yield required\n\n            for r in self.requires(required.name):\n                if r.name in results:\n                    raise RuntimeError(\"cycle detected in plugin argument config\")\n                results.add(r.name)\n                yield r","method_summary":"Find all the arguments required by name","original_method_code":"def requires(self, name):\n        \"\"\"\n        Find all the arguments required by name\n\n        :param name: name of the argument the find the dependencies\n\n        :return: list of dependant arguments\n        \"\"\"\n        results = set([name])\n        argument = self.get(name)\n        for reqname in argument.requires:\n            required = self.get(reqname)\n            if not required:\n                raise KeyError(\"{0} is not a valid argument for this plugin\".format(reqname))\n\n            if required.name in results:\n                raise RuntimeError(\"cycle detected in plugin argument config\")\n            results.add(required.name)\n            yield required\n\n            for r in self.requires(required.name):\n                if r.name in results:\n                    raise RuntimeError(\"cycle detected in plugin argument config\")\n                results.add(r.name)\n                yield r","method_path":"src\/streamlink\/options.py"}
{"repo_name":"streamlink\/streamlink","method_name":"check_file_output","method_code":"def check_file_output(filename, force):\n    log.debug(\"Checking file output\")\n\n    if os.path.isfile(filename) and not force:\n        if sys.stdin.isatty():\n            answer = console.ask(\"File {0} already exists! Overwrite it? [y\/N] \",\n                                 filename)\n\n            if answer.lower() != \"y\":\n                sys.exit()\n        else:\n            log.error(\"File {0} already exists, use --force to overwrite it.\".format(filename))\n            sys.exit()\n\n    return FileOutput(filename)","method_summary":"Checks if file already exists and ask the user if it should be overwritten if it does.","original_method_code":"def check_file_output(filename, force):\n    \"\"\"Checks if file already exists and ask the user if it should\n    be overwritten if it does.\"\"\"\n\n    log.debug(\"Checking file output\")\n\n    if os.path.isfile(filename) and not force:\n        if sys.stdin.isatty():\n            answer = console.ask(\"File {0} already exists! Overwrite it? [y\/N] \",\n                                 filename)\n\n            if answer.lower() != \"y\":\n                sys.exit()\n        else:\n            log.error(\"File {0} already exists, use --force to overwrite it.\".format(filename))\n            sys.exit()\n\n    return FileOutput(filename)","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"create_output","method_code":"def create_output(plugin):\n    if (args.output or args.stdout) and (args.record or args.record_and_pipe):\n        console.exit(\"Cannot use record options with other file output options.\")\n\n    if args.output:\n        if args.output == \"-\":\n            out = FileOutput(fd=stdout)\n        else:\n            out = check_file_output(args.output, args.force)\n    elif args.stdout:\n        out = FileOutput(fd=stdout)\n    elif args.record_and_pipe:\n        record = check_file_output(args.record_and_pipe, args.force)\n        out = FileOutput(fd=stdout, record=record)\n    else:\n        http = namedpipe = record = None\n\n        if not args.player:\n            console.exit(\"The default player (VLC) does not seem to be \"\n                         \"installed. You must specify the path to a player \"\n                         \"executable with --player.\")\n\n        if args.player_fifo:\n            pipename = \"streamlinkpipe-{0}\".format(os.getpid())\n            log.info(\"Creating pipe {0}\", pipename)\n\n            try:\n                namedpipe = NamedPipe(pipename)\n            except IOError as err:\n                console.exit(\"Failed to create pipe: {0}\", err)\n        elif args.player_http:\n            http = create_http_server()\n\n        title = create_title(plugin)\n\n        if args.record:\n            record = check_file_output(args.record, args.force)\n\n        log.info(\"Starting player: {0}\", args.player)\n\n        out = PlayerOutput(args.player, args=args.player_args,\n                           quiet=not args.verbose_player,\n                           kill=not args.player_no_close,\n                           namedpipe=namedpipe, http=http,\n                           record=record, title=title)\n\n    return out","method_summary":"Decides where to write the stream. Depending on arguments it can be one of","original_method_code":"def create_output(plugin):\n    \"\"\"Decides where to write the stream.\n\n    Depending on arguments it can be one of these:\n     - The stdout pipe\n     - A subprocess' stdin pipe\n     - A named pipe that the subprocess reads from\n     - A regular file\n\n    \"\"\"\n\n    if (args.output or args.stdout) and (args.record or args.record_and_pipe):\n        console.exit(\"Cannot use record options with other file output options.\")\n\n    if args.output:\n        if args.output == \"-\":\n            out = FileOutput(fd=stdout)\n        else:\n            out = check_file_output(args.output, args.force)\n    elif args.stdout:\n        out = FileOutput(fd=stdout)\n    elif args.record_and_pipe:\n        record = check_file_output(args.record_and_pipe, args.force)\n        out = FileOutput(fd=stdout, record=record)\n    else:\n        http = namedpipe = record = None\n\n        if not args.player:\n            console.exit(\"The default player (VLC) does not seem to be \"\n                         \"installed. You must specify the path to a player \"\n                         \"executable with --player.\")\n\n        if args.player_fifo:\n            pipename = \"streamlinkpipe-{0}\".format(os.getpid())\n            log.info(\"Creating pipe {0}\", pipename)\n\n            try:\n                namedpipe = NamedPipe(pipename)\n            except IOError as err:\n                console.exit(\"Failed to create pipe: {0}\", err)\n        elif args.player_http:\n            http = create_http_server()\n\n        title = create_title(plugin)\n\n        if args.record:\n            record = check_file_output(args.record, args.force)\n\n        log.info(\"Starting player: {0}\", args.player)\n\n        out = PlayerOutput(args.player, args=args.player_args,\n                           quiet=not args.verbose_player,\n                           kill=not args.player_no_close,\n                           namedpipe=namedpipe, http=http,\n                           record=record, title=title)\n\n    return out","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"create_http_server","method_code":"def create_http_server(host=None, port=0):\n    try:\n        http = HTTPServer()\n        http.bind(host=host, port=port)\n    except OSError as err:\n        console.exit(\"Failed to create HTTP server: {0}\", err)\n\n    return http","method_summary":"Creates a HTTP server listening on a given host and port. If host is empty, listen on all available interfaces, and if port is 0, listen on a random high port.","original_method_code":"def create_http_server(host=None, port=0):\n    \"\"\"Creates a HTTP server listening on a given host and port.\n\n    If host is empty, listen on all available interfaces, and if port is 0,\n    listen on a random high port.\n    \"\"\"\n\n    try:\n        http = HTTPServer()\n        http.bind(host=host, port=port)\n    except OSError as err:\n        console.exit(\"Failed to create HTTP server: {0}\", err)\n\n    return http","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"iter_http_requests","method_code":"def iter_http_requests(server, player):\n    while not player or player.running:\n        try:\n            yield server.open(timeout=2.5)\n        except OSError:\n            continue","method_summary":"Repeatedly accept HTTP connections on a server. Forever if the serving externally, or while a player is running if it is not empty.","original_method_code":"def iter_http_requests(server, player):\n    \"\"\"Repeatedly accept HTTP connections on a server.\n\n    Forever if the serving externally, or while a player is running if it is not\n    empty.\n    \"\"\"\n\n    while not player or player.running:\n        try:\n            yield server.open(timeout=2.5)\n        except OSError:\n            continue","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"output_stream_http","method_code":"def output_stream_http(plugin, initial_streams, external=False, port=0):\n    global output\n\n    if not external:\n        if not args.player:\n            console.exit(\"The default player (VLC) does not seem to be \"\n                         \"installed. You must specify the path to a player \"\n                         \"executable with --player.\")\n\n        title = create_title(plugin)\n        server = create_http_server()\n        player = output = PlayerOutput(args.player, args=args.player_args,\n                                       filename=server.url,\n                                       quiet=not args.verbose_player,\n                                       title=title)\n\n        try:\n            log.info(\"Starting player: {0}\", args.player)\n            if player:\n                player.open()\n        except OSError as err:\n            console.exit(\"Failed to start player: {0} ({1})\",\n                         args.player, err)\n    else:\n        server = create_http_server(host=None, port=port)\n        player = None\n\n        log.info(\"Starting server, access with one of:\")\n        for url in server.urls:\n            log.info(\" \" + url)\n\n    for req in iter_http_requests(server, player):\n        user_agent = req.headers.get(\"User-Agent\") or \"unknown player\"\n        log.info(\"Got HTTP request from {0}\".format(user_agent))\n\n        stream_fd = prebuffer = None\n        while not stream_fd and (not player or player.running):\n            try:\n                streams = initial_streams or fetch_streams(plugin)\n                initial_streams = None\n\n                for stream_name in (resolve_stream_name(streams, s) for s in args.stream):\n                    if stream_name in streams:\n                        stream = streams[stream_name]\n                        break\n                else:\n                    log.info(\"Stream not available, will re-fetch \"\n                             \"streams in 10 sec\")\n                    sleep(10)\n                    continue\n            except PluginError as err:\n                log.error(u\"Unable to fetch new streams: {0}\", err)\n                continue\n\n            try:\n                log.info(\"Opening stream: {0} ({1})\", stream_name,\n                         type(stream).shortname())\n                stream_fd, prebuffer = open_stream(stream)\n            except StreamError as err:\n                log.error(\"{0}\", err)\n\n        if stream_fd and prebuffer:\n            log.debug(\"Writing stream to player\")\n            read_stream(stream_fd, server, prebuffer)\n\n        server.close(True)\n\n    player.close()\n    server.close()","method_summary":"Continuously output the stream over HTTP.","original_method_code":"def output_stream_http(plugin, initial_streams, external=False, port=0):\n    \"\"\"Continuously output the stream over HTTP.\"\"\"\n    global output\n\n    if not external:\n        if not args.player:\n            console.exit(\"The default player (VLC) does not seem to be \"\n                         \"installed. You must specify the path to a player \"\n                         \"executable with --player.\")\n\n        title = create_title(plugin)\n        server = create_http_server()\n        player = output = PlayerOutput(args.player, args=args.player_args,\n                                       filename=server.url,\n                                       quiet=not args.verbose_player,\n                                       title=title)\n\n        try:\n            log.info(\"Starting player: {0}\", args.player)\n            if player:\n                player.open()\n        except OSError as err:\n            console.exit(\"Failed to start player: {0} ({1})\",\n                         args.player, err)\n    else:\n        server = create_http_server(host=None, port=port)\n        player = None\n\n        log.info(\"Starting server, access with one of:\")\n        for url in server.urls:\n            log.info(\" \" + url)\n\n    for req in iter_http_requests(server, player):\n        user_agent = req.headers.get(\"User-Agent\") or \"unknown player\"\n        log.info(\"Got HTTP request from {0}\".format(user_agent))\n\n        stream_fd = prebuffer = None\n        while not stream_fd and (not player or player.running):\n            try:\n                streams = initial_streams or fetch_streams(plugin)\n                initial_streams = None\n\n                for stream_name in (resolve_stream_name(streams, s) for s in args.stream):\n                    if stream_name in streams:\n                        stream = streams[stream_name]\n                        break\n                else:\n                    log.info(\"Stream not available, will re-fetch \"\n                             \"streams in 10 sec\")\n                    sleep(10)\n                    continue\n            except PluginError as err:\n                log.error(u\"Unable to fetch new streams: {0}\", err)\n                continue\n\n            try:\n                log.info(\"Opening stream: {0} ({1})\", stream_name,\n                         type(stream).shortname())\n                stream_fd, prebuffer = open_stream(stream)\n            except StreamError as err:\n                log.error(\"{0}\", err)\n\n        if stream_fd and prebuffer:\n            log.debug(\"Writing stream to player\")\n            read_stream(stream_fd, server, prebuffer)\n\n        server.close(True)\n\n    player.close()\n    server.close()","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"output_stream_passthrough","method_code":"def output_stream_passthrough(plugin, stream):\n    global output\n\n    title = create_title(plugin)\n    filename = '\"{0}\"'.format(stream_to_url(stream))\n    output = PlayerOutput(args.player, args=args.player_args,\n                          filename=filename, call=True,\n                          quiet=not args.verbose_player,\n                          title=title)\n\n    try:\n        log.info(\"Starting player: {0}\", args.player)\n        output.open()\n    except OSError as err:\n        console.exit(\"Failed to start player: {0} ({1})\", args.player, err)\n        return False\n\n    return True","method_summary":"Prepares a filename to be passed to the player.","original_method_code":"def output_stream_passthrough(plugin, stream):\n    \"\"\"Prepares a filename to be passed to the player.\"\"\"\n    global output\n\n    title = create_title(plugin)\n    filename = '\"{0}\"'.format(stream_to_url(stream))\n    output = PlayerOutput(args.player, args=args.player_args,\n                          filename=filename, call=True,\n                          quiet=not args.verbose_player,\n                          title=title)\n\n    try:\n        log.info(\"Starting player: {0}\", args.player)\n        output.open()\n    except OSError as err:\n        console.exit(\"Failed to start player: {0} ({1})\", args.player, err)\n        return False\n\n    return True","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"open_stream","method_code":"def open_stream(stream):\n    global stream_fd\n\n    \n    try:\n        stream_fd = stream.open()\n    except StreamError as err:\n        raise StreamError(\"Could not open stream: {0}\".format(err))\n\n    \n    \n    try:\n        log.debug(\"Pre-buffering 8192 bytes\")\n        prebuffer = stream_fd.read(8192)\n    except IOError as err:\n        stream_fd.close()\n        raise StreamError(\"Failed to read data from stream: {0}\".format(err))\n\n    if not prebuffer:\n        stream_fd.close()\n        raise StreamError(\"No data returned from stream\")\n\n    return stream_fd, prebuffer","method_summary":"Opens a stream and reads 8192 bytes from it. This is useful to check if a stream actually has data before opening the output.","original_method_code":"def open_stream(stream):\n    \"\"\"Opens a stream and reads 8192 bytes from it.\n\n    This is useful to check if a stream actually has data\n    before opening the output.\n\n    \"\"\"\n    global stream_fd\n\n    # Attempts to open the stream\n    try:\n        stream_fd = stream.open()\n    except StreamError as err:\n        raise StreamError(\"Could not open stream: {0}\".format(err))\n\n    # Read 8192 bytes before proceeding to check for errors.\n    # This is to avoid opening the output unnecessarily.\n    try:\n        log.debug(\"Pre-buffering 8192 bytes\")\n        prebuffer = stream_fd.read(8192)\n    except IOError as err:\n        stream_fd.close()\n        raise StreamError(\"Failed to read data from stream: {0}\".format(err))\n\n    if not prebuffer:\n        stream_fd.close()\n        raise StreamError(\"No data returned from stream\")\n\n    return stream_fd, prebuffer","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"output_stream","method_code":"def output_stream(plugin, stream):\n    global output\n\n    success_open = False\n    for i in range(args.retry_open):\n        try:\n            stream_fd, prebuffer = open_stream(stream)\n            success_open = True\n            break\n        except StreamError as err:\n            log.error(\"Try {0}\/{1}: Could not open stream {2} ({3})\", i + 1, args.retry_open, stream, err)\n\n    if not success_open:\n        console.exit(\"Could not open stream {0}, tried {1} times, exiting\", stream, args.retry_open)\n\n    output = create_output(plugin)\n\n    try:\n        output.open()\n    except (IOError, OSError) as err:\n        if isinstance(output, PlayerOutput):\n            console.exit(\"Failed to start player: {0} ({1})\",\n                         args.player, err)\n        else:\n            console.exit(\"Failed to open output: {0} ({1})\",\n                         args.output, err)\n\n    with closing(output):\n        log.debug(\"Writing stream to output\")\n        read_stream(stream_fd, output, prebuffer)\n\n    return True","method_summary":"Open stream, create output and finally write the stream to output.","original_method_code":"def output_stream(plugin, stream):\n    \"\"\"Open stream, create output and finally write the stream to output.\"\"\"\n    global output\n\n    success_open = False\n    for i in range(args.retry_open):\n        try:\n            stream_fd, prebuffer = open_stream(stream)\n            success_open = True\n            break\n        except StreamError as err:\n            log.error(\"Try {0}\/{1}: Could not open stream {2} ({3})\", i + 1, args.retry_open, stream, err)\n\n    if not success_open:\n        console.exit(\"Could not open stream {0}, tried {1} times, exiting\", stream, args.retry_open)\n\n    output = create_output(plugin)\n\n    try:\n        output.open()\n    except (IOError, OSError) as err:\n        if isinstance(output, PlayerOutput):\n            console.exit(\"Failed to start player: {0} ({1})\",\n                         args.player, err)\n        else:\n            console.exit(\"Failed to open output: {0} ({1})\",\n                         args.output, err)\n\n    with closing(output):\n        log.debug(\"Writing stream to output\")\n        read_stream(stream_fd, output, prebuffer)\n\n    return True","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"read_stream","method_code":"def read_stream(stream, output, prebuffer, chunk_size=8192):\n    is_player = isinstance(output, PlayerOutput)\n    is_http = isinstance(output, HTTPServer)\n    is_fifo = is_player and output.namedpipe\n    show_progress = isinstance(output, FileOutput) and output.fd is not stdout and sys.stdout.isatty()\n    show_record_progress = hasattr(output, \"record\") and isinstance(output.record, FileOutput) and output.record.fd is not stdout and sys.stdout.isatty()\n\n    stream_iterator = chain(\n        [prebuffer],\n        iter(partial(stream.read, chunk_size), b\"\")\n    )\n    if show_progress:\n        stream_iterator = progress(stream_iterator,\n                                   prefix=os.path.basename(args.output))\n    elif show_record_progress:\n        stream_iterator = progress(stream_iterator,\n                                   prefix=os.path.basename(args.record))\n\n    try:\n        for data in stream_iterator:\n            \n            \n            \n            if is_win32 and is_fifo:\n                output.player.poll()\n\n                if output.player.returncode is not None:\n                    log.info(\"Player closed\")\n                    break\n\n            try:\n                output.write(data)\n            except IOError as err:\n                if is_player and err.errno in ACCEPTABLE_ERRNO:\n                    log.info(\"Player closed\")\n                elif is_http and err.errno in ACCEPTABLE_ERRNO:\n                    log.info(\"HTTP connection closed\")\n                else:\n                    console.exit(\"Error when writing to output: {0}, exiting\", err)\n\n                break\n    except IOError as err:\n        console.exit(\"Error when reading from stream: {0}, exiting\", err)\n    finally:\n        stream.close()\n        log.info(\"Stream ended\")","method_summary":"Reads data from stream and then writes it to the output.","original_method_code":"def read_stream(stream, output, prebuffer, chunk_size=8192):\n    \"\"\"Reads data from stream and then writes it to the output.\"\"\"\n    is_player = isinstance(output, PlayerOutput)\n    is_http = isinstance(output, HTTPServer)\n    is_fifo = is_player and output.namedpipe\n    show_progress = isinstance(output, FileOutput) and output.fd is not stdout and sys.stdout.isatty()\n    show_record_progress = hasattr(output, \"record\") and isinstance(output.record, FileOutput) and output.record.fd is not stdout and sys.stdout.isatty()\n\n    stream_iterator = chain(\n        [prebuffer],\n        iter(partial(stream.read, chunk_size), b\"\")\n    )\n    if show_progress:\n        stream_iterator = progress(stream_iterator,\n                                   prefix=os.path.basename(args.output))\n    elif show_record_progress:\n        stream_iterator = progress(stream_iterator,\n                                   prefix=os.path.basename(args.record))\n\n    try:\n        for data in stream_iterator:\n            # We need to check if the player process still exists when\n            # using named pipes on Windows since the named pipe is not\n            # automatically closed by the player.\n            if is_win32 and is_fifo:\n                output.player.poll()\n\n                if output.player.returncode is not None:\n                    log.info(\"Player closed\")\n                    break\n\n            try:\n                output.write(data)\n            except IOError as err:\n                if is_player and err.errno in ACCEPTABLE_ERRNO:\n                    log.info(\"Player closed\")\n                elif is_http and err.errno in ACCEPTABLE_ERRNO:\n                    log.info(\"HTTP connection closed\")\n                else:\n                    console.exit(\"Error when writing to output: {0}, exiting\", err)\n\n                break\n    except IOError as err:\n        console.exit(\"Error when reading from stream: {0}, exiting\", err)\n    finally:\n        stream.close()\n        log.info(\"Stream ended\")","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"handle_stream","method_code":"def handle_stream(plugin, streams, stream_name):\n    stream_name = resolve_stream_name(streams, stream_name)\n    stream = streams[stream_name]\n\n    \n    \n    if args.subprocess_cmdline:\n        if isinstance(stream, StreamProcess):\n            try:\n                cmdline = stream.cmdline()\n            except StreamError as err:\n                console.exit(\"{0}\", err)\n\n            console.msg(\"{0}\", cmdline)\n        else:\n            console.exit(\"The stream specified cannot be translated to a command\")\n\n    \n    elif console.json:\n        console.msg_json(stream)\n\n    elif args.stream_url:\n        try:\n            console.msg(\"{0}\", stream.to_url())\n        except TypeError:\n            console.exit(\"The stream specified cannot be translated to a URL\")\n\n    \n    else:\n        \n        \n        alt_streams = list(filter(lambda k: stream_name + \"_alt\" in k,\n                                  sorted(streams.keys())))\n        file_output = args.output or args.stdout\n\n        for stream_name in [stream_name] + alt_streams:\n            stream = streams[stream_name]\n            stream_type = type(stream).shortname()\n\n            if stream_type in args.player_passthrough and not file_output:\n                log.info(\"Opening stream: {0} ({1})\", stream_name,\n                         stream_type)\n                success = output_stream_passthrough(plugin, stream)\n            elif args.player_external_http:\n                return output_stream_http(plugin, streams, external=True,\n                                          port=args.player_external_http_port)\n            elif args.player_continuous_http and not file_output:\n                return output_stream_http(plugin, streams)\n            else:\n                log.info(\"Opening stream: {0} ({1})\", stream_name,\n                         stream_type)\n\n                success = output_stream(plugin, stream)\n\n            if success:\n                break","method_summary":"Decides what to do with the selected stream. Depending on arguments it can be one of","original_method_code":"def handle_stream(plugin, streams, stream_name):\n    \"\"\"Decides what to do with the selected stream.\n\n    Depending on arguments it can be one of these:\n     - Output internal command-line\n     - Output JSON represenation\n     - Continuously output the stream over HTTP\n     - Output stream data to selected output\n\n    \"\"\"\n\n    stream_name = resolve_stream_name(streams, stream_name)\n    stream = streams[stream_name]\n\n    # Print internal command-line if this stream\n    # uses a subprocess.\n    if args.subprocess_cmdline:\n        if isinstance(stream, StreamProcess):\n            try:\n                cmdline = stream.cmdline()\n            except StreamError as err:\n                console.exit(\"{0}\", err)\n\n            console.msg(\"{0}\", cmdline)\n        else:\n            console.exit(\"The stream specified cannot be translated to a command\")\n\n    # Print JSON representation of the stream\n    elif console.json:\n        console.msg_json(stream)\n\n    elif args.stream_url:\n        try:\n            console.msg(\"{0}\", stream.to_url())\n        except TypeError:\n            console.exit(\"The stream specified cannot be translated to a URL\")\n\n    # Output the stream\n    else:\n        # Find any streams with a '_alt' suffix and attempt\n        # to use these in case the main stream is not usable.\n        alt_streams = list(filter(lambda k: stream_name + \"_alt\" in k,\n                                  sorted(streams.keys())))\n        file_output = args.output or args.stdout\n\n        for stream_name in [stream_name] + alt_streams:\n            stream = streams[stream_name]\n            stream_type = type(stream).shortname()\n\n            if stream_type in args.player_passthrough and not file_output:\n                log.info(\"Opening stream: {0} ({1})\", stream_name,\n                         stream_type)\n                success = output_stream_passthrough(plugin, stream)\n            elif args.player_external_http:\n                return output_stream_http(plugin, streams, external=True,\n                                          port=args.player_external_http_port)\n            elif args.player_continuous_http and not file_output:\n                return output_stream_http(plugin, streams)\n            else:\n                log.info(\"Opening stream: {0} ({1})\", stream_name,\n                         stream_type)\n\n                success = output_stream(plugin, stream)\n\n            if success:\n                break","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"fetch_streams","method_code":"def fetch_streams(plugin):\n    return plugin.streams(stream_types=args.stream_types,\n                          sorting_excludes=args.stream_sorting_excludes)","method_summary":"Fetches streams using correct parameters.","original_method_code":"def fetch_streams(plugin):\n    \"\"\"Fetches streams using correct parameters.\"\"\"\n\n    return plugin.streams(stream_types=args.stream_types,\n                          sorting_excludes=args.stream_sorting_excludes)","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"fetch_streams_with_retry","method_code":"def fetch_streams_with_retry(plugin, interval, count):\n    try:\n        streams = fetch_streams(plugin)\n    except PluginError as err:\n        log.error(u\"{0}\", err)\n        streams = None\n\n    if not streams:\n        log.info(\"Waiting for streams, retrying every {0} \"\n                 \"second(s)\", interval)\n    attempts = 0\n\n    while not streams:\n        sleep(interval)\n\n        try:\n            streams = fetch_streams(plugin)\n        except FatalPluginError as err:\n            raise\n        except PluginError as err:\n            log.error(u\"{0}\", err)\n\n        if count > 0:\n            attempts += 1\n            if attempts >= count:\n                break\n\n    return streams","method_summary":"Attempts to fetch streams repeatedly until some are returned or limit hit.","original_method_code":"def fetch_streams_with_retry(plugin, interval, count):\n    \"\"\"Attempts to fetch streams repeatedly\n       until some are returned or limit hit.\"\"\"\n\n    try:\n        streams = fetch_streams(plugin)\n    except PluginError as err:\n        log.error(u\"{0}\", err)\n        streams = None\n\n    if not streams:\n        log.info(\"Waiting for streams, retrying every {0} \"\n                 \"second(s)\", interval)\n    attempts = 0\n\n    while not streams:\n        sleep(interval)\n\n        try:\n            streams = fetch_streams(plugin)\n        except FatalPluginError as err:\n            raise\n        except PluginError as err:\n            log.error(u\"{0}\", err)\n\n        if count > 0:\n            attempts += 1\n            if attempts >= count:\n                break\n\n    return streams","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"format_valid_streams","method_code":"def format_valid_streams(plugin, streams):\n    delimiter = \", \"\n    validstreams = []\n\n    for name, stream in sorted(streams.items(),\n                               key=lambda stream: plugin.stream_weight(stream[0])):\n        if name in STREAM_SYNONYMS:\n            continue\n\n        def synonymfilter(n):\n            return stream is streams[n] and n is not name\n\n        synonyms = list(filter(synonymfilter, streams.keys()))\n\n        if len(synonyms) > 0:\n            joined = delimiter.join(synonyms)\n            name = \"{0} ({1})\".format(name, joined)\n\n        validstreams.append(name)\n\n    return delimiter.join(validstreams)","method_summary":"Formats a dict of streams. Filters out synonyms and displays them next to the stream they point to. Streams are sorted according to their quality (based on plugin.stream_weight).","original_method_code":"def format_valid_streams(plugin, streams):\n    \"\"\"Formats a dict of streams.\n\n    Filters out synonyms and displays them next to\n    the stream they point to.\n\n    Streams are sorted according to their quality\n    (based on plugin.stream_weight).\n\n    \"\"\"\n\n    delimiter = \", \"\n    validstreams = []\n\n    for name, stream in sorted(streams.items(),\n                               key=lambda stream: plugin.stream_weight(stream[0])):\n        if name in STREAM_SYNONYMS:\n            continue\n\n        def synonymfilter(n):\n            return stream is streams[n] and n is not name\n\n        synonyms = list(filter(synonymfilter, streams.keys()))\n\n        if len(synonyms) > 0:\n            joined = delimiter.join(synonyms)\n            name = \"{0} ({1})\".format(name, joined)\n\n        validstreams.append(name)\n\n    return delimiter.join(validstreams)","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"print_plugins","method_code":"def print_plugins():\n    pluginlist = list(streamlink.get_plugins().keys())\n    pluginlist_formatted = \", \".join(sorted(pluginlist))\n\n    if console.json:\n        console.msg_json(pluginlist)\n    else:\n        console.msg(\"Loaded plugins: {0}\", pluginlist_formatted)","method_summary":"Outputs a list of all plugins Streamlink has loaded.","original_method_code":"def print_plugins():\n    \"\"\"Outputs a list of all plugins Streamlink has loaded.\"\"\"\n\n    pluginlist = list(streamlink.get_plugins().keys())\n    pluginlist_formatted = \", \".join(sorted(pluginlist))\n\n    if console.json:\n        console.msg_json(pluginlist)\n    else:\n        console.msg(\"Loaded plugins: {0}\", pluginlist_formatted)","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"authenticate_twitch_oauth","method_code":"def authenticate_twitch_oauth():\n    client_id = TWITCH_CLIENT_ID\n    redirect_uri = \"https:\/\/streamlink.github.io\/twitch_oauth.html\"\n    url = (\"https:\/\/api.twitch.tv\/kraken\/oauth2\/authorize\"\n           \"?response_type=token\"\n           \"&client_id={0}\"\n           \"&redirect_uri={1}\"\n           \"&scope=user_read+user_subscriptions\"\n           \"&force_verify=true\").format(client_id, redirect_uri)\n\n    console.msg(\"Attempting to open a browser to let you authenticate \"\n                \"Streamlink with Twitch\")\n\n    try:\n        if not webbrowser.open_new_tab(url):\n            raise webbrowser.Error\n    except webbrowser.Error:\n        console.exit(\"Unable to open a web browser, try accessing this URL \"\n                     \"manually instead:\\n{0}\".format(url))","method_summary":"Opens a web browser to allow the user to grant Streamlink access to their Twitch account.","original_method_code":"def authenticate_twitch_oauth():\n    \"\"\"Opens a web browser to allow the user to grant Streamlink\n       access to their Twitch account.\"\"\"\n\n    client_id = TWITCH_CLIENT_ID\n    redirect_uri = \"https:\/\/streamlink.github.io\/twitch_oauth.html\"\n    url = (\"https:\/\/api.twitch.tv\/kraken\/oauth2\/authorize\"\n           \"?response_type=token\"\n           \"&client_id={0}\"\n           \"&redirect_uri={1}\"\n           \"&scope=user_read+user_subscriptions\"\n           \"&force_verify=true\").format(client_id, redirect_uri)\n\n    console.msg(\"Attempting to open a browser to let you authenticate \"\n                \"Streamlink with Twitch\")\n\n    try:\n        if not webbrowser.open_new_tab(url):\n            raise webbrowser.Error\n    except webbrowser.Error:\n        console.exit(\"Unable to open a web browser, try accessing this URL \"\n                     \"manually instead:\\n{0}\".format(url))","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"load_plugins","method_code":"def load_plugins(dirs):\n    dirs = [os.path.expanduser(d) for d in dirs]\n\n    for directory in dirs:\n        if os.path.isdir(directory):\n            streamlink.load_plugins(directory)\n        else:\n            log.warning(\"Plugin path {0} does not exist or is not \"\n                        \"a directory!\", directory)","method_summary":"Attempts to load plugins from a list of directories.","original_method_code":"def load_plugins(dirs):\n    \"\"\"Attempts to load plugins from a list of directories.\"\"\"\n\n    dirs = [os.path.expanduser(d) for d in dirs]\n\n    for directory in dirs:\n        if os.path.isdir(directory):\n            streamlink.load_plugins(directory)\n        else:\n            log.warning(\"Plugin path {0} does not exist or is not \"\n                        \"a directory!\", directory)","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"setup_args","method_code":"def setup_args(parser, config_files=[], ignore_unknown=False):\n    global args\n    arglist = sys.argv[1:]\n\n    \n    for config_file in filter(os.path.isfile, config_files):\n        arglist.insert(0, \"@\" + config_file)\n\n    args, unknown = parser.parse_known_args(arglist)\n    if unknown and not ignore_unknown:\n        msg = gettext('unrecognized arguments: %s')\n        parser.error(msg % ' '.join(unknown))\n\n    \n    if args.stream:\n        args.stream = [stream.lower() for stream in args.stream]\n\n    if not args.url and args.url_param:\n        args.url = args.url_param","method_summary":"Parses arguments.","original_method_code":"def setup_args(parser, config_files=[], ignore_unknown=False):\n    \"\"\"Parses arguments.\"\"\"\n    global args\n    arglist = sys.argv[1:]\n\n    # Load arguments from config files\n    for config_file in filter(os.path.isfile, config_files):\n        arglist.insert(0, \"@\" + config_file)\n\n    args, unknown = parser.parse_known_args(arglist)\n    if unknown and not ignore_unknown:\n        msg = gettext('unrecognized arguments: %s')\n        parser.error(msg % ' '.join(unknown))\n\n    # Force lowercase to allow case-insensitive lookup\n    if args.stream:\n        args.stream = [stream.lower() for stream in args.stream]\n\n    if not args.url and args.url_param:\n        args.url = args.url_param","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"setup_console","method_code":"def setup_console(output):\n    global console\n\n    \n    console = ConsoleOutput(output, streamlink)\n    console.json = args.json\n\n    \n    signal.signal(signal.SIGTERM, signal.default_int_handler)","method_summary":"Console setup.","original_method_code":"def setup_console(output):\n    \"\"\"Console setup.\"\"\"\n    global console\n\n    # All console related operations is handled via the ConsoleOutput class\n    console = ConsoleOutput(output, streamlink)\n    console.json = args.json\n\n    # Handle SIGTERM just like SIGINT\n    signal.signal(signal.SIGTERM, signal.default_int_handler)","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"setup_http_session","method_code":"def setup_http_session():\n    if args.http_proxy:\n        streamlink.set_option(\"http-proxy\", args.http_proxy)\n\n    if args.https_proxy:\n        streamlink.set_option(\"https-proxy\", args.https_proxy)\n\n    if args.http_cookie:\n        streamlink.set_option(\"http-cookies\", dict(args.http_cookie))\n\n    if args.http_header:\n        streamlink.set_option(\"http-headers\", dict(args.http_header))\n\n    if args.http_query_param:\n        streamlink.set_option(\"http-query-params\", dict(args.http_query_param))\n\n    if args.http_ignore_env:\n        streamlink.set_option(\"http-trust-env\", False)\n\n    if args.http_no_ssl_verify:\n        streamlink.set_option(\"http-ssl-verify\", False)\n\n    if args.http_disable_dh:\n        streamlink.set_option(\"http-disable-dh\", True)\n\n    if args.http_ssl_cert:\n        streamlink.set_option(\"http-ssl-cert\", args.http_ssl_cert)\n\n    if args.http_ssl_cert_crt_key:\n        streamlink.set_option(\"http-ssl-cert\", tuple(args.http_ssl_cert_crt_key))\n\n    if args.http_timeout:\n        streamlink.set_option(\"http-timeout\", args.http_timeout)\n\n    if args.http_cookies:\n        streamlink.set_option(\"http-cookies\", args.http_cookies)\n\n    if args.http_headers:\n        streamlink.set_option(\"http-headers\", args.http_headers)\n\n    if args.http_query_params:\n        streamlink.set_option(\"http-query-params\", args.http_query_params)","method_summary":"Sets the global HTTP settings, such as proxy and headers.","original_method_code":"def setup_http_session():\n    \"\"\"Sets the global HTTP settings, such as proxy and headers.\"\"\"\n    if args.http_proxy:\n        streamlink.set_option(\"http-proxy\", args.http_proxy)\n\n    if args.https_proxy:\n        streamlink.set_option(\"https-proxy\", args.https_proxy)\n\n    if args.http_cookie:\n        streamlink.set_option(\"http-cookies\", dict(args.http_cookie))\n\n    if args.http_header:\n        streamlink.set_option(\"http-headers\", dict(args.http_header))\n\n    if args.http_query_param:\n        streamlink.set_option(\"http-query-params\", dict(args.http_query_param))\n\n    if args.http_ignore_env:\n        streamlink.set_option(\"http-trust-env\", False)\n\n    if args.http_no_ssl_verify:\n        streamlink.set_option(\"http-ssl-verify\", False)\n\n    if args.http_disable_dh:\n        streamlink.set_option(\"http-disable-dh\", True)\n\n    if args.http_ssl_cert:\n        streamlink.set_option(\"http-ssl-cert\", args.http_ssl_cert)\n\n    if args.http_ssl_cert_crt_key:\n        streamlink.set_option(\"http-ssl-cert\", tuple(args.http_ssl_cert_crt_key))\n\n    if args.http_timeout:\n        streamlink.set_option(\"http-timeout\", args.http_timeout)\n\n    if args.http_cookies:\n        streamlink.set_option(\"http-cookies\", args.http_cookies)\n\n    if args.http_headers:\n        streamlink.set_option(\"http-headers\", args.http_headers)\n\n    if args.http_query_params:\n        streamlink.set_option(\"http-query-params\", args.http_query_params)","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"setup_plugins","method_code":"def setup_plugins(extra_plugin_dir=None):\n    if os.path.isdir(PLUGINS_DIR):\n        load_plugins([PLUGINS_DIR])\n\n    if extra_plugin_dir:\n        load_plugins(extra_plugin_dir)","method_summary":"Loads any additional plugins.","original_method_code":"def setup_plugins(extra_plugin_dir=None):\n    \"\"\"Loads any additional plugins.\"\"\"\n    if os.path.isdir(PLUGINS_DIR):\n        load_plugins([PLUGINS_DIR])\n\n    if extra_plugin_dir:\n        load_plugins(extra_plugin_dir)","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"setup_options","method_code":"def setup_options():\n    if args.hls_live_edge:\n        streamlink.set_option(\"hls-live-edge\", args.hls_live_edge)\n\n    if args.hls_segment_attempts:\n        streamlink.set_option(\"hls-segment-attempts\", args.hls_segment_attempts)\n\n    if args.hls_playlist_reload_attempts:\n        streamlink.set_option(\"hls-playlist-reload-attempts\", args.hls_playlist_reload_attempts)\n\n    if args.hls_segment_threads:\n        streamlink.set_option(\"hls-segment-threads\", args.hls_segment_threads)\n\n    if args.hls_segment_timeout:\n        streamlink.set_option(\"hls-segment-timeout\", args.hls_segment_timeout)\n\n    if args.hls_segment_ignore_names:\n        streamlink.set_option(\"hls-segment-ignore-names\", args.hls_segment_ignore_names)\n\n    if args.hls_segment_key_uri:\n        streamlink.set_option(\"hls-segment-key-uri\", args.hls_segment_key_uri)\n\n    if args.hls_timeout:\n        streamlink.set_option(\"hls-timeout\", args.hls_timeout)\n\n    if args.hls_audio_select:\n        streamlink.set_option(\"hls-audio-select\", args.hls_audio_select)\n\n    if args.hls_start_offset:\n        streamlink.set_option(\"hls-start-offset\", args.hls_start_offset)\n\n    if args.hls_duration:\n        streamlink.set_option(\"hls-duration\", args.hls_duration)\n\n    if args.hls_live_restart:\n        streamlink.set_option(\"hls-live-restart\", args.hls_live_restart)\n\n    if args.hds_live_edge:\n        streamlink.set_option(\"hds-live-edge\", args.hds_live_edge)\n\n    if args.hds_segment_attempts:\n        streamlink.set_option(\"hds-segment-attempts\", args.hds_segment_attempts)\n\n    if args.hds_segment_threads:\n        streamlink.set_option(\"hds-segment-threads\", args.hds_segment_threads)\n\n    if args.hds_segment_timeout:\n        streamlink.set_option(\"hds-segment-timeout\", args.hds_segment_timeout)\n\n    if args.hds_timeout:\n        streamlink.set_option(\"hds-timeout\", args.hds_timeout)\n\n    if args.http_stream_timeout:\n        streamlink.set_option(\"http-stream-timeout\", args.http_stream_timeout)\n\n    if args.ringbuffer_size:\n        streamlink.set_option(\"ringbuffer-size\", args.ringbuffer_size)\n\n    if args.rtmp_proxy:\n        streamlink.set_option(\"rtmp-proxy\", args.rtmp_proxy)\n\n    if args.rtmp_rtmpdump:\n        streamlink.set_option(\"rtmp-rtmpdump\", args.rtmp_rtmpdump)\n\n    if args.rtmp_timeout:\n        streamlink.set_option(\"rtmp-timeout\", args.rtmp_timeout)\n\n    if args.stream_segment_attempts:\n        streamlink.set_option(\"stream-segment-attempts\", args.stream_segment_attempts)\n\n    if args.stream_segment_threads:\n        streamlink.set_option(\"stream-segment-threads\", args.stream_segment_threads)\n\n    if args.stream_segment_timeout:\n        streamlink.set_option(\"stream-segment-timeout\", args.stream_segment_timeout)\n\n    if args.stream_timeout:\n        streamlink.set_option(\"stream-timeout\", args.stream_timeout)\n\n    if args.ffmpeg_ffmpeg:\n        streamlink.set_option(\"ffmpeg-ffmpeg\", args.ffmpeg_ffmpeg)\n    if args.ffmpeg_verbose:\n        streamlink.set_option(\"ffmpeg-verbose\", args.ffmpeg_verbose)\n    if args.ffmpeg_verbose_path:\n        streamlink.set_option(\"ffmpeg-verbose-path\", args.ffmpeg_verbose_path)\n    if args.ffmpeg_video_transcode:\n        streamlink.set_option(\"ffmpeg-video-transcode\", args.ffmpeg_video_transcode)\n    if args.ffmpeg_audio_transcode:\n        streamlink.set_option(\"ffmpeg-audio-transcode\", args.ffmpeg_audio_transcode)\n\n    streamlink.set_option(\"subprocess-errorlog\", args.subprocess_errorlog)\n    streamlink.set_option(\"subprocess-errorlog-path\", args.subprocess_errorlog_path)\n    streamlink.set_option(\"locale\", args.locale)","method_summary":"Sets Streamlink options.","original_method_code":"def setup_options():\n    \"\"\"Sets Streamlink options.\"\"\"\n    if args.hls_live_edge:\n        streamlink.set_option(\"hls-live-edge\", args.hls_live_edge)\n\n    if args.hls_segment_attempts:\n        streamlink.set_option(\"hls-segment-attempts\", args.hls_segment_attempts)\n\n    if args.hls_playlist_reload_attempts:\n        streamlink.set_option(\"hls-playlist-reload-attempts\", args.hls_playlist_reload_attempts)\n\n    if args.hls_segment_threads:\n        streamlink.set_option(\"hls-segment-threads\", args.hls_segment_threads)\n\n    if args.hls_segment_timeout:\n        streamlink.set_option(\"hls-segment-timeout\", args.hls_segment_timeout)\n\n    if args.hls_segment_ignore_names:\n        streamlink.set_option(\"hls-segment-ignore-names\", args.hls_segment_ignore_names)\n\n    if args.hls_segment_key_uri:\n        streamlink.set_option(\"hls-segment-key-uri\", args.hls_segment_key_uri)\n\n    if args.hls_timeout:\n        streamlink.set_option(\"hls-timeout\", args.hls_timeout)\n\n    if args.hls_audio_select:\n        streamlink.set_option(\"hls-audio-select\", args.hls_audio_select)\n\n    if args.hls_start_offset:\n        streamlink.set_option(\"hls-start-offset\", args.hls_start_offset)\n\n    if args.hls_duration:\n        streamlink.set_option(\"hls-duration\", args.hls_duration)\n\n    if args.hls_live_restart:\n        streamlink.set_option(\"hls-live-restart\", args.hls_live_restart)\n\n    if args.hds_live_edge:\n        streamlink.set_option(\"hds-live-edge\", args.hds_live_edge)\n\n    if args.hds_segment_attempts:\n        streamlink.set_option(\"hds-segment-attempts\", args.hds_segment_attempts)\n\n    if args.hds_segment_threads:\n        streamlink.set_option(\"hds-segment-threads\", args.hds_segment_threads)\n\n    if args.hds_segment_timeout:\n        streamlink.set_option(\"hds-segment-timeout\", args.hds_segment_timeout)\n\n    if args.hds_timeout:\n        streamlink.set_option(\"hds-timeout\", args.hds_timeout)\n\n    if args.http_stream_timeout:\n        streamlink.set_option(\"http-stream-timeout\", args.http_stream_timeout)\n\n    if args.ringbuffer_size:\n        streamlink.set_option(\"ringbuffer-size\", args.ringbuffer_size)\n\n    if args.rtmp_proxy:\n        streamlink.set_option(\"rtmp-proxy\", args.rtmp_proxy)\n\n    if args.rtmp_rtmpdump:\n        streamlink.set_option(\"rtmp-rtmpdump\", args.rtmp_rtmpdump)\n\n    if args.rtmp_timeout:\n        streamlink.set_option(\"rtmp-timeout\", args.rtmp_timeout)\n\n    if args.stream_segment_attempts:\n        streamlink.set_option(\"stream-segment-attempts\", args.stream_segment_attempts)\n\n    if args.stream_segment_threads:\n        streamlink.set_option(\"stream-segment-threads\", args.stream_segment_threads)\n\n    if args.stream_segment_timeout:\n        streamlink.set_option(\"stream-segment-timeout\", args.stream_segment_timeout)\n\n    if args.stream_timeout:\n        streamlink.set_option(\"stream-timeout\", args.stream_timeout)\n\n    if args.ffmpeg_ffmpeg:\n        streamlink.set_option(\"ffmpeg-ffmpeg\", args.ffmpeg_ffmpeg)\n    if args.ffmpeg_verbose:\n        streamlink.set_option(\"ffmpeg-verbose\", args.ffmpeg_verbose)\n    if args.ffmpeg_verbose_path:\n        streamlink.set_option(\"ffmpeg-verbose-path\", args.ffmpeg_verbose_path)\n    if args.ffmpeg_video_transcode:\n        streamlink.set_option(\"ffmpeg-video-transcode\", args.ffmpeg_video_transcode)\n    if args.ffmpeg_audio_transcode:\n        streamlink.set_option(\"ffmpeg-audio-transcode\", args.ffmpeg_audio_transcode)\n\n    streamlink.set_option(\"subprocess-errorlog\", args.subprocess_errorlog)\n    streamlink.set_option(\"subprocess-errorlog-path\", args.subprocess_errorlog_path)\n    streamlink.set_option(\"locale\", args.locale)","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"log_current_versions","method_code":"def log_current_versions():\n    if logger.root.isEnabledFor(logging.DEBUG):\n        \n        if sys.platform == \"darwin\":\n            os_version = \"macOS {0}\".format(platform.mac_ver()[0])\n        \n        elif sys.platform.startswith(\"win\"):\n            os_version = \"{0} {1}\".format(platform.system(), platform.release())\n        \n        else:\n            os_version = platform.platform()\n\n        log.debug(\"OS:         {0}\".format(os_version))\n        log.debug(\"Python:     {0}\".format(platform.python_version()))\n        log.debug(\"Streamlink: {0}\".format(streamlink_version))\n        log.debug(\"Requests({0}), Socks({1}), Websocket({2})\".format(\n            requests.__version__, socks_version, websocket_version))","method_summary":"Show current installed versions","original_method_code":"def log_current_versions():\n    \"\"\"Show current installed versions\"\"\"\n    if logger.root.isEnabledFor(logging.DEBUG):\n        # MAC OS X\n        if sys.platform == \"darwin\":\n            os_version = \"macOS {0}\".format(platform.mac_ver()[0])\n        # Windows\n        elif sys.platform.startswith(\"win\"):\n            os_version = \"{0} {1}\".format(platform.system(), platform.release())\n        # linux \/ other\n        else:\n            os_version = platform.platform()\n\n        log.debug(\"OS:         {0}\".format(os_version))\n        log.debug(\"Python:     {0}\".format(platform.python_version()))\n        log.debug(\"Streamlink: {0}\".format(streamlink_version))\n        log.debug(\"Requests({0}), Socks({1}), Websocket({2})\".format(\n            requests.__version__, socks_version, websocket_version))","method_path":"src\/streamlink_cli\/main.py"}
{"repo_name":"streamlink\/streamlink","method_name":"Viasat._get_stream_id","method_code":"def _get_stream_id(self, text):\n        m = self._image_re.search(text)\n        if m:\n            return m.group(\"stream_id\")","method_summary":"Try to find a stream_id","original_method_code":"def _get_stream_id(self, text):\n        \"\"\"Try to find a stream_id\"\"\"\n        m = self._image_re.search(text)\n        if m:\n            return m.group(\"stream_id\")","method_path":"src\/streamlink\/plugins\/viasat.py"}
{"repo_name":"streamlink\/streamlink","method_name":"Viasat._get_iframe","method_code":"def _get_iframe(self, text):\n        m = self._iframe_re.search(text)\n        if m:\n            return self.session.streams(m.group(\"url\"))","method_summary":"Fallback if no stream_id was found before","original_method_code":"def _get_iframe(self, text):\n        \"\"\"Fallback if no stream_id was found before\"\"\"\n        m = self._iframe_re.search(text)\n        if m:\n            return self.session.streams(m.group(\"url\"))","method_path":"src\/streamlink\/plugins\/viasat.py"}
{"repo_name":"streamlink\/streamlink","method_name":"Streamlink.set_option","method_code":"def set_option(self, key, value):\n        \n        if key == \"rtmpdump\":\n            key = \"rtmp-rtmpdump\"\n        elif key == \"rtmpdump-proxy\":\n            key = \"rtmp-proxy\"\n        elif key == \"errorlog\":\n            key = \"subprocess-errorlog\"\n        elif key == \"errorlog-path\":\n            key = \"subprocess-errorlog-path\"\n\n        if key == \"http-proxy\":\n            self.http.proxies[\"http\"] = update_scheme(\"http:\/\/\", value)\n        elif key == \"https-proxy\":\n            self.http.proxies[\"https\"] = update_scheme(\"https:\/\/\", value)\n        elif key == \"http-cookies\":\n            if isinstance(value, dict):\n                self.http.cookies.update(value)\n            else:\n                self.http.parse_cookies(value)\n        elif key == \"http-headers\":\n            if isinstance(value, dict):\n                self.http.headers.update(value)\n            else:\n                self.http.parse_headers(value)\n        elif key == \"http-query-params\":\n            if isinstance(value, dict):\n                self.http.params.update(value)\n            else:\n                self.http.parse_query_params(value)\n        elif key == \"http-trust-env\":\n            self.http.trust_env = value\n        elif key == \"http-ssl-verify\":\n            self.http.verify = value\n        elif key == \"http-disable-dh\":\n            if value:\n                requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':!DH'\n                try:\n                    requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST = \\\n                        requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS.encode(\"ascii\")\n                except AttributeError:\n                    \n                    pass\n        elif key == \"http-ssl-cert\":\n            self.http.cert = value\n        elif key == \"http-timeout\":\n            self.http.timeout = value\n        else:\n            self.options.set(key, value)","method_summary":"Sets general options used by plugins and streams originating from this session object.","original_method_code":"def set_option(self, key, value):\n        \"\"\"Sets general options used by plugins and streams originating\n        from this session object.\n\n        :param key: key of the option\n        :param value: value to set the option to\n\n\n        **Available options**:\n\n        ======================== =========================================\n        hds-live-edge            ( float) Specify the time live HDS\n                                 streams will start from the edge of\n                                 stream, default: ``10.0``\n\n        hds-segment-attempts     (int) How many attempts should be done\n                                 to download each HDS segment, default: ``3``\n\n        hds-segment-threads      (int) The size of the thread pool used\n                                 to download segments, default: ``1``\n\n        hds-segment-timeout      (float) HDS segment connect and read\n                                 timeout, default: ``10.0``\n\n        hds-timeout              (float) Timeout for reading data from\n                                 HDS streams, default: ``60.0``\n\n        hls-live-edge            (int) How many segments from the end\n                                 to start live streams on, default: ``3``\n\n        hls-segment-attempts     (int) How many attempts should be done\n                                 to download each HLS segment, default: ``3``\n\n        hls-segment-threads      (int) The size of the thread pool used\n                                 to download segments, default: ``1``\n\n        hls-segment-timeout      (float) HLS segment connect and read\n                                 timeout, default: ``10.0``\n\n        hls-timeout              (float) Timeout for reading data from\n                                 HLS streams, default: ``60.0``\n\n        http-proxy               (str) Specify a HTTP proxy to use for\n                                 all HTTP requests\n\n        https-proxy              (str) Specify a HTTPS proxy to use for\n                                 all HTTPS requests\n\n        http-cookies             (dict or str) A dict or a semi-colon (;)\n                                 delimited str of cookies to add to each\n                                 HTTP request, e.g. ``foo=bar;baz=qux``\n\n        http-headers             (dict or str) A dict or semi-colon (;)\n                                 delimited str of headers to add to each\n                                 HTTP request, e.g. ``foo=bar;baz=qux``\n\n        http-query-params        (dict or str) A dict or a ampersand (&)\n                                 delimited string of query parameters to\n                                 add to each HTTP request,\n                                 e.g. ``foo=bar&baz=qux``\n\n        http-trust-env           (bool) Trust HTTP settings set in the\n                                 environment, such as environment\n                                 variables (HTTP_PROXY, etc) and\n                                 ~\/.netrc authentication\n\n        http-ssl-verify          (bool) Verify SSL certificates,\n                                 default: ``True``\n\n        http-ssl-cert            (str or tuple) SSL certificate to use,\n                                 can be either a .pem file (str) or a\n                                 .crt\/.key pair (tuple)\n\n        http-timeout             (float) General timeout used by all HTTP\n                                 requests except the ones covered by\n                                 other options, default: ``20.0``\n\n        http-stream-timeout      (float) Timeout for reading data from\n                                 HTTP streams, default: ``60.0``\n\n        subprocess-errorlog      (bool) Log errors from subprocesses to\n                                 a file located in the temp directory\n\n        subprocess-errorlog-path (str) Log errors from subprocesses to\n                                 a specific file\n\n        ringbuffer-size          (int) The size of the internal ring\n                                 buffer used by most stream types,\n                                 default: ``16777216`` (16MB)\n\n        rtmp-proxy               (str) Specify a proxy (SOCKS) that RTMP\n                                 streams will use\n\n        rtmp-rtmpdump            (str) Specify the location of the\n                                 rtmpdump executable used by RTMP streams,\n                                 e.g. ``\/usr\/local\/bin\/rtmpdump``\n\n        rtmp-timeout             (float) Timeout for reading data from\n                                 RTMP streams, default: ``60.0``\n\n        ffmpeg-ffmpeg            (str) Specify the location of the\n                                 ffmpeg executable use by Muxing streams\n                                 e.g. ``\/usr\/local\/bin\/ffmpeg``\n\n        ffmpeg-verbose           (bool) Log stderr from ffmpeg to the\n                                 console\n\n        ffmpeg-verbose-path      (str) Specify the location of the\n                                 ffmpeg stderr log file\n\n        ffmpeg-video-transcode   (str) The codec to use if transcoding\n                                 video when muxing with ffmpeg\n                                 e.g. ``h264``\n\n        ffmpeg-audio-transcode   (str) The codec to use if transcoding\n                                 audio when muxing with ffmpeg\n                                 e.g. ``aac``\n\n        stream-segment-attempts  (int) How many attempts should be done\n                                 to download each segment, default: ``3``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-segment-threads   (int) The size of the thread pool used\n                                 to download segments, default: ``1``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-segment-timeout   (float) Segment connect and read\n                                 timeout, default: ``10.0``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-timeout           (float) Timeout for reading data from\n                                 stream, default: ``60.0``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        locale                   (str) Locale setting, in the RFC 1766 format\n                                 eg. en_US or es_ES\n                                 default: ``system locale``.\n\n        user-input-requester     (UserInputRequester) instance of UserInputRequester\n                                 to collect input from the user at runtime. Must be\n                                 set before the plugins are loaded.\n                                 default: ``UserInputRequester``.\n        ======================== =========================================\n\n        \"\"\"\n\n        # Backwards compatibility\n        if key == \"rtmpdump\":\n            key = \"rtmp-rtmpdump\"\n        elif key == \"rtmpdump-proxy\":\n            key = \"rtmp-proxy\"\n        elif key == \"errorlog\":\n            key = \"subprocess-errorlog\"\n        elif key == \"errorlog-path\":\n            key = \"subprocess-errorlog-path\"\n\n        if key == \"http-proxy\":\n            self.http.proxies[\"http\"] = update_scheme(\"http:\/\/\", value)\n        elif key == \"https-proxy\":\n            self.http.proxies[\"https\"] = update_scheme(\"https:\/\/\", value)\n        elif key == \"http-cookies\":\n            if isinstance(value, dict):\n                self.http.cookies.update(value)\n            else:\n                self.http.parse_cookies(value)\n        elif key == \"http-headers\":\n            if isinstance(value, dict):\n                self.http.headers.update(value)\n            else:\n                self.http.parse_headers(value)\n        elif key == \"http-query-params\":\n            if isinstance(value, dict):\n                self.http.params.update(value)\n            else:\n                self.http.parse_query_params(value)\n        elif key == \"http-trust-env\":\n            self.http.trust_env = value\n        elif key == \"http-ssl-verify\":\n            self.http.verify = value\n        elif key == \"http-disable-dh\":\n            if value:\n                requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':!DH'\n                try:\n                    requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST = \\\n                        requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS.encode(\"ascii\")\n                except AttributeError:\n                    # no ssl to disable the cipher on\n                    pass\n        elif key == \"http-ssl-cert\":\n            self.http.cert = value\n        elif key == \"http-timeout\":\n            self.http.timeout = value\n        else:\n            self.options.set(key, value)","method_path":"src\/streamlink\/session.py"}
{"repo_name":"streamlink\/streamlink","method_name":"Streamlink.set_plugin_option","method_code":"def set_plugin_option(self, plugin, key, value):\n        if plugin in self.plugins:\n            plugin = self.plugins[plugin]\n            plugin.set_option(key, value)","method_summary":"Sets plugin specific options used by plugins originating from this session object.","original_method_code":"def set_plugin_option(self, plugin, key, value):\n        \"\"\"Sets plugin specific options used by plugins originating\n        from this session object.\n\n        :param plugin: name of the plugin\n        :param key: key of the option\n        :param value: value to set the option to\n\n        \"\"\"\n\n        if plugin in self.plugins:\n            plugin = self.plugins[plugin]\n            plugin.set_option(key, value)","method_path":"src\/streamlink\/session.py"}
{"repo_name":"streamlink\/streamlink","method_name":"Streamlink.resolve_url","method_code":"def resolve_url(self, url, follow_redirect=True):\n        url = update_scheme(\"http:\/\/\", url)\n\n        available_plugins = []\n        for name, plugin in self.plugins.items():\n            if plugin.can_handle_url(url):\n                available_plugins.append(plugin)\n\n        available_plugins.sort(key=lambda x: x.priority(url), reverse=True)\n        if available_plugins:\n            return available_plugins[0](url)\n\n        if follow_redirect:\n            \n            try:\n                res = self.http.head(url, allow_redirects=True, acceptable_status=[501])\n\n                \n                if res.status_code == 501:\n                    res = self.http.get(url, stream=True)\n\n                if res.url != url:\n                    return self.resolve_url(res.url, follow_redirect=follow_redirect)\n            except PluginError:\n                pass\n\n        raise NoPluginError","method_summary":"Attempts to find a plugin that can use this URL. The default protocol (http) will be prefixed to the URL if not specified.","original_method_code":"def resolve_url(self, url, follow_redirect=True):\n        \"\"\"Attempts to find a plugin that can use this URL.\n\n        The default protocol (http) will be prefixed to the URL if\n        not specified.\n\n        Raises :exc:`NoPluginError` on failure.\n\n        :param url: a URL to match against loaded plugins\n        :param follow_redirect: follow redirects\n\n        \"\"\"\n        url = update_scheme(\"http:\/\/\", url)\n\n        available_plugins = []\n        for name, plugin in self.plugins.items():\n            if plugin.can_handle_url(url):\n                available_plugins.append(plugin)\n\n        available_plugins.sort(key=lambda x: x.priority(url), reverse=True)\n        if available_plugins:\n            return available_plugins[0](url)\n\n        if follow_redirect:\n            # Attempt to handle a redirect URL\n            try:\n                res = self.http.head(url, allow_redirects=True, acceptable_status=[501])\n\n                # Fall back to GET request if server doesn't handle HEAD.\n                if res.status_code == 501:\n                    res = self.http.get(url, stream=True)\n\n                if res.url != url:\n                    return self.resolve_url(res.url, follow_redirect=follow_redirect)\n            except PluginError:\n                pass\n\n        raise NoPluginError","method_path":"src\/streamlink\/session.py"}
{"repo_name":"streamlink\/streamlink","method_name":"Streamlink.load_plugins","method_code":"def load_plugins(self, path):\n        for loader, name, ispkg in pkgutil.iter_modules([path]):\n            file, pathname, desc = imp.find_module(name, [path])\n            \n            module_name = \"streamlink.plugin.{0}\".format(name)\n\n            try:\n                self.load_plugin(module_name, file, pathname, desc)\n            except Exception:\n                sys.stderr.write(\"Failed to load plugin {0}:\\n\".format(name))\n                print_small_exception(\"load_plugin\")\n\n                continue","method_summary":"Attempt to load plugins from the path specified.","original_method_code":"def load_plugins(self, path):\n        \"\"\"Attempt to load plugins from the path specified.\n\n        :param path: full path to a directory where to look for plugins\n\n        \"\"\"\n        for loader, name, ispkg in pkgutil.iter_modules([path]):\n            file, pathname, desc = imp.find_module(name, [path])\n            # set the full plugin module name\n            module_name = \"streamlink.plugin.{0}\".format(name)\n\n            try:\n                self.load_plugin(module_name, file, pathname, desc)\n            except Exception:\n                sys.stderr.write(\"Failed to load plugin {0}:\\n\".format(name))\n                print_small_exception(\"load_plugin\")\n\n                continue","method_path":"src\/streamlink\/session.py"}
{"repo_name":"streamlink\/streamlink","method_name":"hours_minutes_seconds","method_code":"def hours_minutes_seconds(value):\n    try:\n        return int(value)\n    except ValueError:\n        pass\n\n    match = (_hours_minutes_seconds_re.match(value)\n             or _hours_minutes_seconds_2_re.match(value))\n    if not match:\n        raise ValueError\n\n    s = 0\n    s += int(match.group(\"hours\") or \"0\") * 60 * 60\n    s += int(match.group(\"minutes\") or \"0\") * 60\n    s += int(match.group(\"seconds\") or \"0\")\n\n    return s","method_summary":"converts a timestamp to seconds -","original_method_code":"def hours_minutes_seconds(value):\n    \"\"\"converts a timestamp to seconds\n\n      - hours:minutes:seconds to seconds\n      - minutes:seconds to seconds\n      - 11h22m33s to seconds\n      - 11h to seconds\n      - 20h15m to seconds\n      - seconds to seconds\n\n    :param value: hh:mm:ss ; 00h00m00s ; seconds\n    :return: seconds\n    \"\"\"\n    try:\n        return int(value)\n    except ValueError:\n        pass\n\n    match = (_hours_minutes_seconds_re.match(value)\n             or _hours_minutes_seconds_2_re.match(value))\n    if not match:\n        raise ValueError\n\n    s = 0\n    s += int(match.group(\"hours\") or \"0\") * 60 * 60\n    s += int(match.group(\"minutes\") or \"0\") * 60\n    s += int(match.group(\"seconds\") or \"0\")\n\n    return s","method_path":"src\/streamlink\/utils\/times.py"}
{"repo_name":"streamlink\/streamlink","method_name":"startswith","method_code":"def startswith(string):\n    def starts_with(value):\n        validate(text, value)\n        if not value.startswith(string):\n            raise ValueError(\"'{0}' does not start with '{1}'\".format(value, string))\n        return True\n\n    return starts_with","method_summary":"Checks if the string value starts with another string.","original_method_code":"def startswith(string):\n    \"\"\"Checks if the string value starts with another string.\"\"\"\n    def starts_with(value):\n        validate(text, value)\n        if not value.startswith(string):\n            raise ValueError(\"'{0}' does not start with '{1}'\".format(value, string))\n        return True\n\n    return starts_with","method_path":"src\/streamlink\/plugin\/api\/validate.py"}
{"repo_name":"streamlink\/streamlink","method_name":"endswith","method_code":"def endswith(string):\n    def ends_with(value):\n        validate(text, value)\n        if not value.endswith(string):\n            raise ValueError(\"'{0}' does not end with '{1}'\".format(value, string))\n        return True\n\n    return ends_with","method_summary":"Checks if the string value ends with another string.","original_method_code":"def endswith(string):\n    \"\"\"Checks if the string value ends with another string.\"\"\"\n    def ends_with(value):\n        validate(text, value)\n        if not value.endswith(string):\n            raise ValueError(\"'{0}' does not end with '{1}'\".format(value, string))\n        return True\n\n    return ends_with","method_path":"src\/streamlink\/plugin\/api\/validate.py"}
{"repo_name":"streamlink\/streamlink","method_name":"contains","method_code":"def contains(string):\n    def contains_str(value):\n        validate(text, value)\n        if string not in value:\n            raise ValueError(\"'{0}' does not contain '{1}'\".format(value, string))\n        return True\n\n    return contains_str","method_summary":"Checks if the string value contains another string.","original_method_code":"def contains(string):\n    \"\"\"Checks if the string value contains another string.\"\"\"\n    def contains_str(value):\n        validate(text, value)\n        if string not in value:\n            raise ValueError(\"'{0}' does not contain '{1}'\".format(value, string))\n        return True\n\n    return contains_str","method_path":"src\/streamlink\/plugin\/api\/validate.py"}
{"repo_name":"streamlink\/streamlink","method_name":"getattr","method_code":"def getattr(attr, default=None):\n    def getter(value):\n        return _getattr(value, attr, default)\n\n    return transform(getter)","method_summary":"Get a named attribute from an object. When a default argument is given, it is returned when the attribute doesn't exist.","original_method_code":"def getattr(attr, default=None):\n    \"\"\"Get a named attribute from an object.\n\n    When a default argument is given, it is returned when the attribute\n    doesn't exist.\n    \"\"\"\n    def getter(value):\n        return _getattr(value, attr, default)\n\n    return transform(getter)","method_path":"src\/streamlink\/plugin\/api\/validate.py"}
{"repo_name":"streamlink\/streamlink","method_name":"filter","method_code":"def filter(func):\n    def expand_kv(kv):\n        return func(*kv)\n\n    def filter_values(value):\n        cls = type(value)\n        if isinstance(value, dict):\n            return cls(_filter(expand_kv, value.items()))\n        else:\n            return cls(_filter(func, value))\n\n    return transform(filter_values)","method_summary":"Filters out unwanted items using the specified function. Supports both dicts and sequences, key\/value pairs are expanded when applied to a dict.","original_method_code":"def filter(func):\n    \"\"\"Filters out unwanted items using the specified function.\n\n    Supports both dicts and sequences, key\/value pairs are\n    expanded when applied to a dict.\n    \"\"\"\n    def expand_kv(kv):\n        return func(*kv)\n\n    def filter_values(value):\n        cls = type(value)\n        if isinstance(value, dict):\n            return cls(_filter(expand_kv, value.items()))\n        else:\n            return cls(_filter(func, value))\n\n    return transform(filter_values)","method_path":"src\/streamlink\/plugin\/api\/validate.py"}
{"repo_name":"streamlink\/streamlink","method_name":"map","method_code":"def map(func):\n    \n    \n    \n    if is_py2 and text == func:\n        func = unicode\n\n    def expand_kv(kv):\n        return func(*kv)\n\n    def map_values(value):\n        cls = type(value)\n        if isinstance(value, dict):\n            return cls(_map(expand_kv, value.items()))\n        else:\n            return cls(_map(func, value))\n\n    return transform(map_values)","method_summary":"Apply function to each value inside the sequence or dict. Supports both dicts and sequences, key\/value pairs are expanded when applied to a dict.","original_method_code":"def map(func):\n    \"\"\"Apply function to each value inside the sequence or dict.\n\n    Supports both dicts and sequences, key\/value pairs are\n    expanded when applied to a dict.\n    \"\"\"\n    # text is an alias for basestring on Python 2, which cannot be\n    # instantiated and therefore can't be used to transform the value,\n    # so we force to unicode instead.\n    if is_py2 and text == func:\n        func = unicode\n\n    def expand_kv(kv):\n        return func(*kv)\n\n    def map_values(value):\n        cls = type(value)\n        if isinstance(value, dict):\n            return cls(_map(expand_kv, value.items()))\n        else:\n            return cls(_map(func, value))\n\n    return transform(map_values)","method_path":"src\/streamlink\/plugin\/api\/validate.py"}
{"repo_name":"streamlink\/streamlink","method_name":"url","method_code":"def url(**attributes):\n    def check_url(value):\n        validate(text, value)\n        parsed = urlparse(value)\n        if not parsed.netloc:\n            raise ValueError(\"'{0}' is not a valid URL\".format(value))\n\n        for name, schema in attributes.items():\n            if not _hasattr(parsed, name):\n                raise ValueError(\"Invalid URL attribute '{0}'\".format(name))\n\n            try:\n                validate(schema, _getattr(parsed, name))\n            except ValueError as err:\n                raise ValueError(\n                    \"Unable to validate URL attribute '{0}': {1}\".format(\n                        name, err\n                    )\n                )\n\n        return True\n\n    \n    if attributes.get(\"scheme\") == \"http\":\n        attributes[\"scheme\"] = any(\"http\", \"https\")\n\n    return check_url","method_summary":"Parses an URL and validates its attributes.","original_method_code":"def url(**attributes):\n    \"\"\"Parses an URL and validates its attributes.\"\"\"\n    def check_url(value):\n        validate(text, value)\n        parsed = urlparse(value)\n        if not parsed.netloc:\n            raise ValueError(\"'{0}' is not a valid URL\".format(value))\n\n        for name, schema in attributes.items():\n            if not _hasattr(parsed, name):\n                raise ValueError(\"Invalid URL attribute '{0}'\".format(name))\n\n            try:\n                validate(schema, _getattr(parsed, name))\n            except ValueError as err:\n                raise ValueError(\n                    \"Unable to validate URL attribute '{0}': {1}\".format(\n                        name, err\n                    )\n                )\n\n        return True\n\n    # Convert \"http\" to be either any(\"http\", \"https\") for convenience\n    if attributes.get(\"scheme\") == \"http\":\n        attributes[\"scheme\"] = any(\"http\", \"https\")\n\n    return check_url","method_path":"src\/streamlink\/plugin\/api\/validate.py"}
{"repo_name":"streamlink\/streamlink","method_name":"xml_find","method_code":"def xml_find(xpath):\n    def xpath_find(value):\n        validate(ET.iselement, value)\n        value = value.find(xpath)\n        if value is None:\n            raise ValueError(\"XPath '{0}' did not return an element\".format(xpath))\n\n        return validate(ET.iselement, value)\n\n    return transform(xpath_find)","method_summary":"Find a XML element via xpath.","original_method_code":"def xml_find(xpath):\n    \"\"\"Find a XML element via xpath.\"\"\"\n    def xpath_find(value):\n        validate(ET.iselement, value)\n        value = value.find(xpath)\n        if value is None:\n            raise ValueError(\"XPath '{0}' did not return an element\".format(xpath))\n\n        return validate(ET.iselement, value)\n\n    return transform(xpath_find)","method_path":"src\/streamlink\/plugin\/api\/validate.py"}
{"repo_name":"streamlink\/streamlink","method_name":"xml_findall","method_code":"def xml_findall(xpath):\n    def xpath_findall(value):\n        validate(ET.iselement, value)\n        return value.findall(xpath)\n\n    return transform(xpath_findall)","method_summary":"Find a list of XML elements via xpath.","original_method_code":"def xml_findall(xpath):\n    \"\"\"Find a list of XML elements via xpath.\"\"\"\n    def xpath_findall(value):\n        validate(ET.iselement, value)\n        return value.findall(xpath)\n\n    return transform(xpath_findall)","method_path":"src\/streamlink\/plugin\/api\/validate.py"}
{"repo_name":"streamlink\/streamlink","method_name":"_find_player_url","method_code":"def _find_player_url(response):\n    url = ''\n    matches = _player_re.search(response.text)\n    if matches:\n        tmp_url = matches.group(0).replace('&amp;', '&')\n        if 'hash' not in tmp_url:\n            \n            matches = _hash_re.search(response.text)\n            if matches:\n                url = tmp_url + '&hash=' + matches.group(1)\n        else:\n            url = tmp_url\n\n    return 'http:\/\/ceskatelevize.cz\/' + url","method_summary":"Finds embedded player url in HTTP response.","original_method_code":"def _find_player_url(response):\n    \"\"\"\n    Finds embedded player url in HTTP response.\n\n    :param response: Response object.\n    :returns: Player url (str).\n    \"\"\"\n    url = ''\n    matches = _player_re.search(response.text)\n    if matches:\n        tmp_url = matches.group(0).replace('&amp;', '&')\n        if 'hash' not in tmp_url:\n            # there's no hash in the URL, try to find it\n            matches = _hash_re.search(response.text)\n            if matches:\n                url = tmp_url + '&hash=' + matches.group(1)\n        else:\n            url = tmp_url\n\n    return 'http:\/\/ceskatelevize.cz\/' + url","method_path":"src\/streamlink\/plugins\/ceskatelevize.py"}
{"repo_name":"streamlink\/streamlink","method_name":"load","method_code":"def load(data, base_uri=None, parser=M3U8Parser, **kwargs):\n    return parser(base_uri, **kwargs).parse(data)","method_summary":"Attempts to parse a M3U8 playlist from a string of data. If specified, *base_uri","original_method_code":"def load(data, base_uri=None, parser=M3U8Parser, **kwargs):\n    \"\"\"Attempts to parse a M3U8 playlist from a string of data.\n\n    If specified, *base_uri* is the base URI that relative URIs will\n    be joined together with, otherwise relative URIs will be as is.\n\n    If specified, *parser* can be a M3U8Parser subclass to be used\n    to parse the data.\n\n    \"\"\"\n    return parser(base_uri, **kwargs).parse(data)","method_path":"src\/streamlink\/stream\/hls_playlist.py"}
{"repo_name":"streamlink\/streamlink","method_name":"PlayerOutput.supported_player","method_code":"def supported_player(cls, cmd):\n        if not is_win32:\n            \n            \n            cmd = shlex.split(cmd)[0]\n\n        cmd = os.path.basename(cmd.lower())\n        for player, possiblecmds in SUPPORTED_PLAYERS.items():\n            for possiblecmd in possiblecmds:\n                if cmd.startswith(possiblecmd):\n                    return player","method_summary":"Check if the current player supports adding a title","original_method_code":"def supported_player(cls, cmd):\n        \"\"\"\n        Check if the current player supports adding a title\n\n        :param cmd: command to test\n        :return: name of the player|None\n        \"\"\"\n        if not is_win32:\n            # under a POSIX system use shlex to find the actual command\n            # under windows this is not an issue because executables end in .exe\n            cmd = shlex.split(cmd)[0]\n\n        cmd = os.path.basename(cmd.lower())\n        for player, possiblecmds in SUPPORTED_PLAYERS.items():\n            for possiblecmd in possiblecmds:\n                if cmd.startswith(possiblecmd):\n                    return player","method_path":"src\/streamlink_cli\/output.py"}
{"repo_name":"streamlink\/streamlink","method_name":"SteamBroadcastPlugin.dologin","method_code":"def dologin(self, email, password, emailauth=\"\", emailsteamid=\"\", captchagid=\"-1\", captcha_text=\"\", twofactorcode=\"\"):\n        epassword, rsatimestamp = self.encrypt_password(email, password)\n\n        login_data = {\n            'username': email,\n            \"password\": epassword,\n            \"emailauth\": emailauth,\n            \"loginfriendlyname\": \"Streamlink\",\n            \"captchagid\": captchagid,\n            \"captcha_text\": captcha_text,\n            \"emailsteamid\": emailsteamid,\n            \"rsatimestamp\": rsatimestamp,\n            \"remember_login\": True,\n            \"donotcache\": self.donotcache,\n            \"twofactorcode\": twofactorcode\n        }\n\n        res = self.session.http.post(self._dologin_url, data=login_data)\n\n        resp = self.session.http.json(res, schema=self._dologin_schema)\n\n        if not resp[u\"success\"]:\n            if resp.get(u\"captcha_needed\"):\n                \n                captchagid = resp[u\"captcha_gid\"]\n                log.error(\"Captcha result required, open this URL to see the captcha: {}\".format(\n                    self._captcha_url.format(captchagid)))\n                try:\n                    captcha_text = self.input_ask(\"Captcha text\")\n                except FatalPluginError:\n                    captcha_text = None\n                if not captcha_text:\n                    return False\n            else:\n                \n                if resp.get(u\"emailauth_needed\"):\n                    if not emailauth:\n                        try:\n                            emailauth = self.input_ask(\"Email auth code required\")\n                        except FatalPluginError:\n                            emailauth = None\n                        if not emailauth:\n                            return False\n                    else:\n                        raise SteamLoginFailed(\"Email auth key error\")\n\n                \n                if resp.get(u\"requires_twofactor\"):\n                    try:\n                        twofactorcode = self.input_ask(\"Two factor auth code required\")\n                    except FatalPluginError:\n                        twofactorcode = None\n                    if not twofactorcode:\n                        return False\n\n                if resp.get(u\"message\"):\n                    raise SteamLoginFailed(resp[u\"message\"])\n\n            return self.dologin(email, password,\n                                emailauth=emailauth,\n                                emailsteamid=resp.get(u\"emailsteamid\", u\"\"),\n                                captcha_text=captcha_text,\n                                captchagid=captchagid,\n                                twofactorcode=twofactorcode)\n        elif resp.get(\"login_complete\"):\n            return True\n        else:\n            log.error(\"Something when wrong when logging in to Steam\")\n            return False","method_summary":"Logs in to Steam","original_method_code":"def dologin(self, email, password, emailauth=\"\", emailsteamid=\"\", captchagid=\"-1\", captcha_text=\"\", twofactorcode=\"\"):\n        \"\"\"\n        Logs in to Steam\n\n        \"\"\"\n        epassword, rsatimestamp = self.encrypt_password(email, password)\n\n        login_data = {\n            'username': email,\n            \"password\": epassword,\n            \"emailauth\": emailauth,\n            \"loginfriendlyname\": \"Streamlink\",\n            \"captchagid\": captchagid,\n            \"captcha_text\": captcha_text,\n            \"emailsteamid\": emailsteamid,\n            \"rsatimestamp\": rsatimestamp,\n            \"remember_login\": True,\n            \"donotcache\": self.donotcache,\n            \"twofactorcode\": twofactorcode\n        }\n\n        res = self.session.http.post(self._dologin_url, data=login_data)\n\n        resp = self.session.http.json(res, schema=self._dologin_schema)\n\n        if not resp[u\"success\"]:\n            if resp.get(u\"captcha_needed\"):\n                # special case for captcha\n                captchagid = resp[u\"captcha_gid\"]\n                log.error(\"Captcha result required, open this URL to see the captcha: {}\".format(\n                    self._captcha_url.format(captchagid)))\n                try:\n                    captcha_text = self.input_ask(\"Captcha text\")\n                except FatalPluginError:\n                    captcha_text = None\n                if not captcha_text:\n                    return False\n            else:\n                # If the user must enter the code that was emailed to them\n                if resp.get(u\"emailauth_needed\"):\n                    if not emailauth:\n                        try:\n                            emailauth = self.input_ask(\"Email auth code required\")\n                        except FatalPluginError:\n                            emailauth = None\n                        if not emailauth:\n                            return False\n                    else:\n                        raise SteamLoginFailed(\"Email auth key error\")\n\n                # If the user must enter a two factor auth code\n                if resp.get(u\"requires_twofactor\"):\n                    try:\n                        twofactorcode = self.input_ask(\"Two factor auth code required\")\n                    except FatalPluginError:\n                        twofactorcode = None\n                    if not twofactorcode:\n                        return False\n\n                if resp.get(u\"message\"):\n                    raise SteamLoginFailed(resp[u\"message\"])\n\n            return self.dologin(email, password,\n                                emailauth=emailauth,\n                                emailsteamid=resp.get(u\"emailsteamid\", u\"\"),\n                                captcha_text=captcha_text,\n                                captchagid=captchagid,\n                                twofactorcode=twofactorcode)\n        elif resp.get(\"login_complete\"):\n            return True\n        else:\n            log.error(\"Something when wrong when logging in to Steam\")\n            return False","method_path":"src\/streamlink\/plugins\/steam.py"}
{"repo_name":"streamlink\/streamlink","method_name":"ABweb._login","method_code":"def _login(self, username, password):\n        self.logger.debug('login ...')\n\n        res = self.session.http.get(self.login_url)\n        input_list = self._input_re.findall(res.text)\n        if not input_list:\n            raise PluginError('Missing input data on login website.')\n\n        data = {}\n        for _input_data in input_list:\n            try:\n                _input_name = self._name_re.search(_input_data).group(1)\n            except AttributeError:\n                continue\n\n            try:\n                _input_value = self._value_re.search(_input_data).group(1)\n            except AttributeError:\n                _input_value = ''\n\n            data[_input_name] = _input_value\n\n        login_data = {\n            'ctl00$Login1$UserName': username,\n            'ctl00$Login1$Password': password,\n            'ctl00$Login1$LoginButton.x': '0',\n            'ctl00$Login1$LoginButton.y': '0'\n        }\n        data.update(login_data)\n\n        res = self.session.http.post(self.login_url, data=data)\n\n        for cookie in self.session.http.cookies:\n            self._session_attributes.set(cookie.name, cookie.value, expires=3600 * 24)\n\n        if self._session_attributes.get('ASP.NET_SessionId') and self._session_attributes.get('.abportail1'):\n            self.logger.debug('New session data')\n            self.set_expires_time_cache()\n            return True\n        else:\n            self.logger.error('Failed to login, check your username\/password')\n            return False","method_summary":"login and update cached cookies","original_method_code":"def _login(self, username, password):\n        '''login and update cached cookies'''\n        self.logger.debug('login ...')\n\n        res = self.session.http.get(self.login_url)\n        input_list = self._input_re.findall(res.text)\n        if not input_list:\n            raise PluginError('Missing input data on login website.')\n\n        data = {}\n        for _input_data in input_list:\n            try:\n                _input_name = self._name_re.search(_input_data).group(1)\n            except AttributeError:\n                continue\n\n            try:\n                _input_value = self._value_re.search(_input_data).group(1)\n            except AttributeError:\n                _input_value = ''\n\n            data[_input_name] = _input_value\n\n        login_data = {\n            'ctl00$Login1$UserName': username,\n            'ctl00$Login1$Password': password,\n            'ctl00$Login1$LoginButton.x': '0',\n            'ctl00$Login1$LoginButton.y': '0'\n        }\n        data.update(login_data)\n\n        res = self.session.http.post(self.login_url, data=data)\n\n        for cookie in self.session.http.cookies:\n            self._session_attributes.set(cookie.name, cookie.value, expires=3600 * 24)\n\n        if self._session_attributes.get('ASP.NET_SessionId') and self._session_attributes.get('.abportail1'):\n            self.logger.debug('New session data')\n            self.set_expires_time_cache()\n            return True\n        else:\n            self.logger.error('Failed to login, check your username\/password')\n            return False","method_path":"src\/streamlink\/plugins\/abweb.py"}
{"repo_name":"streamlink\/streamlink","method_name":"CrunchyrollAPI._api_call","method_code":"def _api_call(self, entrypoint, params=None, schema=None):\n        url = self._api_url.format(entrypoint)\n\n        \n        params = params or {}\n        if self.session_id:\n            params.update({\n                \"session_id\": self.session_id\n            })\n        else:\n            params.update({\n                \"device_id\": self.device_id,\n                \"device_type\": self._access_type,\n                \"access_token\": self._access_token,\n                \"version\": self._version_code\n            })\n        params.update({\n            \"locale\": self.locale.replace('_', ''),\n        })\n\n        if self.session_id:\n            params[\"session_id\"] = self.session_id\n\n        \n        res = self.session.http.post(url, data=params, headers=self.headers, verify=False)\n        json_res = self.session.http.json(res, schema=_api_schema)\n\n        if json_res[\"error\"]:\n            err_msg = json_res.get(\"message\", \"Unknown error\")\n            err_code = json_res.get(\"code\", \"unknown_error\")\n            raise CrunchyrollAPIError(err_msg, err_code)\n\n        data = json_res.get(\"data\")\n        if schema:\n            data = schema.validate(data, name=\"API response\")\n\n        return data","method_summary":"Makes a call against the api.","original_method_code":"def _api_call(self, entrypoint, params=None, schema=None):\n        \"\"\"Makes a call against the api.\n\n        :param entrypoint: API method to call.\n        :param params: parameters to include in the request data.\n        :param schema: schema to use to validate the data\n        \"\"\"\n        url = self._api_url.format(entrypoint)\n\n        # Default params\n        params = params or {}\n        if self.session_id:\n            params.update({\n                \"session_id\": self.session_id\n            })\n        else:\n            params.update({\n                \"device_id\": self.device_id,\n                \"device_type\": self._access_type,\n                \"access_token\": self._access_token,\n                \"version\": self._version_code\n            })\n        params.update({\n            \"locale\": self.locale.replace('_', ''),\n        })\n\n        if self.session_id:\n            params[\"session_id\"] = self.session_id\n\n        # The certificate used by Crunchyroll cannot be verified in some environments.\n        res = self.session.http.post(url, data=params, headers=self.headers, verify=False)\n        json_res = self.session.http.json(res, schema=_api_schema)\n\n        if json_res[\"error\"]:\n            err_msg = json_res.get(\"message\", \"Unknown error\")\n            err_code = json_res.get(\"code\", \"unknown_error\")\n            raise CrunchyrollAPIError(err_msg, err_code)\n\n        data = json_res.get(\"data\")\n        if schema:\n            data = schema.validate(data, name=\"API response\")\n\n        return data","method_path":"src\/streamlink\/plugins\/crunchyroll.py"}
{"repo_name":"streamlink\/streamlink","method_name":"CrunchyrollAPI.start_session","method_code":"def start_session(self):\n        params = {}\n        if self.auth:\n            params[\"auth\"] = self.auth\n        self.session_id = self._api_call(\"start_session\", params, schema=_session_schema)\n        log.debug(\"Session created with ID: {0}\".format(self.session_id))\n        return self.session_id","method_summary":"Starts a session against Crunchyroll's server. Is recommended that you call this method before making any other calls to make sure you have a valid session against the server.","original_method_code":"def start_session(self):\n        \"\"\"\n            Starts a session against Crunchyroll's server.\n            Is recommended that you call this method before making any other calls\n            to make sure you have a valid session against the server.\n        \"\"\"\n        params = {}\n        if self.auth:\n            params[\"auth\"] = self.auth\n        self.session_id = self._api_call(\"start_session\", params, schema=_session_schema)\n        log.debug(\"Session created with ID: {0}\".format(self.session_id))\n        return self.session_id","method_path":"src\/streamlink\/plugins\/crunchyroll.py"}
{"repo_name":"streamlink\/streamlink","method_name":"Crunchyroll._create_api","method_code":"def _create_api(self):\n        if self.options.get(\"purge_credentials\"):\n            self.cache.set(\"session_id\", None, 0)\n            self.cache.set(\"auth\", None, 0)\n            self.cache.set(\"session_id\", None, 0)\n\n        \n        locale = self.get_option(\"locale\") or self.session.localization.language_code\n        api = CrunchyrollAPI(self.cache,\n                             self.session,\n                             session_id=self.get_option(\"session_id\"),\n                             locale=locale)\n\n        if not self.get_option(\"session_id\"):\n            self.logger.debug(\"Creating session with locale: {0}\", locale)\n            api.start_session()\n\n            if api.auth:\n                self.logger.debug(\"Using saved credentials\")\n                login = api.authenticate()\n                self.logger.info(\"Successfully logged in as '{0}'\",\n                                 login[\"user\"][\"username\"] or login[\"user\"][\"email\"])\n            elif self.options.get(\"username\"):\n                try:\n                    self.logger.debug(\"Attempting to login using username and password\")\n                    api.login(self.options.get(\"username\"),\n                              self.options.get(\"password\"))\n                    login = api.authenticate()\n                    self.logger.info(\"Logged in as '{0}'\",\n                                     login[\"user\"][\"username\"] or login[\"user\"][\"email\"])\n\n                except CrunchyrollAPIError as err:\n                    raise PluginError(u\"Authentication error: {0}\".format(err.msg))\n            else:\n                self.logger.warning(\n                    \"No authentication provided, you won't be able to access \"\n                    \"premium restricted content\"\n                )\n\n        return api","method_summary":"Creates a new CrunchyrollAPI object, initiates it's session and tries to authenticate it either by using saved credentials or the user's username and password.","original_method_code":"def _create_api(self):\n        \"\"\"Creates a new CrunchyrollAPI object, initiates it's session and\n        tries to authenticate it either by using saved credentials or the\n        user's username and password.\n        \"\"\"\n        if self.options.get(\"purge_credentials\"):\n            self.cache.set(\"session_id\", None, 0)\n            self.cache.set(\"auth\", None, 0)\n            self.cache.set(\"session_id\", None, 0)\n\n        # use the crunchyroll locale as an override, for backwards compatibility\n        locale = self.get_option(\"locale\") or self.session.localization.language_code\n        api = CrunchyrollAPI(self.cache,\n                             self.session,\n                             session_id=self.get_option(\"session_id\"),\n                             locale=locale)\n\n        if not self.get_option(\"session_id\"):\n            self.logger.debug(\"Creating session with locale: {0}\", locale)\n            api.start_session()\n\n            if api.auth:\n                self.logger.debug(\"Using saved credentials\")\n                login = api.authenticate()\n                self.logger.info(\"Successfully logged in as '{0}'\",\n                                 login[\"user\"][\"username\"] or login[\"user\"][\"email\"])\n            elif self.options.get(\"username\"):\n                try:\n                    self.logger.debug(\"Attempting to login using username and password\")\n                    api.login(self.options.get(\"username\"),\n                              self.options.get(\"password\"))\n                    login = api.authenticate()\n                    self.logger.info(\"Logged in as '{0}'\",\n                                     login[\"user\"][\"username\"] or login[\"user\"][\"email\"])\n\n                except CrunchyrollAPIError as err:\n                    raise PluginError(u\"Authentication error: {0}\".format(err.msg))\n            else:\n                self.logger.warning(\n                    \"No authentication provided, you won't be able to access \"\n                    \"premium restricted content\"\n                )\n\n        return api","method_path":"src\/streamlink\/plugins\/crunchyroll.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"frames2video","method_code":"def frames2video(frame_dir,\n                 video_file,\n                 fps=30,\n                 fourcc='XVID',\n                 filename_tmpl='{:06d}.jpg',\n                 start=0,\n                 end=0,\n                 show_progress=True):\n    if end == 0:\n        ext = filename_tmpl.split('.')[-1]\n        end = len([name for name in scandir(frame_dir, ext)])\n    first_file = osp.join(frame_dir, filename_tmpl.format(start))\n    check_file_exist(first_file, 'The start frame not found: ' + first_file)\n    img = cv2.imread(first_file)\n    height, width = img.shape[:2]\n    resolution = (width, height)\n    vwriter = cv2.VideoWriter(video_file, VideoWriter_fourcc(*fourcc), fps,\n                              resolution)\n\n    def write_frame(file_idx):\n        filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n        img = cv2.imread(filename)\n        vwriter.write(img)\n\n    if show_progress:\n        track_progress(write_frame, range(start, end))\n    else:\n        for i in range(start, end):\n            filename = osp.join(frame_dir, filename_tmpl.format(i))\n            img = cv2.imread(filename)\n            vwriter.write(img)\n    vwriter.release()","method_summary":"Read the frame images from a directory and join them as a video","original_method_code":"def frames2video(frame_dir,\n                 video_file,\n                 fps=30,\n                 fourcc='XVID',\n                 filename_tmpl='{:06d}.jpg',\n                 start=0,\n                 end=0,\n                 show_progress=True):\n    \"\"\"Read the frame images from a directory and join them as a video\n\n    Args:\n        frame_dir (str): The directory containing video frames.\n        video_file (str): Output filename.\n        fps (float): FPS of the output video.\n        fourcc (str): Fourcc of the output video, this should be compatible\n            with the output file type.\n        filename_tmpl (str): Filename template with the index as the variable.\n        start (int): Starting frame index.\n        end (int): Ending frame index.\n        show_progress (bool): Whether to show a progress bar.\n    \"\"\"\n    if end == 0:\n        ext = filename_tmpl.split('.')[-1]\n        end = len([name for name in scandir(frame_dir, ext)])\n    first_file = osp.join(frame_dir, filename_tmpl.format(start))\n    check_file_exist(first_file, 'The start frame not found: ' + first_file)\n    img = cv2.imread(first_file)\n    height, width = img.shape[:2]\n    resolution = (width, height)\n    vwriter = cv2.VideoWriter(video_file, VideoWriter_fourcc(*fourcc), fps,\n                              resolution)\n\n    def write_frame(file_idx):\n        filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n        img = cv2.imread(filename)\n        vwriter.write(img)\n\n    if show_progress:\n        track_progress(write_frame, range(start, end))\n    else:\n        for i in range(start, end):\n            filename = osp.join(frame_dir, filename_tmpl.format(i))\n            img = cv2.imread(filename)\n            vwriter.write(img)\n    vwriter.release()","method_path":"mmcv\/video\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"VideoReader.read","method_code":"def read(self):\n        \n        if self._cache:\n            img = self._cache.get(self._position)\n            if img is not None:\n                ret = True\n            else:\n                if self._position != self._get_real_position():\n                    self._set_real_position(self._position)\n                ret, img = self._vcap.read()\n                if ret:\n                    self._cache.put(self._position, img)\n        else:\n            ret, img = self._vcap.read()\n        if ret:\n            self._position += 1\n        return img","method_summary":"Read the next frame. If the next frame have been decoded before and in the cache, then return it directly, otherwise decode, cache and return it.","original_method_code":"def read(self):\n        \"\"\"Read the next frame.\n\n        If the next frame have been decoded before and in the cache, then\n        return it directly, otherwise decode, cache and return it.\n\n        Returns:\n            ndarray or None: Return the frame if successful, otherwise None.\n        \"\"\"\n        # pos = self._position\n        if self._cache:\n            img = self._cache.get(self._position)\n            if img is not None:\n                ret = True\n            else:\n                if self._position != self._get_real_position():\n                    self._set_real_position(self._position)\n                ret, img = self._vcap.read()\n                if ret:\n                    self._cache.put(self._position, img)\n        else:\n            ret, img = self._vcap.read()\n        if ret:\n            self._position += 1\n        return img","method_path":"mmcv\/video\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"VideoReader.get_frame","method_code":"def get_frame(self, frame_id):\n        if frame_id < 0 or frame_id >= self._frame_cnt:\n            raise IndexError(\n                '\"frame_id\" must be between 0 and {}'.format(self._frame_cnt -\n                                                             1))\n        if frame_id == self._position:\n            return self.read()\n        if self._cache:\n            img = self._cache.get(frame_id)\n            if img is not None:\n                self._position = frame_id + 1\n                return img\n        self._set_real_position(frame_id)\n        ret, img = self._vcap.read()\n        if ret:\n            if self._cache:\n                self._cache.put(self._position, img)\n            self._position += 1\n        return img","method_summary":"Get frame by index.","original_method_code":"def get_frame(self, frame_id):\n        \"\"\"Get frame by index.\n\n        Args:\n            frame_id (int): Index of the expected frame, 0-based.\n\n        Returns:\n            ndarray or None: Return the frame if successful, otherwise None.\n        \"\"\"\n        if frame_id < 0 or frame_id >= self._frame_cnt:\n            raise IndexError(\n                '\"frame_id\" must be between 0 and {}'.format(self._frame_cnt -\n                                                             1))\n        if frame_id == self._position:\n            return self.read()\n        if self._cache:\n            img = self._cache.get(frame_id)\n            if img is not None:\n                self._position = frame_id + 1\n                return img\n        self._set_real_position(frame_id)\n        ret, img = self._vcap.read()\n        if ret:\n            if self._cache:\n                self._cache.put(self._position, img)\n            self._position += 1\n        return img","method_path":"mmcv\/video\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"VideoReader.cvt2frames","method_code":"def cvt2frames(self,\n                   frame_dir,\n                   file_start=0,\n                   filename_tmpl='{:06d}.jpg',\n                   start=0,\n                   max_num=0,\n                   show_progress=True):\n        mkdir_or_exist(frame_dir)\n        if max_num == 0:\n            task_num = self.frame_cnt - start\n        else:\n            task_num = min(self.frame_cnt - start, max_num)\n        if task_num <= 0:\n            raise ValueError('start must be less than total frame number')\n        if start > 0:\n            self._set_real_position(start)\n\n        def write_frame(file_idx):\n            img = self.read()\n            filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n            cv2.imwrite(filename, img)\n\n        if show_progress:\n            track_progress(write_frame, range(file_start,\n                                              file_start + task_num))\n        else:\n            for i in range(task_num):\n                img = self.read()\n                if img is None:\n                    break\n                filename = osp.join(frame_dir,\n                                    filename_tmpl.format(i + file_start))\n                cv2.imwrite(filename, img)","method_summary":"Convert a video to frame images","original_method_code":"def cvt2frames(self,\n                   frame_dir,\n                   file_start=0,\n                   filename_tmpl='{:06d}.jpg',\n                   start=0,\n                   max_num=0,\n                   show_progress=True):\n        \"\"\"Convert a video to frame images\n\n        Args:\n            frame_dir (str): Output directory to store all the frame images.\n            file_start (int): Filenames will start from the specified number.\n            filename_tmpl (str): Filename template with the index as the\n                placeholder.\n            start (int): The starting frame index.\n            max_num (int): Maximum number of frames to be written.\n            show_progress (bool): Whether to show a progress bar.\n        \"\"\"\n        mkdir_or_exist(frame_dir)\n        if max_num == 0:\n            task_num = self.frame_cnt - start\n        else:\n            task_num = min(self.frame_cnt - start, max_num)\n        if task_num <= 0:\n            raise ValueError('start must be less than total frame number')\n        if start > 0:\n            self._set_real_position(start)\n\n        def write_frame(file_idx):\n            img = self.read()\n            filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n            cv2.imwrite(filename, img)\n\n        if show_progress:\n            track_progress(write_frame, range(file_start,\n                                              file_start + task_num))\n        else:\n            for i in range(task_num):\n                img = self.read()\n                if img is None:\n                    break\n                filename = osp.join(frame_dir,\n                                    filename_tmpl.format(i + file_start))\n                cv2.imwrite(filename, img)","method_path":"mmcv\/video\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"track_progress","method_code":"def track_progress(func, tasks, bar_width=50, **kwargs):\n    if isinstance(tasks, tuple):\n        assert len(tasks) == 2\n        assert isinstance(tasks[0], collections_abc.Iterable)\n        assert isinstance(tasks[1], int)\n        task_num = tasks[1]\n        tasks = tasks[0]\n    elif isinstance(tasks, collections_abc.Iterable):\n        task_num = len(tasks)\n    else:\n        raise TypeError(\n            '\"tasks\" must be an iterable object or a (iterator, int) tuple')\n    prog_bar = ProgressBar(task_num, bar_width)\n    results = []\n    for task in tasks:\n        results.append(func(task, **kwargs))\n        prog_bar.update()\n    sys.stdout.write('\\n')\n    return results","method_summary":"Track the progress of tasks execution with a progress bar. Tasks are done with a simple for-loop.","original_method_code":"def track_progress(func, tasks, bar_width=50, **kwargs):\n    \"\"\"Track the progress of tasks execution with a progress bar.\n\n    Tasks are done with a simple for-loop.\n\n    Args:\n        func (callable): The function to be applied to each task.\n        tasks (list or tuple[Iterable, int]): A list of tasks or\n            (tasks, total num).\n        bar_width (int): Width of progress bar.\n\n    Returns:\n        list: The task results.\n    \"\"\"\n    if isinstance(tasks, tuple):\n        assert len(tasks) == 2\n        assert isinstance(tasks[0], collections_abc.Iterable)\n        assert isinstance(tasks[1], int)\n        task_num = tasks[1]\n        tasks = tasks[0]\n    elif isinstance(tasks, collections_abc.Iterable):\n        task_num = len(tasks)\n    else:\n        raise TypeError(\n            '\"tasks\" must be an iterable object or a (iterator, int) tuple')\n    prog_bar = ProgressBar(task_num, bar_width)\n    results = []\n    for task in tasks:\n        results.append(func(task, **kwargs))\n        prog_bar.update()\n    sys.stdout.write('\\n')\n    return results","method_path":"mmcv\/utils\/progressbar.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imflip","method_code":"def imflip(img, direction='horizontal'):\n    assert direction in ['horizontal', 'vertical']\n    if direction == 'horizontal':\n        return np.flip(img, axis=1)\n    else:\n        return np.flip(img, axis=0)","method_summary":"Flip an image horizontally or vertically.","original_method_code":"def imflip(img, direction='horizontal'):\n    \"\"\"Flip an image horizontally or vertically.\n\n    Args:\n        img (ndarray): Image to be flipped.\n        direction (str): The flip direction, either \"horizontal\" or \"vertical\".\n\n    Returns:\n        ndarray: The flipped image.\n    \"\"\"\n    assert direction in ['horizontal', 'vertical']\n    if direction == 'horizontal':\n        return np.flip(img, axis=1)\n    else:\n        return np.flip(img, axis=0)","method_path":"mmcv\/image\/transforms\/geometry.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imrotate","method_code":"def imrotate(img,\n             angle,\n             center=None,\n             scale=1.0,\n             border_value=0,\n             auto_bound=False):\n    if center is not None and auto_bound:\n        raise ValueError('`auto_bound` conflicts with `center`')\n    h, w = img.shape[:2]\n    if center is None:\n        center = ((w - 1) * 0.5, (h - 1) * 0.5)\n    assert isinstance(center, tuple)\n\n    matrix = cv2.getRotationMatrix2D(center, -angle, scale)\n    if auto_bound:\n        cos = np.abs(matrix[0, 0])\n        sin = np.abs(matrix[0, 1])\n        new_w = h * sin + w * cos\n        new_h = h * cos + w * sin\n        matrix[0, 2] += (new_w - w) * 0.5\n        matrix[1, 2] += (new_h - h) * 0.5\n        w = int(np.round(new_w))\n        h = int(np.round(new_h))\n    rotated = cv2.warpAffine(img, matrix, (w, h), borderValue=border_value)\n    return rotated","method_summary":"Rotate an image.","original_method_code":"def imrotate(img,\n             angle,\n             center=None,\n             scale=1.0,\n             border_value=0,\n             auto_bound=False):\n    \"\"\"Rotate an image.\n\n    Args:\n        img (ndarray): Image to be rotated.\n        angle (float): Rotation angle in degrees, positive values mean\n            clockwise rotation.\n        center (tuple): Center of the rotation in the source image, by default\n            it is the center of the image.\n        scale (float): Isotropic scale factor.\n        border_value (int): Border value.\n        auto_bound (bool): Whether to adjust the image size to cover the whole\n            rotated image.\n\n    Returns:\n        ndarray: The rotated image.\n    \"\"\"\n    if center is not None and auto_bound:\n        raise ValueError('`auto_bound` conflicts with `center`')\n    h, w = img.shape[:2]\n    if center is None:\n        center = ((w - 1) * 0.5, (h - 1) * 0.5)\n    assert isinstance(center, tuple)\n\n    matrix = cv2.getRotationMatrix2D(center, -angle, scale)\n    if auto_bound:\n        cos = np.abs(matrix[0, 0])\n        sin = np.abs(matrix[0, 1])\n        new_w = h * sin + w * cos\n        new_h = h * cos + w * sin\n        matrix[0, 2] += (new_w - w) * 0.5\n        matrix[1, 2] += (new_h - h) * 0.5\n        w = int(np.round(new_w))\n        h = int(np.round(new_h))\n    rotated = cv2.warpAffine(img, matrix, (w, h), borderValue=border_value)\n    return rotated","method_path":"mmcv\/image\/transforms\/geometry.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"bbox_clip","method_code":"def bbox_clip(bboxes, img_shape):\n    assert bboxes.shape[-1] % 4 == 0\n    clipped_bboxes = np.empty_like(bboxes, dtype=bboxes.dtype)\n    clipped_bboxes[..., 0::2] = np.maximum(\n        np.minimum(bboxes[..., 0::2], img_shape[1] - 1), 0)\n    clipped_bboxes[..., 1::2] = np.maximum(\n        np.minimum(bboxes[..., 1::2], img_shape[0] - 1), 0)\n    return clipped_bboxes","method_summary":"Clip bboxes to fit the image shape.","original_method_code":"def bbox_clip(bboxes, img_shape):\n    \"\"\"Clip bboxes to fit the image shape.\n\n    Args:\n        bboxes (ndarray): Shape (..., 4*k)\n        img_shape (tuple): (height, width) of the image.\n\n    Returns:\n        ndarray: Clipped bboxes.\n    \"\"\"\n    assert bboxes.shape[-1] % 4 == 0\n    clipped_bboxes = np.empty_like(bboxes, dtype=bboxes.dtype)\n    clipped_bboxes[..., 0::2] = np.maximum(\n        np.minimum(bboxes[..., 0::2], img_shape[1] - 1), 0)\n    clipped_bboxes[..., 1::2] = np.maximum(\n        np.minimum(bboxes[..., 1::2], img_shape[0] - 1), 0)\n    return clipped_bboxes","method_path":"mmcv\/image\/transforms\/geometry.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"bbox_scaling","method_code":"def bbox_scaling(bboxes, scale, clip_shape=None):\n    if float(scale) == 1.0:\n        scaled_bboxes = bboxes.copy()\n    else:\n        w = bboxes[..., 2] - bboxes[..., 0] + 1\n        h = bboxes[..., 3] - bboxes[..., 1] + 1\n        dw = (w * (scale - 1)) * 0.5\n        dh = (h * (scale - 1)) * 0.5\n        scaled_bboxes = bboxes + np.stack((-dw, -dh, dw, dh), axis=-1)\n    if clip_shape is not None:\n        return bbox_clip(scaled_bboxes, clip_shape)\n    else:\n        return scaled_bboxes","method_summary":"Scaling bboxes w.r.t the box center.","original_method_code":"def bbox_scaling(bboxes, scale, clip_shape=None):\n    \"\"\"Scaling bboxes w.r.t the box center.\n\n    Args:\n        bboxes (ndarray): Shape(..., 4).\n        scale (float): Scaling factor.\n        clip_shape (tuple, optional): If specified, bboxes that exceed the\n            boundary will be clipped according to the given shape (h, w).\n\n    Returns:\n        ndarray: Scaled bboxes.\n    \"\"\"\n    if float(scale) == 1.0:\n        scaled_bboxes = bboxes.copy()\n    else:\n        w = bboxes[..., 2] - bboxes[..., 0] + 1\n        h = bboxes[..., 3] - bboxes[..., 1] + 1\n        dw = (w * (scale - 1)) * 0.5\n        dh = (h * (scale - 1)) * 0.5\n        scaled_bboxes = bboxes + np.stack((-dw, -dh, dw, dh), axis=-1)\n    if clip_shape is not None:\n        return bbox_clip(scaled_bboxes, clip_shape)\n    else:\n        return scaled_bboxes","method_path":"mmcv\/image\/transforms\/geometry.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imcrop","method_code":"def imcrop(img, bboxes, scale=1.0, pad_fill=None):\n    chn = 1 if img.ndim == 2 else img.shape[2]\n    if pad_fill is not None:\n        if isinstance(pad_fill, (int, float)):\n            pad_fill = [pad_fill for _ in range(chn)]\n        assert len(pad_fill) == chn\n\n    _bboxes = bboxes[None, ...] if bboxes.ndim == 1 else bboxes\n    scaled_bboxes = bbox_scaling(_bboxes, scale).astype(np.int32)\n    clipped_bbox = bbox_clip(scaled_bboxes, img.shape)\n\n    patches = []\n    for i in range(clipped_bbox.shape[0]):\n        x1, y1, x2, y2 = tuple(clipped_bbox[i, :])\n        if pad_fill is None:\n            patch = img[y1:y2 + 1, x1:x2 + 1, ...]\n        else:\n            _x1, _y1, _x2, _y2 = tuple(scaled_bboxes[i, :])\n            if chn == 2:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1)\n            else:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1, chn)\n            patch = np.array(\n                pad_fill, dtype=img.dtype) * np.ones(\n                    patch_shape, dtype=img.dtype)\n            x_start = 0 if _x1 >= 0 else -_x1\n            y_start = 0 if _y1 >= 0 else -_y1\n            w = x2 - x1 + 1\n            h = y2 - y1 + 1\n            patch[y_start:y_start + h, x_start:x_start +\n                  w, ...] = img[y1:y1 + h, x1:x1 + w, ...]\n        patches.append(patch)\n\n    if bboxes.ndim == 1:\n        return patches[0]\n    else:\n        return patches","method_summary":"Crop image patches. 3","original_method_code":"def imcrop(img, bboxes, scale=1.0, pad_fill=None):\n    \"\"\"Crop image patches.\n\n    3 steps: scale the bboxes -> clip bboxes -> crop and pad.\n\n    Args:\n        img (ndarray): Image to be cropped.\n        bboxes (ndarray): Shape (k, 4) or (4, ), location of cropped bboxes.\n        scale (float, optional): Scale ratio of bboxes, the default value\n            1.0 means no padding.\n        pad_fill (number or list): Value to be filled for padding, None for\n            no padding.\n\n    Returns:\n        list or ndarray: The cropped image patches.\n    \"\"\"\n    chn = 1 if img.ndim == 2 else img.shape[2]\n    if pad_fill is not None:\n        if isinstance(pad_fill, (int, float)):\n            pad_fill = [pad_fill for _ in range(chn)]\n        assert len(pad_fill) == chn\n\n    _bboxes = bboxes[None, ...] if bboxes.ndim == 1 else bboxes\n    scaled_bboxes = bbox_scaling(_bboxes, scale).astype(np.int32)\n    clipped_bbox = bbox_clip(scaled_bboxes, img.shape)\n\n    patches = []\n    for i in range(clipped_bbox.shape[0]):\n        x1, y1, x2, y2 = tuple(clipped_bbox[i, :])\n        if pad_fill is None:\n            patch = img[y1:y2 + 1, x1:x2 + 1, ...]\n        else:\n            _x1, _y1, _x2, _y2 = tuple(scaled_bboxes[i, :])\n            if chn == 2:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1)\n            else:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1, chn)\n            patch = np.array(\n                pad_fill, dtype=img.dtype) * np.ones(\n                    patch_shape, dtype=img.dtype)\n            x_start = 0 if _x1 >= 0 else -_x1\n            y_start = 0 if _y1 >= 0 else -_y1\n            w = x2 - x1 + 1\n            h = y2 - y1 + 1\n            patch[y_start:y_start + h, x_start:x_start +\n                  w, ...] = img[y1:y1 + h, x1:x1 + w, ...]\n        patches.append(patch)\n\n    if bboxes.ndim == 1:\n        return patches[0]\n    else:\n        return patches","method_path":"mmcv\/image\/transforms\/geometry.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"impad","method_code":"def impad(img, shape, pad_val=0):\n    if not isinstance(pad_val, (int, float)):\n        assert len(pad_val) == img.shape[-1]\n    if len(shape) < len(img.shape):\n        shape = shape + (img.shape[-1], )\n    assert len(shape) == len(img.shape)\n    for i in range(len(shape) - 1):\n        assert shape[i] >= img.shape[i]\n    pad = np.empty(shape, dtype=img.dtype)\n    pad[...] = pad_val\n    pad[:img.shape[0], :img.shape[1], ...] = img\n    return pad","method_summary":"Pad an image to a certain shape.","original_method_code":"def impad(img, shape, pad_val=0):\n    \"\"\"Pad an image to a certain shape.\n\n    Args:\n        img (ndarray): Image to be padded.\n        shape (tuple): Expected padding shape.\n        pad_val (number or sequence): Values to be filled in padding areas.\n\n    Returns:\n        ndarray: The padded image.\n    \"\"\"\n    if not isinstance(pad_val, (int, float)):\n        assert len(pad_val) == img.shape[-1]\n    if len(shape) < len(img.shape):\n        shape = shape + (img.shape[-1], )\n    assert len(shape) == len(img.shape)\n    for i in range(len(shape) - 1):\n        assert shape[i] >= img.shape[i]\n    pad = np.empty(shape, dtype=img.dtype)\n    pad[...] = pad_val\n    pad[:img.shape[0], :img.shape[1], ...] = img\n    return pad","method_path":"mmcv\/image\/transforms\/geometry.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"impad_to_multiple","method_code":"def impad_to_multiple(img, divisor, pad_val=0):\n    pad_h = int(np.ceil(img.shape[0] \/ divisor)) * divisor\n    pad_w = int(np.ceil(img.shape[1] \/ divisor)) * divisor\n    return impad(img, (pad_h, pad_w), pad_val)","method_summary":"Pad an image to ensure each edge to be multiple to some number.","original_method_code":"def impad_to_multiple(img, divisor, pad_val=0):\n    \"\"\"Pad an image to ensure each edge to be multiple to some number.\n\n    Args:\n        img (ndarray): Image to be padded.\n        divisor (int): Padded image edges will be multiple to divisor.\n        pad_val (number or sequence): Same as :func:`impad`.\n\n    Returns:\n        ndarray: The padded image.\n    \"\"\"\n    pad_h = int(np.ceil(img.shape[0] \/ divisor)) * divisor\n    pad_w = int(np.ceil(img.shape[1] \/ divisor)) * divisor\n    return impad(img, (pad_h, pad_w), pad_val)","method_path":"mmcv\/image\/transforms\/geometry.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"_scale_size","method_code":"def _scale_size(size, scale):\n    w, h = size\n    return int(w * float(scale) + 0.5), int(h * float(scale) + 0.5)","method_summary":"Rescale a size by a ratio.","original_method_code":"def _scale_size(size, scale):\n    \"\"\"Rescale a size by a ratio.\n\n    Args:\n        size (tuple): w, h.\n        scale (float): Scaling factor.\n\n    Returns:\n        tuple[int]: scaled size.\n    \"\"\"\n    w, h = size\n    return int(w * float(scale) + 0.5), int(h * float(scale) + 0.5)","method_path":"mmcv\/image\/transforms\/resize.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imresize","method_code":"def imresize(img, size, return_scale=False, interpolation='bilinear'):\n    h, w = img.shape[:2]\n    resized_img = cv2.resize(\n        img, size, interpolation=interp_codes[interpolation])\n    if not return_scale:\n        return resized_img\n    else:\n        w_scale = size[0] \/ w\n        h_scale = size[1] \/ h\n        return resized_img, w_scale, h_scale","method_summary":"Resize image to a given size.","original_method_code":"def imresize(img, size, return_scale=False, interpolation='bilinear'):\n    \"\"\"Resize image to a given size.\n\n    Args:\n        img (ndarray): The input image.\n        size (tuple): Target (w, h).\n        return_scale (bool): Whether to return `w_scale` and `h_scale`.\n        interpolation (str): Interpolation method, accepted values are\n            \"nearest\", \"bilinear\", \"bicubic\", \"area\", \"lanczos\".\n\n    Returns:\n        tuple or ndarray: (`resized_img`, `w_scale`, `h_scale`) or\n            `resized_img`.\n    \"\"\"\n    h, w = img.shape[:2]\n    resized_img = cv2.resize(\n        img, size, interpolation=interp_codes[interpolation])\n    if not return_scale:\n        return resized_img\n    else:\n        w_scale = size[0] \/ w\n        h_scale = size[1] \/ h\n        return resized_img, w_scale, h_scale","method_path":"mmcv\/image\/transforms\/resize.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imresize_like","method_code":"def imresize_like(img, dst_img, return_scale=False, interpolation='bilinear'):\n    h, w = dst_img.shape[:2]\n    return imresize(img, (w, h), return_scale, interpolation)","method_summary":"Resize image to the same size of a given image.","original_method_code":"def imresize_like(img, dst_img, return_scale=False, interpolation='bilinear'):\n    \"\"\"Resize image to the same size of a given image.\n\n    Args:\n        img (ndarray): The input image.\n        dst_img (ndarray): The target image.\n        return_scale (bool): Whether to return `w_scale` and `h_scale`.\n        interpolation (str): Same as :func:`resize`.\n\n    Returns:\n        tuple or ndarray: (`resized_img`, `w_scale`, `h_scale`) or\n            `resized_img`.\n    \"\"\"\n    h, w = dst_img.shape[:2]\n    return imresize(img, (w, h), return_scale, interpolation)","method_path":"mmcv\/image\/transforms\/resize.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imrescale","method_code":"def imrescale(img, scale, return_scale=False, interpolation='bilinear'):\n    h, w = img.shape[:2]\n    if isinstance(scale, (float, int)):\n        if scale <= 0:\n            raise ValueError(\n                'Invalid scale {}, must be positive.'.format(scale))\n        scale_factor = scale\n    elif isinstance(scale, tuple):\n        max_long_edge = max(scale)\n        max_short_edge = min(scale)\n        scale_factor = min(max_long_edge \/ max(h, w),\n                           max_short_edge \/ min(h, w))\n    else:\n        raise TypeError(\n            'Scale must be a number or tuple of int, but got {}'.format(\n                type(scale)))\n    new_size = _scale_size((w, h), scale_factor)\n    rescaled_img = imresize(img, new_size, interpolation=interpolation)\n    if return_scale:\n        return rescaled_img, scale_factor\n    else:\n        return rescaled_img","method_summary":"Resize image while keeping the aspect ratio.","original_method_code":"def imrescale(img, scale, return_scale=False, interpolation='bilinear'):\n    \"\"\"Resize image while keeping the aspect ratio.\n\n    Args:\n        img (ndarray): The input image.\n        scale (float or tuple[int]): The scaling factor or maximum size.\n            If it is a float number, then the image will be rescaled by this\n            factor, else if it is a tuple of 2 integers, then the image will\n            be rescaled as large as possible within the scale.\n        return_scale (bool): Whether to return the scaling factor besides the\n            rescaled image.\n        interpolation (str): Same as :func:`resize`.\n\n    Returns:\n        ndarray: The rescaled image.\n    \"\"\"\n    h, w = img.shape[:2]\n    if isinstance(scale, (float, int)):\n        if scale <= 0:\n            raise ValueError(\n                'Invalid scale {}, must be positive.'.format(scale))\n        scale_factor = scale\n    elif isinstance(scale, tuple):\n        max_long_edge = max(scale)\n        max_short_edge = min(scale)\n        scale_factor = min(max_long_edge \/ max(h, w),\n                           max_short_edge \/ min(h, w))\n    else:\n        raise TypeError(\n            'Scale must be a number or tuple of int, but got {}'.format(\n                type(scale)))\n    new_size = _scale_size((w, h), scale_factor)\n    rescaled_img = imresize(img, new_size, interpolation=interpolation)\n    if return_scale:\n        return rescaled_img, scale_factor\n    else:\n        return rescaled_img","method_path":"mmcv\/image\/transforms\/resize.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"_register_handler","method_code":"def _register_handler(handler, file_formats):\n    if not isinstance(handler, BaseFileHandler):\n        raise TypeError(\n            'handler must be a child of BaseFileHandler, not {}'.format(\n                type(handler)))\n    if isinstance(file_formats, str):\n        file_formats = [file_formats]\n    if not is_list_of(file_formats, str):\n        raise TypeError('file_formats must be a str or a list of str')\n    for ext in file_formats:\n        file_handlers[ext] = handler","method_summary":"Register a handler for some file extensions.","original_method_code":"def _register_handler(handler, file_formats):\n    \"\"\"Register a handler for some file extensions.\n\n    Args:\n        handler (:obj:`BaseFileHandler`): Handler to be registered.\n        file_formats (str or list[str]): File formats to be handled by this\n            handler.\n    \"\"\"\n    if not isinstance(handler, BaseFileHandler):\n        raise TypeError(\n            'handler must be a child of BaseFileHandler, not {}'.format(\n                type(handler)))\n    if isinstance(file_formats, str):\n        file_formats = [file_formats]\n    if not is_list_of(file_formats, str):\n        raise TypeError('file_formats must be a str or a list of str')\n    for ext in file_formats:\n        file_handlers[ext] = handler","method_path":"mmcv\/fileio\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"get_priority","method_code":"def get_priority(priority):\n    if isinstance(priority, int):\n        if priority < 0 or priority > 100:\n            raise ValueError('priority must be between 0 and 100')\n        return priority\n    elif isinstance(priority, Priority):\n        return priority.value\n    elif isinstance(priority, str):\n        return Priority[priority.upper()].value\n    else:\n        raise TypeError('priority must be an integer or Priority enum value')","method_summary":"Get priority value.","original_method_code":"def get_priority(priority):\n    \"\"\"Get priority value.\n\n    Args:\n        priority (int or str or :obj:`Priority`): Priority.\n\n    Returns:\n        int: The priority value.\n    \"\"\"\n    if isinstance(priority, int):\n        if priority < 0 or priority > 100:\n            raise ValueError('priority must be between 0 and 100')\n        return priority\n    elif isinstance(priority, Priority):\n        return priority.value\n    elif isinstance(priority, str):\n        return Priority[priority.upper()].value\n    else:\n        raise TypeError('priority must be an integer or Priority enum value')","method_path":"mmcv\/runner\/priority.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"dequantize","method_code":"def dequantize(arr, min_val, max_val, levels, dtype=np.float64):\n    if not (isinstance(levels, int) and levels > 1):\n        raise ValueError(\n            'levels must be a positive integer, but got {}'.format(levels))\n    if min_val >= max_val:\n        raise ValueError(\n            'min_val ({}) must be smaller than max_val ({})'.format(\n                min_val, max_val))\n\n    dequantized_arr = (arr + 0.5).astype(dtype) * (\n        max_val - min_val) \/ levels + min_val\n\n    return dequantized_arr","method_summary":"Dequantize an array.","original_method_code":"def dequantize(arr, min_val, max_val, levels, dtype=np.float64):\n    \"\"\"Dequantize an array.\n\n    Args:\n        arr (ndarray): Input array.\n        min_val (scalar): Minimum value to be clipped.\n        max_val (scalar): Maximum value to be clipped.\n        levels (int): Quantization levels.\n        dtype (np.type): The type of the dequantized array.\n\n    Returns:\n        tuple: Dequantized array.\n    \"\"\"\n    if not (isinstance(levels, int) and levels > 1):\n        raise ValueError(\n            'levels must be a positive integer, but got {}'.format(levels))\n    if min_val >= max_val:\n        raise ValueError(\n            'min_val ({}) must be smaller than max_val ({})'.format(\n                min_val, max_val))\n\n    dequantized_arr = (arr + 0.5).astype(dtype) * (\n        max_val - min_val) \/ levels + min_val\n\n    return dequantized_arr","method_path":"mmcv\/arraymisc\/quantization.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imshow","method_code":"def imshow(img, win_name='', wait_time=0):\n    cv2.imshow(win_name, imread(img))\n    cv2.waitKey(wait_time)","method_summary":"Show an image.","original_method_code":"def imshow(img, win_name='', wait_time=0):\n    \"\"\"Show an image.\n\n    Args:\n        img (str or ndarray): The image to be displayed.\n        win_name (str): The window name.\n        wait_time (int): Value of waitKey param.\n    \"\"\"\n    cv2.imshow(win_name, imread(img))\n    cv2.waitKey(wait_time)","method_path":"mmcv\/visualization\/image.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imshow_bboxes","method_code":"def imshow_bboxes(img,\n                  bboxes,\n                  colors='green',\n                  top_k=-1,\n                  thickness=1,\n                  show=True,\n                  win_name='',\n                  wait_time=0,\n                  out_file=None):\n    img = imread(img)\n\n    if isinstance(bboxes, np.ndarray):\n        bboxes = [bboxes]\n    if not isinstance(colors, list):\n        colors = [colors for _ in range(len(bboxes))]\n    colors = [color_val(c) for c in colors]\n    assert len(bboxes) == len(colors)\n\n    for i, _bboxes in enumerate(bboxes):\n        _bboxes = _bboxes.astype(np.int32)\n        if top_k <= 0:\n            _top_k = _bboxes.shape[0]\n        else:\n            _top_k = min(top_k, _bboxes.shape[0])\n        for j in range(_top_k):\n            left_top = (_bboxes[j, 0], _bboxes[j, 1])\n            right_bottom = (_bboxes[j, 2], _bboxes[j, 3])\n            cv2.rectangle(\n                img, left_top, right_bottom, colors[i], thickness=thickness)\n\n    if show:\n        imshow(img, win_name, wait_time)\n    if out_file is not None:\n        imwrite(img, out_file)","method_summary":"Draw bboxes on an image.","original_method_code":"def imshow_bboxes(img,\n                  bboxes,\n                  colors='green',\n                  top_k=-1,\n                  thickness=1,\n                  show=True,\n                  win_name='',\n                  wait_time=0,\n                  out_file=None):\n    \"\"\"Draw bboxes on an image.\n\n    Args:\n        img (str or ndarray): The image to be displayed.\n        bboxes (list or ndarray): A list of ndarray of shape (k, 4).\n        colors (list[str or tuple or Color]): A list of colors.\n        top_k (int): Plot the first k bboxes only if set positive.\n        thickness (int): Thickness of lines.\n        show (bool): Whether to show the image.\n        win_name (str): The window name.\n        wait_time (int): Value of waitKey param.\n        out_file (str, optional): The filename to write the image.\n    \"\"\"\n    img = imread(img)\n\n    if isinstance(bboxes, np.ndarray):\n        bboxes = [bboxes]\n    if not isinstance(colors, list):\n        colors = [colors for _ in range(len(bboxes))]\n    colors = [color_val(c) for c in colors]\n    assert len(bboxes) == len(colors)\n\n    for i, _bboxes in enumerate(bboxes):\n        _bboxes = _bboxes.astype(np.int32)\n        if top_k <= 0:\n            _top_k = _bboxes.shape[0]\n        else:\n            _top_k = min(top_k, _bboxes.shape[0])\n        for j in range(_top_k):\n            left_top = (_bboxes[j, 0], _bboxes[j, 1])\n            right_bottom = (_bboxes[j, 2], _bboxes[j, 3])\n            cv2.rectangle(\n                img, left_top, right_bottom, colors[i], thickness=thickness)\n\n    if show:\n        imshow(img, win_name, wait_time)\n    if out_file is not None:\n        imwrite(img, out_file)","method_path":"mmcv\/visualization\/image.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"flowread","method_code":"def flowread(flow_or_path, quantize=False, concat_axis=0, *args, **kwargs):\n    if isinstance(flow_or_path, np.ndarray):\n        if (flow_or_path.ndim != 3) or (flow_or_path.shape[-1] != 2):\n            raise ValueError('Invalid flow with shape {}'.format(\n                flow_or_path.shape))\n        return flow_or_path\n    elif not is_str(flow_or_path):\n        raise TypeError(\n            '\"flow_or_path\" must be a filename or numpy array, not {}'.format(\n                type(flow_or_path)))\n\n    if not quantize:\n        with open(flow_or_path, 'rb') as f:\n            try:\n                header = f.read(4).decode('utf-8')\n            except Exception:\n                raise IOError('Invalid flow file: {}'.format(flow_or_path))\n            else:\n                if header != 'PIEH':\n                    raise IOError(\n                        'Invalid flow file: {}, header does not contain PIEH'.\n                        format(flow_or_path))\n\n            w = np.fromfile(f, np.int32, 1).squeeze()\n            h = np.fromfile(f, np.int32, 1).squeeze()\n            flow = np.fromfile(f, np.float32, w * h * 2).reshape((h, w, 2))\n    else:\n        assert concat_axis in [0, 1]\n        cat_flow = imread(flow_or_path, flag='unchanged')\n        if cat_flow.ndim != 2:\n            raise IOError(\n                '{} is not a valid quantized flow file, its dimension is {}.'.\n                format(flow_or_path, cat_flow.ndim))\n        assert cat_flow.shape[concat_axis] % 2 == 0\n        dx, dy = np.split(cat_flow, 2, axis=concat_axis)\n        flow = dequantize_flow(dx, dy, *args, **kwargs)\n\n    return flow.astype(np.float32)","method_summary":"Read an optical flow map.","original_method_code":"def flowread(flow_or_path, quantize=False, concat_axis=0, *args, **kwargs):\n    \"\"\"Read an optical flow map.\n\n    Args:\n        flow_or_path (ndarray or str): A flow map or filepath.\n        quantize (bool): whether to read quantized pair, if set to True,\n            remaining args will be passed to :func:`dequantize_flow`.\n        concat_axis (int): The axis that dx and dy are concatenated,\n            can be either 0 or 1. Ignored if quantize is False.\n\n    Returns:\n        ndarray: Optical flow represented as a (h, w, 2) numpy array\n    \"\"\"\n    if isinstance(flow_or_path, np.ndarray):\n        if (flow_or_path.ndim != 3) or (flow_or_path.shape[-1] != 2):\n            raise ValueError('Invalid flow with shape {}'.format(\n                flow_or_path.shape))\n        return flow_or_path\n    elif not is_str(flow_or_path):\n        raise TypeError(\n            '\"flow_or_path\" must be a filename or numpy array, not {}'.format(\n                type(flow_or_path)))\n\n    if not quantize:\n        with open(flow_or_path, 'rb') as f:\n            try:\n                header = f.read(4).decode('utf-8')\n            except Exception:\n                raise IOError('Invalid flow file: {}'.format(flow_or_path))\n            else:\n                if header != 'PIEH':\n                    raise IOError(\n                        'Invalid flow file: {}, header does not contain PIEH'.\n                        format(flow_or_path))\n\n            w = np.fromfile(f, np.int32, 1).squeeze()\n            h = np.fromfile(f, np.int32, 1).squeeze()\n            flow = np.fromfile(f, np.float32, w * h * 2).reshape((h, w, 2))\n    else:\n        assert concat_axis in [0, 1]\n        cat_flow = imread(flow_or_path, flag='unchanged')\n        if cat_flow.ndim != 2:\n            raise IOError(\n                '{} is not a valid quantized flow file, its dimension is {}.'.\n                format(flow_or_path, cat_flow.ndim))\n        assert cat_flow.shape[concat_axis] % 2 == 0\n        dx, dy = np.split(cat_flow, 2, axis=concat_axis)\n        flow = dequantize_flow(dx, dy, *args, **kwargs)\n\n    return flow.astype(np.float32)","method_path":"mmcv\/video\/optflow.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"dequantize_flow","method_code":"def dequantize_flow(dx, dy, max_val=0.02, denorm=True):\n    assert dx.shape == dy.shape\n    assert dx.ndim == 2 or (dx.ndim == 3 and dx.shape[-1] == 1)\n\n    dx, dy = [dequantize(d, -max_val, max_val, 255) for d in [dx, dy]]\n\n    if denorm:\n        dx *= dx.shape[1]\n        dy *= dx.shape[0]\n    flow = np.dstack((dx, dy))\n    return flow","method_summary":"Recover from quantized flow.","original_method_code":"def dequantize_flow(dx, dy, max_val=0.02, denorm=True):\n    \"\"\"Recover from quantized flow.\n\n    Args:\n        dx (ndarray): Quantized dx.\n        dy (ndarray): Quantized dy.\n        max_val (float): Maximum value used when quantizing.\n        denorm (bool): Whether to multiply flow values with width\/height.\n\n    Returns:\n        ndarray: Dequantized flow.\n    \"\"\"\n    assert dx.shape == dy.shape\n    assert dx.ndim == 2 or (dx.ndim == 3 and dx.shape[-1] == 1)\n\n    dx, dy = [dequantize(d, -max_val, max_val, 255) for d in [dx, dy]]\n\n    if denorm:\n        dx *= dx.shape[1]\n        dy *= dx.shape[0]\n    flow = np.dstack((dx, dy))\n    return flow","method_path":"mmcv\/video\/optflow.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"load_checkpoint","method_code":"def load_checkpoint(model,\n                    filename,\n                    map_location=None,\n                    strict=False,\n                    logger=None):\n    \n    if filename.startswith('modelzoo:\/\/'):\n        import torchvision\n        model_urls = dict()\n        for _, name, ispkg in pkgutil.walk_packages(\n                torchvision.models.__path__):\n            if not ispkg:\n                _zoo = import_module('torchvision.models.{}'.format(name))\n                _urls = getattr(_zoo, 'model_urls')\n                model_urls.update(_urls)\n        model_name = filename[11:]\n        checkpoint = model_zoo.load_url(model_urls[model_name])\n    elif filename.startswith('open-mmlab:\/\/'):\n        model_name = filename[13:]\n        checkpoint = model_zoo.load_url(open_mmlab_model_urls[model_name])\n    elif filename.startswith(('http:\/\/', 'https:\/\/')):\n        checkpoint = model_zoo.load_url(filename)\n    else:\n        if not osp.isfile(filename):\n            raise IOError('{} is not a checkpoint file'.format(filename))\n        checkpoint = torch.load(filename, map_location=map_location)\n    \n    if isinstance(checkpoint, OrderedDict):\n        state_dict = checkpoint\n    elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n        state_dict = checkpoint['state_dict']\n    else:\n        raise RuntimeError(\n            'No state_dict found in checkpoint file {}'.format(filename))\n    \n    if list(state_dict.keys())[0].startswith('module.'):\n        state_dict = {k[7:]: v for k, v in checkpoint['state_dict'].items()}\n    \n    if hasattr(model, 'module'):\n        load_state_dict(model.module, state_dict, strict, logger)\n    else:\n        load_state_dict(model, state_dict, strict, logger)\n    return checkpoint","method_summary":"Load checkpoint from a file or URI.","original_method_code":"def load_checkpoint(model,\n                    filename,\n                    map_location=None,\n                    strict=False,\n                    logger=None):\n    \"\"\"Load checkpoint from a file or URI.\n\n    Args:\n        model (Module): Module to load checkpoint.\n        filename (str): Either a filepath or URL or modelzoo:\/\/xxxxxxx.\n        map_location (str): Same as :func:`torch.load`.\n        strict (bool): Whether to allow different params for the model and\n            checkpoint.\n        logger (:mod:`logging.Logger` or None): The logger for error message.\n\n    Returns:\n        dict or OrderedDict: The loaded checkpoint.\n    \"\"\"\n    # load checkpoint from modelzoo or file or url\n    if filename.startswith('modelzoo:\/\/'):\n        import torchvision\n        model_urls = dict()\n        for _, name, ispkg in pkgutil.walk_packages(\n                torchvision.models.__path__):\n            if not ispkg:\n                _zoo = import_module('torchvision.models.{}'.format(name))\n                _urls = getattr(_zoo, 'model_urls')\n                model_urls.update(_urls)\n        model_name = filename[11:]\n        checkpoint = model_zoo.load_url(model_urls[model_name])\n    elif filename.startswith('open-mmlab:\/\/'):\n        model_name = filename[13:]\n        checkpoint = model_zoo.load_url(open_mmlab_model_urls[model_name])\n    elif filename.startswith(('http:\/\/', 'https:\/\/')):\n        checkpoint = model_zoo.load_url(filename)\n    else:\n        if not osp.isfile(filename):\n            raise IOError('{} is not a checkpoint file'.format(filename))\n        checkpoint = torch.load(filename, map_location=map_location)\n    # get state_dict from checkpoint\n    if isinstance(checkpoint, OrderedDict):\n        state_dict = checkpoint\n    elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n        state_dict = checkpoint['state_dict']\n    else:\n        raise RuntimeError(\n            'No state_dict found in checkpoint file {}'.format(filename))\n    # strip prefix of state_dict\n    if list(state_dict.keys())[0].startswith('module.'):\n        state_dict = {k[7:]: v for k, v in checkpoint['state_dict'].items()}\n    # load state_dict\n    if hasattr(model, 'module'):\n        load_state_dict(model.module, state_dict, strict, logger)\n    else:\n        load_state_dict(model, state_dict, strict, logger)\n    return checkpoint","method_path":"mmcv\/runner\/checkpoint.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"weights_to_cpu","method_code":"def weights_to_cpu(state_dict):\n    state_dict_cpu = OrderedDict()\n    for key, val in state_dict.items():\n        state_dict_cpu[key] = val.cpu()\n    return state_dict_cpu","method_summary":"Copy a model state_dict to cpu.","original_method_code":"def weights_to_cpu(state_dict):\n    \"\"\"Copy a model state_dict to cpu.\n\n    Args:\n        state_dict (OrderedDict): Model weights on GPU.\n\n    Returns:\n        OrderedDict: Model weights on GPU.\n    \"\"\"\n    state_dict_cpu = OrderedDict()\n    for key, val in state_dict.items():\n        state_dict_cpu[key] = val.cpu()\n    return state_dict_cpu","method_path":"mmcv\/runner\/checkpoint.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"save_checkpoint","method_code":"def save_checkpoint(model, filename, optimizer=None, meta=None):\n    if meta is None:\n        meta = {}\n    elif not isinstance(meta, dict):\n        raise TypeError('meta must be a dict or None, but got {}'.format(\n            type(meta)))\n    meta.update(mmcv_version=mmcv.__version__, time=time.asctime())\n\n    mmcv.mkdir_or_exist(osp.dirname(filename))\n    if hasattr(model, 'module'):\n        model = model.module\n\n    checkpoint = {\n        'meta': meta,\n        'state_dict': weights_to_cpu(model.state_dict())\n    }\n    if optimizer is not None:\n        checkpoint['optimizer'] = optimizer.state_dict()\n\n    torch.save(checkpoint, filename)","method_summary":"Save checkpoint to file. The checkpoint will have 3","original_method_code":"def save_checkpoint(model, filename, optimizer=None, meta=None):\n    \"\"\"Save checkpoint to file.\n\n    The checkpoint will have 3 fields: ``meta``, ``state_dict`` and\n    ``optimizer``. By default ``meta`` will contain version and time info.\n\n    Args:\n        model (Module): Module whose params are to be saved.\n        filename (str): Checkpoint filename.\n        optimizer (:obj:`Optimizer`, optional): Optimizer to be saved.\n        meta (dict, optional): Metadata to be saved in checkpoint.\n    \"\"\"\n    if meta is None:\n        meta = {}\n    elif not isinstance(meta, dict):\n        raise TypeError('meta must be a dict or None, but got {}'.format(\n            type(meta)))\n    meta.update(mmcv_version=mmcv.__version__, time=time.asctime())\n\n    mmcv.mkdir_or_exist(osp.dirname(filename))\n    if hasattr(model, 'module'):\n        model = model.module\n\n    checkpoint = {\n        'meta': meta,\n        'state_dict': weights_to_cpu(model.state_dict())\n    }\n    if optimizer is not None:\n        checkpoint['optimizer'] = optimizer.state_dict()\n\n    torch.save(checkpoint, filename)","method_path":"mmcv\/runner\/checkpoint.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"Runner.init_optimizer","method_code":"def init_optimizer(self, optimizer):\n        if isinstance(optimizer, dict):\n            optimizer = obj_from_dict(\n                optimizer, torch.optim, dict(params=self.model.parameters()))\n        elif not isinstance(optimizer, torch.optim.Optimizer):\n            raise TypeError(\n                'optimizer must be either an Optimizer object or a dict, '\n                'but got {}'.format(type(optimizer)))\n        return optimizer","method_summary":"Init the optimizer.","original_method_code":"def init_optimizer(self, optimizer):\n        \"\"\"Init the optimizer.\n\n        Args:\n            optimizer (dict or :obj:`~torch.optim.Optimizer`): Either an\n                optimizer object or a dict used for constructing the optimizer.\n\n        Returns:\n            :obj:`~torch.optim.Optimizer`: An optimizer object.\n\n        Examples:\n            >>> optimizer = dict(type='SGD', lr=0.01, momentum=0.9)\n            >>> type(runner.init_optimizer(optimizer))\n            <class 'torch.optim.sgd.SGD'>\n        \"\"\"\n        if isinstance(optimizer, dict):\n            optimizer = obj_from_dict(\n                optimizer, torch.optim, dict(params=self.model.parameters()))\n        elif not isinstance(optimizer, torch.optim.Optimizer):\n            raise TypeError(\n                'optimizer must be either an Optimizer object or a dict, '\n                'but got {}'.format(type(optimizer)))\n        return optimizer","method_path":"mmcv\/runner\/runner.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"Runner.init_logger","method_code":"def init_logger(self, log_dir=None, level=logging.INFO):\n        logging.basicConfig(\n            format='%(asctime)s - %(levelname)s - %(message)s', level=level)\n        logger = logging.getLogger(__name__)\n        if log_dir and self.rank == 0:\n            filename = '{}.log'.format(self.timestamp)\n            log_file = osp.join(log_dir, filename)\n            self._add_file_handler(logger, log_file, level=level)\n        return logger","method_summary":"Init the logger.","original_method_code":"def init_logger(self, log_dir=None, level=logging.INFO):\n        \"\"\"Init the logger.\n\n        Args:\n            log_dir(str, optional): Log file directory. If not specified, no\n                log file will be used.\n            level (int or str): See the built-in python logging module.\n\n        Returns:\n            :obj:`~logging.Logger`: Python logger.\n        \"\"\"\n        logging.basicConfig(\n            format='%(asctime)s - %(levelname)s - %(message)s', level=level)\n        logger = logging.getLogger(__name__)\n        if log_dir and self.rank == 0:\n            filename = '{}.log'.format(self.timestamp)\n            log_file = osp.join(log_dir, filename)\n            self._add_file_handler(logger, log_file, level=level)\n        return logger","method_path":"mmcv\/runner\/runner.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"Runner.current_lr","method_code":"def current_lr(self):\n        if self.optimizer is None:\n            raise RuntimeError(\n                'lr is not applicable because optimizer does not exist.')\n        return [group['lr'] for group in self.optimizer.param_groups]","method_summary":"Get current learning rates.","original_method_code":"def current_lr(self):\n        \"\"\"Get current learning rates.\n\n        Returns:\n            list: Current learning rate of all param groups.\n        \"\"\"\n        if self.optimizer is None:\n            raise RuntimeError(\n                'lr is not applicable because optimizer does not exist.')\n        return [group['lr'] for group in self.optimizer.param_groups]","method_path":"mmcv\/runner\/runner.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"Runner.register_hook","method_code":"def register_hook(self, hook, priority='NORMAL'):\n        assert isinstance(hook, Hook)\n        if hasattr(hook, 'priority'):\n            raise ValueError('\"priority\" is a reserved attribute for hooks')\n        priority = get_priority(priority)\n        hook.priority = priority\n        \n        inserted = False\n        for i in range(len(self._hooks) - 1, -1, -1):\n            if priority >= self._hooks[i].priority:\n                self._hooks.insert(i + 1, hook)\n                inserted = True\n                break\n        if not inserted:\n            self._hooks.insert(0, hook)","method_summary":"Register a hook into the hook list.","original_method_code":"def register_hook(self, hook, priority='NORMAL'):\n        \"\"\"Register a hook into the hook list.\n\n        Args:\n            hook (:obj:`Hook`): The hook to be registered.\n            priority (int or str or :obj:`Priority`): Hook priority.\n                Lower value means higher priority.\n        \"\"\"\n        assert isinstance(hook, Hook)\n        if hasattr(hook, 'priority'):\n            raise ValueError('\"priority\" is a reserved attribute for hooks')\n        priority = get_priority(priority)\n        hook.priority = priority\n        # insert the hook to a sorted list\n        inserted = False\n        for i in range(len(self._hooks) - 1, -1, -1):\n            if priority >= self._hooks[i].priority:\n                self._hooks.insert(i + 1, hook)\n                inserted = True\n                break\n        if not inserted:\n            self._hooks.insert(0, hook)","method_path":"mmcv\/runner\/runner.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"Runner.run","method_code":"def run(self, data_loaders, workflow, max_epochs, **kwargs):\n        assert isinstance(data_loaders, list)\n        assert mmcv.is_list_of(workflow, tuple)\n        assert len(data_loaders) == len(workflow)\n\n        self._max_epochs = max_epochs\n        work_dir = self.work_dir if self.work_dir is not None else 'NONE'\n        self.logger.info('Start running, host: %s, work_dir: %s',\n                         get_host_info(), work_dir)\n        self.logger.info('workflow: %s, max: %d epochs', workflow, max_epochs)\n        self.call_hook('before_run')\n\n        while self.epoch < max_epochs:\n            for i, flow in enumerate(workflow):\n                mode, epochs = flow\n                if isinstance(mode, str):  \n                    if not hasattr(self, mode):\n                        raise ValueError(\n                            'runner has no method named \"{}\" to run an epoch'.\n                            format(mode))\n                    epoch_runner = getattr(self, mode)\n                elif callable(mode):  \n                    epoch_runner = mode\n                else:\n                    raise TypeError('mode in workflow must be a str or '\n                                    'callable function, not {}'.format(\n                                        type(mode)))\n                for _ in range(epochs):\n                    if mode == 'train' and self.epoch >= max_epochs:\n                        return\n                    epoch_runner(data_loaders[i], **kwargs)\n\n        time.sleep(1)  \n        self.call_hook('after_run')","method_summary":"Start running.","original_method_code":"def run(self, data_loaders, workflow, max_epochs, **kwargs):\n        \"\"\"Start running.\n\n        Args:\n            data_loaders (list[:obj:`DataLoader`]): Dataloaders for training\n                and validation.\n            workflow (list[tuple]): A list of (phase, epochs) to specify the\n                running order and epochs. E.g, [('train', 2), ('val', 1)] means\n                running 2 epochs for training and 1 epoch for validation,\n                iteratively.\n            max_epochs (int): Total training epochs.\n        \"\"\"\n        assert isinstance(data_loaders, list)\n        assert mmcv.is_list_of(workflow, tuple)\n        assert len(data_loaders) == len(workflow)\n\n        self._max_epochs = max_epochs\n        work_dir = self.work_dir if self.work_dir is not None else 'NONE'\n        self.logger.info('Start running, host: %s, work_dir: %s',\n                         get_host_info(), work_dir)\n        self.logger.info('workflow: %s, max: %d epochs', workflow, max_epochs)\n        self.call_hook('before_run')\n\n        while self.epoch < max_epochs:\n            for i, flow in enumerate(workflow):\n                mode, epochs = flow\n                if isinstance(mode, str):  # self.train()\n                    if not hasattr(self, mode):\n                        raise ValueError(\n                            'runner has no method named \"{}\" to run an epoch'.\n                            format(mode))\n                    epoch_runner = getattr(self, mode)\n                elif callable(mode):  # custom train()\n                    epoch_runner = mode\n                else:\n                    raise TypeError('mode in workflow must be a str or '\n                                    'callable function, not {}'.format(\n                                        type(mode)))\n                for _ in range(epochs):\n                    if mode == 'train' and self.epoch >= max_epochs:\n                        return\n                    epoch_runner(data_loaders[i], **kwargs)\n\n        time.sleep(1)  # wait for some hooks like loggers to finish\n        self.call_hook('after_run')","method_path":"mmcv\/runner\/runner.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"Runner.register_training_hooks","method_code":"def register_training_hooks(self,\n                                lr_config,\n                                optimizer_config=None,\n                                checkpoint_config=None,\n                                log_config=None):\n        if optimizer_config is None:\n            optimizer_config = {}\n        if checkpoint_config is None:\n            checkpoint_config = {}\n        self.register_lr_hooks(lr_config)\n        self.register_hook(self.build_hook(optimizer_config, OptimizerHook))\n        self.register_hook(self.build_hook(checkpoint_config, CheckpointHook))\n        self.register_hook(IterTimerHook())\n        if log_config is not None:\n            self.register_logger_hooks(log_config)","method_summary":"Register default hooks for training. Default hooks","original_method_code":"def register_training_hooks(self,\n                                lr_config,\n                                optimizer_config=None,\n                                checkpoint_config=None,\n                                log_config=None):\n        \"\"\"Register default hooks for training.\n\n        Default hooks include:\n\n        - LrUpdaterHook\n        - OptimizerStepperHook\n        - CheckpointSaverHook\n        - IterTimerHook\n        - LoggerHook(s)\n        \"\"\"\n        if optimizer_config is None:\n            optimizer_config = {}\n        if checkpoint_config is None:\n            checkpoint_config = {}\n        self.register_lr_hooks(lr_config)\n        self.register_hook(self.build_hook(optimizer_config, OptimizerHook))\n        self.register_hook(self.build_hook(checkpoint_config, CheckpointHook))\n        self.register_hook(IterTimerHook())\n        if log_config is not None:\n            self.register_logger_hooks(log_config)","method_path":"mmcv\/runner\/runner.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"convert_video","method_code":"def convert_video(in_file, out_file, print_cmd=False, pre_options='',\n                  **kwargs):\n    options = []\n    for k, v in kwargs.items():\n        if isinstance(v, bool):\n            if v:\n                options.append('-{}'.format(k))\n        elif k == 'log_level':\n            assert v in [\n                'quiet', 'panic', 'fatal', 'error', 'warning', 'info',\n                'verbose', 'debug', 'trace'\n            ]\n            options.append('-loglevel {}'.format(v))\n        else:\n            options.append('-{} {}'.format(k, v))\n    cmd = 'ffmpeg -y {} -i {} {} {}'.format(pre_options, in_file,\n                                            ' '.join(options), out_file)\n    if print_cmd:\n        print(cmd)\n    subprocess.call(cmd, shell=True)","method_summary":"Convert a video with ffmpeg. This provides a general api to ffmpeg, the executed command","original_method_code":"def convert_video(in_file, out_file, print_cmd=False, pre_options='',\n                  **kwargs):\n    \"\"\"Convert a video with ffmpeg.\n\n    This provides a general api to ffmpeg, the executed command is::\n\n        `ffmpeg -y <pre_options> -i <in_file> <options> <out_file>`\n\n    Options(kwargs) are mapped to ffmpeg commands with the following rules:\n\n    - key=val: \"-key val\"\n    - key=True: \"-key\"\n    - key=False: \"\"\n\n    Args:\n        in_file (str): Input video filename.\n        out_file (str): Output video filename.\n        pre_options (str): Options appears before \"-i <in_file>\".\n        print_cmd (bool): Whether to print the final ffmpeg command.\n    \"\"\"\n    options = []\n    for k, v in kwargs.items():\n        if isinstance(v, bool):\n            if v:\n                options.append('-{}'.format(k))\n        elif k == 'log_level':\n            assert v in [\n                'quiet', 'panic', 'fatal', 'error', 'warning', 'info',\n                'verbose', 'debug', 'trace'\n            ]\n            options.append('-loglevel {}'.format(v))\n        else:\n            options.append('-{} {}'.format(k, v))\n    cmd = 'ffmpeg -y {} -i {} {} {}'.format(pre_options, in_file,\n                                            ' '.join(options), out_file)\n    if print_cmd:\n        print(cmd)\n    subprocess.call(cmd, shell=True)","method_path":"mmcv\/video\/processing.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"resize_video","method_code":"def resize_video(in_file,\n                 out_file,\n                 size=None,\n                 ratio=None,\n                 keep_ar=False,\n                 log_level='info',\n                 print_cmd=False,\n                 **kwargs):\n    if size is None and ratio is None:\n        raise ValueError('expected size or ratio must be specified')\n    elif size is not None and ratio is not None:\n        raise ValueError('size and ratio cannot be specified at the same time')\n    options = {'log_level': log_level}\n    if size:\n        if not keep_ar:\n            options['vf'] = 'scale={}:{}'.format(size[0], size[1])\n        else:\n            options['vf'] = ('scale=w={}:h={}:force_original_aspect_ratio'\n                             '=decrease'.format(size[0], size[1]))\n    else:\n        if not isinstance(ratio, tuple):\n            ratio = (ratio, ratio)\n        options['vf'] = 'scale=\"trunc(iw*{}):trunc(ih*{})\"'.format(\n            ratio[0], ratio[1])\n    convert_video(in_file, out_file, print_cmd, **options)","method_summary":"Resize a video.","original_method_code":"def resize_video(in_file,\n                 out_file,\n                 size=None,\n                 ratio=None,\n                 keep_ar=False,\n                 log_level='info',\n                 print_cmd=False,\n                 **kwargs):\n    \"\"\"Resize a video.\n\n    Args:\n        in_file (str): Input video filename.\n        out_file (str): Output video filename.\n        size (tuple): Expected size (w, h), eg, (320, 240) or (320, -1).\n        ratio (tuple or float): Expected resize ratio, (2, 0.5) means\n            (w*2, h*0.5).\n        keep_ar (bool): Whether to keep original aspect ratio.\n        log_level (str): Logging level of ffmpeg.\n        print_cmd (bool): Whether to print the final ffmpeg command.\n    \"\"\"\n    if size is None and ratio is None:\n        raise ValueError('expected size or ratio must be specified')\n    elif size is not None and ratio is not None:\n        raise ValueError('size and ratio cannot be specified at the same time')\n    options = {'log_level': log_level}\n    if size:\n        if not keep_ar:\n            options['vf'] = 'scale={}:{}'.format(size[0], size[1])\n        else:\n            options['vf'] = ('scale=w={}:h={}:force_original_aspect_ratio'\n                             '=decrease'.format(size[0], size[1]))\n    else:\n        if not isinstance(ratio, tuple):\n            ratio = (ratio, ratio)\n        options['vf'] = 'scale=\"trunc(iw*{}):trunc(ih*{})\"'.format(\n            ratio[0], ratio[1])\n    convert_video(in_file, out_file, print_cmd, **options)","method_path":"mmcv\/video\/processing.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"cut_video","method_code":"def cut_video(in_file,\n              out_file,\n              start=None,\n              end=None,\n              vcodec=None,\n              acodec=None,\n              log_level='info',\n              print_cmd=False,\n              **kwargs):\n    options = {'log_level': log_level}\n    if vcodec is None:\n        options['vcodec'] = 'copy'\n    if acodec is None:\n        options['acodec'] = 'copy'\n    if start:\n        options['ss'] = start\n    else:\n        start = 0\n    if end:\n        options['t'] = end - start\n    convert_video(in_file, out_file, print_cmd, **options)","method_summary":"Cut a clip from a video.","original_method_code":"def cut_video(in_file,\n              out_file,\n              start=None,\n              end=None,\n              vcodec=None,\n              acodec=None,\n              log_level='info',\n              print_cmd=False,\n              **kwargs):\n    \"\"\"Cut a clip from a video.\n\n    Args:\n        in_file (str): Input video filename.\n        out_file (str): Output video filename.\n        start (None or float): Start time (in seconds).\n        end (None or float): End time (in seconds).\n        vcodec (None or str): Output video codec, None for unchanged.\n        acodec (None or str): Output audio codec, None for unchanged.\n        log_level (str): Logging level of ffmpeg.\n        print_cmd (bool): Whether to print the final ffmpeg command.\n    \"\"\"\n    options = {'log_level': log_level}\n    if vcodec is None:\n        options['vcodec'] = 'copy'\n    if acodec is None:\n        options['acodec'] = 'copy'\n    if start:\n        options['ss'] = start\n    else:\n        start = 0\n    if end:\n        options['t'] = end - start\n    convert_video(in_file, out_file, print_cmd, **options)","method_path":"mmcv\/video\/processing.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"concat_video","method_code":"def concat_video(video_list,\n                 out_file,\n                 vcodec=None,\n                 acodec=None,\n                 log_level='info',\n                 print_cmd=False,\n                 **kwargs):\n    _, tmp_filename = tempfile.mkstemp(suffix='.txt', text=True)\n    with open(tmp_filename, 'w') as f:\n        for filename in video_list:\n            f.write('file {}\\n'.format(osp.abspath(filename)))\n    options = {'log_level': log_level}\n    if vcodec is None:\n        options['vcodec'] = 'copy'\n    if acodec is None:\n        options['acodec'] = 'copy'\n    convert_video(\n        tmp_filename,\n        out_file,\n        print_cmd,\n        pre_options='-f concat -safe 0',\n        **options)\n    os.remove(tmp_filename)","method_summary":"Concatenate multiple videos into a single one.","original_method_code":"def concat_video(video_list,\n                 out_file,\n                 vcodec=None,\n                 acodec=None,\n                 log_level='info',\n                 print_cmd=False,\n                 **kwargs):\n    \"\"\"Concatenate multiple videos into a single one.\n\n    Args:\n        video_list (list): A list of video filenames\n        out_file (str): Output video filename\n        vcodec (None or str): Output video codec, None for unchanged\n        acodec (None or str): Output audio codec, None for unchanged\n        log_level (str): Logging level of ffmpeg.\n        print_cmd (bool): Whether to print the final ffmpeg command.\n    \"\"\"\n    _, tmp_filename = tempfile.mkstemp(suffix='.txt', text=True)\n    with open(tmp_filename, 'w') as f:\n        for filename in video_list:\n            f.write('file {}\\n'.format(osp.abspath(filename)))\n    options = {'log_level': log_level}\n    if vcodec is None:\n        options['vcodec'] = 'copy'\n    if acodec is None:\n        options['acodec'] = 'copy'\n    convert_video(\n        tmp_filename,\n        out_file,\n        print_cmd,\n        pre_options='-f concat -safe 0',\n        **options)\n    os.remove(tmp_filename)","method_path":"mmcv\/video\/processing.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"list_from_file","method_code":"def list_from_file(filename, prefix='', offset=0, max_num=0):\n    cnt = 0\n    item_list = []\n    with open(filename, 'r') as f:\n        for _ in range(offset):\n            f.readline()\n        for line in f:\n            if max_num > 0 and cnt >= max_num:\n                break\n            item_list.append(prefix + line.rstrip('\\n'))\n            cnt += 1\n    return item_list","method_summary":"Load a text file and parse the content as a list of strings.","original_method_code":"def list_from_file(filename, prefix='', offset=0, max_num=0):\n    \"\"\"Load a text file and parse the content as a list of strings.\n\n    Args:\n        filename (str): Filename.\n        prefix (str): The prefix to be inserted to the begining of each item.\n        offset (int): The offset of lines.\n        max_num (int): The maximum number of lines to be read,\n            zeros and negatives mean no limitation.\n\n    Returns:\n        list[str]: A list of strings.\n    \"\"\"\n    cnt = 0\n    item_list = []\n    with open(filename, 'r') as f:\n        for _ in range(offset):\n            f.readline()\n        for line in f:\n            if max_num > 0 and cnt >= max_num:\n                break\n            item_list.append(prefix + line.rstrip('\\n'))\n            cnt += 1\n    return item_list","method_path":"mmcv\/fileio\/parse.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"conv3x3","method_code":"def conv3x3(in_planes, out_planes, dilation=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        padding=dilation,\n        dilation=dilation)","method_summary":"3x3 convolution with padding","original_method_code":"def conv3x3(in_planes, out_planes, dilation=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        padding=dilation,\n        dilation=dilation)","method_path":"mmcv\/cnn\/vgg.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imread","method_code":"def imread(img_or_path, flag='color'):\n    if isinstance(img_or_path, np.ndarray):\n        return img_or_path\n    elif is_str(img_or_path):\n        flag = imread_flags[flag] if is_str(flag) else flag\n        check_file_exist(img_or_path,\n                         'img file does not exist: {}'.format(img_or_path))\n        return cv2.imread(img_or_path, flag)\n    else:\n        raise TypeError('\"img\" must be a numpy array or a filename')","method_summary":"Read an image.","original_method_code":"def imread(img_or_path, flag='color'):\n    \"\"\"Read an image.\n\n    Args:\n        img_or_path (ndarray or str): Either a numpy array or image path.\n            If it is a numpy array (loaded image), then it will be returned\n            as is.\n        flag (str): Flags specifying the color type of a loaded image,\n            candidates are `color`, `grayscale` and `unchanged`.\n\n    Returns:\n        ndarray: Loaded image array.\n    \"\"\"\n    if isinstance(img_or_path, np.ndarray):\n        return img_or_path\n    elif is_str(img_or_path):\n        flag = imread_flags[flag] if is_str(flag) else flag\n        check_file_exist(img_or_path,\n                         'img file does not exist: {}'.format(img_or_path))\n        return cv2.imread(img_or_path, flag)\n    else:\n        raise TypeError('\"img\" must be a numpy array or a filename')","method_path":"mmcv\/image\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imfrombytes","method_code":"def imfrombytes(content, flag='color'):\n    img_np = np.frombuffer(content, np.uint8)\n    flag = imread_flags[flag] if is_str(flag) else flag\n    img = cv2.imdecode(img_np, flag)\n    return img","method_summary":"Read an image from bytes.","original_method_code":"def imfrombytes(content, flag='color'):\n    \"\"\"Read an image from bytes.\n\n    Args:\n        content (bytes): Image bytes got from files or other streams.\n        flag (str): Same as :func:`imread`.\n\n    Returns:\n        ndarray: Loaded image array.\n    \"\"\"\n    img_np = np.frombuffer(content, np.uint8)\n    flag = imread_flags[flag] if is_str(flag) else flag\n    img = cv2.imdecode(img_np, flag)\n    return img","method_path":"mmcv\/image\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imwrite","method_code":"def imwrite(img, file_path, params=None, auto_mkdir=True):\n    if auto_mkdir:\n        dir_name = osp.abspath(osp.dirname(file_path))\n        mkdir_or_exist(dir_name)\n    return cv2.imwrite(file_path, img, params)","method_summary":"Write image to file","original_method_code":"def imwrite(img, file_path, params=None, auto_mkdir=True):\n    \"\"\"Write image to file\n\n    Args:\n        img (ndarray): Image array to be written.\n        file_path (str): Image file path.\n        params (None or list): Same as opencv's :func:`imwrite` interface.\n        auto_mkdir (bool): If the parent folder of `file_path` does not exist,\n            whether to create it automatically.\n\n    Returns:\n        bool: Successful or not.\n    \"\"\"\n    if auto_mkdir:\n        dir_name = osp.abspath(osp.dirname(file_path))\n        mkdir_or_exist(dir_name)\n    return cv2.imwrite(file_path, img, params)","method_path":"mmcv\/image\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"bgr2gray","method_code":"def bgr2gray(img, keepdim=False):\n    out_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    if keepdim:\n        out_img = out_img[..., None]\n    return out_img","method_summary":"Convert a BGR image to grayscale image.","original_method_code":"def bgr2gray(img, keepdim=False):\n    \"\"\"Convert a BGR image to grayscale image.\n\n    Args:\n        img (ndarray): The input image.\n        keepdim (bool): If False (by default), then return the grayscale image\n            with 2 dims, otherwise 3 dims.\n\n    Returns:\n        ndarray: The converted grayscale image.\n    \"\"\"\n    out_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    if keepdim:\n        out_img = out_img[..., None]\n    return out_img","method_path":"mmcv\/image\/transforms\/colorspace.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"gray2bgr","method_code":"def gray2bgr(img):\n    img = img[..., None] if img.ndim == 2 else img\n    out_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n    return out_img","method_summary":"Convert a grayscale image to BGR image.","original_method_code":"def gray2bgr(img):\n    \"\"\"Convert a grayscale image to BGR image.\n\n    Args:\n        img (ndarray or str): The input image.\n\n    Returns:\n        ndarray: The converted BGR image.\n    \"\"\"\n    img = img[..., None] if img.ndim == 2 else img\n    out_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n    return out_img","method_path":"mmcv\/image\/transforms\/colorspace.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"iter_cast","method_code":"def iter_cast(inputs, dst_type, return_type=None):\n    if not isinstance(inputs, collections_abc.Iterable):\n        raise TypeError('inputs must be an iterable object')\n    if not isinstance(dst_type, type):\n        raise TypeError('\"dst_type\" must be a valid type')\n\n    out_iterable = six.moves.map(dst_type, inputs)\n\n    if return_type is None:\n        return out_iterable\n    else:\n        return return_type(out_iterable)","method_summary":"Cast elements of an iterable object into some type.","original_method_code":"def iter_cast(inputs, dst_type, return_type=None):\n    \"\"\"Cast elements of an iterable object into some type.\n\n    Args:\n        inputs (Iterable): The input object.\n        dst_type (type): Destination type.\n        return_type (type, optional): If specified, the output object will be\n            converted to this type, otherwise an iterator.\n\n    Returns:\n        iterator or specified type: The converted object.\n    \"\"\"\n    if not isinstance(inputs, collections_abc.Iterable):\n        raise TypeError('inputs must be an iterable object')\n    if not isinstance(dst_type, type):\n        raise TypeError('\"dst_type\" must be a valid type')\n\n    out_iterable = six.moves.map(dst_type, inputs)\n\n    if return_type is None:\n        return out_iterable\n    else:\n        return return_type(out_iterable)","method_path":"mmcv\/utils\/misc.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"is_seq_of","method_code":"def is_seq_of(seq, expected_type, seq_type=None):\n    if seq_type is None:\n        exp_seq_type = collections_abc.Sequence\n    else:\n        assert isinstance(seq_type, type)\n        exp_seq_type = seq_type\n    if not isinstance(seq, exp_seq_type):\n        return False\n    for item in seq:\n        if not isinstance(item, expected_type):\n            return False\n    return True","method_summary":"Check whether it is a sequence of some type.","original_method_code":"def is_seq_of(seq, expected_type, seq_type=None):\n    \"\"\"Check whether it is a sequence of some type.\n\n    Args:\n        seq (Sequence): The sequence to be checked.\n        expected_type (type): Expected type of sequence items.\n        seq_type (type, optional): Expected sequence type.\n\n    Returns:\n        bool: Whether the sequence is valid.\n    \"\"\"\n    if seq_type is None:\n        exp_seq_type = collections_abc.Sequence\n    else:\n        assert isinstance(seq_type, type)\n        exp_seq_type = seq_type\n    if not isinstance(seq, exp_seq_type):\n        return False\n    for item in seq:\n        if not isinstance(item, expected_type):\n            return False\n    return True","method_path":"mmcv\/utils\/misc.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"slice_list","method_code":"def slice_list(in_list, lens):\n    if not isinstance(lens, list):\n        raise TypeError('\"indices\" must be a list of integers')\n    elif sum(lens) != len(in_list):\n        raise ValueError(\n            'sum of lens and list length does not match: {} != {}'.format(\n                sum(lens), len(in_list)))\n    out_list = []\n    idx = 0\n    for i in range(len(lens)):\n        out_list.append(in_list[idx:idx + lens[i]])\n        idx += lens[i]\n    return out_list","method_summary":"Slice a list into several sub lists by a list of given length.","original_method_code":"def slice_list(in_list, lens):\n    \"\"\"Slice a list into several sub lists by a list of given length.\n\n    Args:\n        in_list (list): The list to be sliced.\n        lens(int or list): The expected length of each out list.\n\n    Returns:\n        list: A list of sliced list.\n    \"\"\"\n    if not isinstance(lens, list):\n        raise TypeError('\"indices\" must be a list of integers')\n    elif sum(lens) != len(in_list):\n        raise ValueError(\n            'sum of lens and list length does not match: {} != {}'.format(\n                sum(lens), len(in_list)))\n    out_list = []\n    idx = 0\n    for i in range(len(lens)):\n        out_list.append(in_list[idx:idx + lens[i]])\n        idx += lens[i]\n    return out_list","method_path":"mmcv\/utils\/misc.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"check_prerequisites","method_code":"def check_prerequisites(\n        prerequisites,\n        checker,\n        msg_tmpl='Prerequisites \"{}\" are required in method \"{}\" but not '\n        'found, please install them first.'):\n    def wrap(func):\n\n        @functools.wraps(func)\n        def wrapped_func(*args, **kwargs):\n            requirements = [prerequisites] if isinstance(\n                prerequisites, str) else prerequisites\n            missing = []\n            for item in requirements:\n                if not checker(item):\n                    missing.append(item)\n            if missing:\n                print(msg_tmpl.format(', '.join(missing), func.__name__))\n                raise RuntimeError('Prerequisites not meet.')\n            else:\n                return func(*args, **kwargs)\n\n        return wrapped_func\n\n    return wrap","method_summary":"A decorator factory to check if prerequisites are satisfied.","original_method_code":"def check_prerequisites(\n        prerequisites,\n        checker,\n        msg_tmpl='Prerequisites \"{}\" are required in method \"{}\" but not '\n        'found, please install them first.'):\n    \"\"\"A decorator factory to check if prerequisites are satisfied.\n\n    Args:\n        prerequisites (str of list[str]): Prerequisites to be checked.\n        checker (callable): The checker method that returns True if a\n            prerequisite is meet, False otherwise.\n        msg_tmpl (str): The message template with two variables.\n\n    Returns:\n        decorator: A specific decorator.\n    \"\"\"\n\n    def wrap(func):\n\n        @functools.wraps(func)\n        def wrapped_func(*args, **kwargs):\n            requirements = [prerequisites] if isinstance(\n                prerequisites, str) else prerequisites\n            missing = []\n            for item in requirements:\n                if not checker(item):\n                    missing.append(item)\n            if missing:\n                print(msg_tmpl.format(', '.join(missing), func.__name__))\n                raise RuntimeError('Prerequisites not meet.')\n            else:\n                return func(*args, **kwargs)\n\n        return wrapped_func\n\n    return wrap","method_path":"mmcv\/utils\/misc.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"LogBuffer.average","method_code":"def average(self, n=0):\n        assert n >= 0\n        for key in self.val_history:\n            values = np.array(self.val_history[key][-n:])\n            nums = np.array(self.n_history[key][-n:])\n            avg = np.sum(values * nums) \/ np.sum(nums)\n            self.output[key] = avg\n        self.ready = True","method_summary":"Average latest n values or all values","original_method_code":"def average(self, n=0):\n        \"\"\"Average latest n values or all values\"\"\"\n        assert n >= 0\n        for key in self.val_history:\n            values = np.array(self.val_history[key][-n:])\n            nums = np.array(self.n_history[key][-n:])\n            avg = np.sum(values * nums) \/ np.sum(nums)\n            self.output[key] = avg\n        self.ready = True","method_path":"mmcv\/runner\/log_buffer.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"scatter","method_code":"def scatter(input, devices, streams=None):\n    if streams is None:\n        streams = [None] * len(devices)\n\n    if isinstance(input, list):\n        chunk_size = (len(input) - 1) \/\/ len(devices) + 1\n        outputs = [\n            scatter(input[i], [devices[i \/\/ chunk_size]],\n                    [streams[i \/\/ chunk_size]]) for i in range(len(input))\n        ]\n        return outputs\n    elif isinstance(input, torch.Tensor):\n        output = input.contiguous()\n        \n        stream = streams[0] if output.numel() > 0 else None\n        with torch.cuda.device(devices[0]), torch.cuda.stream(stream):\n            output = output.cuda(devices[0], non_blocking=True)\n        return output\n    else:\n        raise Exception('Unknown type {}.'.format(type(input)))","method_summary":"Scatters tensor across multiple GPUs.","original_method_code":"def scatter(input, devices, streams=None):\n    \"\"\"Scatters tensor across multiple GPUs.\n    \"\"\"\n    if streams is None:\n        streams = [None] * len(devices)\n\n    if isinstance(input, list):\n        chunk_size = (len(input) - 1) \/\/ len(devices) + 1\n        outputs = [\n            scatter(input[i], [devices[i \/\/ chunk_size]],\n                    [streams[i \/\/ chunk_size]]) for i in range(len(input))\n        ]\n        return outputs\n    elif isinstance(input, torch.Tensor):\n        output = input.contiguous()\n        # TODO: copy to a pinned buffer first (if copying from CPU)\n        stream = streams[0] if output.numel() > 0 else None\n        with torch.cuda.device(devices[0]), torch.cuda.stream(stream):\n            output = output.cuda(devices[0], non_blocking=True)\n        return output\n    else:\n        raise Exception('Unknown type {}.'.format(type(input)))","method_path":"mmcv\/parallel\/_functions.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"color_val","method_code":"def color_val(color):\n    if is_str(color):\n        return Color[color].value\n    elif isinstance(color, Color):\n        return color.value\n    elif isinstance(color, tuple):\n        assert len(color) == 3\n        for channel in color:\n            assert channel >= 0 and channel <= 255\n        return color\n    elif isinstance(color, int):\n        assert color >= 0 and color <= 255\n        return color, color, color\n    elif isinstance(color, np.ndarray):\n        assert color.ndim == 1 and color.size == 3\n        assert np.all((color >= 0) & (color <= 255))\n        color = color.astype(np.uint8)\n        return tuple(color)\n    else:\n        raise TypeError('Invalid type for color: {}'.format(type(color)))","method_summary":"Convert various input to color tuples.","original_method_code":"def color_val(color):\n    \"\"\"Convert various input to color tuples.\n\n    Args:\n        color (:obj:`Color`\/str\/tuple\/int\/ndarray): Color inputs\n\n    Returns:\n        tuple[int]: A tuple of 3 integers indicating BGR channels.\n    \"\"\"\n    if is_str(color):\n        return Color[color].value\n    elif isinstance(color, Color):\n        return color.value\n    elif isinstance(color, tuple):\n        assert len(color) == 3\n        for channel in color:\n            assert channel >= 0 and channel <= 255\n        return color\n    elif isinstance(color, int):\n        assert color >= 0 and color <= 255\n        return color, color, color\n    elif isinstance(color, np.ndarray):\n        assert color.ndim == 1 and color.size == 3\n        assert np.all((color >= 0) & (color <= 255))\n        color = color.astype(np.uint8)\n        return tuple(color)\n    else:\n        raise TypeError('Invalid type for color: {}'.format(type(color)))","method_path":"mmcv\/visualization\/color.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"check_time","method_code":"def check_time(timer_id):\n    if timer_id not in _g_timers:\n        _g_timers[timer_id] = Timer()\n        return 0\n    else:\n        return _g_timers[timer_id].since_last_check()","method_summary":"Add check points in a single line. This method is suitable for running a task on a list of items. A timer will be registered when the method is called for the first time.","original_method_code":"def check_time(timer_id):\n    \"\"\"Add check points in a single line.\n\n    This method is suitable for running a task on a list of items. A timer will\n    be registered when the method is called for the first time.\n\n    :Example:\n\n    >>> import time\n    >>> import mmcv\n    >>> for i in range(1, 6):\n    >>>     # simulate a code block\n    >>>     time.sleep(i)\n    >>>     mmcv.check_time('task1')\n    2.000\n    3.000\n    4.000\n    5.000\n\n    Args:\n        timer_id (str): Timer identifier.\n    \"\"\"\n    if timer_id not in _g_timers:\n        _g_timers[timer_id] = Timer()\n        return 0\n    else:\n        return _g_timers[timer_id].since_last_check()","method_path":"mmcv\/utils\/timer.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"Timer.start","method_code":"def start(self):\n        if not self._is_running:\n            self._t_start = time()\n            self._is_running = True\n        self._t_last = time()","method_summary":"Start the timer.","original_method_code":"def start(self):\n        \"\"\"Start the timer.\"\"\"\n        if not self._is_running:\n            self._t_start = time()\n            self._is_running = True\n        self._t_last = time()","method_path":"mmcv\/utils\/timer.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"Timer.since_start","method_code":"def since_start(self):\n        if not self._is_running:\n            raise TimerError('timer is not running')\n        self._t_last = time()\n        return self._t_last - self._t_start","method_summary":"Total time since the timer is started.","original_method_code":"def since_start(self):\n        \"\"\"Total time since the timer is started.\n\n        Returns (float): Time in seconds.\n        \"\"\"\n        if not self._is_running:\n            raise TimerError('timer is not running')\n        self._t_last = time()\n        return self._t_last - self._t_start","method_path":"mmcv\/utils\/timer.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"Timer.since_last_check","method_code":"def since_last_check(self):\n        if not self._is_running:\n            raise TimerError('timer is not running')\n        dur = time() - self._t_last\n        self._t_last = time()\n        return dur","method_summary":"Time since the last checking. Either :func:`since_start` or :func:`since_last_check` is a checking operation.","original_method_code":"def since_last_check(self):\n        \"\"\"Time since the last checking.\n\n        Either :func:`since_start` or :func:`since_last_check` is a checking\n        operation.\n\n        Returns (float): Time in seconds.\n        \"\"\"\n        if not self._is_running:\n            raise TimerError('timer is not running')\n        dur = time() - self._t_last\n        self._t_last = time()\n        return dur","method_path":"mmcv\/utils\/timer.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"flowshow","method_code":"def flowshow(flow, win_name='', wait_time=0):\n    flow = flowread(flow)\n    flow_img = flow2rgb(flow)\n    imshow(rgb2bgr(flow_img), win_name, wait_time)","method_summary":"Show optical flow.","original_method_code":"def flowshow(flow, win_name='', wait_time=0):\n    \"\"\"Show optical flow.\n\n    Args:\n        flow (ndarray or str): The optical flow to be displayed.\n        win_name (str): The window name.\n        wait_time (int): Value of waitKey param.\n    \"\"\"\n    flow = flowread(flow)\n    flow_img = flow2rgb(flow)\n    imshow(rgb2bgr(flow_img), win_name, wait_time)","method_path":"mmcv\/visualization\/optflow.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"flow2rgb","method_code":"def flow2rgb(flow, color_wheel=None, unknown_thr=1e6):\n    assert flow.ndim == 3 and flow.shape[-1] == 2\n    if color_wheel is None:\n        color_wheel = make_color_wheel()\n    assert color_wheel.ndim == 2 and color_wheel.shape[1] == 3\n    num_bins = color_wheel.shape[0]\n\n    dx = flow[:, :, 0].copy()\n    dy = flow[:, :, 1].copy()\n\n    ignore_inds = (np.isnan(dx) | np.isnan(dy) | (np.abs(dx) > unknown_thr) |\n                   (np.abs(dy) > unknown_thr))\n    dx[ignore_inds] = 0\n    dy[ignore_inds] = 0\n\n    rad = np.sqrt(dx**2 + dy**2)\n    if np.any(rad > np.finfo(float).eps):\n        max_rad = np.max(rad)\n        dx \/= max_rad\n        dy \/= max_rad\n\n    [h, w] = dx.shape\n\n    rad = np.sqrt(dx**2 + dy**2)\n    angle = np.arctan2(-dy, -dx) \/ np.pi\n\n    bin_real = (angle + 1) \/ 2 * (num_bins - 1)\n    bin_left = np.floor(bin_real).astype(int)\n    bin_right = (bin_left + 1) % num_bins\n    w = (bin_real - bin_left.astype(np.float32))[..., None]\n    flow_img = (\n        1 - w) * color_wheel[bin_left, :] + w * color_wheel[bin_right, :]\n    small_ind = rad <= 1\n    flow_img[small_ind] = 1 - rad[small_ind, None] * (1 - flow_img[small_ind])\n    flow_img[np.logical_not(small_ind)] *= 0.75\n\n    flow_img[ignore_inds, :] = 0\n\n    return flow_img","method_summary":"Convert flow map to RGB image.","original_method_code":"def flow2rgb(flow, color_wheel=None, unknown_thr=1e6):\n    \"\"\"Convert flow map to RGB image.\n\n    Args:\n        flow (ndarray): Array of optical flow.\n        color_wheel (ndarray or None): Color wheel used to map flow field to\n            RGB colorspace. Default color wheel will be used if not specified.\n        unknown_thr (str): Values above this threshold will be marked as\n            unknown and thus ignored.\n\n    Returns:\n        ndarray: RGB image that can be visualized.\n    \"\"\"\n    assert flow.ndim == 3 and flow.shape[-1] == 2\n    if color_wheel is None:\n        color_wheel = make_color_wheel()\n    assert color_wheel.ndim == 2 and color_wheel.shape[1] == 3\n    num_bins = color_wheel.shape[0]\n\n    dx = flow[:, :, 0].copy()\n    dy = flow[:, :, 1].copy()\n\n    ignore_inds = (np.isnan(dx) | np.isnan(dy) | (np.abs(dx) > unknown_thr) |\n                   (np.abs(dy) > unknown_thr))\n    dx[ignore_inds] = 0\n    dy[ignore_inds] = 0\n\n    rad = np.sqrt(dx**2 + dy**2)\n    if np.any(rad > np.finfo(float).eps):\n        max_rad = np.max(rad)\n        dx \/= max_rad\n        dy \/= max_rad\n\n    [h, w] = dx.shape\n\n    rad = np.sqrt(dx**2 + dy**2)\n    angle = np.arctan2(-dy, -dx) \/ np.pi\n\n    bin_real = (angle + 1) \/ 2 * (num_bins - 1)\n    bin_left = np.floor(bin_real).astype(int)\n    bin_right = (bin_left + 1) % num_bins\n    w = (bin_real - bin_left.astype(np.float32))[..., None]\n    flow_img = (\n        1 - w) * color_wheel[bin_left, :] + w * color_wheel[bin_right, :]\n    small_ind = rad <= 1\n    flow_img[small_ind] = 1 - rad[small_ind, None] * (1 - flow_img[small_ind])\n    flow_img[np.logical_not(small_ind)] *= 0.75\n\n    flow_img[ignore_inds, :] = 0\n\n    return flow_img","method_path":"mmcv\/visualization\/optflow.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"make_color_wheel","method_code":"def make_color_wheel(bins=None):\n    if bins is None:\n        bins = [15, 6, 4, 11, 13, 6]\n    assert len(bins) == 6\n\n    RY, YG, GC, CB, BM, MR = tuple(bins)\n\n    ry = [1, np.arange(RY) \/ RY, 0]\n    yg = [1 - np.arange(YG) \/ YG, 1, 0]\n    gc = [0, 1, np.arange(GC) \/ GC]\n    cb = [0, 1 - np.arange(CB) \/ CB, 1]\n    bm = [np.arange(BM) \/ BM, 0, 1]\n    mr = [1, 0, 1 - np.arange(MR) \/ MR]\n\n    num_bins = RY + YG + GC + CB + BM + MR\n\n    color_wheel = np.zeros((3, num_bins), dtype=np.float32)\n\n    col = 0\n    for i, color in enumerate([ry, yg, gc, cb, bm, mr]):\n        for j in range(3):\n            color_wheel[j, col:col + bins[i]] = color[j]\n        col += bins[i]\n\n    return color_wheel.T","method_summary":"Build a color wheel.","original_method_code":"def make_color_wheel(bins=None):\n    \"\"\"Build a color wheel.\n\n    Args:\n        bins(list or tuple, optional): Specify the number of bins for each\n            color range, corresponding to six ranges: red -> yellow,\n            yellow -> green, green -> cyan, cyan -> blue, blue -> magenta,\n            magenta -> red. [15, 6, 4, 11, 13, 6] is used for default\n            (see Middlebury).\n\n    Returns:\n        ndarray: Color wheel of shape (total_bins, 3).\n    \"\"\"\n    if bins is None:\n        bins = [15, 6, 4, 11, 13, 6]\n    assert len(bins) == 6\n\n    RY, YG, GC, CB, BM, MR = tuple(bins)\n\n    ry = [1, np.arange(RY) \/ RY, 0]\n    yg = [1 - np.arange(YG) \/ YG, 1, 0]\n    gc = [0, 1, np.arange(GC) \/ GC]\n    cb = [0, 1 - np.arange(CB) \/ CB, 1]\n    bm = [np.arange(BM) \/ BM, 0, 1]\n    mr = [1, 0, 1 - np.arange(MR) \/ MR]\n\n    num_bins = RY + YG + GC + CB + BM + MR\n\n    color_wheel = np.zeros((3, num_bins), dtype=np.float32)\n\n    col = 0\n    for i, color in enumerate([ry, yg, gc, cb, bm, mr]):\n        for j in range(3):\n            color_wheel[j, col:col + bins[i]] = color[j]\n        col += bins[i]\n\n    return color_wheel.T","method_path":"mmcv\/visualization\/optflow.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"accuracy","method_code":"def accuracy(output, target, topk=(1, )):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 \/ batch_size))\n        return res","method_summary":"Computes the precision@k for the specified values of k","original_method_code":"def accuracy(output, target, topk=(1, )):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 \/ batch_size))\n        return res","method_path":"examples\/train_cifar10.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"scatter","method_code":"def scatter(inputs, target_gpus, dim=0):\n    def scatter_map(obj):\n        if isinstance(obj, torch.Tensor):\n            return OrigScatter.apply(target_gpus, None, dim, obj)\n        if isinstance(obj, DataContainer):\n            if obj.cpu_only:\n                return obj.data\n            else:\n                return Scatter.forward(target_gpus, obj.data)\n        if isinstance(obj, tuple) and len(obj) > 0:\n            return list(zip(*map(scatter_map, obj)))\n        if isinstance(obj, list) and len(obj) > 0:\n            out = list(map(list, zip(*map(scatter_map, obj))))\n            return out\n        if isinstance(obj, dict) and len(obj) > 0:\n            out = list(map(type(obj), zip(*map(scatter_map, obj.items()))))\n            return out\n        return [obj for targets in target_gpus]\n\n    \n    \n    \n    \n    \n    try:\n        return scatter_map(inputs)\n    finally:\n        scatter_map = None","method_summary":"Scatter inputs to target gpus. The only difference from original :func:`scatter` is to add support for","original_method_code":"def scatter(inputs, target_gpus, dim=0):\n    \"\"\"Scatter inputs to target gpus.\n\n    The only difference from original :func:`scatter` is to add support for\n    :type:`~mmcv.parallel.DataContainer`.\n    \"\"\"\n\n    def scatter_map(obj):\n        if isinstance(obj, torch.Tensor):\n            return OrigScatter.apply(target_gpus, None, dim, obj)\n        if isinstance(obj, DataContainer):\n            if obj.cpu_only:\n                return obj.data\n            else:\n                return Scatter.forward(target_gpus, obj.data)\n        if isinstance(obj, tuple) and len(obj) > 0:\n            return list(zip(*map(scatter_map, obj)))\n        if isinstance(obj, list) and len(obj) > 0:\n            out = list(map(list, zip(*map(scatter_map, obj))))\n            return out\n        if isinstance(obj, dict) and len(obj) > 0:\n            out = list(map(type(obj), zip(*map(scatter_map, obj.items()))))\n            return out\n        return [obj for targets in target_gpus]\n\n    # After scatter_map is called, a scatter_map cell will exist. This cell\n    # has a reference to the actual function scatter_map, which has references\n    # to a closure that has a reference to the scatter_map cell (because the\n    # fn is recursive). To avoid this reference cycle, we set the function to\n    # None, clearing the cell\n    try:\n        return scatter_map(inputs)\n    finally:\n        scatter_map = None","method_path":"mmcv\/parallel\/scatter_gather.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"scatter_kwargs","method_code":"def scatter_kwargs(inputs, kwargs, target_gpus, dim=0):\n    inputs = scatter(inputs, target_gpus, dim) if inputs else []\n    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n    if len(inputs) < len(kwargs):\n        inputs.extend([() for _ in range(len(kwargs) - len(inputs))])\n    elif len(kwargs) < len(inputs):\n        kwargs.extend([{} for _ in range(len(inputs) - len(kwargs))])\n    inputs = tuple(inputs)\n    kwargs = tuple(kwargs)\n    return inputs, kwargs","method_summary":"Scatter with support for kwargs dictionary","original_method_code":"def scatter_kwargs(inputs, kwargs, target_gpus, dim=0):\n    \"\"\"Scatter with support for kwargs dictionary\"\"\"\n    inputs = scatter(inputs, target_gpus, dim) if inputs else []\n    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n    if len(inputs) < len(kwargs):\n        inputs.extend([() for _ in range(len(kwargs) - len(inputs))])\n    elif len(kwargs) < len(inputs):\n        kwargs.extend([{} for _ in range(len(inputs) - len(kwargs))])\n    inputs = tuple(inputs)\n    kwargs = tuple(kwargs)\n    return inputs, kwargs","method_path":"mmcv\/parallel\/scatter_gather.py"}
