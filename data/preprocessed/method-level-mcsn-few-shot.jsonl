{"repo_name":"apache\/airflow","method_name":"BaseExecutor.has_task","method_code":"def has_task(self, task_instance):\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True","method_summary":"Checks if a task is either queued or running in this executor","original_method_code":"def has_task(self, task_instance):\n        \"\"\"\n        Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor\n        \"\"\"\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True","method_path":"airflow\/executors\/base_executor.py"}
{"repo_name":"apache\/airflow","method_name":"SnowflakeHook._get_aws_credentials","method_code":"def _get_aws_credentials(self):\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key","method_summary":"returns aws_access_key_id, aws_secret_access_key from extra intended to be used by external import and export statements","original_method_code":"def _get_aws_credentials(self):\n        \"\"\"\n        returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements\n        \"\"\"\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key","method_path":"airflow\/contrib\/hooks\/snowflake_hook.py"}
{"repo_name":"apache\/airflow","method_name":"PostgresHook.copy_expert","method_code":"def copy_expert(self, sql, filename, open=open):\n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n\n        with open(filename, 'r+') as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()","method_summary":"Executes SQL using psycopg2 copy_expert method. Necessary to execute COPY command without access to a superuser.","original_method_code":"def copy_expert(self, sql, filename, open=open):\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n\n        with open(filename, 'r+') as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()","method_path":"airflow\/hooks\/postgres_hook.py"}
{"repo_name":"apache\/airflow","method_name":"PostgresHook.bulk_dump","method_code":"def bulk_dump(self, table, tmp_file):\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)","method_summary":"Dumps a database table into a tab-delimited file","original_method_code":"def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)","method_path":"airflow\/hooks\/postgres_hook.py"}
{"repo_name":"apache\/airflow","method_name":"FileToGoogleCloudStorageOperator.execute","method_code":"def execute(self, context):\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )","method_summary":"Uploads the file to Google cloud storage","original_method_code":"def execute(self, context):\n        \"\"\"\n        Uploads the file to Google cloud storage\n        \"\"\"\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )","method_path":"airflow\/contrib\/operators\/file_to_gcs.py"}
{"repo_name":"apache\/airflow","method_name":"max_partition","method_code":"def max_partition(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.max_partition(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)","method_summary":"Gets the max partition for a table.","original_method_code":"def max_partition(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    \"\"\"\n    Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'\n    \"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.max_partition(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)","method_path":"airflow\/macros\/hive.py"}
{"repo_name":"apache\/airflow","method_name":"CloudTranslateHook.get_conn","method_code":"def get_conn(self):\n        if not self._client:\n            self._client = Client(credentials=self._get_credentials())\n        return self._client","method_summary":"Retrieves connection to Cloud Translate","original_method_code":"def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client\n        \"\"\"\n        if not self._client:\n            self._client = Client(credentials=self._get_credentials())\n        return self._client","method_path":"airflow\/contrib\/hooks\/gcp_translate_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudTranslateHook.translate","method_code":"def translate(\n        self, values, target_language, format_=None, source_language=None, model=None\n    ):\n        client = self.get_conn()\n\n        return client.translate(\n            values=values,\n            target_language=target_language,\n            format_=format_,\n            source_language=source_language,\n            model=model,\n        )","method_summary":"Translate a string or list of strings. See","original_method_code":"def translate(\n        self, values, target_language, format_=None, source_language=None, model=None\n    ):\n        \"\"\"Translate a string or list of strings.\n\n        See https:\/\/cloud.google.com\/translate\/docs\/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.\n        \"\"\"\n        client = self.get_conn()\n\n        return client.translate(\n            values=values,\n            target_language=target_language,\n            format_=format_,\n            source_language=source_language,\n            model=model,\n        )","method_path":"airflow\/contrib\/hooks\/gcp_translate_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSqlHook.get_instance","method_code":"def get_instance(self, instance, project_id=None):\n        return self.get_conn().instances().get(\n            project=project_id,\n            instance=instance\n        ).execute(num_retries=self.num_retries)","method_summary":"Retrieves a resource containing information about a Cloud SQL instance.","original_method_code":"def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().instances().get(\n            project=project_id,\n            instance=instance\n        ).execute(num_retries=self.num_retries)","method_path":"airflow\/contrib\/hooks\/gcp_sql_hook.py"}
{"repo_name":"apache\/airflow","method_name":"CloudSqlHook.create_instance","method_code":"def create_instance(self, body, project_id=None):\n        response = self.get_conn().instances().insert(\n            project=project_id,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)","method_summary":"Creates a new Cloud SQL instance.","original_method_code":"def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https:\/\/cloud.google.com\/sql\/docs\/mysql\/admin-api\/v1beta4\/instances\/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().insert(\n            project=project_id,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)","method_path":"airflow\/contrib\/hooks\/gcp_sql_hook.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"Message.schedule","method_code":"def schedule(self, schedule_time):\n        if not self.properties.message_id:\n            self.properties.message_id = str(uuid.uuid4())\n        if not self.message.annotations:\n            self.message.annotations = {}\n        self.message.annotations[types.AMQPSymbol(self._x_OPT_SCHEDULED_ENQUEUE_TIME)] = schedule_time","method_summary":"Add a specific enqueue time to the message.","original_method_code":"def schedule(self, schedule_time):\n        \"\"\"Add a specific enqueue time to the message.\n\n        :param schedule_time: The scheduled time to enqueue the message.\n        :type schedule_time: ~datetime.datetime\n        \"\"\"\n        if not self.properties.message_id:\n            self.properties.message_id = str(uuid.uuid4())\n        if not self.message.annotations:\n            self.message.annotations = {}\n        self.message.annotations[types.AMQPSymbol(self._x_OPT_SCHEDULED_ENQUEUE_TIME)] = schedule_time","method_path":"azure-servicebus\/azure\/servicebus\/common\/message.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"VpnSitesConfigurationOperations.download","method_code":"def download(\n            self, resource_group_name, virtual_wan_name, vpn_sites=None, output_blob_sas_url=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._download_initial(\n            resource_group_name=resource_group_name,\n            virtual_wan_name=virtual_wan_name,\n            vpn_sites=vpn_sites,\n            output_blob_sas_url=output_blob_sas_url,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Gives the sas-url to download the configurations for vpn-sites in a resource group.","original_method_code":"def download(\n            self, resource_group_name, virtual_wan_name, vpn_sites=None, output_blob_sas_url=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Gives the sas-url to download the configurations for vpn-sites in a\n        resource group.\n\n        :param resource_group_name: The resource group name.\n        :type resource_group_name: str\n        :param virtual_wan_name: The name of the VirtualWAN for which\n         configuration of all vpn-sites is needed.\n        :type virtual_wan_name: str\n        :param vpn_sites: List of resource-ids of the vpn-sites for which\n         config is to be downloaded.\n        :type vpn_sites:\n         list[~azure.mgmt.network.v2018_04_01.models.SubResource]\n        :param output_blob_sas_url: The sas-url to download the configurations\n         for vpn-sites\n        :type output_blob_sas_url: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorException<azure.mgmt.network.v2018_04_01.models.ErrorException>`\n        \"\"\"\n        raw_result = self._download_initial(\n            resource_group_name=resource_group_name,\n            virtual_wan_name=virtual_wan_name,\n            vpn_sites=vpn_sites,\n            output_blob_sas_url=output_blob_sas_url,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-network\/azure\/mgmt\/network\/v2018_04_01\/operations\/vpn_sites_configuration_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"guess_service_info_from_path","method_code":"def guess_service_info_from_path(spec_path):\n    spec_path = spec_path.lower()\n    spec_path = spec_path[spec_path.index(\"specification\"):] \n    split_spec_path = spec_path.split(\"\/\")\n\n    rp_name = split_spec_path[1]\n    is_arm = split_spec_path[2] == \"resource-manager\"\n\n    return {\n        \"rp_name\": rp_name,\n        \"is_arm\": is_arm\n    }","method_summary":"Guess Python Autorest options based on the spec path. Expected","original_method_code":"def guess_service_info_from_path(spec_path):\n    \"\"\"Guess Python Autorest options based on the spec path.\n\n    Expected path:\n    specification\/compute\/resource-manager\/readme.md\n    \"\"\"\n    spec_path = spec_path.lower()\n    spec_path = spec_path[spec_path.index(\"specification\"):] # Might raise and it's ok\n    split_spec_path = spec_path.split(\"\/\")\n\n    rp_name = split_spec_path[1]\n    is_arm = split_spec_path[2] == \"resource-manager\"\n\n    return {\n        \"rp_name\": rp_name,\n        \"is_arm\": is_arm\n    }","method_path":"scripts\/build_sdk.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"PowerShellOperations.update_command","method_code":"def update_command(\n            self, resource_group_name, node_name, session, pssession, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._update_command_initial(\n            resource_group_name=resource_group_name,\n            node_name=node_name,\n            session=session,\n            pssession=pssession,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('PowerShellCommandResults', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Updates a running PowerShell command with more data.","original_method_code":"def update_command(\n            self, resource_group_name, node_name, session, pssession, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Updates a running PowerShell command with more data.\n\n        :param resource_group_name: The resource group name uniquely\n         identifies the resource group within the user subscriptionId.\n        :type resource_group_name: str\n        :param node_name: The node name (256 characters maximum).\n        :type node_name: str\n        :param session: The sessionId from the user.\n        :type session: str\n        :param pssession: The PowerShell sessionId from the user.\n        :type pssession: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns\n         PowerShellCommandResults or\n         ClientRawResponse<PowerShellCommandResults> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.servermanager.models.PowerShellCommandResults]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.servermanager.models.PowerShellCommandResults]]\n        :raises:\n         :class:`ErrorException<azure.mgmt.servermanager.models.ErrorException>`\n        \"\"\"\n        raw_result = self._update_command_initial(\n            resource_group_name=resource_group_name,\n            node_name=node_name,\n            session=session,\n            pssession=pssession,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('PowerShellCommandResults', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-servermanager\/azure\/mgmt\/servermanager\/operations\/power_shell_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ApplicationDefinitionsOperations.delete_by_id","method_code":"def delete_by_id(\n            self, application_definition_id, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._delete_by_id_initial(\n            application_definition_id=application_definition_id,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Deletes the managed application definition.","original_method_code":"def delete_by_id(\n            self, application_definition_id, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Deletes the managed application definition.\n\n        :param application_definition_id: The fully qualified ID of the\n         managed application definition, including the managed application name\n         and the managed application definition resource type. Use the format,\n         \/subscriptions\/{guid}\/resourceGroups\/{resource-group-name}\/Microsoft.Solutions\/applicationDefinitions\/{applicationDefinition-name}\n        :type application_definition_id: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.resource.managedapplications.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._delete_by_id_initial(\n            application_definition_id=application_definition_id,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-resource\/azure\/mgmt\/resource\/managedapplications\/operations\/application_definitions_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ApplicationDefinitionsOperations.create_or_update_by_id","method_code":"def create_or_update_by_id(\n            self, application_definition_id, parameters, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._create_or_update_by_id_initial(\n            application_definition_id=application_definition_id,\n            parameters=parameters,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('ApplicationDefinition', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Creates a new managed application definition.","original_method_code":"def create_or_update_by_id(\n            self, application_definition_id, parameters, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Creates a new managed application definition.\n\n        :param application_definition_id: The fully qualified ID of the\n         managed application definition, including the managed application name\n         and the managed application definition resource type. Use the format,\n         \/subscriptions\/{guid}\/resourceGroups\/{resource-group-name}\/Microsoft.Solutions\/applicationDefinitions\/{applicationDefinition-name}\n        :type application_definition_id: str\n        :param parameters: Parameters supplied to the create or update a\n         managed application definition.\n        :type parameters:\n         ~azure.mgmt.resource.managedapplications.models.ApplicationDefinition\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns ApplicationDefinition\n         or ClientRawResponse<ApplicationDefinition> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.resource.managedapplications.models.ApplicationDefinition]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.resource.managedapplications.models.ApplicationDefinition]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.resource.managedapplications.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._create_or_update_by_id_initial(\n            application_definition_id=application_definition_id,\n            parameters=parameters,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('ApplicationDefinition', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-resource\/azure\/mgmt\/resource\/managedapplications\/operations\/application_definitions_operations.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_HTTPClient.get_uri","method_code":"def get_uri(self, request):\n        protocol = request.protocol_override \\\n            if request.protocol_override else self.protocol\n        protocol = protocol.lower()\n        port = HTTP_PORT if protocol == 'http' else HTTPS_PORT\n        return protocol + ':\/\/' + request.host + ':' + str(port) + request.path","method_summary":"Return the target uri for the request.","original_method_code":"def get_uri(self, request):\n        ''' Return the target uri for the request.'''\n        protocol = request.protocol_override \\\n            if request.protocol_override else self.protocol\n        protocol = protocol.lower()\n        port = HTTP_PORT if protocol == 'http' else HTTPS_PORT\n        return protocol + ':\/\/' + request.host + ':' + str(port) + request.path","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_http\/httpclient.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_HTTPClient.get_connection","method_code":"def get_connection(self, request):\n        protocol = request.protocol_override \\\n            if request.protocol_override else self.protocol\n        protocol = protocol.lower()\n        target_host = request.host\n        \n\n        connection = _RequestsConnection(\n            target_host, protocol, self.request_session, self.timeout)\n        proxy_host = self.proxy_host\n        proxy_port = self.proxy_port\n\n        if self.proxy_host:\n            headers = None\n            if self.proxy_user and self.proxy_password:\n                auth = base64.b64encode(\"{0}:{1}\".format(self.proxy_user, self.proxy_password).encode())\n                headers = {'Proxy-Authorization': 'Basic {0}'.format(auth.decode())}\n            connection.set_tunnel(proxy_host, int(proxy_port), headers)\n\n        return connection","method_summary":"Create connection for the request.","original_method_code":"def get_connection(self, request):\n        ''' Create connection for the request. '''\n        protocol = request.protocol_override \\\n            if request.protocol_override else self.protocol\n        protocol = protocol.lower()\n        target_host = request.host\n        # target_port = HTTP_PORT if protocol == 'http' else HTTPS_PORT\n\n        connection = _RequestsConnection(\n            target_host, protocol, self.request_session, self.timeout)\n        proxy_host = self.proxy_host\n        proxy_port = self.proxy_port\n\n        if self.proxy_host:\n            headers = None\n            if self.proxy_user and self.proxy_password:\n                auth = base64.b64encode(\"{0}:{1}\".format(self.proxy_user, self.proxy_password).encode())\n                headers = {'Proxy-Authorization': 'Basic {0}'.format(auth.decode())}\n            connection.set_tunnel(proxy_host, int(proxy_port), headers)\n\n        return connection","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_http\/httpclient.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"_HTTPClient.perform_request","method_code":"def perform_request(self, request):\n        connection = self.get_connection(request)\n        try:\n            connection.putrequest(request.method, request.path)\n\n            self.send_request_headers(connection, request.headers)\n            self.send_request_body(connection, request.body)\n\n            if DEBUG_REQUESTS and request.body:\n                print('request:')\n                try:\n                    print(request.body)\n                except:  \n                    pass\n\n            resp = connection.getresponse()\n            status = int(resp.status)\n            message = resp.reason\n            respheaders = resp.getheaders()\n\n            \n            for i, value in enumerate(respheaders):\n                respheaders[i] = (value[0].lower(), value[1])\n\n            respbody = None\n            if resp.length is None:\n                respbody = resp.read()\n            elif resp.length > 0:\n                respbody = resp.read(resp.length)\n\n            if DEBUG_RESPONSES and respbody:\n                print('response:')\n                try:\n                    print(respbody)\n                except:  \n                    pass\n\n            response = HTTPResponse(\n                status, resp.reason, respheaders, respbody)\n            if status == 307:\n                new_url = urlparse(dict(respheaders)['location'])\n                request.host = new_url.hostname\n                request.path = new_url.path\n                request.path, request.query = self._update_request_uri_query(request)\n                return self.perform_request(request)\n            if status >= 300:\n                raise HTTPError(status, message, respheaders, respbody)\n\n            return response\n        finally:\n            connection.close()","method_summary":"Sends request to cloud service server and return the response.","original_method_code":"def perform_request(self, request):\n        ''' Sends request to cloud service server and return the response. '''\n        connection = self.get_connection(request)\n        try:\n            connection.putrequest(request.method, request.path)\n\n            self.send_request_headers(connection, request.headers)\n            self.send_request_body(connection, request.body)\n\n            if DEBUG_REQUESTS and request.body:\n                print('request:')\n                try:\n                    print(request.body)\n                except:  # pylint: disable=bare-except\n                    pass\n\n            resp = connection.getresponse()\n            status = int(resp.status)\n            message = resp.reason\n            respheaders = resp.getheaders()\n\n            # for consistency across platforms, make header names lowercase\n            for i, value in enumerate(respheaders):\n                respheaders[i] = (value[0].lower(), value[1])\n\n            respbody = None\n            if resp.length is None:\n                respbody = resp.read()\n            elif resp.length > 0:\n                respbody = resp.read(resp.length)\n\n            if DEBUG_RESPONSES and respbody:\n                print('response:')\n                try:\n                    print(respbody)\n                except:  # pylint: disable=bare-except\n                    pass\n\n            response = HTTPResponse(\n                status, resp.reason, respheaders, respbody)\n            if status == 307:\n                new_url = urlparse(dict(respheaders)['location'])\n                request.host = new_url.hostname\n                request.path = new_url.path\n                request.path, request.query = self._update_request_uri_query(request)\n                return self.perform_request(request)\n            if status >= 300:\n                raise HTTPError(status, message, respheaders, respbody)\n\n            return response\n        finally:\n            connection.close()","method_path":"azure-servicebus\/azure\/servicebus\/control_client\/_http\/httpclient.py"}
{"repo_name":"Azure\/azure-sdk-for-python","method_name":"ClustersOperations.execute_script_actions","method_code":"def execute_script_actions(\n            self, resource_group_name, cluster_name, persist_on_success, script_actions=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        raw_result = self._execute_script_actions_initial(\n            resource_group_name=resource_group_name,\n            cluster_name=cluster_name,\n            persist_on_success=persist_on_success,\n            script_actions=script_actions,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_summary":"Executes script actions on the specified HDInsight cluster.","original_method_code":"def execute_script_actions(\n            self, resource_group_name, cluster_name, persist_on_success, script_actions=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Executes script actions on the specified HDInsight cluster.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param cluster_name: The name of the cluster.\n        :type cluster_name: str\n        :param persist_on_success: Gets or sets if the scripts needs to be\n         persisted.\n        :type persist_on_success: bool\n        :param script_actions: The list of run time script actions.\n        :type script_actions:\n         list[~azure.mgmt.hdinsight.models.RuntimeScriptAction]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.hdinsight.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._execute_script_actions_initial(\n            resource_group_name=resource_group_name,\n            cluster_name=cluster_name,\n            persist_on_success=persist_on_success,\n            script_actions=script_actions,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)","method_path":"azure-mgmt-hdinsight\/azure\/mgmt\/hdinsight\/operations\/clusters_operations.py"}
{"repo_name":"streamlink\/streamlink","method_name":"Plugin.load_cookies","method_code":"def load_cookies(self):\n        if not self.session or not self.cache:\n            raise RuntimeError(\"Cannot loaded cached cookies in unbound plugin\")\n\n        restored = []\n\n        for key, value in self.cache.get_all().items():\n            if key.startswith(\"__cookie\"):\n                cookie = requests.cookies.create_cookie(**value)\n                self.session.http.cookies.set_cookie(cookie)\n                restored.append(cookie.name)\n\n        if restored:\n            self.logger.debug(\"Restored cookies: {0}\".format(\", \".join(restored)))\n        return restored","method_summary":"Load any stored cookies for the plugin that have not expired.","original_method_code":"def load_cookies(self):\n        \"\"\"\n        Load any stored cookies for the plugin that have not expired.\n\n        :return: list of the restored cookie names\n        \"\"\"\n        if not self.session or not self.cache:\n            raise RuntimeError(\"Cannot loaded cached cookies in unbound plugin\")\n\n        restored = []\n\n        for key, value in self.cache.get_all().items():\n            if key.startswith(\"__cookie\"):\n                cookie = requests.cookies.create_cookie(**value)\n                self.session.http.cookies.set_cookie(cookie)\n                restored.append(cookie.name)\n\n        if restored:\n            self.logger.debug(\"Restored cookies: {0}\".format(\", \".join(restored)))\n        return restored","method_path":"src\/streamlink\/plugin\/plugin.py"}
{"repo_name":"streamlink\/streamlink","method_name":"get_cut_prefix","method_code":"def get_cut_prefix(value, max_len):\n    should_convert = isinstance(value, bytes)\n    if should_convert:\n        value = value.decode(\"utf8\", \"ignore\")\n    for i in range(len(value)):\n        if terminal_width(value[i:]) <= max_len:\n            break\n    return value[i:].encode(\"utf8\", \"ignore\") if should_convert else value[i:]","method_summary":"Drops Characters by unicode not by bytes.","original_method_code":"def get_cut_prefix(value, max_len):\n    \"\"\"Drops Characters by unicode not by bytes.\"\"\"\n    should_convert = isinstance(value, bytes)\n    if should_convert:\n        value = value.decode(\"utf8\", \"ignore\")\n    for i in range(len(value)):\n        if terminal_width(value[i:]) <= max_len:\n            break\n    return value[i:].encode(\"utf8\", \"ignore\") if should_convert else value[i:]","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"print_inplace","method_code":"def print_inplace(msg):\n    term_width = get_terminal_size().columns\n    spacing = term_width - terminal_width(msg)\n\n    \n    if is_win32:\n        spacing -= 1\n\n    sys.stderr.write(\"\\r{0}\".format(msg))\n    sys.stderr.write(\" \" * max(0, spacing))\n    sys.stderr.flush()","method_summary":"Clears out the previous line and prints a new one.","original_method_code":"def print_inplace(msg):\n    \"\"\"Clears out the previous line and prints a new one.\"\"\"\n    term_width = get_terminal_size().columns\n    spacing = term_width - terminal_width(msg)\n\n    # On windows we need one less space or we overflow the line for some reason.\n    if is_win32:\n        spacing -= 1\n\n    sys.stderr.write(\"\\r{0}\".format(msg))\n    sys.stderr.write(\" \" * max(0, spacing))\n    sys.stderr.flush()","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"format_filesize","method_code":"def format_filesize(size):\n    for suffix in (\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if size < 1024.0:\n            if suffix in (\"GB\", \"TB\"):\n                return \"{0:3.2f} {1}\".format(size, suffix)\n            else:\n                return \"{0:3.1f} {1}\".format(size, suffix)\n\n        size \/= 1024.0","method_summary":"Formats the file size into a human readable format.","original_method_code":"def format_filesize(size):\n    \"\"\"Formats the file size into a human readable format.\"\"\"\n    for suffix in (\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if size < 1024.0:\n            if suffix in (\"GB\", \"TB\"):\n                return \"{0:3.2f} {1}\".format(size, suffix)\n            else:\n                return \"{0:3.1f} {1}\".format(size, suffix)\n\n        size \/= 1024.0","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"format_time","method_code":"def format_time(elapsed):\n    hours = int(elapsed \/ (60 * 60))\n    minutes = int((elapsed % (60 * 60)) \/ 60)\n    seconds = int(elapsed % 60)\n\n    rval = \"\"\n    if hours:\n        rval += \"{0}h\".format(hours)\n\n    if elapsed > 60:\n        rval += \"{0}m\".format(minutes)\n\n    rval += \"{0}s\".format(seconds)\n    return rval","method_summary":"Formats elapsed seconds into a human readable format.","original_method_code":"def format_time(elapsed):\n    \"\"\"Formats elapsed seconds into a human readable format.\"\"\"\n    hours = int(elapsed \/ (60 * 60))\n    minutes = int((elapsed % (60 * 60)) \/ 60)\n    seconds = int(elapsed % 60)\n\n    rval = \"\"\n    if hours:\n        rval += \"{0}h\".format(hours)\n\n    if elapsed > 60:\n        rval += \"{0}m\".format(minutes)\n\n    rval += \"{0}s\".format(seconds)\n    return rval","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"create_status_line","method_code":"def create_status_line(**params):\n    max_size = get_terminal_size().columns - 1\n\n    for fmt in PROGRESS_FORMATS:\n        status = fmt.format(**params)\n\n        if len(status) <= max_size:\n            break\n\n    return status","method_summary":"Creates a status line with appropriate size.","original_method_code":"def create_status_line(**params):\n    \"\"\"Creates a status line with appropriate size.\"\"\"\n    max_size = get_terminal_size().columns - 1\n\n    for fmt in PROGRESS_FORMATS:\n        status = fmt.format(**params)\n\n        if len(status) <= max_size:\n            break\n\n    return status","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"progress","method_code":"def progress(iterator, prefix):\n    if terminal_width(prefix) > 25:\n        prefix = (\"..\" + get_cut_prefix(prefix, 23))\n    speed_updated = start = time()\n    speed_written = written = 0\n    speed_history = deque(maxlen=5)\n\n    for data in iterator:\n        yield data\n\n        now = time()\n        elapsed = now - start\n        written += len(data)\n\n        speed_elapsed = now - speed_updated\n        if speed_elapsed >= 0.5:\n            speed_history.appendleft((\n                written - speed_written,\n                speed_updated,\n            ))\n            speed_updated = now\n            speed_written = written\n\n            speed_history_written = sum(h[0] for h in speed_history)\n            speed_history_elapsed = now - speed_history[-1][1]\n            speed = speed_history_written \/ speed_history_elapsed\n\n            status = create_status_line(\n                prefix=prefix,\n                written=format_filesize(written),\n                elapsed=format_time(elapsed),\n                speed=format_filesize(speed)\n            )\n            print_inplace(status)\n    sys.stderr.write(\"\\n\")\n    sys.stderr.flush()","method_summary":"Progress an iterator and updates a pretty status line to the terminal. The status line","original_method_code":"def progress(iterator, prefix):\n    \"\"\"Progress an iterator and updates a pretty status line to the terminal.\n\n    The status line contains:\n     - Amount of data read from the iterator\n     - Time elapsed\n     - Average speed, based on the last few seconds.\n    \"\"\"\n    if terminal_width(prefix) > 25:\n        prefix = (\"..\" + get_cut_prefix(prefix, 23))\n    speed_updated = start = time()\n    speed_written = written = 0\n    speed_history = deque(maxlen=5)\n\n    for data in iterator:\n        yield data\n\n        now = time()\n        elapsed = now - start\n        written += len(data)\n\n        speed_elapsed = now - speed_updated\n        if speed_elapsed >= 0.5:\n            speed_history.appendleft((\n                written - speed_written,\n                speed_updated,\n            ))\n            speed_updated = now\n            speed_written = written\n\n            speed_history_written = sum(h[0] for h in speed_history)\n            speed_history_elapsed = now - speed_history[-1][1]\n            speed = speed_history_written \/ speed_history_elapsed\n\n            status = create_status_line(\n                prefix=prefix,\n                written=format_filesize(written),\n                elapsed=format_time(elapsed),\n                speed=format_filesize(speed)\n            )\n            print_inplace(status)\n    sys.stderr.write(\"\\n\")\n    sys.stderr.flush()","method_path":"src\/streamlink_cli\/utils\/progress.py"}
{"repo_name":"streamlink\/streamlink","method_name":"SegmentedStreamWorker.wait","method_code":"def wait(self, time):\n        self._wait = Event()\n        return not self._wait.wait(time)","method_summary":"Pauses the thread for a specified time.","original_method_code":"def wait(self, time):\n        \"\"\"Pauses the thread for a specified time.\n\n        Returns False if interrupted by another thread and True if the\n        time runs out normally.\n        \"\"\"\n        self._wait = Event()\n        return not self._wait.wait(time)","method_path":"src\/streamlink\/stream\/segmented.py"}
{"repo_name":"streamlink\/streamlink","method_name":"SegmentedStreamWriter.put","method_code":"def put(self, segment):\n        if self.closed:\n            return\n\n        if segment is not None:\n            future = self.executor.submit(self.fetch, segment,\n                                          retries=self.retries)\n        else:\n            future = None\n\n        self.queue(self.futures, (segment, future))","method_summary":"Adds a segment to the download pool and write queue.","original_method_code":"def put(self, segment):\n        \"\"\"Adds a segment to the download pool and write queue.\"\"\"\n        if self.closed:\n            return\n\n        if segment is not None:\n            future = self.executor.submit(self.fetch, segment,\n                                          retries=self.retries)\n        else:\n            future = None\n\n        self.queue(self.futures, (segment, future))","method_path":"src\/streamlink\/stream\/segmented.py"}
{"repo_name":"streamlink\/streamlink","method_name":"SegmentedStreamWriter.queue","method_code":"def queue(self, queue_, value):\n        while not self.closed:\n            try:\n                queue_.put(value, block=True, timeout=1)\n                return\n            except queue.Full:\n                continue","method_summary":"Puts a value into a queue but aborts if this thread is closed.","original_method_code":"def queue(self, queue_, value):\n        \"\"\"Puts a value into a queue but aborts if this thread is closed.\"\"\"\n        while not self.closed:\n            try:\n                queue_.put(value, block=True, timeout=1)\n                return\n            except queue.Full:\n                continue","method_path":"src\/streamlink\/stream\/segmented.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"frames2video","method_code":"def frames2video(frame_dir,\n                 video_file,\n                 fps=30,\n                 fourcc='XVID',\n                 filename_tmpl='{:06d}.jpg',\n                 start=0,\n                 end=0,\n                 show_progress=True):\n    if end == 0:\n        ext = filename_tmpl.split('.')[-1]\n        end = len([name for name in scandir(frame_dir, ext)])\n    first_file = osp.join(frame_dir, filename_tmpl.format(start))\n    check_file_exist(first_file, 'The start frame not found: ' + first_file)\n    img = cv2.imread(first_file)\n    height, width = img.shape[:2]\n    resolution = (width, height)\n    vwriter = cv2.VideoWriter(video_file, VideoWriter_fourcc(*fourcc), fps,\n                              resolution)\n\n    def write_frame(file_idx):\n        filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n        img = cv2.imread(filename)\n        vwriter.write(img)\n\n    if show_progress:\n        track_progress(write_frame, range(start, end))\n    else:\n        for i in range(start, end):\n            filename = osp.join(frame_dir, filename_tmpl.format(i))\n            img = cv2.imread(filename)\n            vwriter.write(img)\n    vwriter.release()","method_summary":"Read the frame images from a directory and join them as a video","original_method_code":"def frames2video(frame_dir,\n                 video_file,\n                 fps=30,\n                 fourcc='XVID',\n                 filename_tmpl='{:06d}.jpg',\n                 start=0,\n                 end=0,\n                 show_progress=True):\n    \"\"\"Read the frame images from a directory and join them as a video\n\n    Args:\n        frame_dir (str): The directory containing video frames.\n        video_file (str): Output filename.\n        fps (float): FPS of the output video.\n        fourcc (str): Fourcc of the output video, this should be compatible\n            with the output file type.\n        filename_tmpl (str): Filename template with the index as the variable.\n        start (int): Starting frame index.\n        end (int): Ending frame index.\n        show_progress (bool): Whether to show a progress bar.\n    \"\"\"\n    if end == 0:\n        ext = filename_tmpl.split('.')[-1]\n        end = len([name for name in scandir(frame_dir, ext)])\n    first_file = osp.join(frame_dir, filename_tmpl.format(start))\n    check_file_exist(first_file, 'The start frame not found: ' + first_file)\n    img = cv2.imread(first_file)\n    height, width = img.shape[:2]\n    resolution = (width, height)\n    vwriter = cv2.VideoWriter(video_file, VideoWriter_fourcc(*fourcc), fps,\n                              resolution)\n\n    def write_frame(file_idx):\n        filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n        img = cv2.imread(filename)\n        vwriter.write(img)\n\n    if show_progress:\n        track_progress(write_frame, range(start, end))\n    else:\n        for i in range(start, end):\n            filename = osp.join(frame_dir, filename_tmpl.format(i))\n            img = cv2.imread(filename)\n            vwriter.write(img)\n    vwriter.release()","method_path":"mmcv\/video\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"VideoReader.read","method_code":"def read(self):\n        \n        if self._cache:\n            img = self._cache.get(self._position)\n            if img is not None:\n                ret = True\n            else:\n                if self._position != self._get_real_position():\n                    self._set_real_position(self._position)\n                ret, img = self._vcap.read()\n                if ret:\n                    self._cache.put(self._position, img)\n        else:\n            ret, img = self._vcap.read()\n        if ret:\n            self._position += 1\n        return img","method_summary":"Read the next frame. If the next frame have been decoded before and in the cache, then return it directly, otherwise decode, cache and return it.","original_method_code":"def read(self):\n        \"\"\"Read the next frame.\n\n        If the next frame have been decoded before and in the cache, then\n        return it directly, otherwise decode, cache and return it.\n\n        Returns:\n            ndarray or None: Return the frame if successful, otherwise None.\n        \"\"\"\n        # pos = self._position\n        if self._cache:\n            img = self._cache.get(self._position)\n            if img is not None:\n                ret = True\n            else:\n                if self._position != self._get_real_position():\n                    self._set_real_position(self._position)\n                ret, img = self._vcap.read()\n                if ret:\n                    self._cache.put(self._position, img)\n        else:\n            ret, img = self._vcap.read()\n        if ret:\n            self._position += 1\n        return img","method_path":"mmcv\/video\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"VideoReader.get_frame","method_code":"def get_frame(self, frame_id):\n        if frame_id < 0 or frame_id >= self._frame_cnt:\n            raise IndexError(\n                '\"frame_id\" must be between 0 and {}'.format(self._frame_cnt -\n                                                             1))\n        if frame_id == self._position:\n            return self.read()\n        if self._cache:\n            img = self._cache.get(frame_id)\n            if img is not None:\n                self._position = frame_id + 1\n                return img\n        self._set_real_position(frame_id)\n        ret, img = self._vcap.read()\n        if ret:\n            if self._cache:\n                self._cache.put(self._position, img)\n            self._position += 1\n        return img","method_summary":"Get frame by index.","original_method_code":"def get_frame(self, frame_id):\n        \"\"\"Get frame by index.\n\n        Args:\n            frame_id (int): Index of the expected frame, 0-based.\n\n        Returns:\n            ndarray or None: Return the frame if successful, otherwise None.\n        \"\"\"\n        if frame_id < 0 or frame_id >= self._frame_cnt:\n            raise IndexError(\n                '\"frame_id\" must be between 0 and {}'.format(self._frame_cnt -\n                                                             1))\n        if frame_id == self._position:\n            return self.read()\n        if self._cache:\n            img = self._cache.get(frame_id)\n            if img is not None:\n                self._position = frame_id + 1\n                return img\n        self._set_real_position(frame_id)\n        ret, img = self._vcap.read()\n        if ret:\n            if self._cache:\n                self._cache.put(self._position, img)\n            self._position += 1\n        return img","method_path":"mmcv\/video\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"VideoReader.cvt2frames","method_code":"def cvt2frames(self,\n                   frame_dir,\n                   file_start=0,\n                   filename_tmpl='{:06d}.jpg',\n                   start=0,\n                   max_num=0,\n                   show_progress=True):\n        mkdir_or_exist(frame_dir)\n        if max_num == 0:\n            task_num = self.frame_cnt - start\n        else:\n            task_num = min(self.frame_cnt - start, max_num)\n        if task_num <= 0:\n            raise ValueError('start must be less than total frame number')\n        if start > 0:\n            self._set_real_position(start)\n\n        def write_frame(file_idx):\n            img = self.read()\n            filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n            cv2.imwrite(filename, img)\n\n        if show_progress:\n            track_progress(write_frame, range(file_start,\n                                              file_start + task_num))\n        else:\n            for i in range(task_num):\n                img = self.read()\n                if img is None:\n                    break\n                filename = osp.join(frame_dir,\n                                    filename_tmpl.format(i + file_start))\n                cv2.imwrite(filename, img)","method_summary":"Convert a video to frame images","original_method_code":"def cvt2frames(self,\n                   frame_dir,\n                   file_start=0,\n                   filename_tmpl='{:06d}.jpg',\n                   start=0,\n                   max_num=0,\n                   show_progress=True):\n        \"\"\"Convert a video to frame images\n\n        Args:\n            frame_dir (str): Output directory to store all the frame images.\n            file_start (int): Filenames will start from the specified number.\n            filename_tmpl (str): Filename template with the index as the\n                placeholder.\n            start (int): The starting frame index.\n            max_num (int): Maximum number of frames to be written.\n            show_progress (bool): Whether to show a progress bar.\n        \"\"\"\n        mkdir_or_exist(frame_dir)\n        if max_num == 0:\n            task_num = self.frame_cnt - start\n        else:\n            task_num = min(self.frame_cnt - start, max_num)\n        if task_num <= 0:\n            raise ValueError('start must be less than total frame number')\n        if start > 0:\n            self._set_real_position(start)\n\n        def write_frame(file_idx):\n            img = self.read()\n            filename = osp.join(frame_dir, filename_tmpl.format(file_idx))\n            cv2.imwrite(filename, img)\n\n        if show_progress:\n            track_progress(write_frame, range(file_start,\n                                              file_start + task_num))\n        else:\n            for i in range(task_num):\n                img = self.read()\n                if img is None:\n                    break\n                filename = osp.join(frame_dir,\n                                    filename_tmpl.format(i + file_start))\n                cv2.imwrite(filename, img)","method_path":"mmcv\/video\/io.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"track_progress","method_code":"def track_progress(func, tasks, bar_width=50, **kwargs):\n    if isinstance(tasks, tuple):\n        assert len(tasks) == 2\n        assert isinstance(tasks[0], collections_abc.Iterable)\n        assert isinstance(tasks[1], int)\n        task_num = tasks[1]\n        tasks = tasks[0]\n    elif isinstance(tasks, collections_abc.Iterable):\n        task_num = len(tasks)\n    else:\n        raise TypeError(\n            '\"tasks\" must be an iterable object or a (iterator, int) tuple')\n    prog_bar = ProgressBar(task_num, bar_width)\n    results = []\n    for task in tasks:\n        results.append(func(task, **kwargs))\n        prog_bar.update()\n    sys.stdout.write('\\n')\n    return results","method_summary":"Track the progress of tasks execution with a progress bar. Tasks are done with a simple for-loop.","original_method_code":"def track_progress(func, tasks, bar_width=50, **kwargs):\n    \"\"\"Track the progress of tasks execution with a progress bar.\n\n    Tasks are done with a simple for-loop.\n\n    Args:\n        func (callable): The function to be applied to each task.\n        tasks (list or tuple[Iterable, int]): A list of tasks or\n            (tasks, total num).\n        bar_width (int): Width of progress bar.\n\n    Returns:\n        list: The task results.\n    \"\"\"\n    if isinstance(tasks, tuple):\n        assert len(tasks) == 2\n        assert isinstance(tasks[0], collections_abc.Iterable)\n        assert isinstance(tasks[1], int)\n        task_num = tasks[1]\n        tasks = tasks[0]\n    elif isinstance(tasks, collections_abc.Iterable):\n        task_num = len(tasks)\n    else:\n        raise TypeError(\n            '\"tasks\" must be an iterable object or a (iterator, int) tuple')\n    prog_bar = ProgressBar(task_num, bar_width)\n    results = []\n    for task in tasks:\n        results.append(func(task, **kwargs))\n        prog_bar.update()\n    sys.stdout.write('\\n')\n    return results","method_path":"mmcv\/utils\/progressbar.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imflip","method_code":"def imflip(img, direction='horizontal'):\n    assert direction in ['horizontal', 'vertical']\n    if direction == 'horizontal':\n        return np.flip(img, axis=1)\n    else:\n        return np.flip(img, axis=0)","method_summary":"Flip an image horizontally or vertically.","original_method_code":"def imflip(img, direction='horizontal'):\n    \"\"\"Flip an image horizontally or vertically.\n\n    Args:\n        img (ndarray): Image to be flipped.\n        direction (str): The flip direction, either \"horizontal\" or \"vertical\".\n\n    Returns:\n        ndarray: The flipped image.\n    \"\"\"\n    assert direction in ['horizontal', 'vertical']\n    if direction == 'horizontal':\n        return np.flip(img, axis=1)\n    else:\n        return np.flip(img, axis=0)","method_path":"mmcv\/image\/transforms\/geometry.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imrotate","method_code":"def imrotate(img,\n             angle,\n             center=None,\n             scale=1.0,\n             border_value=0,\n             auto_bound=False):\n    if center is not None and auto_bound:\n        raise ValueError('`auto_bound` conflicts with `center`')\n    h, w = img.shape[:2]\n    if center is None:\n        center = ((w - 1) * 0.5, (h - 1) * 0.5)\n    assert isinstance(center, tuple)\n\n    matrix = cv2.getRotationMatrix2D(center, -angle, scale)\n    if auto_bound:\n        cos = np.abs(matrix[0, 0])\n        sin = np.abs(matrix[0, 1])\n        new_w = h * sin + w * cos\n        new_h = h * cos + w * sin\n        matrix[0, 2] += (new_w - w) * 0.5\n        matrix[1, 2] += (new_h - h) * 0.5\n        w = int(np.round(new_w))\n        h = int(np.round(new_h))\n    rotated = cv2.warpAffine(img, matrix, (w, h), borderValue=border_value)\n    return rotated","method_summary":"Rotate an image.","original_method_code":"def imrotate(img,\n             angle,\n             center=None,\n             scale=1.0,\n             border_value=0,\n             auto_bound=False):\n    \"\"\"Rotate an image.\n\n    Args:\n        img (ndarray): Image to be rotated.\n        angle (float): Rotation angle in degrees, positive values mean\n            clockwise rotation.\n        center (tuple): Center of the rotation in the source image, by default\n            it is the center of the image.\n        scale (float): Isotropic scale factor.\n        border_value (int): Border value.\n        auto_bound (bool): Whether to adjust the image size to cover the whole\n            rotated image.\n\n    Returns:\n        ndarray: The rotated image.\n    \"\"\"\n    if center is not None and auto_bound:\n        raise ValueError('`auto_bound` conflicts with `center`')\n    h, w = img.shape[:2]\n    if center is None:\n        center = ((w - 1) * 0.5, (h - 1) * 0.5)\n    assert isinstance(center, tuple)\n\n    matrix = cv2.getRotationMatrix2D(center, -angle, scale)\n    if auto_bound:\n        cos = np.abs(matrix[0, 0])\n        sin = np.abs(matrix[0, 1])\n        new_w = h * sin + w * cos\n        new_h = h * cos + w * sin\n        matrix[0, 2] += (new_w - w) * 0.5\n        matrix[1, 2] += (new_h - h) * 0.5\n        w = int(np.round(new_w))\n        h = int(np.round(new_h))\n    rotated = cv2.warpAffine(img, matrix, (w, h), borderValue=border_value)\n    return rotated","method_path":"mmcv\/image\/transforms\/geometry.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"bbox_clip","method_code":"def bbox_clip(bboxes, img_shape):\n    assert bboxes.shape[-1] % 4 == 0\n    clipped_bboxes = np.empty_like(bboxes, dtype=bboxes.dtype)\n    clipped_bboxes[..., 0::2] = np.maximum(\n        np.minimum(bboxes[..., 0::2], img_shape[1] - 1), 0)\n    clipped_bboxes[..., 1::2] = np.maximum(\n        np.minimum(bboxes[..., 1::2], img_shape[0] - 1), 0)\n    return clipped_bboxes","method_summary":"Clip bboxes to fit the image shape.","original_method_code":"def bbox_clip(bboxes, img_shape):\n    \"\"\"Clip bboxes to fit the image shape.\n\n    Args:\n        bboxes (ndarray): Shape (..., 4*k)\n        img_shape (tuple): (height, width) of the image.\n\n    Returns:\n        ndarray: Clipped bboxes.\n    \"\"\"\n    assert bboxes.shape[-1] % 4 == 0\n    clipped_bboxes = np.empty_like(bboxes, dtype=bboxes.dtype)\n    clipped_bboxes[..., 0::2] = np.maximum(\n        np.minimum(bboxes[..., 0::2], img_shape[1] - 1), 0)\n    clipped_bboxes[..., 1::2] = np.maximum(\n        np.minimum(bboxes[..., 1::2], img_shape[0] - 1), 0)\n    return clipped_bboxes","method_path":"mmcv\/image\/transforms\/geometry.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"bbox_scaling","method_code":"def bbox_scaling(bboxes, scale, clip_shape=None):\n    if float(scale) == 1.0:\n        scaled_bboxes = bboxes.copy()\n    else:\n        w = bboxes[..., 2] - bboxes[..., 0] + 1\n        h = bboxes[..., 3] - bboxes[..., 1] + 1\n        dw = (w * (scale - 1)) * 0.5\n        dh = (h * (scale - 1)) * 0.5\n        scaled_bboxes = bboxes + np.stack((-dw, -dh, dw, dh), axis=-1)\n    if clip_shape is not None:\n        return bbox_clip(scaled_bboxes, clip_shape)\n    else:\n        return scaled_bboxes","method_summary":"Scaling bboxes w.r.t the box center.","original_method_code":"def bbox_scaling(bboxes, scale, clip_shape=None):\n    \"\"\"Scaling bboxes w.r.t the box center.\n\n    Args:\n        bboxes (ndarray): Shape(..., 4).\n        scale (float): Scaling factor.\n        clip_shape (tuple, optional): If specified, bboxes that exceed the\n            boundary will be clipped according to the given shape (h, w).\n\n    Returns:\n        ndarray: Scaled bboxes.\n    \"\"\"\n    if float(scale) == 1.0:\n        scaled_bboxes = bboxes.copy()\n    else:\n        w = bboxes[..., 2] - bboxes[..., 0] + 1\n        h = bboxes[..., 3] - bboxes[..., 1] + 1\n        dw = (w * (scale - 1)) * 0.5\n        dh = (h * (scale - 1)) * 0.5\n        scaled_bboxes = bboxes + np.stack((-dw, -dh, dw, dh), axis=-1)\n    if clip_shape is not None:\n        return bbox_clip(scaled_bboxes, clip_shape)\n    else:\n        return scaled_bboxes","method_path":"mmcv\/image\/transforms\/geometry.py"}
{"repo_name":"open-mmlab\/mmcv","method_name":"imcrop","method_code":"def imcrop(img, bboxes, scale=1.0, pad_fill=None):\n    chn = 1 if img.ndim == 2 else img.shape[2]\n    if pad_fill is not None:\n        if isinstance(pad_fill, (int, float)):\n            pad_fill = [pad_fill for _ in range(chn)]\n        assert len(pad_fill) == chn\n\n    _bboxes = bboxes[None, ...] if bboxes.ndim == 1 else bboxes\n    scaled_bboxes = bbox_scaling(_bboxes, scale).astype(np.int32)\n    clipped_bbox = bbox_clip(scaled_bboxes, img.shape)\n\n    patches = []\n    for i in range(clipped_bbox.shape[0]):\n        x1, y1, x2, y2 = tuple(clipped_bbox[i, :])\n        if pad_fill is None:\n            patch = img[y1:y2 + 1, x1:x2 + 1, ...]\n        else:\n            _x1, _y1, _x2, _y2 = tuple(scaled_bboxes[i, :])\n            if chn == 2:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1)\n            else:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1, chn)\n            patch = np.array(\n                pad_fill, dtype=img.dtype) * np.ones(\n                    patch_shape, dtype=img.dtype)\n            x_start = 0 if _x1 >= 0 else -_x1\n            y_start = 0 if _y1 >= 0 else -_y1\n            w = x2 - x1 + 1\n            h = y2 - y1 + 1\n            patch[y_start:y_start + h, x_start:x_start +\n                  w, ...] = img[y1:y1 + h, x1:x1 + w, ...]\n        patches.append(patch)\n\n    if bboxes.ndim == 1:\n        return patches[0]\n    else:\n        return patches","method_summary":"Crop image patches. 3","original_method_code":"def imcrop(img, bboxes, scale=1.0, pad_fill=None):\n    \"\"\"Crop image patches.\n\n    3 steps: scale the bboxes -> clip bboxes -> crop and pad.\n\n    Args:\n        img (ndarray): Image to be cropped.\n        bboxes (ndarray): Shape (k, 4) or (4, ), location of cropped bboxes.\n        scale (float, optional): Scale ratio of bboxes, the default value\n            1.0 means no padding.\n        pad_fill (number or list): Value to be filled for padding, None for\n            no padding.\n\n    Returns:\n        list or ndarray: The cropped image patches.\n    \"\"\"\n    chn = 1 if img.ndim == 2 else img.shape[2]\n    if pad_fill is not None:\n        if isinstance(pad_fill, (int, float)):\n            pad_fill = [pad_fill for _ in range(chn)]\n        assert len(pad_fill) == chn\n\n    _bboxes = bboxes[None, ...] if bboxes.ndim == 1 else bboxes\n    scaled_bboxes = bbox_scaling(_bboxes, scale).astype(np.int32)\n    clipped_bbox = bbox_clip(scaled_bboxes, img.shape)\n\n    patches = []\n    for i in range(clipped_bbox.shape[0]):\n        x1, y1, x2, y2 = tuple(clipped_bbox[i, :])\n        if pad_fill is None:\n            patch = img[y1:y2 + 1, x1:x2 + 1, ...]\n        else:\n            _x1, _y1, _x2, _y2 = tuple(scaled_bboxes[i, :])\n            if chn == 2:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1)\n            else:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1, chn)\n            patch = np.array(\n                pad_fill, dtype=img.dtype) * np.ones(\n                    patch_shape, dtype=img.dtype)\n            x_start = 0 if _x1 >= 0 else -_x1\n            y_start = 0 if _y1 >= 0 else -_y1\n            w = x2 - x1 + 1\n            h = y2 - y1 + 1\n            patch[y_start:y_start + h, x_start:x_start +\n                  w, ...] = img[y1:y1 + h, x1:x1 + w, ...]\n        patches.append(patch)\n\n    if bboxes.ndim == 1:\n        return patches[0]\n    else:\n        return patches","method_path":"mmcv\/image\/transforms\/geometry.py"}
